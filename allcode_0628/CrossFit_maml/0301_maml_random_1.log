nohup: ignoring input
Task: freebase_qa, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic__ccbjxft/none_suise19x
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic__ccbjxft/none_suise19x/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic__ccbjxft/none_suise19x/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/01/2022 15:48:12 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 15:48:12 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa
03/01/2022 15:48:12 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 15:48:12 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa
03/01/2022 15:48:12 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 15:48:12 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 15:48:12 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 15:48:12 - INFO - __main__ - args.device: cuda:0
03/01/2022 15:48:12 - INFO - __main__ - Using 2 gpus
03/01/2022 15:48:12 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 15:48:12 - INFO - __main__ - args.device: cuda:1
03/01/2022 15:48:12 - INFO - __main__ - Using 2 gpus
03/01/2022 15:48:12 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
03/01/2022 15:48:12 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 15:48:20 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.5, bsz=8 ...
03/01/2022 15:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 15:48:21 - INFO - __main__ - ['orange is the new black']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 15:48:21 - INFO - __main__ - ['western australia']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 15:48:21 - INFO - __main__ - ['turkey']
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 15:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 15:48:21 - INFO - __main__ - ['orange is the new black']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 15:48:21 - INFO - __main__ - ['western australia']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 15:48:21 - INFO - __main__ - ['turkey']
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:48:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 15:48:21 - INFO - __main__ - ['benito mussolini']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 15:48:21 - INFO - __main__ - ['the kinks']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 15:48:21 - INFO - __main__ - ['saigon']
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:48:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:48:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:48:21 - INFO - __main__ - Printing 3 examples
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 15:48:21 - INFO - __main__ - ['benito mussolini']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 15:48:21 - INFO - __main__ - ['the kinks']
03/01/2022 15:48:21 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 15:48:21 - INFO - __main__ - ['saigon']
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:48:21 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:48:21 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:48:21 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:48:36 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 15:48:36 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 15:48:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:48:37 - INFO - __main__ - Starting training!
03/01/2022 15:48:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:48:42 - INFO - __main__ - Starting training!
03/01/2022 15:48:45 - INFO - __main__ - Step 10 Global step 10 Train loss 3.18 on epoch=4
03/01/2022 15:48:47 - INFO - __main__ - Step 20 Global step 20 Train loss 2.53 on epoch=9
03/01/2022 15:48:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.39 on epoch=14
03/01/2022 15:48:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.31 on epoch=19
03/01/2022 15:48:53 - INFO - __main__ - Step 50 Global step 50 Train loss 2.19 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 15:48:55 - INFO - __main__ - Global step 50 Train loss 2.52 EM 0.0 on epoch=24
03/01/2022 15:48:55 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 15:48:57 - INFO - __main__ - Step 60 Global step 60 Train loss 2.20 on epoch=29
03/01/2022 15:48:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.13 on epoch=34
03/01/2022 15:49:01 - INFO - __main__ - Step 80 Global step 80 Train loss 2.07 on epoch=39
03/01/2022 15:49:03 - INFO - __main__ - Step 90 Global step 90 Train loss 1.94 on epoch=44
03/01/2022 15:49:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.96 on epoch=49
03/01/2022 15:49:07 - INFO - __main__ - Global step 100 Train loss 2.06 EM 0.0 on epoch=49
03/01/2022 15:49:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.89 on epoch=54
03/01/2022 15:49:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.83 on epoch=59
03/01/2022 15:49:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.80 on epoch=64
03/01/2022 15:49:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.75 on epoch=69
03/01/2022 15:49:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.67 on epoch=74
03/01/2022 15:49:19 - INFO - __main__ - Global step 150 Train loss 1.79 EM 0.0 on epoch=74
03/01/2022 15:49:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.70 on epoch=79
03/01/2022 15:49:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.63 on epoch=84
03/01/2022 15:49:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.54 on epoch=89
03/01/2022 15:49:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.46 on epoch=94
03/01/2022 15:49:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.38 on epoch=99
03/01/2022 15:49:31 - INFO - __main__ - Global step 200 Train loss 1.54 EM 0.0 on epoch=99
03/01/2022 15:49:33 - INFO - __main__ - Step 210 Global step 210 Train loss 1.40 on epoch=104
03/01/2022 15:49:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.31 on epoch=109
03/01/2022 15:49:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.30 on epoch=114
03/01/2022 15:49:40 - INFO - __main__ - Step 240 Global step 240 Train loss 1.31 on epoch=119
03/01/2022 15:49:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.28 on epoch=124
03/01/2022 15:49:43 - INFO - __main__ - Global step 250 Train loss 1.32 EM 0.0 on epoch=124
03/01/2022 15:49:45 - INFO - __main__ - Step 260 Global step 260 Train loss 1.22 on epoch=129
03/01/2022 15:49:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.22 on epoch=134
03/01/2022 15:49:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.14 on epoch=139
03/01/2022 15:49:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.20 on epoch=144
03/01/2022 15:49:54 - INFO - __main__ - Step 300 Global step 300 Train loss 1.22 on epoch=149
03/01/2022 15:49:55 - INFO - __main__ - Global step 300 Train loss 1.20 EM 0.0 on epoch=149
03/01/2022 15:49:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.17 on epoch=154
03/01/2022 15:50:00 - INFO - __main__ - Step 320 Global step 320 Train loss 1.10 on epoch=159
03/01/2022 15:50:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.02 on epoch=164
03/01/2022 15:50:04 - INFO - __main__ - Step 340 Global step 340 Train loss 1.04 on epoch=169
03/01/2022 15:50:06 - INFO - __main__ - Step 350 Global step 350 Train loss 1.11 on epoch=174
03/01/2022 15:50:07 - INFO - __main__ - Global step 350 Train loss 1.09 EM 0.0 on epoch=174
03/01/2022 15:50:09 - INFO - __main__ - Step 360 Global step 360 Train loss 1.04 on epoch=179
03/01/2022 15:50:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.05 on epoch=184
03/01/2022 15:50:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=189
03/01/2022 15:50:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=194
03/01/2022 15:50:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=199
03/01/2022 15:50:19 - INFO - __main__ - Global step 400 Train loss 0.95 EM 0.0 on epoch=199
03/01/2022 15:50:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.84 on epoch=204
03/01/2022 15:50:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.91 on epoch=209
03/01/2022 15:50:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=214
03/01/2022 15:50:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
03/01/2022 15:50:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=224
03/01/2022 15:50:32 - INFO - __main__ - Global step 450 Train loss 0.85 EM 0.0 on epoch=224
03/01/2022 15:50:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=229
03/01/2022 15:50:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=234
03/01/2022 15:50:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=239
03/01/2022 15:50:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=244
03/01/2022 15:50:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=249
03/01/2022 15:50:44 - INFO - __main__ - Global step 500 Train loss 0.76 EM 0.0 on epoch=249
03/01/2022 15:50:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=254
03/01/2022 15:50:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
03/01/2022 15:50:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=264
03/01/2022 15:50:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=269
03/01/2022 15:50:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
03/01/2022 15:50:56 - INFO - __main__ - Global step 550 Train loss 0.68 EM 0.0 on epoch=274
03/01/2022 15:50:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
03/01/2022 15:51:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/01/2022 15:51:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
03/01/2022 15:51:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=294
03/01/2022 15:51:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=299
03/01/2022 15:51:08 - INFO - __main__ - Global step 600 Train loss 0.60 EM 0.0 on epoch=299
03/01/2022 15:51:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=304
03/01/2022 15:51:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.57 on epoch=309
03/01/2022 15:51:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=314
03/01/2022 15:51:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=319
03/01/2022 15:51:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=324
03/01/2022 15:51:20 - INFO - __main__ - Global step 650 Train loss 0.54 EM 0.0 on epoch=324
03/01/2022 15:51:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
03/01/2022 15:51:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=334
03/01/2022 15:51:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=339
03/01/2022 15:51:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/01/2022 15:51:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=349
03/01/2022 15:51:32 - INFO - __main__ - Global step 700 Train loss 0.45 EM 0.0 on epoch=349
03/01/2022 15:51:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=354
03/01/2022 15:51:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=359
03/01/2022 15:51:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/01/2022 15:51:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=369
03/01/2022 15:51:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/01/2022 15:51:44 - INFO - __main__ - Global step 750 Train loss 0.42 EM 0.0 on epoch=374
03/01/2022 15:51:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=379
03/01/2022 15:51:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=384
03/01/2022 15:51:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
03/01/2022 15:51:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=394
03/01/2022 15:51:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/01/2022 15:51:56 - INFO - __main__ - Global step 800 Train loss 0.38 EM 0.0 on epoch=399
03/01/2022 15:51:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=404
03/01/2022 15:52:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=409
03/01/2022 15:52:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
03/01/2022 15:52:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=419
03/01/2022 15:52:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=424
03/01/2022 15:52:09 - INFO - __main__ - Global step 850 Train loss 0.32 EM 0.0 on epoch=424
03/01/2022 15:52:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=429
03/01/2022 15:52:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
03/01/2022 15:52:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=439
03/01/2022 15:52:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/01/2022 15:52:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=449
03/01/2022 15:52:22 - INFO - __main__ - Global step 900 Train loss 0.32 EM 0.0 on epoch=449
03/01/2022 15:52:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
03/01/2022 15:52:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
03/01/2022 15:52:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=464
03/01/2022 15:52:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=469
03/01/2022 15:52:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=474
03/01/2022 15:52:34 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.0 on epoch=474
03/01/2022 15:52:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=479
03/01/2022 15:52:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 15:52:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=489
03/01/2022 15:52:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=494
03/01/2022 15:52:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=499
03/01/2022 15:52:47 - INFO - __main__ - Global step 1000 Train loss 0.28 EM 0.0 on epoch=499
03/01/2022 15:52:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
03/01/2022 15:52:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/01/2022 15:52:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=514
03/01/2022 15:52:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=519
03/01/2022 15:52:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=524
03/01/2022 15:53:00 - INFO - __main__ - Global step 1050 Train loss 0.25 EM 0.0 on epoch=524
03/01/2022 15:53:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/01/2022 15:53:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/01/2022 15:53:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=539
03/01/2022 15:53:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/01/2022 15:53:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
03/01/2022 15:53:13 - INFO - __main__ - Global step 1100 Train loss 0.22 EM 0.0 on epoch=549
03/01/2022 15:53:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=554
03/01/2022 15:53:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=559
03/01/2022 15:53:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/01/2022 15:53:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/01/2022 15:53:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
03/01/2022 15:53:25 - INFO - __main__ - Global step 1150 Train loss 0.20 EM 0.0 on epoch=574
03/01/2022 15:53:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=579
03/01/2022 15:53:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=584
03/01/2022 15:53:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=589
03/01/2022 15:53:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/01/2022 15:53:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/01/2022 15:53:38 - INFO - __main__ - Global step 1200 Train loss 0.20 EM 0.0 on epoch=599
03/01/2022 15:53:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=604
03/01/2022 15:53:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/01/2022 15:53:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/01/2022 15:53:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 15:53:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 15:53:51 - INFO - __main__ - Global step 1250 Train loss 0.23 EM 0.0 on epoch=624
03/01/2022 15:53:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=629
03/01/2022 15:53:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
03/01/2022 15:53:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=639
03/01/2022 15:54:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=644
03/01/2022 15:54:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=649
03/01/2022 15:54:03 - INFO - __main__ - Global step 1300 Train loss 0.17 EM 0.0 on epoch=649
03/01/2022 15:54:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=654
03/01/2022 15:54:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=659
03/01/2022 15:54:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=664
03/01/2022 15:54:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 15:54:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/01/2022 15:54:16 - INFO - __main__ - Global step 1350 Train loss 0.18 EM 0.0 on epoch=674
03/01/2022 15:54:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/01/2022 15:54:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/01/2022 15:54:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=689
03/01/2022 15:54:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 15:54:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=699
03/01/2022 15:54:29 - INFO - __main__ - Global step 1400 Train loss 0.16 EM 0.0 on epoch=699
03/01/2022 15:54:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=704
03/01/2022 15:54:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
03/01/2022 15:54:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=714
03/01/2022 15:54:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=719
03/01/2022 15:54:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=724
03/01/2022 15:54:41 - INFO - __main__ - Global step 1450 Train loss 0.15 EM 0.0 on epoch=724
03/01/2022 15:54:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/01/2022 15:54:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=734
03/01/2022 15:54:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/01/2022 15:54:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
03/01/2022 15:54:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/01/2022 15:54:54 - INFO - __main__ - Global step 1500 Train loss 0.14 EM 0.03125 on epoch=749
03/01/2022 15:54:54 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=749, global_step=1500
03/01/2022 15:54:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/01/2022 15:54:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=759
03/01/2022 15:55:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=764
03/01/2022 15:55:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
03/01/2022 15:55:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
03/01/2022 15:55:07 - INFO - __main__ - Global step 1550 Train loss 0.14 EM 0.0 on epoch=774
03/01/2022 15:55:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/01/2022 15:55:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/01/2022 15:55:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=789
03/01/2022 15:55:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
03/01/2022 15:55:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 15:55:19 - INFO - __main__ - Global step 1600 Train loss 0.12 EM 0.0 on epoch=799
03/01/2022 15:55:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=804
03/01/2022 15:55:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=809
03/01/2022 15:55:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=814
03/01/2022 15:55:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
03/01/2022 15:55:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
03/01/2022 15:55:32 - INFO - __main__ - Global step 1650 Train loss 0.12 EM 0.0 on epoch=824
03/01/2022 15:55:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
03/01/2022 15:55:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
03/01/2022 15:55:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
03/01/2022 15:55:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
03/01/2022 15:55:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=849
03/01/2022 15:55:45 - INFO - __main__ - Global step 1700 Train loss 0.10 EM 0.0 on epoch=849
03/01/2022 15:55:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=854
03/01/2022 15:55:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/01/2022 15:55:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=864
03/01/2022 15:55:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/01/2022 15:55:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/01/2022 15:55:57 - INFO - __main__ - Global step 1750 Train loss 0.11 EM 0.03125 on epoch=874
03/01/2022 15:56:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/01/2022 15:56:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
03/01/2022 15:56:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=889
03/01/2022 15:56:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/01/2022 15:56:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/01/2022 15:56:10 - INFO - __main__ - Global step 1800 Train loss 0.10 EM 0.0 on epoch=899
03/01/2022 15:56:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/01/2022 15:56:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
03/01/2022 15:56:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/01/2022 15:56:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 15:56:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/01/2022 15:56:23 - INFO - __main__ - Global step 1850 Train loss 0.09 EM 0.0 on epoch=924
03/01/2022 15:56:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 15:56:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 15:56:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 15:56:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/01/2022 15:56:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
03/01/2022 15:56:35 - INFO - __main__ - Global step 1900 Train loss 0.10 EM 0.03125 on epoch=949
03/01/2022 15:56:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/01/2022 15:56:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/01/2022 15:56:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/01/2022 15:56:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=969
03/01/2022 15:56:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/01/2022 15:56:48 - INFO - __main__ - Global step 1950 Train loss 0.09 EM 0.0 on epoch=974
03/01/2022 15:56:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
03/01/2022 15:56:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=984
03/01/2022 15:56:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
03/01/2022 15:56:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
03/01/2022 15:56:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/01/2022 15:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:57:00 - INFO - __main__ - Printing 3 examples
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 15:57:00 - INFO - __main__ - ['orange is the new black']
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 15:57:00 - INFO - __main__ - ['western australia']
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 15:57:00 - INFO - __main__ - ['turkey']
03/01/2022 15:57:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 15:57:00 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:57:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 15:57:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 15:57:00 - INFO - __main__ - Printing 3 examples
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 15:57:00 - INFO - __main__ - ['benito mussolini']
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 15:57:00 - INFO - __main__ - ['the kinks']
03/01/2022 15:57:00 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 15:57:00 - INFO - __main__ - ['saigon']
03/01/2022 15:57:00 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:57:00 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:57:00 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 15:57:01 - INFO - __main__ - Global step 2000 Train loss 0.08 EM 0.0 on epoch=999
03/01/2022 15:57:01 - INFO - __main__ - save last model!
03/01/2022 15:57:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 15:57:01 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 15:57:01 - INFO - __main__ - Printing 3 examples
03/01/2022 15:57:01 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 15:57:01 - INFO - __main__ - ['taming of the shrew']
03/01/2022 15:57:01 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 15:57:01 - INFO - __main__ - ['henry fonda']
03/01/2022 15:57:01 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 15:57:01 - INFO - __main__ - ['tchaikovsky']
03/01/2022 15:57:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 15:57:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 15:57:06 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 15:57:12 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 15:57:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 15:57:13 - INFO - __main__ - Starting training!
03/01/2022 16:00:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.5_8_predictions.txt
03/01/2022 16:00:17 - INFO - __main__ - EM on test data: 0.0095
03/01/2022 16:00:19 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.009514271407110666
03/01/2022 16:00:19 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.4, bsz=8 ...
03/01/2022 16:00:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:00:20 - INFO - __main__ - Printing 3 examples
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 16:00:20 - INFO - __main__ - ['orange is the new black']
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 16:00:20 - INFO - __main__ - ['western australia']
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 16:00:20 - INFO - __main__ - ['turkey']
03/01/2022 16:00:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:00:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:00:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:00:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:00:20 - INFO - __main__ - Printing 3 examples
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 16:00:20 - INFO - __main__ - ['benito mussolini']
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 16:00:20 - INFO - __main__ - ['the kinks']
03/01/2022 16:00:20 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 16:00:20 - INFO - __main__ - ['saigon']
03/01/2022 16:00:20 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:00:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:00:20 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:00:33 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:00:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:00:34 - INFO - __main__ - Starting training!
03/01/2022 16:00:37 - INFO - __main__ - Step 10 Global step 10 Train loss 3.17 on epoch=4
03/01/2022 16:00:39 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=9
03/01/2022 16:00:41 - INFO - __main__ - Step 30 Global step 30 Train loss 2.36 on epoch=14
03/01/2022 16:00:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.37 on epoch=19
03/01/2022 16:00:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.30 on epoch=24
03/01/2022 16:00:47 - INFO - __main__ - Global step 50 Train loss 2.58 EM 0.0 on epoch=24
03/01/2022 16:00:48 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 16:00:50 - INFO - __main__ - Step 60 Global step 60 Train loss 2.26 on epoch=29
03/01/2022 16:00:52 - INFO - __main__ - Step 70 Global step 70 Train loss 2.21 on epoch=34
03/01/2022 16:00:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.16 on epoch=39
03/01/2022 16:00:57 - INFO - __main__ - Step 90 Global step 90 Train loss 2.07 on epoch=44
03/01/2022 16:00:59 - INFO - __main__ - Step 100 Global step 100 Train loss 2.07 on epoch=49
03/01/2022 16:01:00 - INFO - __main__ - Global step 100 Train loss 2.15 EM 0.0 on epoch=49
03/01/2022 16:01:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.03 on epoch=54
03/01/2022 16:01:05 - INFO - __main__ - Step 120 Global step 120 Train loss 1.95 on epoch=59
03/01/2022 16:01:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.97 on epoch=64
03/01/2022 16:01:10 - INFO - __main__ - Step 140 Global step 140 Train loss 1.88 on epoch=69
03/01/2022 16:01:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.85 on epoch=74
03/01/2022 16:01:13 - INFO - __main__ - Global step 150 Train loss 1.94 EM 0.0 on epoch=74
03/01/2022 16:01:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.79 on epoch=79
03/01/2022 16:01:18 - INFO - __main__ - Step 170 Global step 170 Train loss 1.77 on epoch=84
03/01/2022 16:01:20 - INFO - __main__ - Step 180 Global step 180 Train loss 1.66 on epoch=89
03/01/2022 16:01:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.77 on epoch=94
03/01/2022 16:01:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.67 on epoch=99
03/01/2022 16:01:26 - INFO - __main__ - Global step 200 Train loss 1.73 EM 0.0 on epoch=99
03/01/2022 16:01:28 - INFO - __main__ - Step 210 Global step 210 Train loss 1.55 on epoch=104
03/01/2022 16:01:30 - INFO - __main__ - Step 220 Global step 220 Train loss 1.55 on epoch=109
03/01/2022 16:01:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.63 on epoch=114
03/01/2022 16:01:35 - INFO - __main__ - Step 240 Global step 240 Train loss 1.50 on epoch=119
03/01/2022 16:01:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.42 on epoch=124
03/01/2022 16:01:38 - INFO - __main__ - Global step 250 Train loss 1.53 EM 0.0 on epoch=124
03/01/2022 16:01:41 - INFO - __main__ - Step 260 Global step 260 Train loss 1.44 on epoch=129
03/01/2022 16:01:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.33 on epoch=134
03/01/2022 16:01:45 - INFO - __main__ - Step 280 Global step 280 Train loss 1.44 on epoch=139
03/01/2022 16:01:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.35 on epoch=144
03/01/2022 16:01:50 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=149
03/01/2022 16:01:51 - INFO - __main__ - Global step 300 Train loss 1.37 EM 0.0 on epoch=149
03/01/2022 16:01:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.37 on epoch=154
03/01/2022 16:01:56 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
03/01/2022 16:01:58 - INFO - __main__ - Step 330 Global step 330 Train loss 1.26 on epoch=164
03/01/2022 16:02:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
03/01/2022 16:02:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
03/01/2022 16:02:04 - INFO - __main__ - Global step 350 Train loss 1.27 EM 0.0 on epoch=174
03/01/2022 16:02:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.19 on epoch=179
03/01/2022 16:02:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=184
03/01/2022 16:02:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.21 on epoch=189
03/01/2022 16:02:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.07 on epoch=194
03/01/2022 16:02:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.06 on epoch=199
03/01/2022 16:02:16 - INFO - __main__ - Global step 400 Train loss 1.14 EM 0.0 on epoch=199
03/01/2022 16:02:18 - INFO - __main__ - Step 410 Global step 410 Train loss 1.11 on epoch=204
03/01/2022 16:02:21 - INFO - __main__ - Step 420 Global step 420 Train loss 1.12 on epoch=209
03/01/2022 16:02:23 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
03/01/2022 16:02:25 - INFO - __main__ - Step 440 Global step 440 Train loss 1.02 on epoch=219
03/01/2022 16:02:27 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=224
03/01/2022 16:02:29 - INFO - __main__ - Global step 450 Train loss 1.08 EM 0.0 on epoch=224
03/01/2022 16:02:31 - INFO - __main__ - Step 460 Global step 460 Train loss 1.04 on epoch=229
03/01/2022 16:02:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.96 on epoch=234
03/01/2022 16:02:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.94 on epoch=239
03/01/2022 16:02:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=244
03/01/2022 16:02:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=249
03/01/2022 16:02:41 - INFO - __main__ - Global step 500 Train loss 0.96 EM 0.0 on epoch=249
03/01/2022 16:02:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=254
03/01/2022 16:02:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=259
03/01/2022 16:02:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=264
03/01/2022 16:02:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
03/01/2022 16:02:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=274
03/01/2022 16:02:54 - INFO - __main__ - Global step 550 Train loss 0.85 EM 0.0 on epoch=274
03/01/2022 16:02:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 16:02:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=284
03/01/2022 16:03:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.79 on epoch=289
03/01/2022 16:03:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=294
03/01/2022 16:03:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
03/01/2022 16:03:07 - INFO - __main__ - Global step 600 Train loss 0.82 EM 0.0 on epoch=299
03/01/2022 16:03:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=304
03/01/2022 16:03:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
03/01/2022 16:03:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=314
03/01/2022 16:03:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=319
03/01/2022 16:03:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=324
03/01/2022 16:03:19 - INFO - __main__ - Global step 650 Train loss 0.74 EM 0.0 on epoch=324
03/01/2022 16:03:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=329
03/01/2022 16:03:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.66 on epoch=334
03/01/2022 16:03:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=339
03/01/2022 16:03:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=344
03/01/2022 16:03:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=349
03/01/2022 16:03:32 - INFO - __main__ - Global step 700 Train loss 0.64 EM 0.0 on epoch=349
03/01/2022 16:03:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/01/2022 16:03:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=359
03/01/2022 16:03:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=364
03/01/2022 16:03:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.62 on epoch=369
03/01/2022 16:03:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 16:03:45 - INFO - __main__ - Global step 750 Train loss 0.59 EM 0.0 on epoch=374
03/01/2022 16:03:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=379
03/01/2022 16:03:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=384
03/01/2022 16:03:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=389
03/01/2022 16:03:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=394
03/01/2022 16:03:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=399
03/01/2022 16:03:58 - INFO - __main__ - Global step 800 Train loss 0.53 EM 0.0 on epoch=399
03/01/2022 16:04:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=404
03/01/2022 16:04:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=409
03/01/2022 16:04:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=414
03/01/2022 16:04:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
03/01/2022 16:04:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=424
03/01/2022 16:04:11 - INFO - __main__ - Global step 850 Train loss 0.49 EM 0.0 on epoch=424
03/01/2022 16:04:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=429
03/01/2022 16:04:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=434
03/01/2022 16:04:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
03/01/2022 16:04:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
03/01/2022 16:04:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=449
03/01/2022 16:04:24 - INFO - __main__ - Global step 900 Train loss 0.46 EM 0.0 on epoch=449
03/01/2022 16:04:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
03/01/2022 16:04:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
03/01/2022 16:04:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
03/01/2022 16:04:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
03/01/2022 16:04:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 16:04:37 - INFO - __main__ - Global step 950 Train loss 0.41 EM 0.0 on epoch=474
03/01/2022 16:04:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/01/2022 16:04:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/01/2022 16:04:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/01/2022 16:04:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/01/2022 16:04:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=499
03/01/2022 16:04:50 - INFO - __main__ - Global step 1000 Train loss 0.39 EM 0.0 on epoch=499
03/01/2022 16:04:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
03/01/2022 16:04:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=509
03/01/2022 16:04:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=514
03/01/2022 16:04:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/01/2022 16:05:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
03/01/2022 16:05:02 - INFO - __main__ - Global step 1050 Train loss 0.33 EM 0.0 on epoch=524
03/01/2022 16:05:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
03/01/2022 16:05:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=534
03/01/2022 16:05:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=539
03/01/2022 16:05:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/01/2022 16:05:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/01/2022 16:05:15 - INFO - __main__ - Global step 1100 Train loss 0.31 EM 0.0 on epoch=549
03/01/2022 16:05:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
03/01/2022 16:05:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
03/01/2022 16:05:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=564
03/01/2022 16:05:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=569
03/01/2022 16:05:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/01/2022 16:05:28 - INFO - __main__ - Global step 1150 Train loss 0.31 EM 0.0 on epoch=574
03/01/2022 16:05:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=579
03/01/2022 16:05:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/01/2022 16:05:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=589
03/01/2022 16:05:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
03/01/2022 16:05:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=599
03/01/2022 16:05:41 - INFO - __main__ - Global step 1200 Train loss 0.29 EM 0.0 on epoch=599
03/01/2022 16:05:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=604
03/01/2022 16:05:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/01/2022 16:05:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/01/2022 16:05:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=619
03/01/2022 16:05:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/01/2022 16:05:54 - INFO - __main__ - Global step 1250 Train loss 0.26 EM 0.0 on epoch=624
03/01/2022 16:05:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=629
03/01/2022 16:05:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/01/2022 16:06:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=639
03/01/2022 16:06:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/01/2022 16:06:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=649
03/01/2022 16:06:06 - INFO - __main__ - Global step 1300 Train loss 0.25 EM 0.0 on epoch=649
03/01/2022 16:06:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
03/01/2022 16:06:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/01/2022 16:06:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=664
03/01/2022 16:06:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=669
03/01/2022 16:06:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
03/01/2022 16:06:19 - INFO - __main__ - Global step 1350 Train loss 0.25 EM 0.0 on epoch=674
03/01/2022 16:06:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/01/2022 16:06:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/01/2022 16:06:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
03/01/2022 16:06:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/01/2022 16:06:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=699
03/01/2022 16:06:32 - INFO - __main__ - Global step 1400 Train loss 0.24 EM 0.0 on epoch=699
03/01/2022 16:06:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/01/2022 16:06:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
03/01/2022 16:06:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/01/2022 16:06:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=719
03/01/2022 16:06:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=724
03/01/2022 16:06:45 - INFO - __main__ - Global step 1450 Train loss 0.22 EM 0.0 on epoch=724
03/01/2022 16:06:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
03/01/2022 16:06:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 16:06:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=739
03/01/2022 16:06:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=744
03/01/2022 16:06:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=749
03/01/2022 16:06:58 - INFO - __main__ - Global step 1500 Train loss 0.20 EM 0.0 on epoch=749
03/01/2022 16:07:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=754
03/01/2022 16:07:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=759
03/01/2022 16:07:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
03/01/2022 16:07:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
03/01/2022 16:07:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
03/01/2022 16:07:10 - INFO - __main__ - Global step 1550 Train loss 0.20 EM 0.0 on epoch=774
03/01/2022 16:07:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=779
03/01/2022 16:07:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=784
03/01/2022 16:07:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/01/2022 16:07:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/01/2022 16:07:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=799
03/01/2022 16:07:23 - INFO - __main__ - Global step 1600 Train loss 0.19 EM 0.0 on epoch=799
03/01/2022 16:07:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/01/2022 16:07:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/01/2022 16:07:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 16:07:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/01/2022 16:07:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/01/2022 16:07:36 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 16:07:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=829
03/01/2022 16:07:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 16:07:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
03/01/2022 16:07:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=844
03/01/2022 16:07:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=849
03/01/2022 16:07:49 - INFO - __main__ - Global step 1700 Train loss 0.16 EM 0.0 on epoch=849
03/01/2022 16:07:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=854
03/01/2022 16:07:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 16:07:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=864
03/01/2022 16:07:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=869
03/01/2022 16:08:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/01/2022 16:08:01 - INFO - __main__ - Global step 1750 Train loss 0.16 EM 0.0 on epoch=874
03/01/2022 16:08:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/01/2022 16:08:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/01/2022 16:08:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 16:08:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/01/2022 16:08:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=899
03/01/2022 16:08:14 - INFO - __main__ - Global step 1800 Train loss 0.16 EM 0.0 on epoch=899
03/01/2022 16:08:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=904
03/01/2022 16:08:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/01/2022 16:08:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/01/2022 16:08:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=919
03/01/2022 16:08:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/01/2022 16:08:27 - INFO - __main__ - Global step 1850 Train loss 0.16 EM 0.0 on epoch=924
03/01/2022 16:08:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
03/01/2022 16:08:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 16:08:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/01/2022 16:08:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 16:08:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=949
03/01/2022 16:08:40 - INFO - __main__ - Global step 1900 Train loss 0.12 EM 0.0 on epoch=949
03/01/2022 16:08:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 16:08:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=959
03/01/2022 16:08:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
03/01/2022 16:08:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/01/2022 16:08:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=974
03/01/2022 16:08:52 - INFO - __main__ - Global step 1950 Train loss 0.11 EM 0.0 on epoch=974
03/01/2022 16:08:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=979
03/01/2022 16:08:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=984
03/01/2022 16:08:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=989
03/01/2022 16:09:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 16:09:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=999
03/01/2022 16:09:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:09:05 - INFO - __main__ - Printing 3 examples
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 16:09:05 - INFO - __main__ - ['orange is the new black']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 16:09:05 - INFO - __main__ - ['western australia']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 16:09:05 - INFO - __main__ - ['turkey']
03/01/2022 16:09:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:09:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:09:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:09:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:09:05 - INFO - __main__ - Printing 3 examples
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 16:09:05 - INFO - __main__ - ['benito mussolini']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 16:09:05 - INFO - __main__ - ['the kinks']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 16:09:05 - INFO - __main__ - ['saigon']
03/01/2022 16:09:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:09:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:09:05 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:09:05 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 16:09:05 - INFO - __main__ - save last model!
03/01/2022 16:09:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 16:09:05 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 16:09:05 - INFO - __main__ - Printing 3 examples
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 16:09:05 - INFO - __main__ - ['taming of the shrew']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 16:09:05 - INFO - __main__ - ['henry fonda']
03/01/2022 16:09:05 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 16:09:05 - INFO - __main__ - ['tchaikovsky']
03/01/2022 16:09:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:09:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:09:11 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 16:09:17 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:09:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:09:18 - INFO - __main__ - Starting training!
03/01/2022 16:11:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.4_8_predictions.txt
03/01/2022 16:11:57 - INFO - __main__ - EM on test data: 0.0085
03/01/2022 16:11:57 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.008512769153730596
03/01/2022 16:11:57 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.3, bsz=8 ...
03/01/2022 16:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:11:58 - INFO - __main__ - Printing 3 examples
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 16:11:58 - INFO - __main__ - ['orange is the new black']
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 16:11:58 - INFO - __main__ - ['western australia']
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 16:11:58 - INFO - __main__ - ['turkey']
03/01/2022 16:11:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 16:11:58 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:11:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:11:58 - INFO - __main__ - Printing 3 examples
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 16:11:58 - INFO - __main__ - ['benito mussolini']
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 16:11:58 - INFO - __main__ - ['the kinks']
03/01/2022 16:11:58 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 16:11:58 - INFO - __main__ - ['saigon']
03/01/2022 16:11:58 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:11:58 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:11:58 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:12:12 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:12:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:12:13 - INFO - __main__ - Starting training!
03/01/2022 16:12:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.23 on epoch=4
03/01/2022 16:12:18 - INFO - __main__ - Step 20 Global step 20 Train loss 2.74 on epoch=9
03/01/2022 16:12:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.49 on epoch=14
03/01/2022 16:12:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.48 on epoch=19
03/01/2022 16:12:25 - INFO - __main__ - Step 50 Global step 50 Train loss 2.31 on epoch=24
03/01/2022 16:12:26 - INFO - __main__ - Global step 50 Train loss 2.65 EM 0.0 on epoch=24
03/01/2022 16:12:26 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 16:12:28 - INFO - __main__ - Step 60 Global step 60 Train loss 2.31 on epoch=29
03/01/2022 16:12:31 - INFO - __main__ - Step 70 Global step 70 Train loss 2.32 on epoch=34
03/01/2022 16:12:33 - INFO - __main__ - Step 80 Global step 80 Train loss 2.23 on epoch=39
03/01/2022 16:12:35 - INFO - __main__ - Step 90 Global step 90 Train loss 2.19 on epoch=44
03/01/2022 16:12:37 - INFO - __main__ - Step 100 Global step 100 Train loss 2.13 on epoch=49
03/01/2022 16:12:39 - INFO - __main__ - Global step 100 Train loss 2.24 EM 0.0 on epoch=49
03/01/2022 16:12:41 - INFO - __main__ - Step 110 Global step 110 Train loss 2.11 on epoch=54
03/01/2022 16:12:43 - INFO - __main__ - Step 120 Global step 120 Train loss 2.09 on epoch=59
03/01/2022 16:12:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.99 on epoch=64
03/01/2022 16:12:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.98 on epoch=69
03/01/2022 16:12:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.97 on epoch=74
03/01/2022 16:12:52 - INFO - __main__ - Global step 150 Train loss 2.03 EM 0.0 on epoch=74
03/01/2022 16:12:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.99 on epoch=79
03/01/2022 16:12:56 - INFO - __main__ - Step 170 Global step 170 Train loss 1.85 on epoch=84
03/01/2022 16:12:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.80 on epoch=89
03/01/2022 16:13:01 - INFO - __main__ - Step 190 Global step 190 Train loss 1.82 on epoch=94
03/01/2022 16:13:03 - INFO - __main__ - Step 200 Global step 200 Train loss 1.76 on epoch=99
03/01/2022 16:13:05 - INFO - __main__ - Global step 200 Train loss 1.84 EM 0.0 on epoch=99
03/01/2022 16:13:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.73 on epoch=104
03/01/2022 16:13:09 - INFO - __main__ - Step 220 Global step 220 Train loss 1.70 on epoch=109
03/01/2022 16:13:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.67 on epoch=114
03/01/2022 16:13:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.61 on epoch=119
03/01/2022 16:13:16 - INFO - __main__ - Step 250 Global step 250 Train loss 1.65 on epoch=124
03/01/2022 16:13:17 - INFO - __main__ - Global step 250 Train loss 1.67 EM 0.0 on epoch=124
03/01/2022 16:13:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.56 on epoch=129
03/01/2022 16:13:22 - INFO - __main__ - Step 270 Global step 270 Train loss 1.58 on epoch=134
03/01/2022 16:13:24 - INFO - __main__ - Step 280 Global step 280 Train loss 1.48 on epoch=139
03/01/2022 16:13:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=144
03/01/2022 16:13:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.54 on epoch=149
03/01/2022 16:13:30 - INFO - __main__ - Global step 300 Train loss 1.54 EM 0.0 on epoch=149
03/01/2022 16:13:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.49 on epoch=154
03/01/2022 16:13:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.47 on epoch=159
03/01/2022 16:13:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.34 on epoch=164
03/01/2022 16:13:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.37 on epoch=169
03/01/2022 16:13:41 - INFO - __main__ - Step 350 Global step 350 Train loss 1.34 on epoch=174
03/01/2022 16:13:43 - INFO - __main__ - Global step 350 Train loss 1.40 EM 0.0 on epoch=174
03/01/2022 16:13:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.32 on epoch=179
03/01/2022 16:13:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.32 on epoch=184
03/01/2022 16:13:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.25 on epoch=189
03/01/2022 16:13:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.27 on epoch=194
03/01/2022 16:13:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.27 on epoch=199
03/01/2022 16:13:55 - INFO - __main__ - Global step 400 Train loss 1.29 EM 0.0 on epoch=199
03/01/2022 16:13:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.21 on epoch=204
03/01/2022 16:14:00 - INFO - __main__ - Step 420 Global step 420 Train loss 1.17 on epoch=209
03/01/2022 16:14:02 - INFO - __main__ - Step 430 Global step 430 Train loss 1.24 on epoch=214
03/01/2022 16:14:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
03/01/2022 16:14:07 - INFO - __main__ - Step 450 Global step 450 Train loss 1.09 on epoch=224
03/01/2022 16:14:08 - INFO - __main__ - Global step 450 Train loss 1.18 EM 0.0 on epoch=224
03/01/2022 16:14:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.20 on epoch=229
03/01/2022 16:14:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.21 on epoch=234
03/01/2022 16:14:15 - INFO - __main__ - Step 480 Global step 480 Train loss 1.11 on epoch=239
03/01/2022 16:14:17 - INFO - __main__ - Step 490 Global step 490 Train loss 1.07 on epoch=244
03/01/2022 16:14:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.10 on epoch=249
03/01/2022 16:14:20 - INFO - __main__ - Global step 500 Train loss 1.14 EM 0.0 on epoch=249
03/01/2022 16:14:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.05 on epoch=254
03/01/2022 16:14:25 - INFO - __main__ - Step 520 Global step 520 Train loss 1.06 on epoch=259
03/01/2022 16:14:27 - INFO - __main__ - Step 530 Global step 530 Train loss 1.03 on epoch=264
03/01/2022 16:14:29 - INFO - __main__ - Step 540 Global step 540 Train loss 1.15 on epoch=269
03/01/2022 16:14:32 - INFO - __main__ - Step 550 Global step 550 Train loss 1.00 on epoch=274
03/01/2022 16:14:33 - INFO - __main__ - Global step 550 Train loss 1.06 EM 0.0 on epoch=274
03/01/2022 16:14:35 - INFO - __main__ - Step 560 Global step 560 Train loss 1.04 on epoch=279
03/01/2022 16:14:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.98 on epoch=284
03/01/2022 16:14:40 - INFO - __main__ - Step 580 Global step 580 Train loss 1.08 on epoch=289
03/01/2022 16:14:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=294
03/01/2022 16:14:44 - INFO - __main__ - Step 600 Global step 600 Train loss 1.03 on epoch=299
03/01/2022 16:14:46 - INFO - __main__ - Global step 600 Train loss 1.02 EM 0.0 on epoch=299
03/01/2022 16:14:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.95 on epoch=304
03/01/2022 16:14:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.87 on epoch=309
03/01/2022 16:14:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=314
03/01/2022 16:14:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=319
03/01/2022 16:14:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=324
03/01/2022 16:14:58 - INFO - __main__ - Global step 650 Train loss 0.88 EM 0.0 on epoch=324
03/01/2022 16:15:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=329
03/01/2022 16:15:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.83 on epoch=334
03/01/2022 16:15:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=339
03/01/2022 16:15:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.76 on epoch=344
03/01/2022 16:15:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=349
03/01/2022 16:15:11 - INFO - __main__ - Global step 700 Train loss 0.82 EM 0.0 on epoch=349
03/01/2022 16:15:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.82 on epoch=354
03/01/2022 16:15:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=359
03/01/2022 16:15:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.77 on epoch=364
03/01/2022 16:15:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=369
03/01/2022 16:15:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=374
03/01/2022 16:15:24 - INFO - __main__ - Global step 750 Train loss 0.79 EM 0.0 on epoch=374
03/01/2022 16:15:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.75 on epoch=379
03/01/2022 16:15:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.65 on epoch=384
03/01/2022 16:15:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.73 on epoch=389
03/01/2022 16:15:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=394
03/01/2022 16:15:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.61 on epoch=399
03/01/2022 16:15:36 - INFO - __main__ - Global step 800 Train loss 0.68 EM 0.0 on epoch=399
03/01/2022 16:15:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.70 on epoch=404
03/01/2022 16:15:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
03/01/2022 16:15:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=414
03/01/2022 16:15:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
03/01/2022 16:15:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
03/01/2022 16:15:49 - INFO - __main__ - Global step 850 Train loss 0.66 EM 0.0 on epoch=424
03/01/2022 16:15:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=429
03/01/2022 16:15:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=434
03/01/2022 16:15:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.67 on epoch=439
03/01/2022 16:15:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.58 on epoch=444
03/01/2022 16:16:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=449
03/01/2022 16:16:01 - INFO - __main__ - Global step 900 Train loss 0.62 EM 0.0 on epoch=449
03/01/2022 16:16:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
03/01/2022 16:16:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
03/01/2022 16:16:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.58 on epoch=464
03/01/2022 16:16:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=469
03/01/2022 16:16:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.53 on epoch=474
03/01/2022 16:16:14 - INFO - __main__ - Global step 950 Train loss 0.57 EM 0.0 on epoch=474
03/01/2022 16:16:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
03/01/2022 16:16:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=484
03/01/2022 16:16:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=489
03/01/2022 16:16:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=494
03/01/2022 16:16:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=499
03/01/2022 16:16:27 - INFO - __main__ - Global step 1000 Train loss 0.52 EM 0.0 on epoch=499
03/01/2022 16:16:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
03/01/2022 16:16:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
03/01/2022 16:16:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
03/01/2022 16:16:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=519
03/01/2022 16:16:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=524
03/01/2022 16:16:39 - INFO - __main__ - Global step 1050 Train loss 0.47 EM 0.0 on epoch=524
03/01/2022 16:16:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=529
03/01/2022 16:16:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
03/01/2022 16:16:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
03/01/2022 16:16:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/01/2022 16:16:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
03/01/2022 16:16:52 - INFO - __main__ - Global step 1100 Train loss 0.42 EM 0.0 on epoch=549
03/01/2022 16:16:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
03/01/2022 16:16:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
03/01/2022 16:16:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=564
03/01/2022 16:17:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=569
03/01/2022 16:17:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
03/01/2022 16:17:05 - INFO - __main__ - Global step 1150 Train loss 0.42 EM 0.0 on epoch=574
03/01/2022 16:17:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
03/01/2022 16:17:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=584
03/01/2022 16:17:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
03/01/2022 16:17:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=594
03/01/2022 16:17:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
03/01/2022 16:17:17 - INFO - __main__ - Global step 1200 Train loss 0.40 EM 0.0 on epoch=599
03/01/2022 16:17:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
03/01/2022 16:17:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=609
03/01/2022 16:17:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
03/01/2022 16:17:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
03/01/2022 16:17:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
03/01/2022 16:17:30 - INFO - __main__ - Global step 1250 Train loss 0.37 EM 0.0 on epoch=624
03/01/2022 16:17:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=629
03/01/2022 16:17:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=634
03/01/2022 16:17:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
03/01/2022 16:17:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=644
03/01/2022 16:17:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
03/01/2022 16:17:42 - INFO - __main__ - Global step 1300 Train loss 0.35 EM 0.0 on epoch=649
03/01/2022 16:17:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
03/01/2022 16:17:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=659
03/01/2022 16:17:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/01/2022 16:17:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
03/01/2022 16:17:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/01/2022 16:17:53 - INFO - __main__ - Global step 1350 Train loss 0.34 EM 0.0 on epoch=674
03/01/2022 16:17:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=679
03/01/2022 16:17:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
03/01/2022 16:18:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
03/01/2022 16:18:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/01/2022 16:18:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
03/01/2022 16:18:05 - INFO - __main__ - Global step 1400 Train loss 0.32 EM 0.0 on epoch=699
03/01/2022 16:18:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=704
03/01/2022 16:18:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
03/01/2022 16:18:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
03/01/2022 16:18:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/01/2022 16:18:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/01/2022 16:18:17 - INFO - __main__ - Global step 1450 Train loss 0.28 EM 0.0 on epoch=724
03/01/2022 16:18:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
03/01/2022 16:18:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=734
03/01/2022 16:18:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=739
03/01/2022 16:18:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
03/01/2022 16:18:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
03/01/2022 16:18:29 - INFO - __main__ - Global step 1500 Train loss 0.27 EM 0.0 on epoch=749
03/01/2022 16:18:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=754
03/01/2022 16:18:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=759
03/01/2022 16:18:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
03/01/2022 16:18:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
03/01/2022 16:18:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=774
03/01/2022 16:18:41 - INFO - __main__ - Global step 1550 Train loss 0.26 EM 0.0 on epoch=774
03/01/2022 16:18:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 16:18:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/01/2022 16:18:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
03/01/2022 16:18:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/01/2022 16:18:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/01/2022 16:18:52 - INFO - __main__ - Global step 1600 Train loss 0.25 EM 0.0 on epoch=799
03/01/2022 16:18:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=804
03/01/2022 16:18:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
03/01/2022 16:18:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 16:19:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/01/2022 16:19:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/01/2022 16:19:04 - INFO - __main__ - Global step 1650 Train loss 0.23 EM 0.0 on epoch=824
03/01/2022 16:19:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
03/01/2022 16:19:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
03/01/2022 16:19:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=839
03/01/2022 16:19:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=844
03/01/2022 16:19:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 16:19:16 - INFO - __main__ - Global step 1700 Train loss 0.23 EM 0.0 on epoch=849
03/01/2022 16:19:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=854
03/01/2022 16:19:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/01/2022 16:19:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=864
03/01/2022 16:19:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/01/2022 16:19:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=874
03/01/2022 16:19:28 - INFO - __main__ - Global step 1750 Train loss 0.23 EM 0.0 on epoch=874
03/01/2022 16:19:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=879
03/01/2022 16:19:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=884
03/01/2022 16:19:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
03/01/2022 16:19:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
03/01/2022 16:19:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/01/2022 16:19:40 - INFO - __main__ - Global step 1800 Train loss 0.21 EM 0.0 on epoch=899
03/01/2022 16:19:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 16:19:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=909
03/01/2022 16:19:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=914
03/01/2022 16:19:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=919
03/01/2022 16:19:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
03/01/2022 16:19:52 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 16:19:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/01/2022 16:19:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/01/2022 16:19:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=939
03/01/2022 16:20:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=944
03/01/2022 16:20:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
03/01/2022 16:20:04 - INFO - __main__ - Global step 1900 Train loss 0.19 EM 0.0 on epoch=949
03/01/2022 16:20:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/01/2022 16:20:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=959
03/01/2022 16:20:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
03/01/2022 16:20:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 16:20:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=974
03/01/2022 16:20:15 - INFO - __main__ - Global step 1950 Train loss 0.17 EM 0.0 on epoch=974
03/01/2022 16:20:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/01/2022 16:20:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=984
03/01/2022 16:20:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=989
03/01/2022 16:20:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/01/2022 16:20:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
03/01/2022 16:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:20:27 - INFO - __main__ - Printing 3 examples
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 16:20:27 - INFO - __main__ - ['orange is the new black']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 16:20:27 - INFO - __main__ - ['western australia']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 16:20:27 - INFO - __main__ - ['turkey']
03/01/2022 16:20:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:20:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:20:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:20:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:20:27 - INFO - __main__ - Printing 3 examples
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 16:20:27 - INFO - __main__ - ['benito mussolini']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 16:20:27 - INFO - __main__ - ['the kinks']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 16:20:27 - INFO - __main__ - ['saigon']
03/01/2022 16:20:27 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:20:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:20:27 - INFO - __main__ - Global step 2000 Train loss 0.16 EM 0.0 on epoch=999
03/01/2022 16:20:27 - INFO - __main__ - save last model!
03/01/2022 16:20:27 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:20:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 16:20:27 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 16:20:27 - INFO - __main__ - Printing 3 examples
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 16:20:27 - INFO - __main__ - ['taming of the shrew']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 16:20:27 - INFO - __main__ - ['henry fonda']
03/01/2022 16:20:27 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 16:20:27 - INFO - __main__ - ['tchaikovsky']
03/01/2022 16:20:27 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:20:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:20:33 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 16:20:41 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:20:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:20:42 - INFO - __main__ - Starting training!
03/01/2022 16:23:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.3_8_predictions.txt
03/01/2022 16:23:10 - INFO - __main__ - EM on test data: 0.0078
03/01/2022 16:23:11 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.007761642463695543
03/01/2022 16:23:11 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.2, bsz=8 ...
03/01/2022 16:23:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:23:12 - INFO - __main__ - Printing 3 examples
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 16:23:12 - INFO - __main__ - ['orange is the new black']
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 16:23:12 - INFO - __main__ - ['western australia']
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 16:23:12 - INFO - __main__ - ['turkey']
03/01/2022 16:23:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 16:23:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:23:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:23:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:23:12 - INFO - __main__ - Printing 3 examples
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 16:23:12 - INFO - __main__ - ['benito mussolini']
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 16:23:12 - INFO - __main__ - ['the kinks']
03/01/2022 16:23:12 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 16:23:12 - INFO - __main__ - ['saigon']
03/01/2022 16:23:12 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:23:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:23:12 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:23:26 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:23:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:23:27 - INFO - __main__ - Starting training!
03/01/2022 16:23:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=4
03/01/2022 16:23:32 - INFO - __main__ - Step 20 Global step 20 Train loss 2.98 on epoch=9
03/01/2022 16:23:34 - INFO - __main__ - Step 30 Global step 30 Train loss 2.71 on epoch=14
03/01/2022 16:23:36 - INFO - __main__ - Step 40 Global step 40 Train loss 2.54 on epoch=19
03/01/2022 16:23:39 - INFO - __main__ - Step 50 Global step 50 Train loss 2.48 on epoch=24
03/01/2022 16:23:40 - INFO - __main__ - Global step 50 Train loss 2.83 EM 0.0 on epoch=24
03/01/2022 16:23:40 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 16:23:42 - INFO - __main__ - Step 60 Global step 60 Train loss 2.44 on epoch=29
03/01/2022 16:23:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.36 on epoch=34
03/01/2022 16:23:47 - INFO - __main__ - Step 80 Global step 80 Train loss 2.33 on epoch=39
03/01/2022 16:23:49 - INFO - __main__ - Step 90 Global step 90 Train loss 2.29 on epoch=44
03/01/2022 16:23:51 - INFO - __main__ - Step 100 Global step 100 Train loss 2.27 on epoch=49
03/01/2022 16:23:53 - INFO - __main__ - Global step 100 Train loss 2.33 EM 0.0 on epoch=49
03/01/2022 16:23:55 - INFO - __main__ - Step 110 Global step 110 Train loss 2.22 on epoch=54
03/01/2022 16:23:57 - INFO - __main__ - Step 120 Global step 120 Train loss 2.19 on epoch=59
03/01/2022 16:24:00 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/01/2022 16:24:02 - INFO - __main__ - Step 140 Global step 140 Train loss 2.13 on epoch=69
03/01/2022 16:24:04 - INFO - __main__ - Step 150 Global step 150 Train loss 2.13 on epoch=74
03/01/2022 16:24:06 - INFO - __main__ - Global step 150 Train loss 2.16 EM 0.0 on epoch=74
03/01/2022 16:24:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.08 on epoch=79
03/01/2022 16:24:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.02 on epoch=84
03/01/2022 16:24:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.08 on epoch=89
03/01/2022 16:24:15 - INFO - __main__ - Step 190 Global step 190 Train loss 2.01 on epoch=94
03/01/2022 16:24:17 - INFO - __main__ - Step 200 Global step 200 Train loss 2.06 on epoch=99
03/01/2022 16:24:18 - INFO - __main__ - Global step 200 Train loss 2.05 EM 0.0 on epoch=99
03/01/2022 16:24:21 - INFO - __main__ - Step 210 Global step 210 Train loss 1.96 on epoch=104
03/01/2022 16:24:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.95 on epoch=109
03/01/2022 16:24:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.87 on epoch=114
03/01/2022 16:24:28 - INFO - __main__ - Step 240 Global step 240 Train loss 1.91 on epoch=119
03/01/2022 16:24:30 - INFO - __main__ - Step 250 Global step 250 Train loss 1.90 on epoch=124
03/01/2022 16:24:31 - INFO - __main__ - Global step 250 Train loss 1.92 EM 0.0 on epoch=124
03/01/2022 16:24:34 - INFO - __main__ - Step 260 Global step 260 Train loss 1.87 on epoch=129
03/01/2022 16:24:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.90 on epoch=134
03/01/2022 16:24:38 - INFO - __main__ - Step 280 Global step 280 Train loss 1.80 on epoch=139
03/01/2022 16:24:40 - INFO - __main__ - Step 290 Global step 290 Train loss 1.72 on epoch=144
03/01/2022 16:24:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.71 on epoch=149
03/01/2022 16:24:44 - INFO - __main__ - Global step 300 Train loss 1.80 EM 0.0 on epoch=149
03/01/2022 16:24:46 - INFO - __main__ - Step 310 Global step 310 Train loss 1.69 on epoch=154
03/01/2022 16:24:49 - INFO - __main__ - Step 320 Global step 320 Train loss 1.71 on epoch=159
03/01/2022 16:24:51 - INFO - __main__ - Step 330 Global step 330 Train loss 1.69 on epoch=164
03/01/2022 16:24:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.73 on epoch=169
03/01/2022 16:24:55 - INFO - __main__ - Step 350 Global step 350 Train loss 1.61 on epoch=174
03/01/2022 16:24:57 - INFO - __main__ - Global step 350 Train loss 1.69 EM 0.0 on epoch=174
03/01/2022 16:24:59 - INFO - __main__ - Step 360 Global step 360 Train loss 1.55 on epoch=179
03/01/2022 16:25:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.60 on epoch=184
03/01/2022 16:25:03 - INFO - __main__ - Step 380 Global step 380 Train loss 1.59 on epoch=189
03/01/2022 16:25:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.52 on epoch=194
03/01/2022 16:25:08 - INFO - __main__ - Step 400 Global step 400 Train loss 1.56 on epoch=199
03/01/2022 16:25:09 - INFO - __main__ - Global step 400 Train loss 1.57 EM 0.0 on epoch=199
03/01/2022 16:25:12 - INFO - __main__ - Step 410 Global step 410 Train loss 1.57 on epoch=204
03/01/2022 16:25:14 - INFO - __main__ - Step 420 Global step 420 Train loss 1.49 on epoch=209
03/01/2022 16:25:16 - INFO - __main__ - Step 430 Global step 430 Train loss 1.36 on epoch=214
03/01/2022 16:25:18 - INFO - __main__ - Step 440 Global step 440 Train loss 1.43 on epoch=219
03/01/2022 16:25:21 - INFO - __main__ - Step 450 Global step 450 Train loss 1.38 on epoch=224
03/01/2022 16:25:22 - INFO - __main__ - Global step 450 Train loss 1.45 EM 0.0 on epoch=224
03/01/2022 16:25:24 - INFO - __main__ - Step 460 Global step 460 Train loss 1.40 on epoch=229
03/01/2022 16:25:26 - INFO - __main__ - Step 470 Global step 470 Train loss 1.39 on epoch=234
03/01/2022 16:25:29 - INFO - __main__ - Step 480 Global step 480 Train loss 1.39 on epoch=239
03/01/2022 16:25:31 - INFO - __main__ - Step 490 Global step 490 Train loss 1.36 on epoch=244
03/01/2022 16:25:33 - INFO - __main__ - Step 500 Global step 500 Train loss 1.40 on epoch=249
03/01/2022 16:25:34 - INFO - __main__ - Global step 500 Train loss 1.39 EM 0.0 on epoch=249
03/01/2022 16:25:37 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
03/01/2022 16:25:39 - INFO - __main__ - Step 520 Global step 520 Train loss 1.31 on epoch=259
03/01/2022 16:25:41 - INFO - __main__ - Step 530 Global step 530 Train loss 1.31 on epoch=264
03/01/2022 16:25:43 - INFO - __main__ - Step 540 Global step 540 Train loss 1.24 on epoch=269
03/01/2022 16:25:46 - INFO - __main__ - Step 550 Global step 550 Train loss 1.23 on epoch=274
03/01/2022 16:25:47 - INFO - __main__ - Global step 550 Train loss 1.28 EM 0.0 on epoch=274
03/01/2022 16:25:49 - INFO - __main__ - Step 560 Global step 560 Train loss 1.26 on epoch=279
03/01/2022 16:25:51 - INFO - __main__ - Step 570 Global step 570 Train loss 1.23 on epoch=284
03/01/2022 16:25:54 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
03/01/2022 16:25:56 - INFO - __main__ - Step 590 Global step 590 Train loss 1.17 on epoch=294
03/01/2022 16:25:58 - INFO - __main__ - Step 600 Global step 600 Train loss 1.27 on epoch=299
03/01/2022 16:25:59 - INFO - __main__ - Global step 600 Train loss 1.22 EM 0.0 on epoch=299
03/01/2022 16:26:02 - INFO - __main__ - Step 610 Global step 610 Train loss 1.22 on epoch=304
03/01/2022 16:26:04 - INFO - __main__ - Step 620 Global step 620 Train loss 1.14 on epoch=309
03/01/2022 16:26:06 - INFO - __main__ - Step 630 Global step 630 Train loss 1.19 on epoch=314
03/01/2022 16:26:08 - INFO - __main__ - Step 640 Global step 640 Train loss 1.09 on epoch=319
03/01/2022 16:26:11 - INFO - __main__ - Step 650 Global step 650 Train loss 1.08 on epoch=324
03/01/2022 16:26:12 - INFO - __main__ - Global step 650 Train loss 1.15 EM 0.0 on epoch=324
03/01/2022 16:26:14 - INFO - __main__ - Step 660 Global step 660 Train loss 1.20 on epoch=329
03/01/2022 16:26:17 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
03/01/2022 16:26:19 - INFO - __main__ - Step 680 Global step 680 Train loss 1.13 on epoch=339
03/01/2022 16:26:21 - INFO - __main__ - Step 690 Global step 690 Train loss 1.01 on epoch=344
03/01/2022 16:26:23 - INFO - __main__ - Step 700 Global step 700 Train loss 1.02 on epoch=349
03/01/2022 16:26:25 - INFO - __main__ - Global step 700 Train loss 1.08 EM 0.0 on epoch=349
03/01/2022 16:26:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.98 on epoch=354
03/01/2022 16:26:29 - INFO - __main__ - Step 720 Global step 720 Train loss 1.03 on epoch=359
03/01/2022 16:26:31 - INFO - __main__ - Step 730 Global step 730 Train loss 1.04 on epoch=364
03/01/2022 16:26:34 - INFO - __main__ - Step 740 Global step 740 Train loss 1.00 on epoch=369
03/01/2022 16:26:36 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
03/01/2022 16:26:37 - INFO - __main__ - Global step 750 Train loss 1.02 EM 0.0 on epoch=374
03/01/2022 16:26:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.96 on epoch=379
03/01/2022 16:26:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
03/01/2022 16:26:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.95 on epoch=389
03/01/2022 16:26:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.87 on epoch=394
03/01/2022 16:26:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.86 on epoch=399
03/01/2022 16:26:50 - INFO - __main__ - Global step 800 Train loss 0.92 EM 0.0 on epoch=399
03/01/2022 16:26:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=404
03/01/2022 16:26:54 - INFO - __main__ - Step 820 Global step 820 Train loss 1.01 on epoch=409
03/01/2022 16:26:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.88 on epoch=414
03/01/2022 16:26:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
03/01/2022 16:27:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=424
03/01/2022 16:27:03 - INFO - __main__ - Global step 850 Train loss 0.89 EM 0.0 on epoch=424
03/01/2022 16:27:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=429
03/01/2022 16:27:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=434
03/01/2022 16:27:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=439
03/01/2022 16:27:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=444
03/01/2022 16:27:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.88 on epoch=449
03/01/2022 16:27:15 - INFO - __main__ - Global step 900 Train loss 0.83 EM 0.0 on epoch=449
03/01/2022 16:27:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=454
03/01/2022 16:27:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.80 on epoch=459
03/01/2022 16:27:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.77 on epoch=464
03/01/2022 16:27:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=469
03/01/2022 16:27:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.79 on epoch=474
03/01/2022 16:27:28 - INFO - __main__ - Global step 950 Train loss 0.79 EM 0.0 on epoch=474
03/01/2022 16:27:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.74 on epoch=479
03/01/2022 16:27:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
03/01/2022 16:27:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.72 on epoch=489
03/01/2022 16:27:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=494
03/01/2022 16:27:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.71 on epoch=499
03/01/2022 16:27:41 - INFO - __main__ - Global step 1000 Train loss 0.75 EM 0.0 on epoch=499
03/01/2022 16:27:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.72 on epoch=504
03/01/2022 16:27:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.69 on epoch=509
03/01/2022 16:27:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.66 on epoch=514
03/01/2022 16:27:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
03/01/2022 16:27:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.77 on epoch=524
03/01/2022 16:27:53 - INFO - __main__ - Global step 1050 Train loss 0.71 EM 0.0 on epoch=524
03/01/2022 16:27:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
03/01/2022 16:27:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.72 on epoch=534
03/01/2022 16:28:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=539
03/01/2022 16:28:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.61 on epoch=544
03/01/2022 16:28:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.69 on epoch=549
03/01/2022 16:28:06 - INFO - __main__ - Global step 1100 Train loss 0.66 EM 0.0 on epoch=549
03/01/2022 16:28:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
03/01/2022 16:28:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=559
03/01/2022 16:28:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=564
03/01/2022 16:28:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.63 on epoch=569
03/01/2022 16:28:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.72 on epoch=574
03/01/2022 16:28:19 - INFO - __main__ - Global step 1150 Train loss 0.64 EM 0.0 on epoch=574
03/01/2022 16:28:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=579
03/01/2022 16:28:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
03/01/2022 16:28:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.69 on epoch=589
03/01/2022 16:28:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=594
03/01/2022 16:28:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.62 on epoch=599
03/01/2022 16:28:32 - INFO - __main__ - Global step 1200 Train loss 0.61 EM 0.0 on epoch=599
03/01/2022 16:28:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=604
03/01/2022 16:28:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.59 on epoch=609
03/01/2022 16:28:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.62 on epoch=614
03/01/2022 16:28:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.55 on epoch=619
03/01/2022 16:28:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.61 on epoch=624
03/01/2022 16:28:44 - INFO - __main__ - Global step 1250 Train loss 0.59 EM 0.0 on epoch=624
03/01/2022 16:28:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.61 on epoch=629
03/01/2022 16:28:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=634
03/01/2022 16:28:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.54 on epoch=639
03/01/2022 16:28:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.64 on epoch=644
03/01/2022 16:28:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
03/01/2022 16:28:57 - INFO - __main__ - Global step 1300 Train loss 0.58 EM 0.0 on epoch=649
03/01/2022 16:28:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.60 on epoch=654
03/01/2022 16:29:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
03/01/2022 16:29:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=664
03/01/2022 16:29:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
03/01/2022 16:29:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=674
03/01/2022 16:29:10 - INFO - __main__ - Global step 1350 Train loss 0.52 EM 0.0 on epoch=674
03/01/2022 16:29:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=679
03/01/2022 16:29:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.52 on epoch=684
03/01/2022 16:29:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=689
03/01/2022 16:29:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=694
03/01/2022 16:29:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=699
03/01/2022 16:29:22 - INFO - __main__ - Global step 1400 Train loss 0.52 EM 0.0 on epoch=699
03/01/2022 16:29:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
03/01/2022 16:29:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
03/01/2022 16:29:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=714
03/01/2022 16:29:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=719
03/01/2022 16:29:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
03/01/2022 16:29:35 - INFO - __main__ - Global step 1450 Train loss 0.48 EM 0.0 on epoch=724
03/01/2022 16:29:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=729
03/01/2022 16:29:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
03/01/2022 16:29:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=739
03/01/2022 16:29:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
03/01/2022 16:29:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
03/01/2022 16:29:48 - INFO - __main__ - Global step 1500 Train loss 0.46 EM 0.0 on epoch=749
03/01/2022 16:29:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
03/01/2022 16:29:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
03/01/2022 16:29:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=764
03/01/2022 16:29:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
03/01/2022 16:29:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
03/01/2022 16:30:01 - INFO - __main__ - Global step 1550 Train loss 0.44 EM 0.0 on epoch=774
03/01/2022 16:30:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
03/01/2022 16:30:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
03/01/2022 16:30:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
03/01/2022 16:30:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
03/01/2022 16:30:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=799
03/01/2022 16:30:13 - INFO - __main__ - Global step 1600 Train loss 0.41 EM 0.0 on epoch=799
03/01/2022 16:30:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
03/01/2022 16:30:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
03/01/2022 16:30:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
03/01/2022 16:30:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
03/01/2022 16:30:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
03/01/2022 16:30:26 - INFO - __main__ - Global step 1650 Train loss 0.42 EM 0.0 on epoch=824
03/01/2022 16:30:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
03/01/2022 16:30:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
03/01/2022 16:30:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
03/01/2022 16:30:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
03/01/2022 16:30:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
03/01/2022 16:30:39 - INFO - __main__ - Global step 1700 Train loss 0.39 EM 0.0 on epoch=849
03/01/2022 16:30:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
03/01/2022 16:30:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
03/01/2022 16:30:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
03/01/2022 16:30:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=869
03/01/2022 16:30:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
03/01/2022 16:30:51 - INFO - __main__ - Global step 1750 Train loss 0.40 EM 0.0 on epoch=874
03/01/2022 16:30:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
03/01/2022 16:30:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
03/01/2022 16:30:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
03/01/2022 16:31:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=894
03/01/2022 16:31:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=899
03/01/2022 16:31:04 - INFO - __main__ - Global step 1800 Train loss 0.37 EM 0.0 on epoch=899
03/01/2022 16:31:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
03/01/2022 16:31:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
03/01/2022 16:31:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=914
03/01/2022 16:31:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
03/01/2022 16:31:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
03/01/2022 16:31:17 - INFO - __main__ - Global step 1850 Train loss 0.38 EM 0.0 on epoch=924
03/01/2022 16:31:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
03/01/2022 16:31:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=934
03/01/2022 16:31:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
03/01/2022 16:31:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
03/01/2022 16:31:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
03/01/2022 16:31:30 - INFO - __main__ - Global step 1900 Train loss 0.36 EM 0.0 on epoch=949
03/01/2022 16:31:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
03/01/2022 16:31:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/01/2022 16:31:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/01/2022 16:31:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
03/01/2022 16:31:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
03/01/2022 16:31:42 - INFO - __main__ - Global step 1950 Train loss 0.34 EM 0.0 on epoch=974
03/01/2022 16:31:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
03/01/2022 16:31:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
03/01/2022 16:31:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
03/01/2022 16:31:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
03/01/2022 16:31:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
03/01/2022 16:31:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:31:55 - INFO - __main__ - Printing 3 examples
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:31:55 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:31:55 - INFO - __main__ - ['uranus']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:31:55 - INFO - __main__ - ['rob thomas']
03/01/2022 16:31:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:31:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:31:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:31:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:31:55 - INFO - __main__ - Printing 3 examples
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:31:55 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:31:55 - INFO - __main__ - ['casino royale']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:31:55 - INFO - __main__ - ['offenbach']
03/01/2022 16:31:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:31:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:31:55 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:31:55 - INFO - __main__ - Global step 2000 Train loss 0.32 EM 0.0 on epoch=999
03/01/2022 16:31:55 - INFO - __main__ - save last model!
03/01/2022 16:31:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 16:31:55 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 16:31:55 - INFO - __main__ - Printing 3 examples
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 16:31:55 - INFO - __main__ - ['taming of the shrew']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 16:31:55 - INFO - __main__ - ['henry fonda']
03/01/2022 16:31:55 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 16:31:55 - INFO - __main__ - ['tchaikovsky']
03/01/2022 16:31:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:31:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:32:00 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 16:32:07 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:32:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:32:08 - INFO - __main__ - Starting training!
03/01/2022 16:34:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.2_8_predictions.txt
03/01/2022 16:34:44 - INFO - __main__ - EM on test data: 0.0083
03/01/2022 16:34:45 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.008262393590385579
03/01/2022 16:34:45 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.5, bsz=8 ...
03/01/2022 16:34:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:34:45 - INFO - __main__ - Printing 3 examples
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:34:45 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:34:45 - INFO - __main__ - ['uranus']
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:34:45 - INFO - __main__ - ['rob thomas']
03/01/2022 16:34:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 16:34:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:34:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:34:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:34:45 - INFO - __main__ - Printing 3 examples
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:34:45 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:34:45 - INFO - __main__ - ['casino royale']
03/01/2022 16:34:45 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:34:45 - INFO - __main__ - ['offenbach']
03/01/2022 16:34:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:34:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:34:46 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:34:59 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:35:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:35:00 - INFO - __main__ - Starting training!
03/01/2022 16:35:03 - INFO - __main__ - Step 10 Global step 10 Train loss 3.22 on epoch=4
03/01/2022 16:35:05 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=9
03/01/2022 16:35:07 - INFO - __main__ - Step 30 Global step 30 Train loss 2.43 on epoch=14
03/01/2022 16:35:10 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=19
03/01/2022 16:35:12 - INFO - __main__ - Step 50 Global step 50 Train loss 2.28 on epoch=24
03/01/2022 16:35:14 - INFO - __main__ - Global step 50 Train loss 2.58 EM 0.03125 on epoch=24
03/01/2022 16:35:14 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/01/2022 16:35:16 - INFO - __main__ - Step 60 Global step 60 Train loss 2.24 on epoch=29
03/01/2022 16:35:18 - INFO - __main__ - Step 70 Global step 70 Train loss 2.10 on epoch=34
03/01/2022 16:35:21 - INFO - __main__ - Step 80 Global step 80 Train loss 2.11 on epoch=39
03/01/2022 16:35:23 - INFO - __main__ - Step 90 Global step 90 Train loss 1.90 on epoch=44
03/01/2022 16:35:25 - INFO - __main__ - Step 100 Global step 100 Train loss 1.87 on epoch=49
03/01/2022 16:35:27 - INFO - __main__ - Global step 100 Train loss 2.05 EM 0.0 on epoch=49
03/01/2022 16:35:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.82 on epoch=54
03/01/2022 16:35:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.74 on epoch=59
03/01/2022 16:35:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.70 on epoch=64
03/01/2022 16:35:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.70 on epoch=69
03/01/2022 16:35:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.60 on epoch=74
03/01/2022 16:35:39 - INFO - __main__ - Global step 150 Train loss 1.71 EM 0.0 on epoch=74
03/01/2022 16:35:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.58 on epoch=79
03/01/2022 16:35:44 - INFO - __main__ - Step 170 Global step 170 Train loss 1.56 on epoch=84
03/01/2022 16:35:46 - INFO - __main__ - Step 180 Global step 180 Train loss 1.45 on epoch=89
03/01/2022 16:35:49 - INFO - __main__ - Step 190 Global step 190 Train loss 1.44 on epoch=94
03/01/2022 16:35:51 - INFO - __main__ - Step 200 Global step 200 Train loss 1.34 on epoch=99
03/01/2022 16:35:52 - INFO - __main__ - Global step 200 Train loss 1.47 EM 0.0 on epoch=99
03/01/2022 16:35:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.34 on epoch=104
03/01/2022 16:35:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.25 on epoch=109
03/01/2022 16:35:59 - INFO - __main__ - Step 230 Global step 230 Train loss 1.23 on epoch=114
03/01/2022 16:36:02 - INFO - __main__ - Step 240 Global step 240 Train loss 1.20 on epoch=119
03/01/2022 16:36:04 - INFO - __main__ - Step 250 Global step 250 Train loss 1.18 on epoch=124
03/01/2022 16:36:05 - INFO - __main__ - Global step 250 Train loss 1.24 EM 0.0 on epoch=124
03/01/2022 16:36:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.13 on epoch=129
03/01/2022 16:36:10 - INFO - __main__ - Step 270 Global step 270 Train loss 1.13 on epoch=134
03/01/2022 16:36:12 - INFO - __main__ - Step 280 Global step 280 Train loss 1.11 on epoch=139
03/01/2022 16:36:14 - INFO - __main__ - Step 290 Global step 290 Train loss 1.11 on epoch=144
03/01/2022 16:36:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.06 on epoch=149
03/01/2022 16:36:18 - INFO - __main__ - Global step 300 Train loss 1.11 EM 0.0 on epoch=149
03/01/2022 16:36:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=154
03/01/2022 16:36:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=159
03/01/2022 16:36:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=164
03/01/2022 16:36:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=169
03/01/2022 16:36:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=174
03/01/2022 16:36:31 - INFO - __main__ - Global step 350 Train loss 0.94 EM 0.0 on epoch=174
03/01/2022 16:36:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.94 on epoch=179
03/01/2022 16:36:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=184
03/01/2022 16:36:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=189
03/01/2022 16:36:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=194
03/01/2022 16:36:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=199
03/01/2022 16:36:44 - INFO - __main__ - Global step 400 Train loss 0.83 EM 0.0 on epoch=199
03/01/2022 16:36:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=204
03/01/2022 16:36:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=209
03/01/2022 16:36:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=214
03/01/2022 16:36:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=219
03/01/2022 16:36:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=224
03/01/2022 16:36:57 - INFO - __main__ - Global step 450 Train loss 0.72 EM 0.0 on epoch=224
03/01/2022 16:36:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=229
03/01/2022 16:37:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=234
03/01/2022 16:37:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=239
03/01/2022 16:37:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=244
03/01/2022 16:37:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=249
03/01/2022 16:37:09 - INFO - __main__ - Global step 500 Train loss 0.62 EM 0.0 on epoch=249
03/01/2022 16:37:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=254
03/01/2022 16:37:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=259
03/01/2022 16:37:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=264
03/01/2022 16:37:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=269
03/01/2022 16:37:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=274
03/01/2022 16:37:22 - INFO - __main__ - Global step 550 Train loss 0.55 EM 0.0 on epoch=274
03/01/2022 16:37:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=279
03/01/2022 16:37:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=284
03/01/2022 16:37:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=289
03/01/2022 16:37:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=294
03/01/2022 16:37:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
03/01/2022 16:37:35 - INFO - __main__ - Global step 600 Train loss 0.47 EM 0.0 on epoch=299
03/01/2022 16:37:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=304
03/01/2022 16:37:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
03/01/2022 16:37:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=314
03/01/2022 16:37:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=319
03/01/2022 16:37:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=324
03/01/2022 16:37:48 - INFO - __main__ - Global step 650 Train loss 0.45 EM 0.0 on epoch=324
03/01/2022 16:37:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=329
03/01/2022 16:37:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/01/2022 16:37:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/01/2022 16:37:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=344
03/01/2022 16:38:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/01/2022 16:38:01 - INFO - __main__ - Global step 700 Train loss 0.39 EM 0.0 on epoch=349
03/01/2022 16:38:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
03/01/2022 16:38:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/01/2022 16:38:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=364
03/01/2022 16:38:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/01/2022 16:38:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
03/01/2022 16:38:14 - INFO - __main__ - Global step 750 Train loss 0.36 EM 0.0 on epoch=374
03/01/2022 16:38:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=379
03/01/2022 16:38:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=384
03/01/2022 16:38:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/01/2022 16:38:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=394
03/01/2022 16:38:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=399
03/01/2022 16:38:27 - INFO - __main__ - Global step 800 Train loss 0.32 EM 0.0 on epoch=399
03/01/2022 16:38:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/01/2022 16:38:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=409
03/01/2022 16:38:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=414
03/01/2022 16:38:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=419
03/01/2022 16:38:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=424
03/01/2022 16:38:40 - INFO - __main__ - Global step 850 Train loss 0.29 EM 0.0 on epoch=424
03/01/2022 16:38:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/01/2022 16:38:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=434
03/01/2022 16:38:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=439
03/01/2022 16:38:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/01/2022 16:38:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=449
03/01/2022 16:38:53 - INFO - __main__ - Global step 900 Train loss 0.25 EM 0.0 on epoch=449
03/01/2022 16:38:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=454
03/01/2022 16:38:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=459
03/01/2022 16:38:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
03/01/2022 16:39:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=469
03/01/2022 16:39:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
03/01/2022 16:39:05 - INFO - __main__ - Global step 950 Train loss 0.26 EM 0.0 on epoch=474
03/01/2022 16:39:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/01/2022 16:39:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/01/2022 16:39:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
03/01/2022 16:39:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
03/01/2022 16:39:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
03/01/2022 16:39:18 - INFO - __main__ - Global step 1000 Train loss 0.25 EM 0.0 on epoch=499
03/01/2022 16:39:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
03/01/2022 16:39:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/01/2022 16:39:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
03/01/2022 16:39:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
03/01/2022 16:39:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/01/2022 16:39:31 - INFO - __main__ - Global step 1050 Train loss 0.21 EM 0.0 on epoch=524
03/01/2022 16:39:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
03/01/2022 16:39:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=534
03/01/2022 16:39:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/01/2022 16:39:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=544
03/01/2022 16:39:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
03/01/2022 16:39:44 - INFO - __main__ - Global step 1100 Train loss 0.20 EM 0.0 on epoch=549
03/01/2022 16:39:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
03/01/2022 16:39:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=559
03/01/2022 16:39:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
03/01/2022 16:39:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/01/2022 16:39:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=574
03/01/2022 16:39:57 - INFO - __main__ - Global step 1150 Train loss 0.21 EM 0.0 on epoch=574
03/01/2022 16:39:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=579
03/01/2022 16:40:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
03/01/2022 16:40:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
03/01/2022 16:40:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/01/2022 16:40:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=599
03/01/2022 16:40:10 - INFO - __main__ - Global step 1200 Train loss 0.19 EM 0.0 on epoch=599
03/01/2022 16:40:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=604
03/01/2022 16:40:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=609
03/01/2022 16:40:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=614
03/01/2022 16:40:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=619
03/01/2022 16:40:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
03/01/2022 16:40:23 - INFO - __main__ - Global step 1250 Train loss 0.19 EM 0.0 on epoch=624
03/01/2022 16:40:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=629
03/01/2022 16:40:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/01/2022 16:40:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=639
03/01/2022 16:40:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=644
03/01/2022 16:40:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 16:40:36 - INFO - __main__ - Global step 1300 Train loss 0.19 EM 0.0 on epoch=649
03/01/2022 16:40:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=654
03/01/2022 16:40:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
03/01/2022 16:40:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=664
03/01/2022 16:40:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=669
03/01/2022 16:40:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/01/2022 16:40:48 - INFO - __main__ - Global step 1350 Train loss 0.14 EM 0.0 on epoch=674
03/01/2022 16:40:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/01/2022 16:40:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=684
03/01/2022 16:40:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
03/01/2022 16:40:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=694
03/01/2022 16:41:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=699
03/01/2022 16:41:01 - INFO - __main__ - Global step 1400 Train loss 0.16 EM 0.0 on epoch=699
03/01/2022 16:41:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 16:41:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
03/01/2022 16:41:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=714
03/01/2022 16:41:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=719
03/01/2022 16:41:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
03/01/2022 16:41:14 - INFO - __main__ - Global step 1450 Train loss 0.14 EM 0.0 on epoch=724
03/01/2022 16:41:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=729
03/01/2022 16:41:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 16:41:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 16:41:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=744
03/01/2022 16:41:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=749
03/01/2022 16:41:27 - INFO - __main__ - Global step 1500 Train loss 0.15 EM 0.0 on epoch=749
03/01/2022 16:41:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=754
03/01/2022 16:41:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
03/01/2022 16:41:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/01/2022 16:41:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=769
03/01/2022 16:41:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=774
03/01/2022 16:41:40 - INFO - __main__ - Global step 1550 Train loss 0.13 EM 0.0 on epoch=774
03/01/2022 16:41:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/01/2022 16:41:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/01/2022 16:41:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/01/2022 16:41:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 16:41:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/01/2022 16:41:53 - INFO - __main__ - Global step 1600 Train loss 0.14 EM 0.0 on epoch=799
03/01/2022 16:41:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
03/01/2022 16:41:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/01/2022 16:42:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
03/01/2022 16:42:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 16:42:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/01/2022 16:42:06 - INFO - __main__ - Global step 1650 Train loss 0.12 EM 0.0 on epoch=824
03/01/2022 16:42:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
03/01/2022 16:42:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/01/2022 16:42:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
03/01/2022 16:42:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/01/2022 16:42:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=849
03/01/2022 16:42:18 - INFO - __main__ - Global step 1700 Train loss 0.10 EM 0.0 on epoch=849
03/01/2022 16:42:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
03/01/2022 16:42:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/01/2022 16:42:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 16:42:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/01/2022 16:42:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/01/2022 16:42:31 - INFO - __main__ - Global step 1750 Train loss 0.11 EM 0.0 on epoch=874
03/01/2022 16:42:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 16:42:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=884
03/01/2022 16:42:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=889
03/01/2022 16:42:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
03/01/2022 16:42:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 16:42:44 - INFO - __main__ - Global step 1800 Train loss 0.09 EM 0.0 on epoch=899
03/01/2022 16:42:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=904
03/01/2022 16:42:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=909
03/01/2022 16:42:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=914
03/01/2022 16:42:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=919
03/01/2022 16:42:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=924
03/01/2022 16:42:57 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.0 on epoch=924
03/01/2022 16:42:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
03/01/2022 16:43:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/01/2022 16:43:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=939
03/01/2022 16:43:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
03/01/2022 16:43:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/01/2022 16:43:10 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 16:43:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
03/01/2022 16:43:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
03/01/2022 16:43:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
03/01/2022 16:43:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=969
03/01/2022 16:43:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/01/2022 16:43:23 - INFO - __main__ - Global step 1950 Train loss 0.08 EM 0.0 on epoch=974
03/01/2022 16:43:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
03/01/2022 16:43:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
03/01/2022 16:43:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
03/01/2022 16:43:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
03/01/2022 16:43:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 16:43:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:43:36 - INFO - __main__ - Printing 3 examples
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:43:36 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:43:36 - INFO - __main__ - ['uranus']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:43:36 - INFO - __main__ - ['rob thomas']
03/01/2022 16:43:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:43:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:43:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:43:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:43:36 - INFO - __main__ - Printing 3 examples
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:43:36 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:43:36 - INFO - __main__ - ['casino royale']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:43:36 - INFO - __main__ - ['offenbach']
03/01/2022 16:43:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:43:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:43:36 - INFO - __main__ - Global step 2000 Train loss 0.08 EM 0.0 on epoch=999
03/01/2022 16:43:36 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:43:36 - INFO - __main__ - save last model!
03/01/2022 16:43:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 16:43:36 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 16:43:36 - INFO - __main__ - Printing 3 examples
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 16:43:36 - INFO - __main__ - ['taming of the shrew']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 16:43:36 - INFO - __main__ - ['henry fonda']
03/01/2022 16:43:36 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 16:43:36 - INFO - __main__ - ['tchaikovsky']
03/01/2022 16:43:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:43:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:43:41 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 16:43:48 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:43:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:43:49 - INFO - __main__ - Starting training!
03/01/2022 16:46:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.5_8_predictions.txt
03/01/2022 16:46:25 - INFO - __main__ - EM on test data: 0.0065
03/01/2022 16:46:25 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.006509764646970456
03/01/2022 16:46:25 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.4, bsz=8 ...
03/01/2022 16:46:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:46:26 - INFO - __main__ - Printing 3 examples
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:46:26 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:46:26 - INFO - __main__ - ['uranus']
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:46:26 - INFO - __main__ - ['rob thomas']
03/01/2022 16:46:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 16:46:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:46:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:46:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:46:26 - INFO - __main__ - Printing 3 examples
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:46:26 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:46:26 - INFO - __main__ - ['casino royale']
03/01/2022 16:46:26 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:46:26 - INFO - __main__ - ['offenbach']
03/01/2022 16:46:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:46:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:46:26 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:46:40 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:46:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:46:41 - INFO - __main__ - Starting training!
03/01/2022 16:46:43 - INFO - __main__ - Step 10 Global step 10 Train loss 3.23 on epoch=4
03/01/2022 16:46:46 - INFO - __main__ - Step 20 Global step 20 Train loss 2.62 on epoch=9
03/01/2022 16:46:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.51 on epoch=14
03/01/2022 16:46:50 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=19
03/01/2022 16:46:52 - INFO - __main__ - Step 50 Global step 50 Train loss 2.30 on epoch=24
03/01/2022 16:46:54 - INFO - __main__ - Global step 50 Train loss 2.61 EM 0.0 on epoch=24
03/01/2022 16:46:54 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 16:46:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.24 on epoch=29
03/01/2022 16:46:58 - INFO - __main__ - Step 70 Global step 70 Train loss 2.25 on epoch=34
03/01/2022 16:47:01 - INFO - __main__ - Step 80 Global step 80 Train loss 2.19 on epoch=39
03/01/2022 16:47:03 - INFO - __main__ - Step 90 Global step 90 Train loss 2.04 on epoch=44
03/01/2022 16:47:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.99 on epoch=49
03/01/2022 16:47:07 - INFO - __main__ - Global step 100 Train loss 2.14 EM 0.03125 on epoch=49
03/01/2022 16:47:07 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/01/2022 16:47:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.89 on epoch=54
03/01/2022 16:47:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.93 on epoch=59
03/01/2022 16:47:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.92 on epoch=64
03/01/2022 16:47:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.80 on epoch=69
03/01/2022 16:47:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.86 on epoch=74
03/01/2022 16:47:20 - INFO - __main__ - Global step 150 Train loss 1.88 EM 0.0 on epoch=74
03/01/2022 16:47:22 - INFO - __main__ - Step 160 Global step 160 Train loss 1.75 on epoch=79
03/01/2022 16:47:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.67 on epoch=84
03/01/2022 16:47:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.71 on epoch=89
03/01/2022 16:47:29 - INFO - __main__ - Step 190 Global step 190 Train loss 1.51 on epoch=94
03/01/2022 16:47:31 - INFO - __main__ - Step 200 Global step 200 Train loss 1.52 on epoch=99
03/01/2022 16:47:33 - INFO - __main__ - Global step 200 Train loss 1.63 EM 0.0 on epoch=99
03/01/2022 16:47:35 - INFO - __main__ - Step 210 Global step 210 Train loss 1.53 on epoch=104
03/01/2022 16:47:37 - INFO - __main__ - Step 220 Global step 220 Train loss 1.42 on epoch=109
03/01/2022 16:47:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.41 on epoch=114
03/01/2022 16:47:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.42 on epoch=119
03/01/2022 16:47:44 - INFO - __main__ - Step 250 Global step 250 Train loss 1.37 on epoch=124
03/01/2022 16:47:45 - INFO - __main__ - Global step 250 Train loss 1.43 EM 0.0 on epoch=124
03/01/2022 16:47:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.32 on epoch=129
03/01/2022 16:47:50 - INFO - __main__ - Step 270 Global step 270 Train loss 1.24 on epoch=134
03/01/2022 16:47:52 - INFO - __main__ - Step 280 Global step 280 Train loss 1.26 on epoch=139
03/01/2022 16:47:54 - INFO - __main__ - Step 290 Global step 290 Train loss 1.25 on epoch=144
03/01/2022 16:47:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=149
03/01/2022 16:47:58 - INFO - __main__ - Global step 300 Train loss 1.23 EM 0.0 on epoch=149
03/01/2022 16:48:00 - INFO - __main__ - Step 310 Global step 310 Train loss 1.12 on epoch=154
03/01/2022 16:48:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.12 on epoch=159
03/01/2022 16:48:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.10 on epoch=164
03/01/2022 16:48:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.07 on epoch=169
03/01/2022 16:48:10 - INFO - __main__ - Step 350 Global step 350 Train loss 1.01 on epoch=174
03/01/2022 16:48:11 - INFO - __main__ - Global step 350 Train loss 1.08 EM 0.0 on epoch=174
03/01/2022 16:48:13 - INFO - __main__ - Step 360 Global step 360 Train loss 1.06 on epoch=179
03/01/2022 16:48:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.03 on epoch=184
03/01/2022 16:48:18 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=189
03/01/2022 16:48:20 - INFO - __main__ - Step 390 Global step 390 Train loss 1.00 on epoch=194
03/01/2022 16:48:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=199
03/01/2022 16:48:24 - INFO - __main__ - Global step 400 Train loss 1.01 EM 0.0 on epoch=199
03/01/2022 16:48:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=204
03/01/2022 16:48:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.90 on epoch=209
03/01/2022 16:48:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=214
03/01/2022 16:48:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
03/01/2022 16:48:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=224
03/01/2022 16:48:36 - INFO - __main__ - Global step 450 Train loss 0.91 EM 0.0 on epoch=224
03/01/2022 16:48:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
03/01/2022 16:48:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=234
03/01/2022 16:48:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=239
03/01/2022 16:48:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=244
03/01/2022 16:48:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.76 on epoch=249
03/01/2022 16:48:49 - INFO - __main__ - Global step 500 Train loss 0.79 EM 0.0 on epoch=249
03/01/2022 16:48:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=254
03/01/2022 16:48:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=259
03/01/2022 16:48:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=264
03/01/2022 16:48:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=269
03/01/2022 16:49:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=274
03/01/2022 16:49:02 - INFO - __main__ - Global step 550 Train loss 0.74 EM 0.0 on epoch=274
03/01/2022 16:49:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=279
03/01/2022 16:49:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.65 on epoch=284
03/01/2022 16:49:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=289
03/01/2022 16:49:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=294
03/01/2022 16:49:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=299
03/01/2022 16:49:15 - INFO - __main__ - Global step 600 Train loss 0.62 EM 0.0 on epoch=299
03/01/2022 16:49:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=304
03/01/2022 16:49:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=309
03/01/2022 16:49:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=314
03/01/2022 16:49:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=319
03/01/2022 16:49:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
03/01/2022 16:49:28 - INFO - __main__ - Global step 650 Train loss 0.58 EM 0.0 on epoch=324
03/01/2022 16:49:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=329
03/01/2022 16:49:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=334
03/01/2022 16:49:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=339
03/01/2022 16:49:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=344
03/01/2022 16:49:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=349
03/01/2022 16:49:40 - INFO - __main__ - Global step 700 Train loss 0.52 EM 0.0 on epoch=349
03/01/2022 16:49:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
03/01/2022 16:49:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
03/01/2022 16:49:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=364
03/01/2022 16:49:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
03/01/2022 16:49:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=374
03/01/2022 16:49:53 - INFO - __main__ - Global step 750 Train loss 0.54 EM 0.0 on epoch=374
03/01/2022 16:49:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=379
03/01/2022 16:49:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=384
03/01/2022 16:50:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
03/01/2022 16:50:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
03/01/2022 16:50:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=399
03/01/2022 16:50:06 - INFO - __main__ - Global step 800 Train loss 0.48 EM 0.0 on epoch=399
03/01/2022 16:50:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
03/01/2022 16:50:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
03/01/2022 16:50:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=414
03/01/2022 16:50:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
03/01/2022 16:50:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=424
03/01/2022 16:50:19 - INFO - __main__ - Global step 850 Train loss 0.44 EM 0.0 on epoch=424
03/01/2022 16:50:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/01/2022 16:50:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=434
03/01/2022 16:50:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/01/2022 16:50:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
03/01/2022 16:50:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=449
03/01/2022 16:50:32 - INFO - __main__ - Global step 900 Train loss 0.41 EM 0.0 on epoch=449
03/01/2022 16:50:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/01/2022 16:50:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
03/01/2022 16:50:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
03/01/2022 16:50:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/01/2022 16:50:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 16:50:44 - INFO - __main__ - Global step 950 Train loss 0.38 EM 0.0 on epoch=474
03/01/2022 16:50:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/01/2022 16:50:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=484
03/01/2022 16:50:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/01/2022 16:50:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/01/2022 16:50:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
03/01/2022 16:50:58 - INFO - __main__ - Global step 1000 Train loss 0.35 EM 0.0 on epoch=499
03/01/2022 16:51:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=504
03/01/2022 16:51:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=509
03/01/2022 16:51:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
03/01/2022 16:51:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
03/01/2022 16:51:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=524
03/01/2022 16:51:10 - INFO - __main__ - Global step 1050 Train loss 0.32 EM 0.0 on epoch=524
03/01/2022 16:51:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=529
03/01/2022 16:51:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=534
03/01/2022 16:51:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=539
03/01/2022 16:51:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/01/2022 16:51:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=549
03/01/2022 16:51:23 - INFO - __main__ - Global step 1100 Train loss 0.28 EM 0.0 on epoch=549
03/01/2022 16:51:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
03/01/2022 16:51:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=559
03/01/2022 16:51:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/01/2022 16:51:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
03/01/2022 16:51:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/01/2022 16:51:36 - INFO - __main__ - Global step 1150 Train loss 0.27 EM 0.0 on epoch=574
03/01/2022 16:51:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/01/2022 16:51:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
03/01/2022 16:51:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/01/2022 16:51:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=594
03/01/2022 16:51:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/01/2022 16:51:48 - INFO - __main__ - Global step 1200 Train loss 0.27 EM 0.0 on epoch=599
03/01/2022 16:51:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/01/2022 16:51:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/01/2022 16:51:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=614
03/01/2022 16:51:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 16:52:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/01/2022 16:52:01 - INFO - __main__ - Global step 1250 Train loss 0.22 EM 0.0 on epoch=624
03/01/2022 16:52:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/01/2022 16:52:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=634
03/01/2022 16:52:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 16:52:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
03/01/2022 16:52:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
03/01/2022 16:52:14 - INFO - __main__ - Global step 1300 Train loss 0.25 EM 0.0 on epoch=649
03/01/2022 16:52:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/01/2022 16:52:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/01/2022 16:52:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/01/2022 16:52:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
03/01/2022 16:52:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=674
03/01/2022 16:52:27 - INFO - __main__ - Global step 1350 Train loss 0.24 EM 0.0 on epoch=674
03/01/2022 16:52:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=679
03/01/2022 16:52:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/01/2022 16:52:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=689
03/01/2022 16:52:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 16:52:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=699
03/01/2022 16:52:39 - INFO - __main__ - Global step 1400 Train loss 0.19 EM 0.0 on epoch=699
03/01/2022 16:52:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
03/01/2022 16:52:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/01/2022 16:52:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=714
03/01/2022 16:52:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=719
03/01/2022 16:52:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=724
03/01/2022 16:52:52 - INFO - __main__ - Global step 1450 Train loss 0.21 EM 0.0 on epoch=724
03/01/2022 16:52:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 16:52:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=734
03/01/2022 16:52:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 16:53:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=744
03/01/2022 16:53:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=749
03/01/2022 16:53:05 - INFO - __main__ - Global step 1500 Train loss 0.20 EM 0.0 on epoch=749
03/01/2022 16:53:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/01/2022 16:53:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/01/2022 16:53:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=764
03/01/2022 16:53:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=769
03/01/2022 16:53:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
03/01/2022 16:53:18 - INFO - __main__ - Global step 1550 Train loss 0.20 EM 0.0 on epoch=774
03/01/2022 16:53:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=779
03/01/2022 16:53:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=784
03/01/2022 16:53:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=789
03/01/2022 16:53:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=794
03/01/2022 16:53:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/01/2022 16:53:30 - INFO - __main__ - Global step 1600 Train loss 0.18 EM 0.0 on epoch=799
03/01/2022 16:53:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=804
03/01/2022 16:53:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=809
03/01/2022 16:53:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 16:53:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=819
03/01/2022 16:53:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/01/2022 16:53:43 - INFO - __main__ - Global step 1650 Train loss 0.18 EM 0.0 on epoch=824
03/01/2022 16:53:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
03/01/2022 16:53:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/01/2022 16:53:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 16:53:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=844
03/01/2022 16:53:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=849
03/01/2022 16:53:56 - INFO - __main__ - Global step 1700 Train loss 0.15 EM 0.0 on epoch=849
03/01/2022 16:53:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=854
03/01/2022 16:54:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 16:54:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 16:54:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 16:54:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=874
03/01/2022 16:54:09 - INFO - __main__ - Global step 1750 Train loss 0.15 EM 0.0 on epoch=874
03/01/2022 16:54:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=879
03/01/2022 16:54:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=884
03/01/2022 16:54:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=889
03/01/2022 16:54:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
03/01/2022 16:54:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=899
03/01/2022 16:54:21 - INFO - __main__ - Global step 1800 Train loss 0.14 EM 0.0 on epoch=899
03/01/2022 16:54:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 16:54:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/01/2022 16:54:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=914
03/01/2022 16:54:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/01/2022 16:54:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/01/2022 16:54:34 - INFO - __main__ - Global step 1850 Train loss 0.14 EM 0.0 on epoch=924
03/01/2022 16:54:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/01/2022 16:54:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=934
03/01/2022 16:54:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/01/2022 16:54:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=944
03/01/2022 16:54:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
03/01/2022 16:54:47 - INFO - __main__ - Global step 1900 Train loss 0.14 EM 0.0 on epoch=949
03/01/2022 16:54:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/01/2022 16:54:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 16:54:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 16:54:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 16:54:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
03/01/2022 16:55:00 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 16:55:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=979
03/01/2022 16:55:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=984
03/01/2022 16:55:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
03/01/2022 16:55:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
03/01/2022 16:55:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=999
03/01/2022 16:55:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:55:12 - INFO - __main__ - Printing 3 examples
03/01/2022 16:55:12 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:55:12 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:55:12 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:55:12 - INFO - __main__ - ['uranus']
03/01/2022 16:55:12 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:55:12 - INFO - __main__ - ['rob thomas']
03/01/2022 16:55:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 16:55:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:55:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:55:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:55:13 - INFO - __main__ - Printing 3 examples
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:55:13 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:55:13 - INFO - __main__ - ['casino royale']
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:55:13 - INFO - __main__ - ['offenbach']
03/01/2022 16:55:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:55:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:55:13 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:55:13 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 16:55:13 - INFO - __main__ - save last model!
03/01/2022 16:55:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 16:55:13 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 16:55:13 - INFO - __main__ - Printing 3 examples
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 16:55:13 - INFO - __main__ - ['taming of the shrew']
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 16:55:13 - INFO - __main__ - ['henry fonda']
03/01/2022 16:55:13 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 16:55:13 - INFO - __main__ - ['tchaikovsky']
03/01/2022 16:55:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:55:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:55:18 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 16:55:25 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:55:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:55:26 - INFO - __main__ - Starting training!
03/01/2022 16:58:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.4_8_predictions.txt
03/01/2022 16:58:10 - INFO - __main__ - EM on test data: 0.0070
03/01/2022 16:58:10 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.4, bsz=8, dev_performance=0.03125, test_performance=0.007010515773660491
03/01/2022 16:58:10 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.3, bsz=8 ...
03/01/2022 16:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:58:11 - INFO - __main__ - Printing 3 examples
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 16:58:11 - INFO - __main__ - ['kieren fallon']
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 16:58:11 - INFO - __main__ - ['uranus']
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 16:58:11 - INFO - __main__ - ['rob thomas']
03/01/2022 16:58:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 16:58:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:58:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 16:58:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 16:58:11 - INFO - __main__ - Printing 3 examples
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 16:58:11 - INFO - __main__ - ['terence rattigan']
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 16:58:11 - INFO - __main__ - ['casino royale']
03/01/2022 16:58:11 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 16:58:11 - INFO - __main__ - ['offenbach']
03/01/2022 16:58:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 16:58:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 16:58:11 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 16:58:25 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 16:58:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 16:58:26 - INFO - __main__ - Starting training!
03/01/2022 16:58:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.33 on epoch=4
03/01/2022 16:58:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.73 on epoch=9
03/01/2022 16:58:33 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=14
03/01/2022 16:58:35 - INFO - __main__ - Step 40 Global step 40 Train loss 2.47 on epoch=19
03/01/2022 16:58:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.40 on epoch=24
03/01/2022 16:58:39 - INFO - __main__ - Global step 50 Train loss 2.68 EM 0.03125 on epoch=24
03/01/2022 16:58:39 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/01/2022 16:58:42 - INFO - __main__ - Step 60 Global step 60 Train loss 2.37 on epoch=29
03/01/2022 16:58:44 - INFO - __main__ - Step 70 Global step 70 Train loss 2.34 on epoch=34
03/01/2022 16:58:46 - INFO - __main__ - Step 80 Global step 80 Train loss 2.24 on epoch=39
03/01/2022 16:58:48 - INFO - __main__ - Step 90 Global step 90 Train loss 2.23 on epoch=44
03/01/2022 16:58:51 - INFO - __main__ - Step 100 Global step 100 Train loss 2.05 on epoch=49
03/01/2022 16:58:52 - INFO - __main__ - Global step 100 Train loss 2.25 EM 0.03125 on epoch=49
03/01/2022 16:58:54 - INFO - __main__ - Step 110 Global step 110 Train loss 2.15 on epoch=54
03/01/2022 16:58:57 - INFO - __main__ - Step 120 Global step 120 Train loss 2.06 on epoch=59
03/01/2022 16:58:59 - INFO - __main__ - Step 130 Global step 130 Train loss 2.00 on epoch=64
03/01/2022 16:59:01 - INFO - __main__ - Step 140 Global step 140 Train loss 2.02 on epoch=69
03/01/2022 16:59:03 - INFO - __main__ - Step 150 Global step 150 Train loss 1.93 on epoch=74
03/01/2022 16:59:05 - INFO - __main__ - Global step 150 Train loss 2.03 EM 0.0 on epoch=74
03/01/2022 16:59:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.79 on epoch=79
03/01/2022 16:59:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.82 on epoch=84
03/01/2022 16:59:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.75 on epoch=89
03/01/2022 16:59:14 - INFO - __main__ - Step 190 Global step 190 Train loss 1.69 on epoch=94
03/01/2022 16:59:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.63 on epoch=99
03/01/2022 16:59:18 - INFO - __main__ - Global step 200 Train loss 1.74 EM 0.0 on epoch=99
03/01/2022 16:59:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.58 on epoch=104
03/01/2022 16:59:22 - INFO - __main__ - Step 220 Global step 220 Train loss 1.53 on epoch=109
03/01/2022 16:59:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.59 on epoch=114
03/01/2022 16:59:27 - INFO - __main__ - Step 240 Global step 240 Train loss 1.49 on epoch=119
03/01/2022 16:59:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.51 on epoch=124
03/01/2022 16:59:30 - INFO - __main__ - Global step 250 Train loss 1.54 EM 0.0 on epoch=124
03/01/2022 16:59:33 - INFO - __main__ - Step 260 Global step 260 Train loss 1.44 on epoch=129
03/01/2022 16:59:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.41 on epoch=134
03/01/2022 16:59:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.36 on epoch=139
03/01/2022 16:59:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.30 on epoch=144
03/01/2022 16:59:42 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=149
03/01/2022 16:59:43 - INFO - __main__ - Global step 300 Train loss 1.37 EM 0.0 on epoch=149
03/01/2022 16:59:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.27 on epoch=154
03/01/2022 16:59:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.28 on epoch=159
03/01/2022 16:59:50 - INFO - __main__ - Step 330 Global step 330 Train loss 1.20 on epoch=164
03/01/2022 16:59:52 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 16:59:55 - INFO - __main__ - Step 350 Global step 350 Train loss 1.24 on epoch=174
03/01/2022 16:59:56 - INFO - __main__ - Global step 350 Train loss 1.24 EM 0.0 on epoch=174
03/01/2022 16:59:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.21 on epoch=179
03/01/2022 17:00:00 - INFO - __main__ - Step 370 Global step 370 Train loss 1.18 on epoch=184
03/01/2022 17:00:03 - INFO - __main__ - Step 380 Global step 380 Train loss 1.13 on epoch=189
03/01/2022 17:00:05 - INFO - __main__ - Step 390 Global step 390 Train loss 1.12 on epoch=194
03/01/2022 17:00:07 - INFO - __main__ - Step 400 Global step 400 Train loss 1.17 on epoch=199
03/01/2022 17:00:09 - INFO - __main__ - Global step 400 Train loss 1.16 EM 0.0 on epoch=199
03/01/2022 17:00:11 - INFO - __main__ - Step 410 Global step 410 Train loss 1.06 on epoch=204
03/01/2022 17:00:13 - INFO - __main__ - Step 420 Global step 420 Train loss 1.11 on epoch=209
03/01/2022 17:00:15 - INFO - __main__ - Step 430 Global step 430 Train loss 1.14 on epoch=214
03/01/2022 17:00:18 - INFO - __main__ - Step 440 Global step 440 Train loss 1.08 on epoch=219
03/01/2022 17:00:20 - INFO - __main__ - Step 450 Global step 450 Train loss 1.06 on epoch=224
03/01/2022 17:00:21 - INFO - __main__ - Global step 450 Train loss 1.09 EM 0.0 on epoch=224
03/01/2022 17:00:24 - INFO - __main__ - Step 460 Global step 460 Train loss 1.09 on epoch=229
03/01/2022 17:00:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.99 on epoch=234
03/01/2022 17:00:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.99 on epoch=239
03/01/2022 17:00:30 - INFO - __main__ - Step 490 Global step 490 Train loss 1.04 on epoch=244
03/01/2022 17:00:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.91 on epoch=249
03/01/2022 17:00:34 - INFO - __main__ - Global step 500 Train loss 1.01 EM 0.0 on epoch=249
03/01/2022 17:00:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=254
03/01/2022 17:00:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.99 on epoch=259
03/01/2022 17:00:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=264
03/01/2022 17:00:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.97 on epoch=269
03/01/2022 17:00:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=274
03/01/2022 17:00:47 - INFO - __main__ - Global step 550 Train loss 0.92 EM 0.0 on epoch=274
03/01/2022 17:00:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 17:00:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=284
03/01/2022 17:00:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=289
03/01/2022 17:00:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
03/01/2022 17:00:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=299
03/01/2022 17:00:59 - INFO - __main__ - Global step 600 Train loss 0.85 EM 0.0 on epoch=299
03/01/2022 17:01:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=304
03/01/2022 17:01:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=309
03/01/2022 17:01:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
03/01/2022 17:01:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=319
03/01/2022 17:01:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.75 on epoch=324
03/01/2022 17:01:12 - INFO - __main__ - Global step 650 Train loss 0.78 EM 0.0 on epoch=324
03/01/2022 17:01:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.72 on epoch=329
03/01/2022 17:01:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=334
03/01/2022 17:01:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=339
03/01/2022 17:01:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.70 on epoch=344
03/01/2022 17:01:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.70 on epoch=349
03/01/2022 17:01:25 - INFO - __main__ - Global step 700 Train loss 0.73 EM 0.0 on epoch=349
03/01/2022 17:01:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
03/01/2022 17:01:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=359
03/01/2022 17:01:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=364
03/01/2022 17:01:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.62 on epoch=369
03/01/2022 17:01:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=374
03/01/2022 17:01:37 - INFO - __main__ - Global step 750 Train loss 0.66 EM 0.0 on epoch=374
03/01/2022 17:01:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.68 on epoch=379
03/01/2022 17:01:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.70 on epoch=384
03/01/2022 17:01:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=389
03/01/2022 17:01:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.61 on epoch=394
03/01/2022 17:01:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=399
03/01/2022 17:01:50 - INFO - __main__ - Global step 800 Train loss 0.64 EM 0.0 on epoch=399
03/01/2022 17:01:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
03/01/2022 17:01:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=409
03/01/2022 17:01:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=414
03/01/2022 17:01:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=419
03/01/2022 17:02:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=424
03/01/2022 17:02:03 - INFO - __main__ - Global step 850 Train loss 0.57 EM 0.0 on epoch=424
03/01/2022 17:02:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=429
03/01/2022 17:02:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=434
03/01/2022 17:02:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.60 on epoch=439
03/01/2022 17:02:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=444
03/01/2022 17:02:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
03/01/2022 17:02:16 - INFO - __main__ - Global step 900 Train loss 0.54 EM 0.0 on epoch=449
03/01/2022 17:02:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
03/01/2022 17:02:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=459
03/01/2022 17:02:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
03/01/2022 17:02:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=469
03/01/2022 17:02:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
03/01/2022 17:02:28 - INFO - __main__ - Global step 950 Train loss 0.51 EM 0.0 on epoch=474
03/01/2022 17:02:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=479
03/01/2022 17:02:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=484
03/01/2022 17:02:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
03/01/2022 17:02:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
03/01/2022 17:02:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=499
03/01/2022 17:02:41 - INFO - __main__ - Global step 1000 Train loss 0.49 EM 0.0 on epoch=499
03/01/2022 17:02:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=504
03/01/2022 17:02:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=509
03/01/2022 17:02:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=514
03/01/2022 17:02:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=519
03/01/2022 17:02:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
03/01/2022 17:02:54 - INFO - __main__ - Global step 1050 Train loss 0.45 EM 0.0 on epoch=524
03/01/2022 17:02:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=529
03/01/2022 17:02:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
03/01/2022 17:03:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
03/01/2022 17:03:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
03/01/2022 17:03:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
03/01/2022 17:03:07 - INFO - __main__ - Global step 1100 Train loss 0.44 EM 0.0 on epoch=549
03/01/2022 17:03:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=554
03/01/2022 17:03:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=559
03/01/2022 17:03:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=564
03/01/2022 17:03:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=569
03/01/2022 17:03:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=574
03/01/2022 17:03:19 - INFO - __main__ - Global step 1150 Train loss 0.42 EM 0.0 on epoch=574
03/01/2022 17:03:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
03/01/2022 17:03:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/01/2022 17:03:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/01/2022 17:03:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=594
03/01/2022 17:03:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
03/01/2022 17:03:32 - INFO - __main__ - Global step 1200 Train loss 0.38 EM 0.0 on epoch=599
03/01/2022 17:03:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
03/01/2022 17:03:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
03/01/2022 17:03:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=614
03/01/2022 17:03:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
03/01/2022 17:03:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=624
03/01/2022 17:03:45 - INFO - __main__ - Global step 1250 Train loss 0.38 EM 0.0 on epoch=624
03/01/2022 17:03:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=629
03/01/2022 17:03:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=634
03/01/2022 17:03:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=639
03/01/2022 17:03:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
03/01/2022 17:03:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
03/01/2022 17:03:57 - INFO - __main__ - Global step 1300 Train loss 0.34 EM 0.0 on epoch=649
03/01/2022 17:04:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=654
03/01/2022 17:04:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=659
03/01/2022 17:04:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/01/2022 17:04:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
03/01/2022 17:04:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/01/2022 17:04:10 - INFO - __main__ - Global step 1350 Train loss 0.34 EM 0.0 on epoch=674
03/01/2022 17:04:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
03/01/2022 17:04:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
03/01/2022 17:04:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=689
03/01/2022 17:04:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=694
03/01/2022 17:04:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
03/01/2022 17:04:23 - INFO - __main__ - Global step 1400 Train loss 0.33 EM 0.0 on epoch=699
03/01/2022 17:04:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=704
03/01/2022 17:04:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=709
03/01/2022 17:04:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=714
03/01/2022 17:04:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=719
03/01/2022 17:04:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=724
03/01/2022 17:04:36 - INFO - __main__ - Global step 1450 Train loss 0.31 EM 0.0 on epoch=724
03/01/2022 17:04:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=729
03/01/2022 17:04:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=734
03/01/2022 17:04:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=739
03/01/2022 17:04:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
03/01/2022 17:04:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=749
03/01/2022 17:04:49 - INFO - __main__ - Global step 1500 Train loss 0.31 EM 0.0 on epoch=749
03/01/2022 17:04:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=754
03/01/2022 17:04:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
03/01/2022 17:04:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=764
03/01/2022 17:04:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=769
03/01/2022 17:05:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=774
03/01/2022 17:05:01 - INFO - __main__ - Global step 1550 Train loss 0.28 EM 0.0 on epoch=774
03/01/2022 17:05:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 17:05:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=784
03/01/2022 17:05:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
03/01/2022 17:05:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
03/01/2022 17:05:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
03/01/2022 17:05:14 - INFO - __main__ - Global step 1600 Train loss 0.28 EM 0.0 on epoch=799
03/01/2022 17:05:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/01/2022 17:05:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
03/01/2022 17:05:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=814
03/01/2022 17:05:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
03/01/2022 17:05:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/01/2022 17:05:27 - INFO - __main__ - Global step 1650 Train loss 0.25 EM 0.0 on epoch=824
03/01/2022 17:05:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=829
03/01/2022 17:05:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=834
03/01/2022 17:05:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
03/01/2022 17:05:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=844
03/01/2022 17:05:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 17:05:39 - INFO - __main__ - Global step 1700 Train loss 0.26 EM 0.0 on epoch=849
03/01/2022 17:05:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/01/2022 17:05:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
03/01/2022 17:05:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
03/01/2022 17:05:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
03/01/2022 17:05:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=874
03/01/2022 17:05:52 - INFO - __main__ - Global step 1750 Train loss 0.24 EM 0.0 on epoch=874
03/01/2022 17:05:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/01/2022 17:05:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/01/2022 17:05:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=889
03/01/2022 17:06:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
03/01/2022 17:06:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/01/2022 17:06:05 - INFO - __main__ - Global step 1800 Train loss 0.22 EM 0.0 on epoch=899
03/01/2022 17:06:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 17:06:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/01/2022 17:06:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=914
03/01/2022 17:06:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=919
03/01/2022 17:06:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
03/01/2022 17:06:18 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 17:06:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/01/2022 17:06:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/01/2022 17:06:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=939
03/01/2022 17:06:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=944
03/01/2022 17:06:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=949
03/01/2022 17:06:31 - INFO - __main__ - Global step 1900 Train loss 0.21 EM 0.0 on epoch=949
03/01/2022 17:06:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=954
03/01/2022 17:06:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=959
03/01/2022 17:06:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=964
03/01/2022 17:06:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=969
03/01/2022 17:06:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
03/01/2022 17:06:44 - INFO - __main__ - Global step 1950 Train loss 0.18 EM 0.0 on epoch=974
03/01/2022 17:06:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/01/2022 17:06:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
03/01/2022 17:06:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=989
03/01/2022 17:06:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
03/01/2022 17:06:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=999
03/01/2022 17:06:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:06:56 - INFO - __main__ - Printing 3 examples
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 17:06:56 - INFO - __main__ - ['kieren fallon']
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 17:06:56 - INFO - __main__ - ['uranus']
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 17:06:56 - INFO - __main__ - ['rob thomas']
03/01/2022 17:06:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:06:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:06:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:06:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:06:56 - INFO - __main__ - Printing 3 examples
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 17:06:56 - INFO - __main__ - ['terence rattigan']
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 17:06:56 - INFO - __main__ - ['casino royale']
03/01/2022 17:06:56 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 17:06:56 - INFO - __main__ - ['offenbach']
03/01/2022 17:06:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:06:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:06:57 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:06:57 - INFO - __main__ - Global step 2000 Train loss 0.19 EM 0.0 on epoch=999
03/01/2022 17:06:57 - INFO - __main__ - save last model!
03/01/2022 17:06:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:06:57 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:06:57 - INFO - __main__ - Printing 3 examples
03/01/2022 17:06:57 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:06:57 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:06:57 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:06:57 - INFO - __main__ - ['henry fonda']
03/01/2022 17:06:57 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:06:57 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:06:57 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:06:58 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:07:02 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:07:09 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:07:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:07:10 - INFO - __main__ - Starting training!
03/01/2022 17:09:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.3_8_predictions.txt
03/01/2022 17:09:47 - INFO - __main__ - EM on test data: 0.0045
03/01/2022 17:09:48 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.004506760140210316
03/01/2022 17:09:48 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.2, bsz=8 ...
03/01/2022 17:09:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:09:49 - INFO - __main__ - Printing 3 examples
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 17:09:49 - INFO - __main__ - ['kieren fallon']
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 17:09:49 - INFO - __main__ - ['uranus']
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 17:09:49 - INFO - __main__ - ['rob thomas']
03/01/2022 17:09:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:09:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:09:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:09:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:09:49 - INFO - __main__ - Printing 3 examples
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 17:09:49 - INFO - __main__ - ['terence rattigan']
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 17:09:49 - INFO - __main__ - ['casino royale']
03/01/2022 17:09:49 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 17:09:49 - INFO - __main__ - ['offenbach']
03/01/2022 17:09:49 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:09:49 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:09:49 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:10:03 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:10:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:10:04 - INFO - __main__ - Starting training!
03/01/2022 17:10:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.41 on epoch=4
03/01/2022 17:10:09 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=9
03/01/2022 17:10:11 - INFO - __main__ - Step 30 Global step 30 Train loss 2.68 on epoch=14
03/01/2022 17:10:13 - INFO - __main__ - Step 40 Global step 40 Train loss 2.58 on epoch=19
03/01/2022 17:10:16 - INFO - __main__ - Step 50 Global step 50 Train loss 2.47 on epoch=24
03/01/2022 17:10:18 - INFO - __main__ - Global step 50 Train loss 2.84 EM 0.0 on epoch=24
03/01/2022 17:10:18 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:10:20 - INFO - __main__ - Step 60 Global step 60 Train loss 2.42 on epoch=29
03/01/2022 17:10:23 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=34
03/01/2022 17:10:25 - INFO - __main__ - Step 80 Global step 80 Train loss 2.36 on epoch=39
03/01/2022 17:10:27 - INFO - __main__ - Step 90 Global step 90 Train loss 2.31 on epoch=44
03/01/2022 17:10:30 - INFO - __main__ - Step 100 Global step 100 Train loss 2.30 on epoch=49
03/01/2022 17:10:31 - INFO - __main__ - Global step 100 Train loss 2.35 EM 0.0 on epoch=49
03/01/2022 17:10:33 - INFO - __main__ - Step 110 Global step 110 Train loss 2.20 on epoch=54
03/01/2022 17:10:36 - INFO - __main__ - Step 120 Global step 120 Train loss 2.26 on epoch=59
03/01/2022 17:10:38 - INFO - __main__ - Step 130 Global step 130 Train loss 2.17 on epoch=64
03/01/2022 17:10:40 - INFO - __main__ - Step 140 Global step 140 Train loss 2.18 on epoch=69
03/01/2022 17:10:42 - INFO - __main__ - Step 150 Global step 150 Train loss 2.17 on epoch=74
03/01/2022 17:10:44 - INFO - __main__ - Global step 150 Train loss 2.19 EM 0.0 on epoch=74
03/01/2022 17:10:46 - INFO - __main__ - Step 160 Global step 160 Train loss 2.12 on epoch=79
03/01/2022 17:10:48 - INFO - __main__ - Step 170 Global step 170 Train loss 2.05 on epoch=84
03/01/2022 17:10:51 - INFO - __main__ - Step 180 Global step 180 Train loss 2.05 on epoch=89
03/01/2022 17:10:53 - INFO - __main__ - Step 190 Global step 190 Train loss 1.98 on epoch=94
03/01/2022 17:10:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.98 on epoch=99
03/01/2022 17:10:57 - INFO - __main__ - Global step 200 Train loss 2.04 EM 0.0 on epoch=99
03/01/2022 17:10:59 - INFO - __main__ - Step 210 Global step 210 Train loss 1.97 on epoch=104
03/01/2022 17:11:01 - INFO - __main__ - Step 220 Global step 220 Train loss 1.90 on epoch=109
03/01/2022 17:11:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.90 on epoch=114
03/01/2022 17:11:06 - INFO - __main__ - Step 240 Global step 240 Train loss 1.90 on epoch=119
03/01/2022 17:11:08 - INFO - __main__ - Step 250 Global step 250 Train loss 1.82 on epoch=124
03/01/2022 17:11:09 - INFO - __main__ - Global step 250 Train loss 1.90 EM 0.0 on epoch=124
03/01/2022 17:11:12 - INFO - __main__ - Step 260 Global step 260 Train loss 1.83 on epoch=129
03/01/2022 17:11:14 - INFO - __main__ - Step 270 Global step 270 Train loss 1.92 on epoch=134
03/01/2022 17:11:16 - INFO - __main__ - Step 280 Global step 280 Train loss 1.78 on epoch=139
03/01/2022 17:11:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.81 on epoch=144
03/01/2022 17:11:21 - INFO - __main__ - Step 300 Global step 300 Train loss 1.74 on epoch=149
03/01/2022 17:11:22 - INFO - __main__ - Global step 300 Train loss 1.82 EM 0.0 on epoch=149
03/01/2022 17:11:24 - INFO - __main__ - Step 310 Global step 310 Train loss 1.80 on epoch=154
03/01/2022 17:11:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.56 on epoch=159
03/01/2022 17:11:29 - INFO - __main__ - Step 330 Global step 330 Train loss 1.71 on epoch=164
03/01/2022 17:11:31 - INFO - __main__ - Step 340 Global step 340 Train loss 1.69 on epoch=169
03/01/2022 17:11:33 - INFO - __main__ - Step 350 Global step 350 Train loss 1.68 on epoch=174
03/01/2022 17:11:35 - INFO - __main__ - Global step 350 Train loss 1.69 EM 0.0 on epoch=174
03/01/2022 17:11:37 - INFO - __main__ - Step 360 Global step 360 Train loss 1.62 on epoch=179
03/01/2022 17:11:39 - INFO - __main__ - Step 370 Global step 370 Train loss 1.55 on epoch=184
03/01/2022 17:11:42 - INFO - __main__ - Step 380 Global step 380 Train loss 1.58 on epoch=189
03/01/2022 17:11:44 - INFO - __main__ - Step 390 Global step 390 Train loss 1.53 on epoch=194
03/01/2022 17:11:46 - INFO - __main__ - Step 400 Global step 400 Train loss 1.52 on epoch=199
03/01/2022 17:11:47 - INFO - __main__ - Global step 400 Train loss 1.56 EM 0.0 on epoch=199
03/01/2022 17:11:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.47 on epoch=204
03/01/2022 17:11:52 - INFO - __main__ - Step 420 Global step 420 Train loss 1.50 on epoch=209
03/01/2022 17:11:54 - INFO - __main__ - Step 430 Global step 430 Train loss 1.53 on epoch=214
03/01/2022 17:11:57 - INFO - __main__ - Step 440 Global step 440 Train loss 1.41 on epoch=219
03/01/2022 17:11:59 - INFO - __main__ - Step 450 Global step 450 Train loss 1.40 on epoch=224
03/01/2022 17:12:00 - INFO - __main__ - Global step 450 Train loss 1.46 EM 0.0 on epoch=224
03/01/2022 17:12:03 - INFO - __main__ - Step 460 Global step 460 Train loss 1.39 on epoch=229
03/01/2022 17:12:05 - INFO - __main__ - Step 470 Global step 470 Train loss 1.46 on epoch=234
03/01/2022 17:12:07 - INFO - __main__ - Step 480 Global step 480 Train loss 1.44 on epoch=239
03/01/2022 17:12:10 - INFO - __main__ - Step 490 Global step 490 Train loss 1.34 on epoch=244
03/01/2022 17:12:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.38 on epoch=249
03/01/2022 17:12:13 - INFO - __main__ - Global step 500 Train loss 1.40 EM 0.0 on epoch=249
03/01/2022 17:12:16 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
03/01/2022 17:12:18 - INFO - __main__ - Step 520 Global step 520 Train loss 1.32 on epoch=259
03/01/2022 17:12:20 - INFO - __main__ - Step 530 Global step 530 Train loss 1.26 on epoch=264
03/01/2022 17:12:23 - INFO - __main__ - Step 540 Global step 540 Train loss 1.28 on epoch=269
03/01/2022 17:12:25 - INFO - __main__ - Step 550 Global step 550 Train loss 1.29 on epoch=274
03/01/2022 17:12:26 - INFO - __main__ - Global step 550 Train loss 1.29 EM 0.0 on epoch=274
03/01/2022 17:12:28 - INFO - __main__ - Step 560 Global step 560 Train loss 1.33 on epoch=279
03/01/2022 17:12:31 - INFO - __main__ - Step 570 Global step 570 Train loss 1.26 on epoch=284
03/01/2022 17:12:33 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
03/01/2022 17:12:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.13 on epoch=294
03/01/2022 17:12:38 - INFO - __main__ - Step 600 Global step 600 Train loss 1.18 on epoch=299
03/01/2022 17:12:39 - INFO - __main__ - Global step 600 Train loss 1.24 EM 0.0 on epoch=299
03/01/2022 17:12:41 - INFO - __main__ - Step 610 Global step 610 Train loss 1.17 on epoch=304
03/01/2022 17:12:44 - INFO - __main__ - Step 620 Global step 620 Train loss 1.21 on epoch=309
03/01/2022 17:12:46 - INFO - __main__ - Step 630 Global step 630 Train loss 1.15 on epoch=314
03/01/2022 17:12:48 - INFO - __main__ - Step 640 Global step 640 Train loss 1.14 on epoch=319
03/01/2022 17:12:51 - INFO - __main__ - Step 650 Global step 650 Train loss 1.20 on epoch=324
03/01/2022 17:12:52 - INFO - __main__ - Global step 650 Train loss 1.17 EM 0.0 on epoch=324
03/01/2022 17:12:54 - INFO - __main__ - Step 660 Global step 660 Train loss 1.09 on epoch=329
03/01/2022 17:12:56 - INFO - __main__ - Step 670 Global step 670 Train loss 1.07 on epoch=334
03/01/2022 17:12:59 - INFO - __main__ - Step 680 Global step 680 Train loss 1.06 on epoch=339
03/01/2022 17:13:01 - INFO - __main__ - Step 690 Global step 690 Train loss 1.06 on epoch=344
03/01/2022 17:13:03 - INFO - __main__ - Step 700 Global step 700 Train loss 1.09 on epoch=349
03/01/2022 17:13:05 - INFO - __main__ - Global step 700 Train loss 1.07 EM 0.0 on epoch=349
03/01/2022 17:13:07 - INFO - __main__ - Step 710 Global step 710 Train loss 1.06 on epoch=354
03/01/2022 17:13:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=359
03/01/2022 17:13:12 - INFO - __main__ - Step 730 Global step 730 Train loss 1.07 on epoch=364
03/01/2022 17:13:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.97 on epoch=369
03/01/2022 17:13:16 - INFO - __main__ - Step 750 Global step 750 Train loss 1.00 on epoch=374
03/01/2022 17:13:17 - INFO - __main__ - Global step 750 Train loss 1.01 EM 0.0 on epoch=374
03/01/2022 17:13:20 - INFO - __main__ - Step 760 Global step 760 Train loss 1.05 on epoch=379
03/01/2022 17:13:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.98 on epoch=384
03/01/2022 17:13:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.98 on epoch=389
03/01/2022 17:13:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.98 on epoch=394
03/01/2022 17:13:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.97 on epoch=399
03/01/2022 17:13:30 - INFO - __main__ - Global step 800 Train loss 0.99 EM 0.0 on epoch=399
03/01/2022 17:13:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.99 on epoch=404
03/01/2022 17:13:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
03/01/2022 17:13:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
03/01/2022 17:13:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=419
03/01/2022 17:13:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.88 on epoch=424
03/01/2022 17:13:43 - INFO - __main__ - Global step 850 Train loss 0.93 EM 0.0 on epoch=424
03/01/2022 17:13:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=429
03/01/2022 17:13:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
03/01/2022 17:13:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=439
03/01/2022 17:13:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=444
03/01/2022 17:13:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.79 on epoch=449
03/01/2022 17:13:56 - INFO - __main__ - Global step 900 Train loss 0.84 EM 0.0 on epoch=449
03/01/2022 17:13:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=454
03/01/2022 17:14:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.79 on epoch=459
03/01/2022 17:14:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.86 on epoch=464
03/01/2022 17:14:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.80 on epoch=469
03/01/2022 17:14:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.83 on epoch=474
03/01/2022 17:14:08 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 17:14:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
03/01/2022 17:14:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.75 on epoch=484
03/01/2022 17:14:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.77 on epoch=489
03/01/2022 17:14:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
03/01/2022 17:14:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.72 on epoch=499
03/01/2022 17:14:21 - INFO - __main__ - Global step 1000 Train loss 0.74 EM 0.0 on epoch=499
03/01/2022 17:14:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.72 on epoch=504
03/01/2022 17:14:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
03/01/2022 17:14:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.75 on epoch=514
03/01/2022 17:14:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.74 on epoch=519
03/01/2022 17:14:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
03/01/2022 17:14:34 - INFO - __main__ - Global step 1050 Train loss 0.73 EM 0.0 on epoch=524
03/01/2022 17:14:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=529
03/01/2022 17:14:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
03/01/2022 17:14:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
03/01/2022 17:14:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=544
03/01/2022 17:14:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=549
03/01/2022 17:14:47 - INFO - __main__ - Global step 1100 Train loss 0.67 EM 0.0 on epoch=549
03/01/2022 17:14:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=554
03/01/2022 17:14:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.71 on epoch=559
03/01/2022 17:14:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.71 on epoch=564
03/01/2022 17:14:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
03/01/2022 17:14:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
03/01/2022 17:14:59 - INFO - __main__ - Global step 1150 Train loss 0.70 EM 0.0 on epoch=574
03/01/2022 17:15:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
03/01/2022 17:15:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.67 on epoch=584
03/01/2022 17:15:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
03/01/2022 17:15:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
03/01/2022 17:15:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.66 on epoch=599
03/01/2022 17:15:12 - INFO - __main__ - Global step 1200 Train loss 0.65 EM 0.0 on epoch=599
03/01/2022 17:15:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.63 on epoch=604
03/01/2022 17:15:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.62 on epoch=609
03/01/2022 17:15:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=614
03/01/2022 17:15:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.61 on epoch=619
03/01/2022 17:15:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
03/01/2022 17:15:25 - INFO - __main__ - Global step 1250 Train loss 0.61 EM 0.0 on epoch=624
03/01/2022 17:15:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=629
03/01/2022 17:15:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.60 on epoch=634
03/01/2022 17:15:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
03/01/2022 17:15:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
03/01/2022 17:15:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.53 on epoch=649
03/01/2022 17:15:38 - INFO - __main__ - Global step 1300 Train loss 0.58 EM 0.0 on epoch=649
03/01/2022 17:15:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.58 on epoch=654
03/01/2022 17:15:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
03/01/2022 17:15:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
03/01/2022 17:15:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=669
03/01/2022 17:15:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=674
03/01/2022 17:15:51 - INFO - __main__ - Global step 1350 Train loss 0.54 EM 0.0 on epoch=674
03/01/2022 17:15:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
03/01/2022 17:15:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
03/01/2022 17:15:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
03/01/2022 17:16:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
03/01/2022 17:16:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=699
03/01/2022 17:16:03 - INFO - __main__ - Global step 1400 Train loss 0.52 EM 0.0 on epoch=699
03/01/2022 17:16:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
03/01/2022 17:16:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
03/01/2022 17:16:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=714
03/01/2022 17:16:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
03/01/2022 17:16:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=724
03/01/2022 17:16:16 - INFO - __main__ - Global step 1450 Train loss 0.51 EM 0.0 on epoch=724
03/01/2022 17:16:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=729
03/01/2022 17:16:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
03/01/2022 17:16:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
03/01/2022 17:16:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
03/01/2022 17:16:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=749
03/01/2022 17:16:29 - INFO - __main__ - Global step 1500 Train loss 0.50 EM 0.0 on epoch=749
03/01/2022 17:16:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=754
03/01/2022 17:16:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.60 on epoch=759
03/01/2022 17:16:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
03/01/2022 17:16:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
03/01/2022 17:16:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=774
03/01/2022 17:16:42 - INFO - __main__ - Global step 1550 Train loss 0.49 EM 0.0 on epoch=774
03/01/2022 17:16:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
03/01/2022 17:16:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=784
03/01/2022 17:16:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
03/01/2022 17:16:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=794
03/01/2022 17:16:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
03/01/2022 17:16:55 - INFO - __main__ - Global step 1600 Train loss 0.44 EM 0.0 on epoch=799
03/01/2022 17:16:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
03/01/2022 17:17:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/01/2022 17:17:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
03/01/2022 17:17:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/01/2022 17:17:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
03/01/2022 17:17:08 - INFO - __main__ - Global step 1650 Train loss 0.41 EM 0.0 on epoch=824
03/01/2022 17:17:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
03/01/2022 17:17:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=834
03/01/2022 17:17:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=839
03/01/2022 17:17:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
03/01/2022 17:17:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
03/01/2022 17:17:21 - INFO - __main__ - Global step 1700 Train loss 0.40 EM 0.0 on epoch=849
03/01/2022 17:17:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
03/01/2022 17:17:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
03/01/2022 17:17:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
03/01/2022 17:17:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=869
03/01/2022 17:17:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
03/01/2022 17:17:33 - INFO - __main__ - Global step 1750 Train loss 0.39 EM 0.0 on epoch=874
03/01/2022 17:17:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
03/01/2022 17:17:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
03/01/2022 17:17:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
03/01/2022 17:17:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
03/01/2022 17:17:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
03/01/2022 17:17:46 - INFO - __main__ - Global step 1800 Train loss 0.35 EM 0.0 on epoch=899
03/01/2022 17:17:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
03/01/2022 17:17:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=909
03/01/2022 17:17:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
03/01/2022 17:17:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
03/01/2022 17:17:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
03/01/2022 17:17:59 - INFO - __main__ - Global step 1850 Train loss 0.37 EM 0.0 on epoch=924
03/01/2022 17:18:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
03/01/2022 17:18:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
03/01/2022 17:18:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=939
03/01/2022 17:18:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=944
03/01/2022 17:18:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
03/01/2022 17:18:11 - INFO - __main__ - Global step 1900 Train loss 0.33 EM 0.0 on epoch=949
03/01/2022 17:18:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=954
03/01/2022 17:18:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
03/01/2022 17:18:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/01/2022 17:18:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=969
03/01/2022 17:18:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
03/01/2022 17:18:24 - INFO - __main__ - Global step 1950 Train loss 0.30 EM 0.0 on epoch=974
03/01/2022 17:18:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
03/01/2022 17:18:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
03/01/2022 17:18:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=989
03/01/2022 17:18:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
03/01/2022 17:18:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
03/01/2022 17:18:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:18:36 - INFO - __main__ - Printing 3 examples
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:18:36 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:18:36 - INFO - __main__ - ['madness']
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:18:36 - INFO - __main__ - ['genevieve']
03/01/2022 17:18:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:18:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:18:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:18:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:18:36 - INFO - __main__ - Printing 3 examples
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:18:36 - INFO - __main__ - ['will hay']
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:18:36 - INFO - __main__ - ['alan sugar']
03/01/2022 17:18:36 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:18:36 - INFO - __main__ - ['cleopatra']
03/01/2022 17:18:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:18:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:18:36 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:18:37 - INFO - __main__ - Global step 2000 Train loss 0.31 EM 0.0 on epoch=999
03/01/2022 17:18:37 - INFO - __main__ - save last model!
03/01/2022 17:18:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:18:37 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:18:37 - INFO - __main__ - Printing 3 examples
03/01/2022 17:18:37 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:18:37 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:18:37 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:18:37 - INFO - __main__ - ['henry fonda']
03/01/2022 17:18:37 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:18:37 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:18:37 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:18:38 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:18:42 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:18:51 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:18:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:18:51 - INFO - __main__ - Starting training!
03/01/2022 17:21:23 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.2_8_predictions.txt
03/01/2022 17:21:23 - INFO - __main__ - EM on test data: 0.0050
03/01/2022 17:21:24 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.005007511266900351
03/01/2022 17:21:24 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.5, bsz=8 ...
03/01/2022 17:21:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:21:24 - INFO - __main__ - Printing 3 examples
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:21:24 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:21:24 - INFO - __main__ - ['madness']
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:21:24 - INFO - __main__ - ['genevieve']
03/01/2022 17:21:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:21:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:21:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:21:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:21:24 - INFO - __main__ - Printing 3 examples
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:21:24 - INFO - __main__ - ['will hay']
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:21:24 - INFO - __main__ - ['alan sugar']
03/01/2022 17:21:24 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:21:24 - INFO - __main__ - ['cleopatra']
03/01/2022 17:21:24 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:21:25 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:21:25 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:21:38 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:21:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:21:39 - INFO - __main__ - Starting training!
03/01/2022 17:21:42 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=4
03/01/2022 17:21:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.23 on epoch=9
03/01/2022 17:21:47 - INFO - __main__ - Step 30 Global step 30 Train loss 3.05 on epoch=14
03/01/2022 17:21:49 - INFO - __main__ - Step 40 Global step 40 Train loss 2.79 on epoch=19
03/01/2022 17:21:51 - INFO - __main__ - Step 50 Global step 50 Train loss 2.77 on epoch=24
03/01/2022 17:21:53 - INFO - __main__ - Global step 50 Train loss 3.18 EM 0.0 on epoch=24
03/01/2022 17:21:53 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:21:55 - INFO - __main__ - Step 60 Global step 60 Train loss 2.64 on epoch=29
03/01/2022 17:21:57 - INFO - __main__ - Step 70 Global step 70 Train loss 2.56 on epoch=34
03/01/2022 17:22:00 - INFO - __main__ - Step 80 Global step 80 Train loss 2.39 on epoch=39
03/01/2022 17:22:02 - INFO - __main__ - Step 90 Global step 90 Train loss 2.35 on epoch=44
03/01/2022 17:22:04 - INFO - __main__ - Step 100 Global step 100 Train loss 2.26 on epoch=49
03/01/2022 17:22:06 - INFO - __main__ - Global step 100 Train loss 2.44 EM 0.0 on epoch=49
03/01/2022 17:22:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.15 on epoch=54
03/01/2022 17:22:10 - INFO - __main__ - Step 120 Global step 120 Train loss 2.06 on epoch=59
03/01/2022 17:22:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.94 on epoch=64
03/01/2022 17:22:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.89 on epoch=69
03/01/2022 17:22:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.76 on epoch=74
03/01/2022 17:22:19 - INFO - __main__ - Global step 150 Train loss 1.96 EM 0.0 on epoch=74
03/01/2022 17:22:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.80 on epoch=79
03/01/2022 17:22:23 - INFO - __main__ - Step 170 Global step 170 Train loss 1.60 on epoch=84
03/01/2022 17:22:25 - INFO - __main__ - Step 180 Global step 180 Train loss 1.54 on epoch=89
03/01/2022 17:22:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.48 on epoch=94
03/01/2022 17:22:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.45 on epoch=99
03/01/2022 17:22:31 - INFO - __main__ - Global step 200 Train loss 1.57 EM 0.0 on epoch=99
03/01/2022 17:22:34 - INFO - __main__ - Step 210 Global step 210 Train loss 1.43 on epoch=104
03/01/2022 17:22:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.29 on epoch=109
03/01/2022 17:22:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.31 on epoch=114
03/01/2022 17:22:41 - INFO - __main__ - Step 240 Global step 240 Train loss 1.27 on epoch=119
03/01/2022 17:22:43 - INFO - __main__ - Step 250 Global step 250 Train loss 1.27 on epoch=124
03/01/2022 17:22:44 - INFO - __main__ - Global step 250 Train loss 1.31 EM 0.0 on epoch=124
03/01/2022 17:22:47 - INFO - __main__ - Step 260 Global step 260 Train loss 1.20 on epoch=129
03/01/2022 17:22:49 - INFO - __main__ - Step 270 Global step 270 Train loss 1.21 on epoch=134
03/01/2022 17:22:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.21 on epoch=139
03/01/2022 17:22:53 - INFO - __main__ - Step 290 Global step 290 Train loss 1.13 on epoch=144
03/01/2022 17:22:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=149
03/01/2022 17:22:57 - INFO - __main__ - Global step 300 Train loss 1.16 EM 0.0 on epoch=149
03/01/2022 17:22:59 - INFO - __main__ - Step 310 Global step 310 Train loss 1.07 on epoch=154
03/01/2022 17:23:01 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=159
03/01/2022 17:23:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.96 on epoch=164
03/01/2022 17:23:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=169
03/01/2022 17:23:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=174
03/01/2022 17:23:10 - INFO - __main__ - Global step 350 Train loss 0.99 EM 0.03125 on epoch=174
03/01/2022 17:23:10 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=174, global_step=350
03/01/2022 17:23:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.94 on epoch=179
03/01/2022 17:23:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=184
03/01/2022 17:23:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=189
03/01/2022 17:23:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=194
03/01/2022 17:23:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=199
03/01/2022 17:23:23 - INFO - __main__ - Global step 400 Train loss 0.87 EM 0.03125 on epoch=199
03/01/2022 17:23:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=204
03/01/2022 17:23:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=209
03/01/2022 17:23:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=214
03/01/2022 17:23:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=219
03/01/2022 17:23:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=224
03/01/2022 17:23:36 - INFO - __main__ - Global step 450 Train loss 0.79 EM 0.03125 on epoch=224
03/01/2022 17:23:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=229
03/01/2022 17:23:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=234
03/01/2022 17:23:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=239
03/01/2022 17:23:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.71 on epoch=244
03/01/2022 17:23:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=249
03/01/2022 17:23:49 - INFO - __main__ - Global step 500 Train loss 0.72 EM 0.03125 on epoch=249
03/01/2022 17:23:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.66 on epoch=254
03/01/2022 17:23:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=259
03/01/2022 17:23:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=264
03/01/2022 17:23:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=269
03/01/2022 17:24:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
03/01/2022 17:24:02 - INFO - __main__ - Global step 550 Train loss 0.66 EM 0.0 on epoch=274
03/01/2022 17:24:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=279
03/01/2022 17:24:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=284
03/01/2022 17:24:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=289
03/01/2022 17:24:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
03/01/2022 17:24:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=299
03/01/2022 17:24:15 - INFO - __main__ - Global step 600 Train loss 0.55 EM 0.0 on epoch=299
03/01/2022 17:24:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=304
03/01/2022 17:24:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=309
03/01/2022 17:24:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=314
03/01/2022 17:24:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=319
03/01/2022 17:24:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=324
03/01/2022 17:24:28 - INFO - __main__ - Global step 650 Train loss 0.51 EM 0.03125 on epoch=324
03/01/2022 17:24:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=329
03/01/2022 17:24:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=334
03/01/2022 17:24:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/01/2022 17:24:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/01/2022 17:24:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/01/2022 17:24:40 - INFO - __main__ - Global step 700 Train loss 0.44 EM 0.03125 on epoch=349
03/01/2022 17:24:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=354
03/01/2022 17:24:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=359
03/01/2022 17:24:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/01/2022 17:24:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=369
03/01/2022 17:24:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=374
03/01/2022 17:24:53 - INFO - __main__ - Global step 750 Train loss 0.44 EM 0.03125 on epoch=374
03/01/2022 17:24:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
03/01/2022 17:24:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=384
03/01/2022 17:25:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
03/01/2022 17:25:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
03/01/2022 17:25:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/01/2022 17:25:06 - INFO - __main__ - Global step 800 Train loss 0.39 EM 0.0 on epoch=399
03/01/2022 17:25:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
03/01/2022 17:25:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
03/01/2022 17:25:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=414
03/01/2022 17:25:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/01/2022 17:25:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=424
03/01/2022 17:25:19 - INFO - __main__ - Global step 850 Train loss 0.37 EM 0.0 on epoch=424
03/01/2022 17:25:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=429
03/01/2022 17:25:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
03/01/2022 17:25:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=439
03/01/2022 17:25:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/01/2022 17:25:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=449
03/01/2022 17:25:32 - INFO - __main__ - Global step 900 Train loss 0.32 EM 0.03125 on epoch=449
03/01/2022 17:25:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=454
03/01/2022 17:25:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=459
03/01/2022 17:25:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=464
03/01/2022 17:25:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=469
03/01/2022 17:25:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=474
03/01/2022 17:25:45 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.03125 on epoch=474
03/01/2022 17:25:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=479
03/01/2022 17:25:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=484
03/01/2022 17:25:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=489
03/01/2022 17:25:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/01/2022 17:25:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=499
03/01/2022 17:25:58 - INFO - __main__ - Global step 1000 Train loss 0.27 EM 0.0 on epoch=499
03/01/2022 17:26:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=504
03/01/2022 17:26:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/01/2022 17:26:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=514
03/01/2022 17:26:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/01/2022 17:26:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/01/2022 17:26:11 - INFO - __main__ - Global step 1050 Train loss 0.24 EM 0.03125 on epoch=524
03/01/2022 17:26:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/01/2022 17:26:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
03/01/2022 17:26:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/01/2022 17:26:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/01/2022 17:26:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/01/2022 17:26:23 - INFO - __main__ - Global step 1100 Train loss 0.24 EM 0.03125 on epoch=549
03/01/2022 17:26:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/01/2022 17:26:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/01/2022 17:26:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=564
03/01/2022 17:26:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 17:26:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/01/2022 17:26:36 - INFO - __main__ - Global step 1150 Train loss 0.23 EM 0.0 on epoch=574
03/01/2022 17:26:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=579
03/01/2022 17:26:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=584
03/01/2022 17:26:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/01/2022 17:26:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/01/2022 17:26:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
03/01/2022 17:26:49 - INFO - __main__ - Global step 1200 Train loss 0.22 EM 0.03125 on epoch=599
03/01/2022 17:26:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/01/2022 17:26:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=609
03/01/2022 17:26:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=614
03/01/2022 17:26:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=619
03/01/2022 17:27:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/01/2022 17:27:02 - INFO - __main__ - Global step 1250 Train loss 0.19 EM 0.0 on epoch=624
03/01/2022 17:27:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/01/2022 17:27:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/01/2022 17:27:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/01/2022 17:27:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=644
03/01/2022 17:27:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 17:27:15 - INFO - __main__ - Global step 1300 Train loss 0.20 EM 0.03125 on epoch=649
03/01/2022 17:27:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/01/2022 17:27:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=659
03/01/2022 17:27:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=664
03/01/2022 17:27:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=669
03/01/2022 17:27:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=674
03/01/2022 17:27:28 - INFO - __main__ - Global step 1350 Train loss 0.17 EM 0.03125 on epoch=674
03/01/2022 17:27:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=679
03/01/2022 17:27:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/01/2022 17:27:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=689
03/01/2022 17:27:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=694
03/01/2022 17:27:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/01/2022 17:27:41 - INFO - __main__ - Global step 1400 Train loss 0.17 EM 0.0 on epoch=699
03/01/2022 17:27:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=704
03/01/2022 17:27:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
03/01/2022 17:27:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
03/01/2022 17:27:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=719
03/01/2022 17:27:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=724
03/01/2022 17:27:53 - INFO - __main__ - Global step 1450 Train loss 0.14 EM 0.03125 on epoch=724
03/01/2022 17:27:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/01/2022 17:27:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
03/01/2022 17:28:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=739
03/01/2022 17:28:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/01/2022 17:28:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/01/2022 17:28:06 - INFO - __main__ - Global step 1500 Train loss 0.15 EM 0.0 on epoch=749
03/01/2022 17:28:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/01/2022 17:28:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=759
03/01/2022 17:28:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=764
03/01/2022 17:28:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
03/01/2022 17:28:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=774
03/01/2022 17:28:19 - INFO - __main__ - Global step 1550 Train loss 0.14 EM 0.0 on epoch=774
03/01/2022 17:28:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=779
03/01/2022 17:28:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/01/2022 17:28:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=789
03/01/2022 17:28:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 17:28:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 17:28:32 - INFO - __main__ - Global step 1600 Train loss 0.14 EM 0.0 on epoch=799
03/01/2022 17:28:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=804
03/01/2022 17:28:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
03/01/2022 17:28:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=814
03/01/2022 17:28:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=819
03/01/2022 17:28:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/01/2022 17:28:45 - INFO - __main__ - Global step 1650 Train loss 0.10 EM 0.0 on epoch=824
03/01/2022 17:28:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
03/01/2022 17:28:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 17:28:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
03/01/2022 17:28:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 17:28:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=849
03/01/2022 17:28:58 - INFO - __main__ - Global step 1700 Train loss 0.11 EM 0.0 on epoch=849
03/01/2022 17:29:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=854
03/01/2022 17:29:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
03/01/2022 17:29:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 17:29:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/01/2022 17:29:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/01/2022 17:29:11 - INFO - __main__ - Global step 1750 Train loss 0.14 EM 0.0 on epoch=874
03/01/2022 17:29:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
03/01/2022 17:29:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=884
03/01/2022 17:29:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=889
03/01/2022 17:29:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=894
03/01/2022 17:29:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/01/2022 17:29:24 - INFO - __main__ - Global step 1800 Train loss 0.09 EM 0.03125 on epoch=899
03/01/2022 17:29:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=904
03/01/2022 17:29:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
03/01/2022 17:29:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
03/01/2022 17:29:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 17:29:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 17:29:36 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.03125 on epoch=924
03/01/2022 17:29:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 17:29:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/01/2022 17:29:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/01/2022 17:29:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 17:29:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=949
03/01/2022 17:29:49 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 17:29:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 17:29:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=959
03/01/2022 17:29:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
03/01/2022 17:29:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/01/2022 17:30:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/01/2022 17:30:02 - INFO - __main__ - Global step 1950 Train loss 0.08 EM 0.0 on epoch=974
03/01/2022 17:30:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=979
03/01/2022 17:30:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 17:30:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
03/01/2022 17:30:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 17:30:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
03/01/2022 17:30:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:30:15 - INFO - __main__ - Printing 3 examples
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:30:15 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:30:15 - INFO - __main__ - ['madness']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:30:15 - INFO - __main__ - ['genevieve']
03/01/2022 17:30:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:30:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:30:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:30:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:30:15 - INFO - __main__ - Printing 3 examples
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:30:15 - INFO - __main__ - ['will hay']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:30:15 - INFO - __main__ - ['alan sugar']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:30:15 - INFO - __main__ - ['cleopatra']
03/01/2022 17:30:15 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:30:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:30:15 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.0 on epoch=999
03/01/2022 17:30:15 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:30:15 - INFO - __main__ - save last model!
03/01/2022 17:30:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:30:15 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:30:15 - INFO - __main__ - Printing 3 examples
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:30:15 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:30:15 - INFO - __main__ - ['henry fonda']
03/01/2022 17:30:15 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:30:15 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:30:15 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:30:16 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:30:20 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:30:27 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:30:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:30:28 - INFO - __main__ - Starting training!
03/01/2022 17:33:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.5_8_predictions.txt
03/01/2022 17:33:17 - INFO - __main__ - EM on test data: 0.0053
03/01/2022 17:33:19 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.005257886830245368
03/01/2022 17:33:19 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.4, bsz=8 ...
03/01/2022 17:33:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:33:20 - INFO - __main__ - Printing 3 examples
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:33:20 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:33:20 - INFO - __main__ - ['madness']
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:33:20 - INFO - __main__ - ['genevieve']
03/01/2022 17:33:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:33:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:33:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:33:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:33:20 - INFO - __main__ - Printing 3 examples
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:33:20 - INFO - __main__ - ['will hay']
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:33:20 - INFO - __main__ - ['alan sugar']
03/01/2022 17:33:20 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:33:20 - INFO - __main__ - ['cleopatra']
03/01/2022 17:33:20 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:33:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:33:20 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:33:34 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:33:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:33:34 - INFO - __main__ - Starting training!
03/01/2022 17:33:37 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=4
03/01/2022 17:33:40 - INFO - __main__ - Step 20 Global step 20 Train loss 3.42 on epoch=9
03/01/2022 17:33:42 - INFO - __main__ - Step 30 Global step 30 Train loss 3.15 on epoch=14
03/01/2022 17:33:44 - INFO - __main__ - Step 40 Global step 40 Train loss 3.03 on epoch=19
03/01/2022 17:33:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.90 on epoch=24
03/01/2022 17:33:48 - INFO - __main__ - Global step 50 Train loss 3.33 EM 0.0 on epoch=24
03/01/2022 17:33:48 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:33:51 - INFO - __main__ - Step 60 Global step 60 Train loss 2.78 on epoch=29
03/01/2022 17:33:53 - INFO - __main__ - Step 70 Global step 70 Train loss 2.67 on epoch=34
03/01/2022 17:33:55 - INFO - __main__ - Step 80 Global step 80 Train loss 2.51 on epoch=39
03/01/2022 17:33:58 - INFO - __main__ - Step 90 Global step 90 Train loss 2.52 on epoch=44
03/01/2022 17:34:00 - INFO - __main__ - Step 100 Global step 100 Train loss 2.37 on epoch=49
03/01/2022 17:34:01 - INFO - __main__ - Global step 100 Train loss 2.57 EM 0.0 on epoch=49
03/01/2022 17:34:03 - INFO - __main__ - Step 110 Global step 110 Train loss 2.28 on epoch=54
03/01/2022 17:34:06 - INFO - __main__ - Step 120 Global step 120 Train loss 2.29 on epoch=59
03/01/2022 17:34:08 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/01/2022 17:34:10 - INFO - __main__ - Step 140 Global step 140 Train loss 2.11 on epoch=69
03/01/2022 17:34:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.99 on epoch=74
03/01/2022 17:34:14 - INFO - __main__ - Global step 150 Train loss 2.16 EM 0.0 on epoch=74
03/01/2022 17:34:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.88 on epoch=79
03/01/2022 17:34:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.91 on epoch=84
03/01/2022 17:34:21 - INFO - __main__ - Step 180 Global step 180 Train loss 1.91 on epoch=89
03/01/2022 17:34:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.81 on epoch=94
03/01/2022 17:34:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.72 on epoch=99
03/01/2022 17:34:27 - INFO - __main__ - Global step 200 Train loss 1.85 EM 0.0 on epoch=99
03/01/2022 17:34:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.70 on epoch=104
03/01/2022 17:34:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.67 on epoch=109
03/01/2022 17:34:34 - INFO - __main__ - Step 230 Global step 230 Train loss 1.55 on epoch=114
03/01/2022 17:34:36 - INFO - __main__ - Step 240 Global step 240 Train loss 1.52 on epoch=119
03/01/2022 17:34:38 - INFO - __main__ - Step 250 Global step 250 Train loss 1.38 on epoch=124
03/01/2022 17:34:40 - INFO - __main__ - Global step 250 Train loss 1.56 EM 0.0 on epoch=124
03/01/2022 17:34:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.37 on epoch=129
03/01/2022 17:34:45 - INFO - __main__ - Step 270 Global step 270 Train loss 1.42 on epoch=134
03/01/2022 17:34:47 - INFO - __main__ - Step 280 Global step 280 Train loss 1.41 on epoch=139
03/01/2022 17:34:49 - INFO - __main__ - Step 290 Global step 290 Train loss 1.22 on epoch=144
03/01/2022 17:34:51 - INFO - __main__ - Step 300 Global step 300 Train loss 1.26 on epoch=149
03/01/2022 17:34:53 - INFO - __main__ - Global step 300 Train loss 1.34 EM 0.0 on epoch=149
03/01/2022 17:34:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.13 on epoch=154
03/01/2022 17:34:57 - INFO - __main__ - Step 320 Global step 320 Train loss 1.21 on epoch=159
03/01/2022 17:35:00 - INFO - __main__ - Step 330 Global step 330 Train loss 1.27 on epoch=164
03/01/2022 17:35:02 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 17:35:04 - INFO - __main__ - Step 350 Global step 350 Train loss 1.08 on epoch=174
03/01/2022 17:35:06 - INFO - __main__ - Global step 350 Train loss 1.18 EM 0.0 on epoch=174
03/01/2022 17:35:08 - INFO - __main__ - Step 360 Global step 360 Train loss 1.03 on epoch=179
03/01/2022 17:35:10 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=184
03/01/2022 17:35:13 - INFO - __main__ - Step 380 Global step 380 Train loss 1.09 on epoch=189
03/01/2022 17:35:15 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=194
03/01/2022 17:35:17 - INFO - __main__ - Step 400 Global step 400 Train loss 1.02 on epoch=199
03/01/2022 17:35:18 - INFO - __main__ - Global step 400 Train loss 1.04 EM 0.0 on epoch=199
03/01/2022 17:35:21 - INFO - __main__ - Step 410 Global step 410 Train loss 1.08 on epoch=204
03/01/2022 17:35:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.97 on epoch=209
03/01/2022 17:35:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=214
03/01/2022 17:35:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=219
03/01/2022 17:35:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.93 on epoch=224
03/01/2022 17:35:31 - INFO - __main__ - Global step 450 Train loss 0.95 EM 0.0 on epoch=224
03/01/2022 17:35:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=229
03/01/2022 17:35:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=234
03/01/2022 17:35:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=239
03/01/2022 17:35:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=244
03/01/2022 17:35:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=249
03/01/2022 17:35:44 - INFO - __main__ - Global step 500 Train loss 0.84 EM 0.0 on epoch=249
03/01/2022 17:35:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=254
03/01/2022 17:35:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
03/01/2022 17:35:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=264
03/01/2022 17:35:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=269
03/01/2022 17:35:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=274
03/01/2022 17:35:57 - INFO - __main__ - Global step 550 Train loss 0.78 EM 0.03125 on epoch=274
03/01/2022 17:35:57 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=274, global_step=550
03/01/2022 17:35:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
03/01/2022 17:36:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/01/2022 17:36:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=289
03/01/2022 17:36:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=294
03/01/2022 17:36:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=299
03/01/2022 17:36:10 - INFO - __main__ - Global step 600 Train loss 0.68 EM 0.0 on epoch=299
03/01/2022 17:36:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
03/01/2022 17:36:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=309
03/01/2022 17:36:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=314
03/01/2022 17:36:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
03/01/2022 17:36:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=324
03/01/2022 17:36:23 - INFO - __main__ - Global step 650 Train loss 0.62 EM 0.0 on epoch=324
03/01/2022 17:36:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=329
03/01/2022 17:36:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=334
03/01/2022 17:36:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.72 on epoch=339
03/01/2022 17:36:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=344
03/01/2022 17:36:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=349
03/01/2022 17:36:35 - INFO - __main__ - Global step 700 Train loss 0.61 EM 0.0 on epoch=349
03/01/2022 17:36:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/01/2022 17:36:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
03/01/2022 17:36:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
03/01/2022 17:36:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=369
03/01/2022 17:36:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 17:36:48 - INFO - __main__ - Global step 750 Train loss 0.54 EM 0.03125 on epoch=374
03/01/2022 17:36:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=379
03/01/2022 17:36:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=384
03/01/2022 17:36:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=389
03/01/2022 17:36:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=394
03/01/2022 17:37:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=399
03/01/2022 17:37:01 - INFO - __main__ - Global step 800 Train loss 0.49 EM 0.0 on epoch=399
03/01/2022 17:37:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=404
03/01/2022 17:37:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
03/01/2022 17:37:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/01/2022 17:37:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/01/2022 17:37:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
03/01/2022 17:37:14 - INFO - __main__ - Global step 850 Train loss 0.45 EM 0.0 on epoch=424
03/01/2022 17:37:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/01/2022 17:37:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
03/01/2022 17:37:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=439
03/01/2022 17:37:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=444
03/01/2022 17:37:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
03/01/2022 17:37:28 - INFO - __main__ - Global step 900 Train loss 0.44 EM 0.0 on epoch=449
03/01/2022 17:37:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=454
03/01/2022 17:37:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=459
03/01/2022 17:37:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=464
03/01/2022 17:37:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
03/01/2022 17:37:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
03/01/2022 17:37:40 - INFO - __main__ - Global step 950 Train loss 0.41 EM 0.03125 on epoch=474
03/01/2022 17:37:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/01/2022 17:37:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 17:37:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=489
03/01/2022 17:37:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/01/2022 17:37:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=499
03/01/2022 17:37:53 - INFO - __main__ - Global step 1000 Train loss 0.33 EM 0.0 on epoch=499
03/01/2022 17:37:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=504
03/01/2022 17:37:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
03/01/2022 17:38:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/01/2022 17:38:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
03/01/2022 17:38:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=524
03/01/2022 17:38:06 - INFO - __main__ - Global step 1050 Train loss 0.31 EM 0.03125 on epoch=524
03/01/2022 17:38:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=529
03/01/2022 17:38:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/01/2022 17:38:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=539
03/01/2022 17:38:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=544
03/01/2022 17:38:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/01/2022 17:38:19 - INFO - __main__ - Global step 1100 Train loss 0.30 EM 0.0 on epoch=549
03/01/2022 17:38:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
03/01/2022 17:38:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=559
03/01/2022 17:38:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
03/01/2022 17:38:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=569
03/01/2022 17:38:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/01/2022 17:38:31 - INFO - __main__ - Global step 1150 Train loss 0.26 EM 0.0 on epoch=574
03/01/2022 17:38:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/01/2022 17:38:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
03/01/2022 17:38:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/01/2022 17:38:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
03/01/2022 17:38:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/01/2022 17:38:43 - INFO - __main__ - Global step 1200 Train loss 0.24 EM 0.03125 on epoch=599
03/01/2022 17:38:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=604
03/01/2022 17:38:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/01/2022 17:38:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
03/01/2022 17:38:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
03/01/2022 17:38:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 17:38:56 - INFO - __main__ - Global step 1250 Train loss 0.21 EM 0.0 on epoch=624
03/01/2022 17:38:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=629
03/01/2022 17:39:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=634
03/01/2022 17:39:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=639
03/01/2022 17:39:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/01/2022 17:39:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/01/2022 17:39:08 - INFO - __main__ - Global step 1300 Train loss 0.23 EM 0.03125 on epoch=649
03/01/2022 17:39:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
03/01/2022 17:39:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/01/2022 17:39:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/01/2022 17:39:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=669
03/01/2022 17:39:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/01/2022 17:39:21 - INFO - __main__ - Global step 1350 Train loss 0.23 EM 0.03125 on epoch=674
03/01/2022 17:39:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
03/01/2022 17:39:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/01/2022 17:39:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/01/2022 17:39:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=694
03/01/2022 17:39:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=699
03/01/2022 17:39:34 - INFO - __main__ - Global step 1400 Train loss 0.18 EM 0.03125 on epoch=699
03/01/2022 17:39:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/01/2022 17:39:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/01/2022 17:39:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/01/2022 17:39:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 17:39:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=724
03/01/2022 17:39:46 - INFO - __main__ - Global step 1450 Train loss 0.18 EM 0.0 on epoch=724
03/01/2022 17:39:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=729
03/01/2022 17:39:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/01/2022 17:39:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 17:39:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/01/2022 17:39:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/01/2022 17:39:59 - INFO - __main__ - Global step 1500 Train loss 0.18 EM 0.03125 on epoch=749
03/01/2022 17:40:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/01/2022 17:40:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=759
03/01/2022 17:40:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
03/01/2022 17:40:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 17:40:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=774
03/01/2022 17:40:12 - INFO - __main__ - Global step 1550 Train loss 0.16 EM 0.03125 on epoch=774
03/01/2022 17:40:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=779
03/01/2022 17:40:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=784
03/01/2022 17:40:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
03/01/2022 17:40:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=794
03/01/2022 17:40:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=799
03/01/2022 17:40:24 - INFO - __main__ - Global step 1600 Train loss 0.16 EM 0.0 on epoch=799
03/01/2022 17:40:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=804
03/01/2022 17:40:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/01/2022 17:40:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/01/2022 17:40:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 17:40:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
03/01/2022 17:40:37 - INFO - __main__ - Global step 1650 Train loss 0.15 EM 0.03125 on epoch=824
03/01/2022 17:40:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
03/01/2022 17:40:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 17:40:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
03/01/2022 17:40:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 17:40:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=849
03/01/2022 17:40:49 - INFO - __main__ - Global step 1700 Train loss 0.13 EM 0.03125 on epoch=849
03/01/2022 17:40:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/01/2022 17:40:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
03/01/2022 17:40:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 17:40:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 17:41:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=874
03/01/2022 17:41:02 - INFO - __main__ - Global step 1750 Train loss 0.13 EM 0.03125 on epoch=874
03/01/2022 17:41:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 17:41:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/01/2022 17:41:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 17:41:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/01/2022 17:41:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=899
03/01/2022 17:41:14 - INFO - __main__ - Global step 1800 Train loss 0.13 EM 0.03125 on epoch=899
03/01/2022 17:41:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 17:41:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/01/2022 17:41:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/01/2022 17:41:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=919
03/01/2022 17:41:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 17:41:27 - INFO - __main__ - Global step 1850 Train loss 0.13 EM 0.03125 on epoch=924
03/01/2022 17:41:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=929
03/01/2022 17:41:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 17:41:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 17:41:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/01/2022 17:41:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
03/01/2022 17:41:40 - INFO - __main__ - Global step 1900 Train loss 0.10 EM 0.0 on epoch=949
03/01/2022 17:41:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/01/2022 17:41:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=959
03/01/2022 17:41:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/01/2022 17:41:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
03/01/2022 17:41:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=974
03/01/2022 17:41:52 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 17:41:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/01/2022 17:41:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 17:41:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 17:42:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
03/01/2022 17:42:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=999
03/01/2022 17:42:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:42:05 - INFO - __main__ - Printing 3 examples
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:42:05 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:42:05 - INFO - __main__ - ['madness']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:42:05 - INFO - __main__ - ['genevieve']
03/01/2022 17:42:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:42:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:42:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:42:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:42:05 - INFO - __main__ - Printing 3 examples
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:42:05 - INFO - __main__ - ['will hay']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:42:05 - INFO - __main__ - ['alan sugar']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:42:05 - INFO - __main__ - ['cleopatra']
03/01/2022 17:42:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:42:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:42:05 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:42:05 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.03125 on epoch=999
03/01/2022 17:42:05 - INFO - __main__ - save last model!
03/01/2022 17:42:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:42:05 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:42:05 - INFO - __main__ - Printing 3 examples
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:42:05 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:42:05 - INFO - __main__ - ['henry fonda']
03/01/2022 17:42:05 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:42:05 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:42:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:42:06 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:42:10 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:42:17 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:42:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:42:18 - INFO - __main__ - Starting training!
03/01/2022 17:45:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.4_8_predictions.txt
03/01/2022 17:45:03 - INFO - __main__ - EM on test data: 0.0060
03/01/2022 17:45:04 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.4, bsz=8, dev_performance=0.03125, test_performance=0.006009013520280421
03/01/2022 17:45:04 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.3, bsz=8 ...
03/01/2022 17:45:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:45:05 - INFO - __main__ - Printing 3 examples
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:45:05 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:45:05 - INFO - __main__ - ['madness']
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:45:05 - INFO - __main__ - ['genevieve']
03/01/2022 17:45:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:45:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:45:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:45:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:45:05 - INFO - __main__ - Printing 3 examples
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:45:05 - INFO - __main__ - ['will hay']
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:45:05 - INFO - __main__ - ['alan sugar']
03/01/2022 17:45:05 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:45:05 - INFO - __main__ - ['cleopatra']
03/01/2022 17:45:05 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:45:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:45:05 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:45:18 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:45:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:45:19 - INFO - __main__ - Starting training!
03/01/2022 17:45:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.25 on epoch=4
03/01/2022 17:45:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.64 on epoch=9
03/01/2022 17:45:29 - INFO - __main__ - Step 30 Global step 30 Train loss 3.25 on epoch=14
03/01/2022 17:45:31 - INFO - __main__ - Step 40 Global step 40 Train loss 3.15 on epoch=19
03/01/2022 17:45:33 - INFO - __main__ - Step 50 Global step 50 Train loss 2.99 on epoch=24
03/01/2022 17:45:35 - INFO - __main__ - Global step 50 Train loss 3.46 EM 0.0 on epoch=24
03/01/2022 17:45:35 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:45:37 - INFO - __main__ - Step 60 Global step 60 Train loss 2.92 on epoch=29
03/01/2022 17:45:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.78 on epoch=34
03/01/2022 17:45:42 - INFO - __main__ - Step 80 Global step 80 Train loss 2.65 on epoch=39
03/01/2022 17:45:44 - INFO - __main__ - Step 90 Global step 90 Train loss 2.57 on epoch=44
03/01/2022 17:45:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.56 on epoch=49
03/01/2022 17:45:48 - INFO - __main__ - Global step 100 Train loss 2.70 EM 0.0 on epoch=49
03/01/2022 17:45:50 - INFO - __main__ - Step 110 Global step 110 Train loss 2.45 on epoch=54
03/01/2022 17:45:52 - INFO - __main__ - Step 120 Global step 120 Train loss 2.41 on epoch=59
03/01/2022 17:45:55 - INFO - __main__ - Step 130 Global step 130 Train loss 2.37 on epoch=64
03/01/2022 17:45:57 - INFO - __main__ - Step 140 Global step 140 Train loss 2.29 on epoch=69
03/01/2022 17:45:59 - INFO - __main__ - Step 150 Global step 150 Train loss 2.28 on epoch=74
03/01/2022 17:46:01 - INFO - __main__ - Global step 150 Train loss 2.36 EM 0.0 on epoch=74
03/01/2022 17:46:03 - INFO - __main__ - Step 160 Global step 160 Train loss 2.11 on epoch=79
03/01/2022 17:46:05 - INFO - __main__ - Step 170 Global step 170 Train loss 2.00 on epoch=84
03/01/2022 17:46:07 - INFO - __main__ - Step 180 Global step 180 Train loss 2.13 on epoch=89
03/01/2022 17:46:10 - INFO - __main__ - Step 190 Global step 190 Train loss 2.09 on epoch=94
03/01/2022 17:46:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.97 on epoch=99
03/01/2022 17:46:14 - INFO - __main__ - Global step 200 Train loss 2.06 EM 0.0 on epoch=99
03/01/2022 17:46:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.94 on epoch=104
03/01/2022 17:46:19 - INFO - __main__ - Step 220 Global step 220 Train loss 1.89 on epoch=109
03/01/2022 17:46:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.76 on epoch=114
03/01/2022 17:46:23 - INFO - __main__ - Step 240 Global step 240 Train loss 1.83 on epoch=119
03/01/2022 17:46:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.73 on epoch=124
03/01/2022 17:46:27 - INFO - __main__ - Global step 250 Train loss 1.83 EM 0.0 on epoch=124
03/01/2022 17:46:29 - INFO - __main__ - Step 260 Global step 260 Train loss 1.74 on epoch=129
03/01/2022 17:46:32 - INFO - __main__ - Step 270 Global step 270 Train loss 1.65 on epoch=134
03/01/2022 17:46:34 - INFO - __main__ - Step 280 Global step 280 Train loss 1.57 on epoch=139
03/01/2022 17:46:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.64 on epoch=144
03/01/2022 17:46:39 - INFO - __main__ - Step 300 Global step 300 Train loss 1.55 on epoch=149
03/01/2022 17:46:40 - INFO - __main__ - Global step 300 Train loss 1.63 EM 0.0 on epoch=149
03/01/2022 17:46:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.58 on epoch=154
03/01/2022 17:46:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.45 on epoch=159
03/01/2022 17:46:47 - INFO - __main__ - Step 330 Global step 330 Train loss 1.48 on epoch=164
03/01/2022 17:46:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.48 on epoch=169
03/01/2022 17:46:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.37 on epoch=174
03/01/2022 17:46:53 - INFO - __main__ - Global step 350 Train loss 1.47 EM 0.0 on epoch=174
03/01/2022 17:46:56 - INFO - __main__ - Step 360 Global step 360 Train loss 1.34 on epoch=179
03/01/2022 17:46:58 - INFO - __main__ - Step 370 Global step 370 Train loss 1.45 on epoch=184
03/01/2022 17:47:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.31 on epoch=189
03/01/2022 17:47:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.25 on epoch=194
03/01/2022 17:47:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.28 on epoch=199
03/01/2022 17:47:06 - INFO - __main__ - Global step 400 Train loss 1.32 EM 0.0 on epoch=199
03/01/2022 17:47:09 - INFO - __main__ - Step 410 Global step 410 Train loss 1.19 on epoch=204
03/01/2022 17:47:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.10 on epoch=209
03/01/2022 17:47:13 - INFO - __main__ - Step 430 Global step 430 Train loss 1.09 on epoch=214
03/01/2022 17:47:16 - INFO - __main__ - Step 440 Global step 440 Train loss 1.09 on epoch=219
03/01/2022 17:47:18 - INFO - __main__ - Step 450 Global step 450 Train loss 1.08 on epoch=224
03/01/2022 17:47:19 - INFO - __main__ - Global step 450 Train loss 1.11 EM 0.0 on epoch=224
03/01/2022 17:47:22 - INFO - __main__ - Step 460 Global step 460 Train loss 1.09 on epoch=229
03/01/2022 17:47:24 - INFO - __main__ - Step 470 Global step 470 Train loss 1.11 on epoch=234
03/01/2022 17:47:26 - INFO - __main__ - Step 480 Global step 480 Train loss 1.02 on epoch=239
03/01/2022 17:47:29 - INFO - __main__ - Step 490 Global step 490 Train loss 1.03 on epoch=244
03/01/2022 17:47:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=249
03/01/2022 17:47:32 - INFO - __main__ - Global step 500 Train loss 1.04 EM 0.0 on epoch=249
03/01/2022 17:47:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.93 on epoch=254
03/01/2022 17:47:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.90 on epoch=259
03/01/2022 17:47:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.99 on epoch=264
03/01/2022 17:47:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.92 on epoch=269
03/01/2022 17:47:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=274
03/01/2022 17:47:45 - INFO - __main__ - Global step 550 Train loss 0.91 EM 0.03125 on epoch=274
03/01/2022 17:47:45 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=274, global_step=550
03/01/2022 17:47:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=279
03/01/2022 17:47:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=284
03/01/2022 17:47:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.85 on epoch=289
03/01/2022 17:47:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=294
03/01/2022 17:47:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=299
03/01/2022 17:47:58 - INFO - __main__ - Global step 600 Train loss 0.84 EM 0.03125 on epoch=299
03/01/2022 17:48:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=304
03/01/2022 17:48:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
03/01/2022 17:48:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=314
03/01/2022 17:48:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.75 on epoch=319
03/01/2022 17:48:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=324
03/01/2022 17:48:11 - INFO - __main__ - Global step 650 Train loss 0.77 EM 0.03125 on epoch=324
03/01/2022 17:48:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.76 on epoch=329
03/01/2022 17:48:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=334
03/01/2022 17:48:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=339
03/01/2022 17:48:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
03/01/2022 17:48:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=349
03/01/2022 17:48:24 - INFO - __main__ - Global step 700 Train loss 0.65 EM 0.03125 on epoch=349
03/01/2022 17:48:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.59 on epoch=354
03/01/2022 17:48:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=359
03/01/2022 17:48:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=364
03/01/2022 17:48:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.63 on epoch=369
03/01/2022 17:48:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=374
03/01/2022 17:48:37 - INFO - __main__ - Global step 750 Train loss 0.61 EM 0.03125 on epoch=374
03/01/2022 17:48:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=379
03/01/2022 17:48:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=384
03/01/2022 17:48:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
03/01/2022 17:48:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.57 on epoch=394
03/01/2022 17:48:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
03/01/2022 17:48:50 - INFO - __main__ - Global step 800 Train loss 0.58 EM 0.0 on epoch=399
03/01/2022 17:48:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.55 on epoch=404
03/01/2022 17:48:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=409
03/01/2022 17:48:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=414
03/01/2022 17:48:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
03/01/2022 17:49:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
03/01/2022 17:49:03 - INFO - __main__ - Global step 850 Train loss 0.52 EM 0.0 on epoch=424
03/01/2022 17:49:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
03/01/2022 17:49:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
03/01/2022 17:49:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=439
03/01/2022 17:49:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.55 on epoch=444
03/01/2022 17:49:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=449
03/01/2022 17:49:15 - INFO - __main__ - Global step 900 Train loss 0.49 EM 0.03125 on epoch=449
03/01/2022 17:49:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=454
03/01/2022 17:49:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=459
03/01/2022 17:49:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=464
03/01/2022 17:49:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=469
03/01/2022 17:49:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 17:49:28 - INFO - __main__ - Global step 950 Train loss 0.48 EM 0.03125 on epoch=474
03/01/2022 17:49:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/01/2022 17:49:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/01/2022 17:49:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
03/01/2022 17:49:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/01/2022 17:49:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=499
03/01/2022 17:49:41 - INFO - __main__ - Global step 1000 Train loss 0.41 EM 0.03125 on epoch=499
03/01/2022 17:49:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
03/01/2022 17:49:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=509
03/01/2022 17:49:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
03/01/2022 17:49:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=519
03/01/2022 17:49:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
03/01/2022 17:49:54 - INFO - __main__ - Global step 1050 Train loss 0.44 EM 0.0 on epoch=524
03/01/2022 17:49:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
03/01/2022 17:49:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=534
03/01/2022 17:50:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=539
03/01/2022 17:50:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/01/2022 17:50:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
03/01/2022 17:50:07 - INFO - __main__ - Global step 1100 Train loss 0.37 EM 0.03125 on epoch=549
03/01/2022 17:50:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
03/01/2022 17:50:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=559
03/01/2022 17:50:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=564
03/01/2022 17:50:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 17:50:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
03/01/2022 17:50:20 - INFO - __main__ - Global step 1150 Train loss 0.34 EM 0.03125 on epoch=574
03/01/2022 17:50:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=579
03/01/2022 17:50:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/01/2022 17:50:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=589
03/01/2022 17:50:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=594
03/01/2022 17:50:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
03/01/2022 17:50:33 - INFO - __main__ - Global step 1200 Train loss 0.34 EM 0.03125 on epoch=599
03/01/2022 17:50:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/01/2022 17:50:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=609
03/01/2022 17:50:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=614
03/01/2022 17:50:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
03/01/2022 17:50:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=624
03/01/2022 17:50:46 - INFO - __main__ - Global step 1250 Train loss 0.31 EM 0.0 on epoch=624
03/01/2022 17:50:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/01/2022 17:50:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=634
03/01/2022 17:50:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 17:50:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=644
03/01/2022 17:50:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=649
03/01/2022 17:50:59 - INFO - __main__ - Global step 1300 Train loss 0.27 EM 0.03125 on epoch=649
03/01/2022 17:51:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
03/01/2022 17:51:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=659
03/01/2022 17:51:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=664
03/01/2022 17:51:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/01/2022 17:51:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
03/01/2022 17:51:12 - INFO - __main__ - Global step 1350 Train loss 0.28 EM 0.03125 on epoch=674
03/01/2022 17:51:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=679
03/01/2022 17:51:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=684
03/01/2022 17:51:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=689
03/01/2022 17:51:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=694
03/01/2022 17:51:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
03/01/2022 17:51:25 - INFO - __main__ - Global step 1400 Train loss 0.27 EM 0.03125 on epoch=699
03/01/2022 17:51:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=704
03/01/2022 17:51:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/01/2022 17:51:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
03/01/2022 17:51:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/01/2022 17:51:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/01/2022 17:51:38 - INFO - __main__ - Global step 1450 Train loss 0.26 EM 0.03125 on epoch=724
03/01/2022 17:51:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 17:51:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=734
03/01/2022 17:51:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=739
03/01/2022 17:51:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=744
03/01/2022 17:51:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=749
03/01/2022 17:51:51 - INFO - __main__ - Global step 1500 Train loss 0.23 EM 0.03125 on epoch=749
03/01/2022 17:51:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=754
03/01/2022 17:51:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
03/01/2022 17:51:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
03/01/2022 17:52:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=769
03/01/2022 17:52:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=774
03/01/2022 17:52:04 - INFO - __main__ - Global step 1550 Train loss 0.22 EM 0.03125 on epoch=774
03/01/2022 17:52:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
03/01/2022 17:52:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=784
03/01/2022 17:52:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=789
03/01/2022 17:52:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=794
03/01/2022 17:52:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=799
03/01/2022 17:52:17 - INFO - __main__ - Global step 1600 Train loss 0.20 EM 0.0 on epoch=799
03/01/2022 17:52:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=804
03/01/2022 17:52:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/01/2022 17:52:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=814
03/01/2022 17:52:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/01/2022 17:52:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/01/2022 17:52:30 - INFO - __main__ - Global step 1650 Train loss 0.20 EM 0.03125 on epoch=824
03/01/2022 17:52:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 17:52:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/01/2022 17:52:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 17:52:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=844
03/01/2022 17:52:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 17:52:43 - INFO - __main__ - Global step 1700 Train loss 0.20 EM 0.0 on epoch=849
03/01/2022 17:52:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=854
03/01/2022 17:52:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 17:52:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=864
03/01/2022 17:52:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=869
03/01/2022 17:52:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/01/2022 17:52:56 - INFO - __main__ - Global step 1750 Train loss 0.14 EM 0.03125 on epoch=874
03/01/2022 17:52:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=879
03/01/2022 17:53:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/01/2022 17:53:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 17:53:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/01/2022 17:53:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=899
03/01/2022 17:53:09 - INFO - __main__ - Global step 1800 Train loss 0.18 EM 0.03125 on epoch=899
03/01/2022 17:53:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
03/01/2022 17:53:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/01/2022 17:53:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=914
03/01/2022 17:53:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/01/2022 17:53:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 17:53:22 - INFO - __main__ - Global step 1850 Train loss 0.15 EM 0.0 on epoch=924
03/01/2022 17:53:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=929
03/01/2022 17:53:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=934
03/01/2022 17:53:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
03/01/2022 17:53:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 17:53:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/01/2022 17:53:35 - INFO - __main__ - Global step 1900 Train loss 0.15 EM 0.0 on epoch=949
03/01/2022 17:53:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=954
03/01/2022 17:53:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=959
03/01/2022 17:53:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 17:53:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=969
03/01/2022 17:53:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=974
03/01/2022 17:53:48 - INFO - __main__ - Global step 1950 Train loss 0.15 EM 0.03125 on epoch=974
03/01/2022 17:53:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/01/2022 17:53:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/01/2022 17:53:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=989
03/01/2022 17:53:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=994
03/01/2022 17:54:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=999
03/01/2022 17:54:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:54:01 - INFO - __main__ - Printing 3 examples
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:54:01 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:54:01 - INFO - __main__ - ['madness']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:54:01 - INFO - __main__ - ['genevieve']
03/01/2022 17:54:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 17:54:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:54:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:54:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:54:01 - INFO - __main__ - Printing 3 examples
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:54:01 - INFO - __main__ - ['will hay']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:54:01 - INFO - __main__ - ['alan sugar']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:54:01 - INFO - __main__ - ['cleopatra']
03/01/2022 17:54:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:54:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:54:01 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:54:01 - INFO - __main__ - Global step 2000 Train loss 0.15 EM 0.03125 on epoch=999
03/01/2022 17:54:01 - INFO - __main__ - save last model!
03/01/2022 17:54:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 17:54:01 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 17:54:01 - INFO - __main__ - Printing 3 examples
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 17:54:01 - INFO - __main__ - ['taming of the shrew']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 17:54:01 - INFO - __main__ - ['henry fonda']
03/01/2022 17:54:01 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 17:54:01 - INFO - __main__ - ['tchaikovsky']
03/01/2022 17:54:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:54:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:54:07 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 17:54:13 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:54:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:54:14 - INFO - __main__ - Starting training!
03/01/2022 17:57:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.3_8_predictions.txt
03/01/2022 17:57:01 - INFO - __main__ - EM on test data: 0.0045
03/01/2022 17:57:02 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.004506760140210316
03/01/2022 17:57:02 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.2, bsz=8 ...
03/01/2022 17:57:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:57:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 17:57:03 - INFO - __main__ - ['camille saint-saens']
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 17:57:03 - INFO - __main__ - ['madness']
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 17:57:03 - INFO - __main__ - ['genevieve']
03/01/2022 17:57:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 17:57:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:57:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 17:57:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 17:57:03 - INFO - __main__ - Printing 3 examples
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 17:57:03 - INFO - __main__ - ['will hay']
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 17:57:03 - INFO - __main__ - ['alan sugar']
03/01/2022 17:57:03 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 17:57:03 - INFO - __main__ - ['cleopatra']
03/01/2022 17:57:03 - INFO - __main__ - Tokenizing Input ...
03/01/2022 17:57:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 17:57:03 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 17:57:16 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 17:57:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 17:57:17 - INFO - __main__ - Starting training!
03/01/2022 17:57:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.38 on epoch=4
03/01/2022 17:57:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.96 on epoch=9
03/01/2022 17:57:28 - INFO - __main__ - Step 30 Global step 30 Train loss 3.51 on epoch=14
03/01/2022 17:57:30 - INFO - __main__ - Step 40 Global step 40 Train loss 3.32 on epoch=19
03/01/2022 17:57:33 - INFO - __main__ - Step 50 Global step 50 Train loss 3.19 on epoch=24
03/01/2022 17:57:34 - INFO - __main__ - Global step 50 Train loss 3.67 EM 0.0 on epoch=24
03/01/2022 17:57:34 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 17:57:36 - INFO - __main__ - Step 60 Global step 60 Train loss 3.02 on epoch=29
03/01/2022 17:57:39 - INFO - __main__ - Step 70 Global step 70 Train loss 3.00 on epoch=34
03/01/2022 17:57:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.90 on epoch=39
03/01/2022 17:57:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.90 on epoch=44
03/01/2022 17:57:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.79 on epoch=49
03/01/2022 17:57:47 - INFO - __main__ - Global step 100 Train loss 2.92 EM 0.0 on epoch=49
03/01/2022 17:57:49 - INFO - __main__ - Step 110 Global step 110 Train loss 2.74 on epoch=54
03/01/2022 17:57:52 - INFO - __main__ - Step 120 Global step 120 Train loss 2.62 on epoch=59
03/01/2022 17:57:54 - INFO - __main__ - Step 130 Global step 130 Train loss 2.62 on epoch=64
03/01/2022 17:57:56 - INFO - __main__ - Step 140 Global step 140 Train loss 2.62 on epoch=69
03/01/2022 17:57:58 - INFO - __main__ - Step 150 Global step 150 Train loss 2.56 on epoch=74
03/01/2022 17:58:00 - INFO - __main__ - Global step 150 Train loss 2.63 EM 0.0 on epoch=74
03/01/2022 17:58:02 - INFO - __main__ - Step 160 Global step 160 Train loss 2.44 on epoch=79
03/01/2022 17:58:04 - INFO - __main__ - Step 170 Global step 170 Train loss 2.38 on epoch=84
03/01/2022 17:58:07 - INFO - __main__ - Step 180 Global step 180 Train loss 2.37 on epoch=89
03/01/2022 17:58:09 - INFO - __main__ - Step 190 Global step 190 Train loss 2.41 on epoch=94
03/01/2022 17:58:11 - INFO - __main__ - Step 200 Global step 200 Train loss 2.23 on epoch=99
03/01/2022 17:58:13 - INFO - __main__ - Global step 200 Train loss 2.36 EM 0.0 on epoch=99
03/01/2022 17:58:15 - INFO - __main__ - Step 210 Global step 210 Train loss 2.27 on epoch=104
03/01/2022 17:58:17 - INFO - __main__ - Step 220 Global step 220 Train loss 2.27 on epoch=109
03/01/2022 17:58:19 - INFO - __main__ - Step 230 Global step 230 Train loss 2.25 on epoch=114
03/01/2022 17:58:22 - INFO - __main__ - Step 240 Global step 240 Train loss 2.17 on epoch=119
03/01/2022 17:58:24 - INFO - __main__ - Step 250 Global step 250 Train loss 2.16 on epoch=124
03/01/2022 17:58:25 - INFO - __main__ - Global step 250 Train loss 2.22 EM 0.0 on epoch=124
03/01/2022 17:58:28 - INFO - __main__ - Step 260 Global step 260 Train loss 2.15 on epoch=129
03/01/2022 17:58:30 - INFO - __main__ - Step 270 Global step 270 Train loss 2.08 on epoch=134
03/01/2022 17:58:32 - INFO - __main__ - Step 280 Global step 280 Train loss 2.07 on epoch=139
03/01/2022 17:58:35 - INFO - __main__ - Step 290 Global step 290 Train loss 1.98 on epoch=144
03/01/2022 17:58:37 - INFO - __main__ - Step 300 Global step 300 Train loss 2.03 on epoch=149
03/01/2022 17:58:38 - INFO - __main__ - Global step 300 Train loss 2.06 EM 0.0 on epoch=149
03/01/2022 17:58:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.92 on epoch=154
03/01/2022 17:58:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.94 on epoch=159
03/01/2022 17:58:45 - INFO - __main__ - Step 330 Global step 330 Train loss 1.87 on epoch=164
03/01/2022 17:58:47 - INFO - __main__ - Step 340 Global step 340 Train loss 1.90 on epoch=169
03/01/2022 17:58:50 - INFO - __main__ - Step 350 Global step 350 Train loss 1.88 on epoch=174
03/01/2022 17:58:51 - INFO - __main__ - Global step 350 Train loss 1.90 EM 0.0 on epoch=174
03/01/2022 17:58:53 - INFO - __main__ - Step 360 Global step 360 Train loss 1.84 on epoch=179
03/01/2022 17:58:56 - INFO - __main__ - Step 370 Global step 370 Train loss 1.78 on epoch=184
03/01/2022 17:58:58 - INFO - __main__ - Step 380 Global step 380 Train loss 1.69 on epoch=189
03/01/2022 17:59:00 - INFO - __main__ - Step 390 Global step 390 Train loss 1.68 on epoch=194
03/01/2022 17:59:02 - INFO - __main__ - Step 400 Global step 400 Train loss 1.74 on epoch=199
03/01/2022 17:59:04 - INFO - __main__ - Global step 400 Train loss 1.75 EM 0.0 on epoch=199
03/01/2022 17:59:06 - INFO - __main__ - Step 410 Global step 410 Train loss 1.66 on epoch=204
03/01/2022 17:59:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.61 on epoch=209
03/01/2022 17:59:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.65 on epoch=214
03/01/2022 17:59:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.59 on epoch=219
03/01/2022 17:59:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
03/01/2022 17:59:17 - INFO - __main__ - Global step 450 Train loss 1.60 EM 0.0 on epoch=224
03/01/2022 17:59:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.52 on epoch=229
03/01/2022 17:59:22 - INFO - __main__ - Step 470 Global step 470 Train loss 1.51 on epoch=234
03/01/2022 17:59:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.56 on epoch=239
03/01/2022 17:59:26 - INFO - __main__ - Step 490 Global step 490 Train loss 1.53 on epoch=244
03/01/2022 17:59:28 - INFO - __main__ - Step 500 Global step 500 Train loss 1.41 on epoch=249
03/01/2022 17:59:30 - INFO - __main__ - Global step 500 Train loss 1.51 EM 0.0 on epoch=249
03/01/2022 17:59:32 - INFO - __main__ - Step 510 Global step 510 Train loss 1.34 on epoch=254
03/01/2022 17:59:34 - INFO - __main__ - Step 520 Global step 520 Train loss 1.38 on epoch=259
03/01/2022 17:59:37 - INFO - __main__ - Step 530 Global step 530 Train loss 1.35 on epoch=264
03/01/2022 17:59:39 - INFO - __main__ - Step 540 Global step 540 Train loss 1.28 on epoch=269
03/01/2022 17:59:41 - INFO - __main__ - Step 550 Global step 550 Train loss 1.33 on epoch=274
03/01/2022 17:59:43 - INFO - __main__ - Global step 550 Train loss 1.34 EM 0.0 on epoch=274
03/01/2022 17:59:45 - INFO - __main__ - Step 560 Global step 560 Train loss 1.34 on epoch=279
03/01/2022 17:59:48 - INFO - __main__ - Step 570 Global step 570 Train loss 1.22 on epoch=284
03/01/2022 17:59:50 - INFO - __main__ - Step 580 Global step 580 Train loss 1.28 on epoch=289
03/01/2022 17:59:52 - INFO - __main__ - Step 590 Global step 590 Train loss 1.23 on epoch=294
03/01/2022 17:59:54 - INFO - __main__ - Step 600 Global step 600 Train loss 1.21 on epoch=299
03/01/2022 17:59:56 - INFO - __main__ - Global step 600 Train loss 1.26 EM 0.0 on epoch=299
03/01/2022 17:59:58 - INFO - __main__ - Step 610 Global step 610 Train loss 1.15 on epoch=304
03/01/2022 18:00:01 - INFO - __main__ - Step 620 Global step 620 Train loss 1.21 on epoch=309
03/01/2022 18:00:03 - INFO - __main__ - Step 630 Global step 630 Train loss 1.17 on epoch=314
03/01/2022 18:00:05 - INFO - __main__ - Step 640 Global step 640 Train loss 1.24 on epoch=319
03/01/2022 18:00:08 - INFO - __main__ - Step 650 Global step 650 Train loss 1.15 on epoch=324
03/01/2022 18:00:09 - INFO - __main__ - Global step 650 Train loss 1.18 EM 0.0 on epoch=324
03/01/2022 18:00:11 - INFO - __main__ - Step 660 Global step 660 Train loss 1.04 on epoch=329
03/01/2022 18:00:14 - INFO - __main__ - Step 670 Global step 670 Train loss 1.02 on epoch=334
03/01/2022 18:00:16 - INFO - __main__ - Step 680 Global step 680 Train loss 1.09 on epoch=339
03/01/2022 18:00:18 - INFO - __main__ - Step 690 Global step 690 Train loss 1.06 on epoch=344
03/01/2022 18:00:21 - INFO - __main__ - Step 700 Global step 700 Train loss 1.00 on epoch=349
03/01/2022 18:00:22 - INFO - __main__ - Global step 700 Train loss 1.04 EM 0.0 on epoch=349
03/01/2022 18:00:24 - INFO - __main__ - Step 710 Global step 710 Train loss 1.07 on epoch=354
03/01/2022 18:00:27 - INFO - __main__ - Step 720 Global step 720 Train loss 1.03 on epoch=359
03/01/2022 18:00:29 - INFO - __main__ - Step 730 Global step 730 Train loss 1.07 on epoch=364
03/01/2022 18:00:31 - INFO - __main__ - Step 740 Global step 740 Train loss 1.00 on epoch=369
03/01/2022 18:00:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.95 on epoch=374
03/01/2022 18:00:35 - INFO - __main__ - Global step 750 Train loss 1.03 EM 0.0 on epoch=374
03/01/2022 18:00:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=379
03/01/2022 18:00:39 - INFO - __main__ - Step 770 Global step 770 Train loss 1.02 on epoch=384
03/01/2022 18:00:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=389
03/01/2022 18:00:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.94 on epoch=394
03/01/2022 18:00:46 - INFO - __main__ - Step 800 Global step 800 Train loss 1.00 on epoch=399
03/01/2022 18:00:48 - INFO - __main__ - Global step 800 Train loss 0.96 EM 0.0 on epoch=399
03/01/2022 18:00:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.91 on epoch=404
03/01/2022 18:00:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
03/01/2022 18:00:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.93 on epoch=414
03/01/2022 18:00:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=419
03/01/2022 18:00:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
03/01/2022 18:01:01 - INFO - __main__ - Global step 850 Train loss 0.91 EM 0.0 on epoch=424
03/01/2022 18:01:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=429
03/01/2022 18:01:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=434
03/01/2022 18:01:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.79 on epoch=439
03/01/2022 18:01:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=444
03/01/2022 18:01:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=449
03/01/2022 18:01:14 - INFO - __main__ - Global step 900 Train loss 0.84 EM 0.0 on epoch=449
03/01/2022 18:01:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=454
03/01/2022 18:01:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.86 on epoch=459
03/01/2022 18:01:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=464
03/01/2022 18:01:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=469
03/01/2022 18:01:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
03/01/2022 18:01:26 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 18:01:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.76 on epoch=479
03/01/2022 18:01:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.73 on epoch=484
03/01/2022 18:01:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.71 on epoch=489
03/01/2022 18:01:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.67 on epoch=494
03/01/2022 18:01:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.70 on epoch=499
03/01/2022 18:01:39 - INFO - __main__ - Global step 1000 Train loss 0.72 EM 0.03125 on epoch=499
03/01/2022 18:01:39 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=499, global_step=1000
03/01/2022 18:01:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=504
03/01/2022 18:01:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.72 on epoch=509
03/01/2022 18:01:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.66 on epoch=514
03/01/2022 18:01:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.61 on epoch=519
03/01/2022 18:01:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.70 on epoch=524
03/01/2022 18:01:52 - INFO - __main__ - Global step 1050 Train loss 0.69 EM 0.03125 on epoch=524
03/01/2022 18:01:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.67 on epoch=529
03/01/2022 18:01:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.63 on epoch=534
03/01/2022 18:01:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=539
03/01/2022 18:02:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.67 on epoch=544
03/01/2022 18:02:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.62 on epoch=549
03/01/2022 18:02:05 - INFO - __main__ - Global step 1100 Train loss 0.65 EM 0.03125 on epoch=549
03/01/2022 18:02:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.64 on epoch=554
03/01/2022 18:02:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.66 on epoch=559
03/01/2022 18:02:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.63 on epoch=564
03/01/2022 18:02:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.60 on epoch=569
03/01/2022 18:02:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.58 on epoch=574
03/01/2022 18:02:18 - INFO - __main__ - Global step 1150 Train loss 0.62 EM 0.0 on epoch=574
03/01/2022 18:02:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=579
03/01/2022 18:02:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.60 on epoch=584
03/01/2022 18:02:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.60 on epoch=589
03/01/2022 18:02:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.56 on epoch=594
03/01/2022 18:02:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.57 on epoch=599
03/01/2022 18:02:30 - INFO - __main__ - Global step 1200 Train loss 0.58 EM 0.0 on epoch=599
03/01/2022 18:02:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
03/01/2022 18:02:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
03/01/2022 18:02:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=614
03/01/2022 18:02:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=619
03/01/2022 18:02:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=624
03/01/2022 18:02:43 - INFO - __main__ - Global step 1250 Train loss 0.50 EM 0.03125 on epoch=624
03/01/2022 18:02:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
03/01/2022 18:02:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=634
03/01/2022 18:02:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=639
03/01/2022 18:02:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=644
03/01/2022 18:02:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/01/2022 18:02:56 - INFO - __main__ - Global step 1300 Train loss 0.48 EM 0.03125 on epoch=649
03/01/2022 18:02:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.55 on epoch=654
03/01/2022 18:03:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=659
03/01/2022 18:03:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
03/01/2022 18:03:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=669
03/01/2022 18:03:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=674
03/01/2022 18:03:09 - INFO - __main__ - Global step 1350 Train loss 0.47 EM 0.03125 on epoch=674
03/01/2022 18:03:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=679
03/01/2022 18:03:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=684
03/01/2022 18:03:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
03/01/2022 18:03:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=694
03/01/2022 18:03:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=699
03/01/2022 18:03:22 - INFO - __main__ - Global step 1400 Train loss 0.46 EM 0.03125 on epoch=699
03/01/2022 18:03:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=704
03/01/2022 18:03:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
03/01/2022 18:03:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
03/01/2022 18:03:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
03/01/2022 18:03:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=724
03/01/2022 18:03:35 - INFO - __main__ - Global step 1450 Train loss 0.40 EM 0.03125 on epoch=724
03/01/2022 18:03:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=729
03/01/2022 18:03:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
03/01/2022 18:03:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=739
03/01/2022 18:03:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=744
03/01/2022 18:03:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=749
03/01/2022 18:03:47 - INFO - __main__ - Global step 1500 Train loss 0.36 EM 0.03125 on epoch=749
03/01/2022 18:03:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=754
03/01/2022 18:03:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=759
03/01/2022 18:03:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
03/01/2022 18:03:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
03/01/2022 18:03:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=774
03/01/2022 18:04:00 - INFO - __main__ - Global step 1550 Train loss 0.36 EM 0.03125 on epoch=774
03/01/2022 18:04:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
03/01/2022 18:04:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=784
03/01/2022 18:04:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
03/01/2022 18:04:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
03/01/2022 18:04:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
03/01/2022 18:04:13 - INFO - __main__ - Global step 1600 Train loss 0.34 EM 0.03125 on epoch=799
03/01/2022 18:04:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
03/01/2022 18:04:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=809
03/01/2022 18:04:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
03/01/2022 18:04:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=819
03/01/2022 18:04:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=824
03/01/2022 18:04:26 - INFO - __main__ - Global step 1650 Train loss 0.32 EM 0.03125 on epoch=824
03/01/2022 18:04:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
03/01/2022 18:04:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=834
03/01/2022 18:04:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=839
03/01/2022 18:04:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=844
03/01/2022 18:04:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=849
03/01/2022 18:04:38 - INFO - __main__ - Global step 1700 Train loss 0.30 EM 0.03125 on epoch=849
03/01/2022 18:04:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=854
03/01/2022 18:04:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=859
03/01/2022 18:04:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/01/2022 18:04:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
03/01/2022 18:04:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=874
03/01/2022 18:04:51 - INFO - __main__ - Global step 1750 Train loss 0.30 EM 0.03125 on epoch=874
03/01/2022 18:04:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=879
03/01/2022 18:04:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=884
03/01/2022 18:04:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=889
03/01/2022 18:05:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
03/01/2022 18:05:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/01/2022 18:05:04 - INFO - __main__ - Global step 1800 Train loss 0.27 EM 0.03125 on epoch=899
03/01/2022 18:05:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=904
03/01/2022 18:05:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=909
03/01/2022 18:05:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=914
03/01/2022 18:05:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=919
03/01/2022 18:05:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=924
03/01/2022 18:05:17 - INFO - __main__ - Global step 1850 Train loss 0.26 EM 0.03125 on epoch=924
03/01/2022 18:05:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
03/01/2022 18:05:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=934
03/01/2022 18:05:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=939
03/01/2022 18:05:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=944
03/01/2022 18:05:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=949
03/01/2022 18:05:30 - INFO - __main__ - Global step 1900 Train loss 0.26 EM 0.03125 on epoch=949
03/01/2022 18:05:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
03/01/2022 18:05:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
03/01/2022 18:05:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=964
03/01/2022 18:05:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=969
03/01/2022 18:05:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=974
03/01/2022 18:05:43 - INFO - __main__ - Global step 1950 Train loss 0.26 EM 0.03125 on epoch=974
03/01/2022 18:05:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=979
03/01/2022 18:05:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
03/01/2022 18:05:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=989
03/01/2022 18:05:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/01/2022 18:05:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=999
03/01/2022 18:05:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:05:55 - INFO - __main__ - Printing 3 examples
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:05:55 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:05:55 - INFO - __main__ - ['james callaghan']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:05:55 - INFO - __main__ - ['aberdeen']
03/01/2022 18:05:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:05:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:05:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:05:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:05:55 - INFO - __main__ - Printing 3 examples
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:05:55 - INFO - __main__ - ['tulisa']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:05:55 - INFO - __main__ - ['calgary']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:05:55 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:05:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:05:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:05:55 - INFO - __main__ - Global step 2000 Train loss 0.21 EM 0.03125 on epoch=999
03/01/2022 18:05:55 - INFO - __main__ - save last model!
03/01/2022 18:05:55 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:05:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:05:55 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:05:55 - INFO - __main__ - Printing 3 examples
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:05:55 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:05:55 - INFO - __main__ - ['henry fonda']
03/01/2022 18:05:55 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:05:55 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:05:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:05:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:06:01 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:06:10 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:06:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:06:10 - INFO - __main__ - Starting training!
03/01/2022 18:08:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.2_8_predictions.txt
03/01/2022 18:08:53 - INFO - __main__ - EM on test data: 0.0058
03/01/2022 18:08:53 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.2, bsz=8, dev_performance=0.03125, test_performance=0.005758637956935403
03/01/2022 18:08:53 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.5, bsz=8 ...
03/01/2022 18:08:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:08:54 - INFO - __main__ - Printing 3 examples
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:08:54 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:08:54 - INFO - __main__ - ['james callaghan']
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:08:54 - INFO - __main__ - ['aberdeen']
03/01/2022 18:08:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:08:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:08:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:08:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:08:54 - INFO - __main__ - Printing 3 examples
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:08:54 - INFO - __main__ - ['tulisa']
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:08:54 - INFO - __main__ - ['calgary']
03/01/2022 18:08:54 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:08:54 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:08:54 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:08:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:08:54 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:09:08 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:09:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:09:08 - INFO - __main__ - Starting training!
03/01/2022 18:09:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.80 on epoch=4
03/01/2022 18:09:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.13 on epoch=9
03/01/2022 18:09:16 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=14
03/01/2022 18:09:18 - INFO - __main__ - Step 40 Global step 40 Train loss 2.83 on epoch=19
03/01/2022 18:09:21 - INFO - __main__ - Step 50 Global step 50 Train loss 2.76 on epoch=24
03/01/2022 18:09:22 - INFO - __main__ - Global step 50 Train loss 3.09 EM 0.0 on epoch=24
03/01/2022 18:09:22 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:09:25 - INFO - __main__ - Step 60 Global step 60 Train loss 2.75 on epoch=29
03/01/2022 18:09:27 - INFO - __main__ - Step 70 Global step 70 Train loss 2.55 on epoch=34
03/01/2022 18:09:29 - INFO - __main__ - Step 80 Global step 80 Train loss 2.50 on epoch=39
03/01/2022 18:09:31 - INFO - __main__ - Step 90 Global step 90 Train loss 2.35 on epoch=44
03/01/2022 18:09:34 - INFO - __main__ - Step 100 Global step 100 Train loss 2.28 on epoch=49
03/01/2022 18:09:35 - INFO - __main__ - Global step 100 Train loss 2.49 EM 0.0 on epoch=49
03/01/2022 18:09:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.27 on epoch=54
03/01/2022 18:09:40 - INFO - __main__ - Step 120 Global step 120 Train loss 2.14 on epoch=59
03/01/2022 18:09:42 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/01/2022 18:09:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.99 on epoch=69
03/01/2022 18:09:47 - INFO - __main__ - Step 150 Global step 150 Train loss 2.06 on epoch=74
03/01/2022 18:09:48 - INFO - __main__ - Global step 150 Train loss 2.12 EM 0.0 on epoch=74
03/01/2022 18:09:50 - INFO - __main__ - Step 160 Global step 160 Train loss 2.02 on epoch=79
03/01/2022 18:09:53 - INFO - __main__ - Step 170 Global step 170 Train loss 1.85 on epoch=84
03/01/2022 18:09:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.93 on epoch=89
03/01/2022 18:09:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.85 on epoch=94
03/01/2022 18:09:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.74 on epoch=99
03/01/2022 18:10:01 - INFO - __main__ - Global step 200 Train loss 1.88 EM 0.0 on epoch=99
03/01/2022 18:10:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.75 on epoch=104
03/01/2022 18:10:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.75 on epoch=109
03/01/2022 18:10:08 - INFO - __main__ - Step 230 Global step 230 Train loss 1.71 on epoch=114
03/01/2022 18:10:10 - INFO - __main__ - Step 240 Global step 240 Train loss 1.63 on epoch=119
03/01/2022 18:10:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.71 on epoch=124
03/01/2022 18:10:14 - INFO - __main__ - Global step 250 Train loss 1.71 EM 0.0 on epoch=124
03/01/2022 18:10:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.61 on epoch=129
03/01/2022 18:10:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.44 on epoch=134
03/01/2022 18:10:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.50 on epoch=139
03/01/2022 18:10:23 - INFO - __main__ - Step 290 Global step 290 Train loss 1.46 on epoch=144
03/01/2022 18:10:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.36 on epoch=149
03/01/2022 18:10:26 - INFO - __main__ - Global step 300 Train loss 1.47 EM 0.0 on epoch=149
03/01/2022 18:10:29 - INFO - __main__ - Step 310 Global step 310 Train loss 1.36 on epoch=154
03/01/2022 18:10:31 - INFO - __main__ - Step 320 Global step 320 Train loss 1.36 on epoch=159
03/01/2022 18:10:33 - INFO - __main__ - Step 330 Global step 330 Train loss 1.25 on epoch=164
03/01/2022 18:10:35 - INFO - __main__ - Step 340 Global step 340 Train loss 1.32 on epoch=169
03/01/2022 18:10:38 - INFO - __main__ - Step 350 Global step 350 Train loss 1.31 on epoch=174
03/01/2022 18:10:39 - INFO - __main__ - Global step 350 Train loss 1.32 EM 0.0 on epoch=174
03/01/2022 18:10:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.23 on epoch=179
03/01/2022 18:10:44 - INFO - __main__ - Step 370 Global step 370 Train loss 1.21 on epoch=184
03/01/2022 18:10:46 - INFO - __main__ - Step 380 Global step 380 Train loss 1.17 on epoch=189
03/01/2022 18:10:48 - INFO - __main__ - Step 390 Global step 390 Train loss 1.13 on epoch=194
03/01/2022 18:10:50 - INFO - __main__ - Step 400 Global step 400 Train loss 1.07 on epoch=199
03/01/2022 18:10:52 - INFO - __main__ - Global step 400 Train loss 1.16 EM 0.0 on epoch=199
03/01/2022 18:10:54 - INFO - __main__ - Step 410 Global step 410 Train loss 1.09 on epoch=204
03/01/2022 18:10:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.05 on epoch=209
03/01/2022 18:10:59 - INFO - __main__ - Step 430 Global step 430 Train loss 1.02 on epoch=214
03/01/2022 18:11:01 - INFO - __main__ - Step 440 Global step 440 Train loss 1.00 on epoch=219
03/01/2022 18:11:03 - INFO - __main__ - Step 450 Global step 450 Train loss 1.00 on epoch=224
03/01/2022 18:11:05 - INFO - __main__ - Global step 450 Train loss 1.03 EM 0.0 on epoch=224
03/01/2022 18:11:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.97 on epoch=229
03/01/2022 18:11:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.97 on epoch=234
03/01/2022 18:11:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.90 on epoch=239
03/01/2022 18:11:14 - INFO - __main__ - Step 490 Global step 490 Train loss 1.00 on epoch=244
03/01/2022 18:11:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.90 on epoch=249
03/01/2022 18:11:17 - INFO - __main__ - Global step 500 Train loss 0.95 EM 0.0 on epoch=249
03/01/2022 18:11:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=254
03/01/2022 18:11:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
03/01/2022 18:11:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=264
03/01/2022 18:11:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=269
03/01/2022 18:11:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.88 on epoch=274
03/01/2022 18:11:30 - INFO - __main__ - Global step 550 Train loss 0.88 EM 0.0 on epoch=274
03/01/2022 18:11:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=279
03/01/2022 18:11:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.79 on epoch=284
03/01/2022 18:11:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=289
03/01/2022 18:11:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=294
03/01/2022 18:11:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.79 on epoch=299
03/01/2022 18:11:42 - INFO - __main__ - Global step 600 Train loss 0.77 EM 0.0 on epoch=299
03/01/2022 18:11:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.74 on epoch=304
03/01/2022 18:11:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=309
03/01/2022 18:11:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=314
03/01/2022 18:11:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.67 on epoch=319
03/01/2022 18:11:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=324
03/01/2022 18:11:55 - INFO - __main__ - Global step 650 Train loss 0.72 EM 0.0 on epoch=324
03/01/2022 18:11:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.68 on epoch=329
03/01/2022 18:12:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=334
03/01/2022 18:12:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.63 on epoch=339
03/01/2022 18:12:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=344
03/01/2022 18:12:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=349
03/01/2022 18:12:08 - INFO - __main__ - Global step 700 Train loss 0.63 EM 0.0 on epoch=349
03/01/2022 18:12:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
03/01/2022 18:12:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
03/01/2022 18:12:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=364
03/01/2022 18:12:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=369
03/01/2022 18:12:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 18:12:21 - INFO - __main__ - Global step 750 Train loss 0.58 EM 0.0 on epoch=374
03/01/2022 18:12:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=379
03/01/2022 18:12:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=384
03/01/2022 18:12:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
03/01/2022 18:12:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=394
03/01/2022 18:12:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=399
03/01/2022 18:12:33 - INFO - __main__ - Global step 800 Train loss 0.48 EM 0.0 on epoch=399
03/01/2022 18:12:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=404
03/01/2022 18:12:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=409
03/01/2022 18:12:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/01/2022 18:12:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=419
03/01/2022 18:12:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=424
03/01/2022 18:12:46 - INFO - __main__ - Global step 850 Train loss 0.42 EM 0.0 on epoch=424
03/01/2022 18:12:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/01/2022 18:12:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=434
03/01/2022 18:12:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=439
03/01/2022 18:12:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=444
03/01/2022 18:12:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=449
03/01/2022 18:12:59 - INFO - __main__ - Global step 900 Train loss 0.38 EM 0.0 on epoch=449
03/01/2022 18:13:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=454
03/01/2022 18:13:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=459
03/01/2022 18:13:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=464
03/01/2022 18:13:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=469
03/01/2022 18:13:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=474
03/01/2022 18:13:11 - INFO - __main__ - Global step 950 Train loss 0.33 EM 0.0 on epoch=474
03/01/2022 18:13:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=479
03/01/2022 18:13:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 18:13:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=489
03/01/2022 18:13:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/01/2022 18:13:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/01/2022 18:13:24 - INFO - __main__ - Global step 1000 Train loss 0.30 EM 0.0 on epoch=499
03/01/2022 18:13:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
03/01/2022 18:13:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=509
03/01/2022 18:13:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=514
03/01/2022 18:13:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
03/01/2022 18:13:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=524
03/01/2022 18:13:37 - INFO - __main__ - Global step 1050 Train loss 0.29 EM 0.0 on epoch=524
03/01/2022 18:13:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=529
03/01/2022 18:13:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/01/2022 18:13:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
03/01/2022 18:13:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/01/2022 18:13:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/01/2022 18:13:50 - INFO - __main__ - Global step 1100 Train loss 0.24 EM 0.0 on epoch=549
03/01/2022 18:13:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
03/01/2022 18:13:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=559
03/01/2022 18:13:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/01/2022 18:13:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
03/01/2022 18:14:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=574
03/01/2022 18:14:02 - INFO - __main__ - Global step 1150 Train loss 0.25 EM 0.0 on epoch=574
03/01/2022 18:14:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=579
03/01/2022 18:14:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=584
03/01/2022 18:14:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
03/01/2022 18:14:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=594
03/01/2022 18:14:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/01/2022 18:14:15 - INFO - __main__ - Global step 1200 Train loss 0.21 EM 0.0 on epoch=599
03/01/2022 18:14:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=604
03/01/2022 18:14:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/01/2022 18:14:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=614
03/01/2022 18:14:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=619
03/01/2022 18:14:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=624
03/01/2022 18:14:28 - INFO - __main__ - Global step 1250 Train loss 0.20 EM 0.0 on epoch=624
03/01/2022 18:14:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=629
03/01/2022 18:14:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
03/01/2022 18:14:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/01/2022 18:14:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
03/01/2022 18:14:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=649
03/01/2022 18:14:40 - INFO - __main__ - Global step 1300 Train loss 0.19 EM 0.0 on epoch=649
03/01/2022 18:14:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=654
03/01/2022 18:14:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=659
03/01/2022 18:14:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
03/01/2022 18:14:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 18:14:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=674
03/01/2022 18:14:53 - INFO - __main__ - Global step 1350 Train loss 0.18 EM 0.0 on epoch=674
03/01/2022 18:14:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
03/01/2022 18:14:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/01/2022 18:15:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/01/2022 18:15:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=694
03/01/2022 18:15:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=699
03/01/2022 18:15:06 - INFO - __main__ - Global step 1400 Train loss 0.17 EM 0.0 on epoch=699
03/01/2022 18:15:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
03/01/2022 18:15:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/01/2022 18:15:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=714
03/01/2022 18:15:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=719
03/01/2022 18:15:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=724
03/01/2022 18:15:19 - INFO - __main__ - Global step 1450 Train loss 0.15 EM 0.0 on epoch=724
03/01/2022 18:15:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=729
03/01/2022 18:15:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=734
03/01/2022 18:15:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/01/2022 18:15:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/01/2022 18:15:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=749
03/01/2022 18:15:31 - INFO - __main__ - Global step 1500 Train loss 0.14 EM 0.0 on epoch=749
03/01/2022 18:15:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/01/2022 18:15:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=759
03/01/2022 18:15:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=764
03/01/2022 18:15:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=769
03/01/2022 18:15:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/01/2022 18:15:44 - INFO - __main__ - Global step 1550 Train loss 0.13 EM 0.0 on epoch=774
03/01/2022 18:15:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=779
03/01/2022 18:15:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=784
03/01/2022 18:15:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
03/01/2022 18:15:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=794
03/01/2022 18:15:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=799
03/01/2022 18:15:57 - INFO - __main__ - Global step 1600 Train loss 0.11 EM 0.0 on epoch=799
03/01/2022 18:15:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=804
03/01/2022 18:16:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=809
03/01/2022 18:16:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=814
03/01/2022 18:16:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
03/01/2022 18:16:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=824
03/01/2022 18:16:10 - INFO - __main__ - Global step 1650 Train loss 0.13 EM 0.0 on epoch=824
03/01/2022 18:16:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
03/01/2022 18:16:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=834
03/01/2022 18:16:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
03/01/2022 18:16:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=844
03/01/2022 18:16:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=849
03/01/2022 18:16:22 - INFO - __main__ - Global step 1700 Train loss 0.11 EM 0.0 on epoch=849
03/01/2022 18:16:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=854
03/01/2022 18:16:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=859
03/01/2022 18:16:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=864
03/01/2022 18:16:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/01/2022 18:16:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=874
03/01/2022 18:16:35 - INFO - __main__ - Global step 1750 Train loss 0.10 EM 0.0 on epoch=874
03/01/2022 18:16:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=879
03/01/2022 18:16:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=884
03/01/2022 18:16:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=889
03/01/2022 18:16:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=894
03/01/2022 18:16:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 18:16:48 - INFO - __main__ - Global step 1800 Train loss 0.10 EM 0.0 on epoch=899
03/01/2022 18:16:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
03/01/2022 18:16:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
03/01/2022 18:16:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
03/01/2022 18:16:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=919
03/01/2022 18:16:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=924
03/01/2022 18:17:00 - INFO - __main__ - Global step 1850 Train loss 0.08 EM 0.0 on epoch=924
03/01/2022 18:17:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 18:17:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/01/2022 18:17:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/01/2022 18:17:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=944
03/01/2022 18:17:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/01/2022 18:17:13 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 18:17:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=954
03/01/2022 18:17:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/01/2022 18:17:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/01/2022 18:17:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 18:17:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
03/01/2022 18:17:26 - INFO - __main__ - Global step 1950 Train loss 0.09 EM 0.0 on epoch=974
03/01/2022 18:17:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=979
03/01/2022 18:17:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=984
03/01/2022 18:17:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 18:17:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
03/01/2022 18:17:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/01/2022 18:17:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:17:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:17:39 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:17:39 - INFO - __main__ - ['james callaghan']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:17:39 - INFO - __main__ - ['aberdeen']
03/01/2022 18:17:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:17:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:17:39 - INFO - __main__ - Global step 2000 Train loss 0.09 EM 0.0 on epoch=999
03/01/2022 18:17:39 - INFO - __main__ - save last model!
03/01/2022 18:17:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:17:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:17:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:17:39 - INFO - __main__ - ['tulisa']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:17:39 - INFO - __main__ - ['calgary']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:17:39 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:17:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:17:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:17:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:17:39 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:17:39 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:17:39 - INFO - __main__ - Printing 3 examples
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:17:39 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:17:39 - INFO - __main__ - ['henry fonda']
03/01/2022 18:17:39 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:17:39 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:17:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:17:40 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:17:44 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:17:51 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:17:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:17:52 - INFO - __main__ - Starting training!
03/01/2022 18:20:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_42_0.5_8_predictions.txt
03/01/2022 18:20:32 - INFO - __main__ - EM on test data: 0.0060
03/01/2022 18:20:32 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.006009013520280421
03/01/2022 18:20:32 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.4, bsz=8 ...
03/01/2022 18:20:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:20:33 - INFO - __main__ - Printing 3 examples
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:20:33 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:20:33 - INFO - __main__ - ['james callaghan']
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:20:33 - INFO - __main__ - ['aberdeen']
03/01/2022 18:20:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:20:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:20:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:20:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:20:33 - INFO - __main__ - Printing 3 examples
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:20:33 - INFO - __main__ - ['tulisa']
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:20:33 - INFO - __main__ - ['calgary']
03/01/2022 18:20:33 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:20:33 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:20:33 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:20:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:20:33 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:20:47 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:20:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:20:48 - INFO - __main__ - Starting training!
03/01/2022 18:20:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.72 on epoch=4
03/01/2022 18:20:57 - INFO - __main__ - Step 20 Global step 20 Train loss 3.16 on epoch=9
03/01/2022 18:20:59 - INFO - __main__ - Step 30 Global step 30 Train loss 2.92 on epoch=14
03/01/2022 18:21:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.93 on epoch=19
03/01/2022 18:21:03 - INFO - __main__ - Step 50 Global step 50 Train loss 2.81 on epoch=24
03/01/2022 18:21:05 - INFO - __main__ - Global step 50 Train loss 3.11 EM 0.0 on epoch=24
03/01/2022 18:21:05 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:21:07 - INFO - __main__ - Step 60 Global step 60 Train loss 2.75 on epoch=29
03/01/2022 18:21:09 - INFO - __main__ - Step 70 Global step 70 Train loss 2.63 on epoch=34
03/01/2022 18:21:11 - INFO - __main__ - Step 80 Global step 80 Train loss 2.66 on epoch=39
03/01/2022 18:21:13 - INFO - __main__ - Step 90 Global step 90 Train loss 2.56 on epoch=44
03/01/2022 18:21:16 - INFO - __main__ - Step 100 Global step 100 Train loss 2.56 on epoch=49
03/01/2022 18:21:17 - INFO - __main__ - Global step 100 Train loss 2.63 EM 0.0 on epoch=49
03/01/2022 18:21:19 - INFO - __main__ - Step 110 Global step 110 Train loss 2.44 on epoch=54
03/01/2022 18:21:21 - INFO - __main__ - Step 120 Global step 120 Train loss 2.45 on epoch=59
03/01/2022 18:21:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.26 on epoch=64
03/01/2022 18:21:26 - INFO - __main__ - Step 140 Global step 140 Train loss 2.32 on epoch=69
03/01/2022 18:21:28 - INFO - __main__ - Step 150 Global step 150 Train loss 2.17 on epoch=74
03/01/2022 18:21:29 - INFO - __main__ - Global step 150 Train loss 2.33 EM 0.0 on epoch=74
03/01/2022 18:21:31 - INFO - __main__ - Step 160 Global step 160 Train loss 2.16 on epoch=79
03/01/2022 18:21:34 - INFO - __main__ - Step 170 Global step 170 Train loss 2.11 on epoch=84
03/01/2022 18:21:36 - INFO - __main__ - Step 180 Global step 180 Train loss 2.09 on epoch=89
03/01/2022 18:21:38 - INFO - __main__ - Step 190 Global step 190 Train loss 2.00 on epoch=94
03/01/2022 18:21:40 - INFO - __main__ - Step 200 Global step 200 Train loss 2.02 on epoch=99
03/01/2022 18:21:41 - INFO - __main__ - Global step 200 Train loss 2.08 EM 0.0 on epoch=99
03/01/2022 18:21:44 - INFO - __main__ - Step 210 Global step 210 Train loss 1.89 on epoch=104
03/01/2022 18:21:46 - INFO - __main__ - Step 220 Global step 220 Train loss 1.80 on epoch=109
03/01/2022 18:21:48 - INFO - __main__ - Step 230 Global step 230 Train loss 1.80 on epoch=114
03/01/2022 18:21:50 - INFO - __main__ - Step 240 Global step 240 Train loss 1.86 on epoch=119
03/01/2022 18:21:53 - INFO - __main__ - Step 250 Global step 250 Train loss 1.72 on epoch=124
03/01/2022 18:21:54 - INFO - __main__ - Global step 250 Train loss 1.82 EM 0.0 on epoch=124
03/01/2022 18:21:56 - INFO - __main__ - Step 260 Global step 260 Train loss 1.74 on epoch=129
03/01/2022 18:21:58 - INFO - __main__ - Step 270 Global step 270 Train loss 1.61 on epoch=134
03/01/2022 18:22:00 - INFO - __main__ - Step 280 Global step 280 Train loss 1.67 on epoch=139
03/01/2022 18:22:03 - INFO - __main__ - Step 290 Global step 290 Train loss 1.58 on epoch=144
03/01/2022 18:22:05 - INFO - __main__ - Step 300 Global step 300 Train loss 1.61 on epoch=149
03/01/2022 18:22:06 - INFO - __main__ - Global step 300 Train loss 1.64 EM 0.0 on epoch=149
03/01/2022 18:22:08 - INFO - __main__ - Step 310 Global step 310 Train loss 1.62 on epoch=154
03/01/2022 18:22:11 - INFO - __main__ - Step 320 Global step 320 Train loss 1.53 on epoch=159
03/01/2022 18:22:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.51 on epoch=164
03/01/2022 18:22:15 - INFO - __main__ - Step 340 Global step 340 Train loss 1.43 on epoch=169
03/01/2022 18:22:17 - INFO - __main__ - Step 350 Global step 350 Train loss 1.41 on epoch=174
03/01/2022 18:22:19 - INFO - __main__ - Global step 350 Train loss 1.50 EM 0.0 on epoch=174
03/01/2022 18:22:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.35 on epoch=179
03/01/2022 18:22:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.42 on epoch=184
03/01/2022 18:22:25 - INFO - __main__ - Step 380 Global step 380 Train loss 1.38 on epoch=189
03/01/2022 18:22:28 - INFO - __main__ - Step 390 Global step 390 Train loss 1.31 on epoch=194
03/01/2022 18:22:30 - INFO - __main__ - Step 400 Global step 400 Train loss 1.25 on epoch=199
03/01/2022 18:22:31 - INFO - __main__ - Global step 400 Train loss 1.34 EM 0.0 on epoch=199
03/01/2022 18:22:33 - INFO - __main__ - Step 410 Global step 410 Train loss 1.32 on epoch=204
03/01/2022 18:22:36 - INFO - __main__ - Step 420 Global step 420 Train loss 1.23 on epoch=209
03/01/2022 18:22:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.23 on epoch=214
03/01/2022 18:22:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.23 on epoch=219
03/01/2022 18:22:42 - INFO - __main__ - Step 450 Global step 450 Train loss 1.15 on epoch=224
03/01/2022 18:22:44 - INFO - __main__ - Global step 450 Train loss 1.23 EM 0.0 on epoch=224
03/01/2022 18:22:46 - INFO - __main__ - Step 460 Global step 460 Train loss 1.16 on epoch=229
03/01/2022 18:22:48 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=234
03/01/2022 18:22:50 - INFO - __main__ - Step 480 Global step 480 Train loss 1.12 on epoch=239
03/01/2022 18:22:53 - INFO - __main__ - Step 490 Global step 490 Train loss 1.01 on epoch=244
03/01/2022 18:22:55 - INFO - __main__ - Step 500 Global step 500 Train loss 1.07 on epoch=249
03/01/2022 18:22:56 - INFO - __main__ - Global step 500 Train loss 1.08 EM 0.0 on epoch=249
03/01/2022 18:22:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.98 on epoch=254
03/01/2022 18:23:01 - INFO - __main__ - Step 520 Global step 520 Train loss 1.01 on epoch=259
03/01/2022 18:23:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.99 on epoch=264
03/01/2022 18:23:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.89 on epoch=269
03/01/2022 18:23:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.91 on epoch=274
03/01/2022 18:23:09 - INFO - __main__ - Global step 550 Train loss 0.96 EM 0.0 on epoch=274
03/01/2022 18:23:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=279
03/01/2022 18:23:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.86 on epoch=284
03/01/2022 18:23:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=289
03/01/2022 18:23:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.90 on epoch=294
03/01/2022 18:23:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.78 on epoch=299
03/01/2022 18:23:22 - INFO - __main__ - Global step 600 Train loss 0.87 EM 0.0 on epoch=299
03/01/2022 18:23:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=304
03/01/2022 18:23:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=309
03/01/2022 18:23:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=314
03/01/2022 18:23:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=319
03/01/2022 18:23:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.76 on epoch=324
03/01/2022 18:23:34 - INFO - __main__ - Global step 650 Train loss 0.78 EM 0.0 on epoch=324
03/01/2022 18:23:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.68 on epoch=329
03/01/2022 18:23:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=334
03/01/2022 18:23:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=339
03/01/2022 18:23:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
03/01/2022 18:23:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.71 on epoch=349
03/01/2022 18:23:47 - INFO - __main__ - Global step 700 Train loss 0.71 EM 0.0 on epoch=349
03/01/2022 18:23:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=354
03/01/2022 18:23:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
03/01/2022 18:23:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=364
03/01/2022 18:23:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.67 on epoch=369
03/01/2022 18:23:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.65 on epoch=374
03/01/2022 18:23:59 - INFO - __main__ - Global step 750 Train loss 0.61 EM 0.0 on epoch=374
03/01/2022 18:24:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.67 on epoch=379
03/01/2022 18:24:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.59 on epoch=384
03/01/2022 18:24:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=389
03/01/2022 18:24:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=394
03/01/2022 18:24:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=399
03/01/2022 18:24:12 - INFO - __main__ - Global step 800 Train loss 0.57 EM 0.0 on epoch=399
03/01/2022 18:24:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=404
03/01/2022 18:24:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=409
03/01/2022 18:24:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=414
03/01/2022 18:24:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=419
03/01/2022 18:24:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
03/01/2022 18:24:24 - INFO - __main__ - Global step 850 Train loss 0.53 EM 0.0 on epoch=424
03/01/2022 18:24:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=429
03/01/2022 18:24:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=434
03/01/2022 18:24:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=439
03/01/2022 18:24:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=444
03/01/2022 18:24:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=449
03/01/2022 18:24:37 - INFO - __main__ - Global step 900 Train loss 0.49 EM 0.0 on epoch=449
03/01/2022 18:24:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=454
03/01/2022 18:24:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/01/2022 18:24:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=464
03/01/2022 18:24:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/01/2022 18:24:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
03/01/2022 18:24:49 - INFO - __main__ - Global step 950 Train loss 0.43 EM 0.0 on epoch=474
03/01/2022 18:24:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=479
03/01/2022 18:24:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
03/01/2022 18:24:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
03/01/2022 18:24:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
03/01/2022 18:25:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
03/01/2022 18:25:02 - INFO - __main__ - Global step 1000 Train loss 0.39 EM 0.0 on epoch=499
03/01/2022 18:25:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
03/01/2022 18:25:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/01/2022 18:25:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=514
03/01/2022 18:25:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=519
03/01/2022 18:25:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=524
03/01/2022 18:25:15 - INFO - __main__ - Global step 1050 Train loss 0.38 EM 0.0 on epoch=524
03/01/2022 18:25:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=529
03/01/2022 18:25:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=534
03/01/2022 18:25:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=539
03/01/2022 18:25:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=544
03/01/2022 18:25:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=549
03/01/2022 18:25:27 - INFO - __main__ - Global step 1100 Train loss 0.34 EM 0.0 on epoch=549
03/01/2022 18:25:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=554
03/01/2022 18:25:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=559
03/01/2022 18:25:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=564
03/01/2022 18:25:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=569
03/01/2022 18:25:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/01/2022 18:25:40 - INFO - __main__ - Global step 1150 Train loss 0.29 EM 0.0 on epoch=574
03/01/2022 18:25:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=579
03/01/2022 18:25:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=584
03/01/2022 18:25:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=589
03/01/2022 18:25:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=594
03/01/2022 18:25:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=599
03/01/2022 18:25:52 - INFO - __main__ - Global step 1200 Train loss 0.33 EM 0.0 on epoch=599
03/01/2022 18:25:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=604
03/01/2022 18:25:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/01/2022 18:25:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
03/01/2022 18:26:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=619
03/01/2022 18:26:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=624
03/01/2022 18:26:05 - INFO - __main__ - Global step 1250 Train loss 0.29 EM 0.0 on epoch=624
03/01/2022 18:26:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/01/2022 18:26:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/01/2022 18:26:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=639
03/01/2022 18:26:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=644
03/01/2022 18:26:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=649
03/01/2022 18:26:18 - INFO - __main__ - Global step 1300 Train loss 0.24 EM 0.0 on epoch=649
03/01/2022 18:26:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/01/2022 18:26:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/01/2022 18:26:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=664
03/01/2022 18:26:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=669
03/01/2022 18:26:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
03/01/2022 18:26:30 - INFO - __main__ - Global step 1350 Train loss 0.23 EM 0.0 on epoch=674
03/01/2022 18:26:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=679
03/01/2022 18:26:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/01/2022 18:26:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=689
03/01/2022 18:26:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
03/01/2022 18:26:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/01/2022 18:26:43 - INFO - __main__ - Global step 1400 Train loss 0.22 EM 0.0 on epoch=699
03/01/2022 18:26:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
03/01/2022 18:26:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=709
03/01/2022 18:26:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/01/2022 18:26:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/01/2022 18:26:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=724
03/01/2022 18:26:55 - INFO - __main__ - Global step 1450 Train loss 0.22 EM 0.0 on epoch=724
03/01/2022 18:26:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/01/2022 18:27:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=734
03/01/2022 18:27:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=739
03/01/2022 18:27:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=744
03/01/2022 18:27:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
03/01/2022 18:27:08 - INFO - __main__ - Global step 1500 Train loss 0.21 EM 0.0 on epoch=749
03/01/2022 18:27:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/01/2022 18:27:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=759
03/01/2022 18:27:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/01/2022 18:27:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=769
03/01/2022 18:27:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
03/01/2022 18:27:20 - INFO - __main__ - Global step 1550 Train loss 0.17 EM 0.0 on epoch=774
03/01/2022 18:27:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=779
03/01/2022 18:27:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=784
03/01/2022 18:27:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=789
03/01/2022 18:27:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 18:27:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 18:27:33 - INFO - __main__ - Global step 1600 Train loss 0.16 EM 0.0 on epoch=799
03/01/2022 18:27:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=804
03/01/2022 18:27:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=809
03/01/2022 18:27:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
03/01/2022 18:27:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
03/01/2022 18:27:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=824
03/01/2022 18:27:46 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 18:27:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=829
03/01/2022 18:27:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=834
03/01/2022 18:27:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
03/01/2022 18:27:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=844
03/01/2022 18:27:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=849
03/01/2022 18:27:58 - INFO - __main__ - Global step 1700 Train loss 0.16 EM 0.0 on epoch=849
03/01/2022 18:28:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=854
03/01/2022 18:28:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=859
03/01/2022 18:28:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
03/01/2022 18:28:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=869
03/01/2022 18:28:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=874
03/01/2022 18:28:11 - INFO - __main__ - Global step 1750 Train loss 0.16 EM 0.0 on epoch=874
03/01/2022 18:28:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 18:28:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=884
03/01/2022 18:28:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=889
03/01/2022 18:28:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
03/01/2022 18:28:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=899
03/01/2022 18:28:23 - INFO - __main__ - Global step 1800 Train loss 0.14 EM 0.0 on epoch=899
03/01/2022 18:28:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=904
03/01/2022 18:28:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/01/2022 18:28:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=914
03/01/2022 18:28:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/01/2022 18:28:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=924
03/01/2022 18:28:36 - INFO - __main__ - Global step 1850 Train loss 0.16 EM 0.0 on epoch=924
03/01/2022 18:28:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=929
03/01/2022 18:28:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=934
03/01/2022 18:28:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=939
03/01/2022 18:28:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/01/2022 18:28:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
03/01/2022 18:28:49 - INFO - __main__ - Global step 1900 Train loss 0.13 EM 0.0 on epoch=949
03/01/2022 18:28:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=954
03/01/2022 18:28:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=959
03/01/2022 18:28:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=964
03/01/2022 18:28:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/01/2022 18:29:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=974
03/01/2022 18:29:01 - INFO - __main__ - Global step 1950 Train loss 0.13 EM 0.0 on epoch=974
03/01/2022 18:29:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=979
03/01/2022 18:29:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
03/01/2022 18:29:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
03/01/2022 18:29:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=994
03/01/2022 18:29:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 18:29:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:29:14 - INFO - __main__ - Printing 3 examples
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:29:14 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:29:14 - INFO - __main__ - ['james callaghan']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:29:14 - INFO - __main__ - ['aberdeen']
03/01/2022 18:29:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:29:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:29:14 - INFO - __main__ - Global step 2000 Train loss 0.10 EM 0.0 on epoch=999
03/01/2022 18:29:14 - INFO - __main__ - save last model!
03/01/2022 18:29:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:29:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:29:14 - INFO - __main__ - Printing 3 examples
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:29:14 - INFO - __main__ - ['tulisa']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:29:14 - INFO - __main__ - ['calgary']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:29:14 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:29:14 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:29:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:29:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:29:14 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:29:14 - INFO - __main__ - Printing 3 examples
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:29:14 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:29:14 - INFO - __main__ - ['henry fonda']
03/01/2022 18:29:14 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:29:14 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:29:14 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:29:14 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:29:16 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:29:19 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:29:27 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:29:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:29:27 - INFO - __main__ - Starting training!
03/01/2022 18:32:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_42_0.4_8_predictions.txt
03/01/2022 18:32:01 - INFO - __main__ - EM on test data: 0.0068
03/01/2022 18:32:01 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.006760140210315473
03/01/2022 18:32:01 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.3, bsz=8 ...
03/01/2022 18:32:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:32:02 - INFO - __main__ - Printing 3 examples
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:32:02 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:32:02 - INFO - __main__ - ['james callaghan']
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:32:02 - INFO - __main__ - ['aberdeen']
03/01/2022 18:32:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:32:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:32:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:32:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:32:02 - INFO - __main__ - Printing 3 examples
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:32:02 - INFO - __main__ - ['tulisa']
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:32:02 - INFO - __main__ - ['calgary']
03/01/2022 18:32:02 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:32:02 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:32:02 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:32:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:32:02 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:32:16 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:32:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:32:17 - INFO - __main__ - Starting training!
03/01/2022 18:32:22 - INFO - __main__ - Step 10 Global step 10 Train loss 3.83 on epoch=4
03/01/2022 18:32:24 - INFO - __main__ - Step 20 Global step 20 Train loss 3.34 on epoch=9
03/01/2022 18:32:27 - INFO - __main__ - Step 30 Global step 30 Train loss 2.98 on epoch=14
03/01/2022 18:32:29 - INFO - __main__ - Step 40 Global step 40 Train loss 2.93 on epoch=19
03/01/2022 18:32:31 - INFO - __main__ - Step 50 Global step 50 Train loss 2.90 on epoch=24
03/01/2022 18:32:33 - INFO - __main__ - Global step 50 Train loss 3.19 EM 0.0 on epoch=24
03/01/2022 18:32:33 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:32:35 - INFO - __main__ - Step 60 Global step 60 Train loss 2.83 on epoch=29
03/01/2022 18:32:38 - INFO - __main__ - Step 70 Global step 70 Train loss 2.77 on epoch=34
03/01/2022 18:32:40 - INFO - __main__ - Step 80 Global step 80 Train loss 2.69 on epoch=39
03/01/2022 18:32:42 - INFO - __main__ - Step 90 Global step 90 Train loss 2.63 on epoch=44
03/01/2022 18:32:45 - INFO - __main__ - Step 100 Global step 100 Train loss 2.61 on epoch=49
03/01/2022 18:32:46 - INFO - __main__ - Global step 100 Train loss 2.70 EM 0.0 on epoch=49
03/01/2022 18:32:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.52 on epoch=54
03/01/2022 18:32:51 - INFO - __main__ - Step 120 Global step 120 Train loss 2.50 on epoch=59
03/01/2022 18:32:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.46 on epoch=64
03/01/2022 18:32:55 - INFO - __main__ - Step 140 Global step 140 Train loss 2.41 on epoch=69
03/01/2022 18:32:58 - INFO - __main__ - Step 150 Global step 150 Train loss 2.44 on epoch=74
03/01/2022 18:32:59 - INFO - __main__ - Global step 150 Train loss 2.46 EM 0.0 on epoch=74
03/01/2022 18:33:01 - INFO - __main__ - Step 160 Global step 160 Train loss 2.43 on epoch=79
03/01/2022 18:33:04 - INFO - __main__ - Step 170 Global step 170 Train loss 2.27 on epoch=84
03/01/2022 18:33:06 - INFO - __main__ - Step 180 Global step 180 Train loss 2.22 on epoch=89
03/01/2022 18:33:08 - INFO - __main__ - Step 190 Global step 190 Train loss 2.23 on epoch=94
03/01/2022 18:33:11 - INFO - __main__ - Step 200 Global step 200 Train loss 2.14 on epoch=99
03/01/2022 18:33:12 - INFO - __main__ - Global step 200 Train loss 2.26 EM 0.0 on epoch=99
03/01/2022 18:33:14 - INFO - __main__ - Step 210 Global step 210 Train loss 2.07 on epoch=104
03/01/2022 18:33:17 - INFO - __main__ - Step 220 Global step 220 Train loss 2.05 on epoch=109
03/01/2022 18:33:19 - INFO - __main__ - Step 230 Global step 230 Train loss 2.11 on epoch=114
03/01/2022 18:33:21 - INFO - __main__ - Step 240 Global step 240 Train loss 2.06 on epoch=119
03/01/2022 18:33:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.95 on epoch=124
03/01/2022 18:33:25 - INFO - __main__ - Global step 250 Train loss 2.05 EM 0.0 on epoch=124
03/01/2022 18:33:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.96 on epoch=129
03/01/2022 18:33:29 - INFO - __main__ - Step 270 Global step 270 Train loss 1.89 on epoch=134
03/01/2022 18:33:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.79 on epoch=139
03/01/2022 18:33:34 - INFO - __main__ - Step 290 Global step 290 Train loss 1.86 on epoch=144
03/01/2022 18:33:36 - INFO - __main__ - Step 300 Global step 300 Train loss 1.82 on epoch=149
03/01/2022 18:33:37 - INFO - __main__ - Global step 300 Train loss 1.86 EM 0.0 on epoch=149
03/01/2022 18:33:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.86 on epoch=154
03/01/2022 18:33:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.79 on epoch=159
03/01/2022 18:33:44 - INFO - __main__ - Step 330 Global step 330 Train loss 1.69 on epoch=164
03/01/2022 18:33:47 - INFO - __main__ - Step 340 Global step 340 Train loss 1.67 on epoch=169
03/01/2022 18:33:49 - INFO - __main__ - Step 350 Global step 350 Train loss 1.66 on epoch=174
03/01/2022 18:33:50 - INFO - __main__ - Global step 350 Train loss 1.73 EM 0.0 on epoch=174
03/01/2022 18:33:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.58 on epoch=179
03/01/2022 18:33:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.61 on epoch=184
03/01/2022 18:33:57 - INFO - __main__ - Step 380 Global step 380 Train loss 1.67 on epoch=189
03/01/2022 18:33:59 - INFO - __main__ - Step 390 Global step 390 Train loss 1.63 on epoch=194
03/01/2022 18:34:01 - INFO - __main__ - Step 400 Global step 400 Train loss 1.52 on epoch=199
03/01/2022 18:34:03 - INFO - __main__ - Global step 400 Train loss 1.60 EM 0.0 on epoch=199
03/01/2022 18:34:05 - INFO - __main__ - Step 410 Global step 410 Train loss 1.49 on epoch=204
03/01/2022 18:34:07 - INFO - __main__ - Step 420 Global step 420 Train loss 1.46 on epoch=209
03/01/2022 18:34:09 - INFO - __main__ - Step 430 Global step 430 Train loss 1.47 on epoch=214
03/01/2022 18:34:12 - INFO - __main__ - Step 440 Global step 440 Train loss 1.55 on epoch=219
03/01/2022 18:34:14 - INFO - __main__ - Step 450 Global step 450 Train loss 1.44 on epoch=224
03/01/2022 18:34:15 - INFO - __main__ - Global step 450 Train loss 1.48 EM 0.0 on epoch=224
03/01/2022 18:34:18 - INFO - __main__ - Step 460 Global step 460 Train loss 1.48 on epoch=229
03/01/2022 18:34:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.46 on epoch=234
03/01/2022 18:34:22 - INFO - __main__ - Step 480 Global step 480 Train loss 1.43 on epoch=239
03/01/2022 18:34:25 - INFO - __main__ - Step 490 Global step 490 Train loss 1.39 on epoch=244
03/01/2022 18:34:27 - INFO - __main__ - Step 500 Global step 500 Train loss 1.31 on epoch=249
03/01/2022 18:34:28 - INFO - __main__ - Global step 500 Train loss 1.42 EM 0.0 on epoch=249
03/01/2022 18:34:30 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
03/01/2022 18:34:33 - INFO - __main__ - Step 520 Global step 520 Train loss 1.34 on epoch=259
03/01/2022 18:34:35 - INFO - __main__ - Step 530 Global step 530 Train loss 1.21 on epoch=264
03/01/2022 18:34:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.29 on epoch=269
03/01/2022 18:34:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.29 on epoch=274
03/01/2022 18:34:41 - INFO - __main__ - Global step 550 Train loss 1.29 EM 0.0 on epoch=274
03/01/2022 18:34:43 - INFO - __main__ - Step 560 Global step 560 Train loss 1.24 on epoch=279
03/01/2022 18:34:45 - INFO - __main__ - Step 570 Global step 570 Train loss 1.22 on epoch=284
03/01/2022 18:34:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.27 on epoch=289
03/01/2022 18:34:50 - INFO - __main__ - Step 590 Global step 590 Train loss 1.30 on epoch=294
03/01/2022 18:34:52 - INFO - __main__ - Step 600 Global step 600 Train loss 1.19 on epoch=299
03/01/2022 18:34:53 - INFO - __main__ - Global step 600 Train loss 1.24 EM 0.0 on epoch=299
03/01/2022 18:34:56 - INFO - __main__ - Step 610 Global step 610 Train loss 1.10 on epoch=304
03/01/2022 18:34:58 - INFO - __main__ - Step 620 Global step 620 Train loss 1.23 on epoch=309
03/01/2022 18:35:00 - INFO - __main__ - Step 630 Global step 630 Train loss 1.07 on epoch=314
03/01/2022 18:35:02 - INFO - __main__ - Step 640 Global step 640 Train loss 1.09 on epoch=319
03/01/2022 18:35:05 - INFO - __main__ - Step 650 Global step 650 Train loss 1.10 on epoch=324
03/01/2022 18:35:06 - INFO - __main__ - Global step 650 Train loss 1.12 EM 0.0 on epoch=324
03/01/2022 18:35:08 - INFO - __main__ - Step 660 Global step 660 Train loss 1.07 on epoch=329
03/01/2022 18:35:10 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
03/01/2022 18:35:13 - INFO - __main__ - Step 680 Global step 680 Train loss 1.04 on epoch=339
03/01/2022 18:35:15 - INFO - __main__ - Step 690 Global step 690 Train loss 1.11 on epoch=344
03/01/2022 18:35:17 - INFO - __main__ - Step 700 Global step 700 Train loss 1.05 on epoch=349
03/01/2022 18:35:19 - INFO - __main__ - Global step 700 Train loss 1.06 EM 0.0 on epoch=349
03/01/2022 18:35:21 - INFO - __main__ - Step 710 Global step 710 Train loss 1.00 on epoch=354
03/01/2022 18:35:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.99 on epoch=359
03/01/2022 18:35:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.92 on epoch=364
03/01/2022 18:35:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.98 on epoch=369
03/01/2022 18:35:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.98 on epoch=374
03/01/2022 18:35:31 - INFO - __main__ - Global step 750 Train loss 0.97 EM 0.0 on epoch=374
03/01/2022 18:35:33 - INFO - __main__ - Step 760 Global step 760 Train loss 1.06 on epoch=379
03/01/2022 18:35:36 - INFO - __main__ - Step 770 Global step 770 Train loss 1.04 on epoch=384
03/01/2022 18:35:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.91 on epoch=389
03/01/2022 18:35:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=394
03/01/2022 18:35:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.88 on epoch=399
03/01/2022 18:35:44 - INFO - __main__ - Global step 800 Train loss 0.96 EM 0.0 on epoch=399
03/01/2022 18:35:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.90 on epoch=404
03/01/2022 18:35:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.91 on epoch=409
03/01/2022 18:35:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.88 on epoch=414
03/01/2022 18:35:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.93 on epoch=419
03/01/2022 18:35:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=424
03/01/2022 18:35:57 - INFO - __main__ - Global step 850 Train loss 0.89 EM 0.0 on epoch=424
03/01/2022 18:35:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.86 on epoch=429
03/01/2022 18:36:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=434
03/01/2022 18:36:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.79 on epoch=439
03/01/2022 18:36:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.79 on epoch=444
03/01/2022 18:36:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.80 on epoch=449
03/01/2022 18:36:09 - INFO - __main__ - Global step 900 Train loss 0.82 EM 0.0 on epoch=449
03/01/2022 18:36:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.73 on epoch=454
03/01/2022 18:36:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.73 on epoch=459
03/01/2022 18:36:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.66 on epoch=464
03/01/2022 18:36:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=469
03/01/2022 18:36:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.66 on epoch=474
03/01/2022 18:36:22 - INFO - __main__ - Global step 950 Train loss 0.69 EM 0.0 on epoch=474
03/01/2022 18:36:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.76 on epoch=479
03/01/2022 18:36:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.71 on epoch=484
03/01/2022 18:36:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.63 on epoch=489
03/01/2022 18:36:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.65 on epoch=494
03/01/2022 18:36:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
03/01/2022 18:36:35 - INFO - __main__ - Global step 1000 Train loss 0.68 EM 0.0 on epoch=499
03/01/2022 18:36:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.68 on epoch=504
03/01/2022 18:36:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=509
03/01/2022 18:36:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.71 on epoch=514
03/01/2022 18:36:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.60 on epoch=519
03/01/2022 18:36:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.62 on epoch=524
03/01/2022 18:36:48 - INFO - __main__ - Global step 1050 Train loss 0.65 EM 0.0 on epoch=524
03/01/2022 18:36:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.62 on epoch=529
03/01/2022 18:36:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.59 on epoch=534
03/01/2022 18:36:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
03/01/2022 18:36:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.55 on epoch=544
03/01/2022 18:36:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.58 on epoch=549
03/01/2022 18:37:00 - INFO - __main__ - Global step 1100 Train loss 0.59 EM 0.0 on epoch=549
03/01/2022 18:37:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.59 on epoch=554
03/01/2022 18:37:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=559
03/01/2022 18:37:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
03/01/2022 18:37:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=569
03/01/2022 18:37:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.52 on epoch=574
03/01/2022 18:37:13 - INFO - __main__ - Global step 1150 Train loss 0.50 EM 0.0 on epoch=574
03/01/2022 18:37:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
03/01/2022 18:37:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=584
03/01/2022 18:37:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=589
03/01/2022 18:37:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=594
03/01/2022 18:37:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
03/01/2022 18:37:26 - INFO - __main__ - Global step 1200 Train loss 0.48 EM 0.0 on epoch=599
03/01/2022 18:37:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=604
03/01/2022 18:37:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/01/2022 18:37:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=614
03/01/2022 18:37:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=619
03/01/2022 18:37:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=624
03/01/2022 18:37:39 - INFO - __main__ - Global step 1250 Train loss 0.44 EM 0.0 on epoch=624
03/01/2022 18:37:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
03/01/2022 18:37:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
03/01/2022 18:37:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=639
03/01/2022 18:37:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=644
03/01/2022 18:37:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/01/2022 18:37:51 - INFO - __main__ - Global step 1300 Train loss 0.43 EM 0.0 on epoch=649
03/01/2022 18:37:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=654
03/01/2022 18:37:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=659
03/01/2022 18:37:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
03/01/2022 18:38:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=669
03/01/2022 18:38:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
03/01/2022 18:38:04 - INFO - __main__ - Global step 1350 Train loss 0.43 EM 0.0 on epoch=674
03/01/2022 18:38:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=679
03/01/2022 18:38:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=684
03/01/2022 18:38:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=689
03/01/2022 18:38:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
03/01/2022 18:38:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=699
03/01/2022 18:38:17 - INFO - __main__ - Global step 1400 Train loss 0.38 EM 0.0 on epoch=699
03/01/2022 18:38:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=704
03/01/2022 18:38:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=709
03/01/2022 18:38:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
03/01/2022 18:38:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
03/01/2022 18:38:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=724
03/01/2022 18:38:29 - INFO - __main__ - Global step 1450 Train loss 0.37 EM 0.0 on epoch=724
03/01/2022 18:38:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=729
03/01/2022 18:38:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=734
03/01/2022 18:38:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/01/2022 18:38:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=744
03/01/2022 18:38:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=749
03/01/2022 18:38:42 - INFO - __main__ - Global step 1500 Train loss 0.34 EM 0.0 on epoch=749
03/01/2022 18:38:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=754
03/01/2022 18:38:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=759
03/01/2022 18:38:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
03/01/2022 18:38:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=769
03/01/2022 18:38:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
03/01/2022 18:38:55 - INFO - __main__ - Global step 1550 Train loss 0.31 EM 0.0 on epoch=774
03/01/2022 18:38:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
03/01/2022 18:38:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=784
03/01/2022 18:39:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
03/01/2022 18:39:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=794
03/01/2022 18:39:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=799
03/01/2022 18:39:07 - INFO - __main__ - Global step 1600 Train loss 0.33 EM 0.0 on epoch=799
03/01/2022 18:39:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
03/01/2022 18:39:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
03/01/2022 18:39:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
03/01/2022 18:39:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=819
03/01/2022 18:39:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=824
03/01/2022 18:39:20 - INFO - __main__ - Global step 1650 Train loss 0.29 EM 0.0 on epoch=824
03/01/2022 18:39:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 18:39:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=834
03/01/2022 18:39:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=839
03/01/2022 18:39:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=844
03/01/2022 18:39:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=849
03/01/2022 18:39:33 - INFO - __main__ - Global step 1700 Train loss 0.27 EM 0.0 on epoch=849
03/01/2022 18:39:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/01/2022 18:39:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=859
03/01/2022 18:39:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=864
03/01/2022 18:39:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/01/2022 18:39:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=874
03/01/2022 18:39:46 - INFO - __main__ - Global step 1750 Train loss 0.24 EM 0.0 on epoch=874
03/01/2022 18:39:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
03/01/2022 18:39:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=884
03/01/2022 18:39:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=889
03/01/2022 18:39:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
03/01/2022 18:39:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=899
03/01/2022 18:39:59 - INFO - __main__ - Global step 1800 Train loss 0.23 EM 0.0 on epoch=899
03/01/2022 18:40:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 18:40:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=909
03/01/2022 18:40:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=914
03/01/2022 18:40:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
03/01/2022 18:40:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=924
03/01/2022 18:40:11 - INFO - __main__ - Global step 1850 Train loss 0.22 EM 0.0 on epoch=924
03/01/2022 18:40:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
03/01/2022 18:40:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/01/2022 18:40:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=939
03/01/2022 18:40:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=944
03/01/2022 18:40:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=949
03/01/2022 18:40:24 - INFO - __main__ - Global step 1900 Train loss 0.22 EM 0.0 on epoch=949
03/01/2022 18:40:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=954
03/01/2022 18:40:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=959
03/01/2022 18:40:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 18:40:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
03/01/2022 18:40:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/01/2022 18:40:37 - INFO - __main__ - Global step 1950 Train loss 0.20 EM 0.0 on epoch=974
03/01/2022 18:40:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=979
03/01/2022 18:40:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/01/2022 18:40:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=989
03/01/2022 18:40:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/01/2022 18:40:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=999
03/01/2022 18:40:50 - INFO - __main__ - Global step 2000 Train loss 0.20 EM 0.0 on epoch=999
03/01/2022 18:40:50 - INFO - __main__ - save last model!
03/01/2022 18:40:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:40:50 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:40:50 - INFO - __main__ - Printing 3 examples
03/01/2022 18:40:50 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:40:50 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:40:50 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:40:50 - INFO - __main__ - ['henry fonda']
03/01/2022 18:40:50 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:40:50 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:40:50 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:40:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:40:51 - INFO - __main__ - Printing 3 examples
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:40:51 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:40:51 - INFO - __main__ - ['james callaghan']
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:40:51 - INFO - __main__ - ['aberdeen']
03/01/2022 18:40:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:40:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:40:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:40:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:40:51 - INFO - __main__ - Printing 3 examples
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:40:51 - INFO - __main__ - ['tulisa']
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:40:51 - INFO - __main__ - ['calgary']
03/01/2022 18:40:51 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:40:51 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:40:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:40:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:40:51 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:40:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:40:55 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:41:03 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:41:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:41:04 - INFO - __main__ - Starting training!
03/01/2022 18:43:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_42_0.3_8_predictions.txt
03/01/2022 18:43:35 - INFO - __main__ - EM on test data: 0.0055
03/01/2022 18:43:36 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.005508262393590386
03/01/2022 18:43:36 - INFO - __main__ - Running ... prefix=freebase_qa_32_42, lr=0.2, bsz=8 ...
03/01/2022 18:43:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:43:37 - INFO - __main__ - Printing 3 examples
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] In May 1994, the Channel Tunnel was formally opened by Queen Elizabeth II and which French President?
03/01/2022 18:43:37 - INFO - __main__ - ['francois mitterrand']
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] b Who was the tallest British Prime Minister of the 20th century?
03/01/2022 18:43:37 - INFO - __main__ - ['james callaghan']
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] Which Scottish football team plays home games at Pittodrie?
03/01/2022 18:43:37 - INFO - __main__ - ['aberdeen']
03/01/2022 18:43:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:43:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:43:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:43:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:43:37 - INFO - __main__ - Printing 3 examples
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] 'Young' was a UK number one hit in May 2012 for which singer?
03/01/2022 18:43:37 - INFO - __main__ - ['tulisa']
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] In which city is the distinctive building of the saddledome?
03/01/2022 18:43:37 - INFO - __main__ - ['calgary']
03/01/2022 18:43:37 - INFO - __main__ -  [freebase_qa] Who won the Oscar for Best Actor in 2010 for his role as Otis Blake in the film 'Crazy Heart'?
03/01/2022 18:43:37 - INFO - __main__ - ['jeff bridges']
03/01/2022 18:43:37 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:43:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:43:37 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:43:51 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:43:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:43:51 - INFO - __main__ - Starting training!
03/01/2022 18:43:55 - INFO - __main__ - Step 10 Global step 10 Train loss 3.94 on epoch=4
03/01/2022 18:43:57 - INFO - __main__ - Step 20 Global step 20 Train loss 3.61 on epoch=9
03/01/2022 18:43:59 - INFO - __main__ - Step 30 Global step 30 Train loss 3.23 on epoch=14
03/01/2022 18:44:02 - INFO - __main__ - Step 40 Global step 40 Train loss 3.08 on epoch=19
03/01/2022 18:44:04 - INFO - __main__ - Step 50 Global step 50 Train loss 2.98 on epoch=24
03/01/2022 18:44:05 - INFO - __main__ - Global step 50 Train loss 3.37 EM 0.0 on epoch=24
03/01/2022 18:44:06 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:44:08 - INFO - __main__ - Step 60 Global step 60 Train loss 2.91 on epoch=29
03/01/2022 18:44:10 - INFO - __main__ - Step 70 Global step 70 Train loss 2.92 on epoch=34
03/01/2022 18:44:12 - INFO - __main__ - Step 80 Global step 80 Train loss 2.83 on epoch=39
03/01/2022 18:44:15 - INFO - __main__ - Step 90 Global step 90 Train loss 2.78 on epoch=44
03/01/2022 18:44:17 - INFO - __main__ - Step 100 Global step 100 Train loss 2.74 on epoch=49
03/01/2022 18:44:19 - INFO - __main__ - Global step 100 Train loss 2.84 EM 0.0 on epoch=49
03/01/2022 18:44:21 - INFO - __main__ - Step 110 Global step 110 Train loss 2.72 on epoch=54
03/01/2022 18:44:23 - INFO - __main__ - Step 120 Global step 120 Train loss 2.77 on epoch=59
03/01/2022 18:44:25 - INFO - __main__ - Step 130 Global step 130 Train loss 2.69 on epoch=64
03/01/2022 18:44:28 - INFO - __main__ - Step 140 Global step 140 Train loss 2.54 on epoch=69
03/01/2022 18:44:30 - INFO - __main__ - Step 150 Global step 150 Train loss 2.63 on epoch=74
03/01/2022 18:44:32 - INFO - __main__ - Global step 150 Train loss 2.67 EM 0.0 on epoch=74
03/01/2022 18:44:34 - INFO - __main__ - Step 160 Global step 160 Train loss 2.56 on epoch=79
03/01/2022 18:44:36 - INFO - __main__ - Step 170 Global step 170 Train loss 2.51 on epoch=84
03/01/2022 18:44:38 - INFO - __main__ - Step 180 Global step 180 Train loss 2.52 on epoch=89
03/01/2022 18:44:41 - INFO - __main__ - Step 190 Global step 190 Train loss 2.51 on epoch=94
03/01/2022 18:44:43 - INFO - __main__ - Step 200 Global step 200 Train loss 2.39 on epoch=99
03/01/2022 18:44:45 - INFO - __main__ - Global step 200 Train loss 2.50 EM 0.0 on epoch=99
03/01/2022 18:44:47 - INFO - __main__ - Step 210 Global step 210 Train loss 2.44 on epoch=104
03/01/2022 18:44:49 - INFO - __main__ - Step 220 Global step 220 Train loss 2.33 on epoch=109
03/01/2022 18:44:51 - INFO - __main__ - Step 230 Global step 230 Train loss 2.32 on epoch=114
03/01/2022 18:44:54 - INFO - __main__ - Step 240 Global step 240 Train loss 2.33 on epoch=119
03/01/2022 18:44:56 - INFO - __main__ - Step 250 Global step 250 Train loss 2.26 on epoch=124
03/01/2022 18:44:57 - INFO - __main__ - Global step 250 Train loss 2.34 EM 0.0 on epoch=124
03/01/2022 18:45:00 - INFO - __main__ - Step 260 Global step 260 Train loss 2.22 on epoch=129
03/01/2022 18:45:02 - INFO - __main__ - Step 270 Global step 270 Train loss 2.25 on epoch=134
03/01/2022 18:45:04 - INFO - __main__ - Step 280 Global step 280 Train loss 2.22 on epoch=139
03/01/2022 18:45:06 - INFO - __main__ - Step 290 Global step 290 Train loss 2.18 on epoch=144
03/01/2022 18:45:09 - INFO - __main__ - Step 300 Global step 300 Train loss 2.15 on epoch=149
03/01/2022 18:45:10 - INFO - __main__ - Global step 300 Train loss 2.20 EM 0.0 on epoch=149
03/01/2022 18:45:12 - INFO - __main__ - Step 310 Global step 310 Train loss 2.08 on epoch=154
03/01/2022 18:45:15 - INFO - __main__ - Step 320 Global step 320 Train loss 2.05 on epoch=159
03/01/2022 18:45:17 - INFO - __main__ - Step 330 Global step 330 Train loss 2.02 on epoch=164
03/01/2022 18:45:19 - INFO - __main__ - Step 340 Global step 340 Train loss 2.05 on epoch=169
03/01/2022 18:45:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.95 on epoch=174
03/01/2022 18:45:23 - INFO - __main__ - Global step 350 Train loss 2.03 EM 0.0 on epoch=174
03/01/2022 18:45:25 - INFO - __main__ - Step 360 Global step 360 Train loss 1.95 on epoch=179
03/01/2022 18:45:27 - INFO - __main__ - Step 370 Global step 370 Train loss 1.94 on epoch=184
03/01/2022 18:45:30 - INFO - __main__ - Step 380 Global step 380 Train loss 1.93 on epoch=189
03/01/2022 18:45:32 - INFO - __main__ - Step 390 Global step 390 Train loss 1.96 on epoch=194
03/01/2022 18:45:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.98 on epoch=199
03/01/2022 18:45:36 - INFO - __main__ - Global step 400 Train loss 1.95 EM 0.0 on epoch=199
03/01/2022 18:45:38 - INFO - __main__ - Step 410 Global step 410 Train loss 1.90 on epoch=204
03/01/2022 18:45:40 - INFO - __main__ - Step 420 Global step 420 Train loss 1.78 on epoch=209
03/01/2022 18:45:43 - INFO - __main__ - Step 430 Global step 430 Train loss 1.83 on epoch=214
03/01/2022 18:45:45 - INFO - __main__ - Step 440 Global step 440 Train loss 1.81 on epoch=219
03/01/2022 18:45:47 - INFO - __main__ - Step 450 Global step 450 Train loss 1.84 on epoch=224
03/01/2022 18:45:48 - INFO - __main__ - Global step 450 Train loss 1.83 EM 0.0 on epoch=224
03/01/2022 18:45:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.77 on epoch=229
03/01/2022 18:45:53 - INFO - __main__ - Step 470 Global step 470 Train loss 1.76 on epoch=234
03/01/2022 18:45:55 - INFO - __main__ - Step 480 Global step 480 Train loss 1.73 on epoch=239
03/01/2022 18:45:58 - INFO - __main__ - Step 490 Global step 490 Train loss 1.68 on epoch=244
03/01/2022 18:46:00 - INFO - __main__ - Step 500 Global step 500 Train loss 1.74 on epoch=249
03/01/2022 18:46:01 - INFO - __main__ - Global step 500 Train loss 1.74 EM 0.0 on epoch=249
03/01/2022 18:46:03 - INFO - __main__ - Step 510 Global step 510 Train loss 1.68 on epoch=254
03/01/2022 18:46:06 - INFO - __main__ - Step 520 Global step 520 Train loss 1.65 on epoch=259
03/01/2022 18:46:08 - INFO - __main__ - Step 530 Global step 530 Train loss 1.60 on epoch=264
03/01/2022 18:46:10 - INFO - __main__ - Step 540 Global step 540 Train loss 1.61 on epoch=269
03/01/2022 18:46:12 - INFO - __main__ - Step 550 Global step 550 Train loss 1.56 on epoch=274
03/01/2022 18:46:14 - INFO - __main__ - Global step 550 Train loss 1.62 EM 0.0 on epoch=274
03/01/2022 18:46:16 - INFO - __main__ - Step 560 Global step 560 Train loss 1.66 on epoch=279
03/01/2022 18:46:18 - INFO - __main__ - Step 570 Global step 570 Train loss 1.56 on epoch=284
03/01/2022 18:46:21 - INFO - __main__ - Step 580 Global step 580 Train loss 1.50 on epoch=289
03/01/2022 18:46:23 - INFO - __main__ - Step 590 Global step 590 Train loss 1.57 on epoch=294
03/01/2022 18:46:25 - INFO - __main__ - Step 600 Global step 600 Train loss 1.54 on epoch=299
03/01/2022 18:46:26 - INFO - __main__ - Global step 600 Train loss 1.57 EM 0.0 on epoch=299
03/01/2022 18:46:29 - INFO - __main__ - Step 610 Global step 610 Train loss 1.40 on epoch=304
03/01/2022 18:46:31 - INFO - __main__ - Step 620 Global step 620 Train loss 1.59 on epoch=309
03/01/2022 18:46:33 - INFO - __main__ - Step 630 Global step 630 Train loss 1.36 on epoch=314
03/01/2022 18:46:35 - INFO - __main__ - Step 640 Global step 640 Train loss 1.43 on epoch=319
03/01/2022 18:46:38 - INFO - __main__ - Step 650 Global step 650 Train loss 1.34 on epoch=324
03/01/2022 18:46:39 - INFO - __main__ - Global step 650 Train loss 1.43 EM 0.0 on epoch=324
03/01/2022 18:46:41 - INFO - __main__ - Step 660 Global step 660 Train loss 1.40 on epoch=329
03/01/2022 18:46:44 - INFO - __main__ - Step 670 Global step 670 Train loss 1.41 on epoch=334
03/01/2022 18:46:46 - INFO - __main__ - Step 680 Global step 680 Train loss 1.35 on epoch=339
03/01/2022 18:46:48 - INFO - __main__ - Step 690 Global step 690 Train loss 1.37 on epoch=344
03/01/2022 18:46:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.36 on epoch=349
03/01/2022 18:46:52 - INFO - __main__ - Global step 700 Train loss 1.38 EM 0.0 on epoch=349
03/01/2022 18:46:54 - INFO - __main__ - Step 710 Global step 710 Train loss 1.30 on epoch=354
03/01/2022 18:46:56 - INFO - __main__ - Step 720 Global step 720 Train loss 1.30 on epoch=359
03/01/2022 18:46:59 - INFO - __main__ - Step 730 Global step 730 Train loss 1.32 on epoch=364
03/01/2022 18:47:01 - INFO - __main__ - Step 740 Global step 740 Train loss 1.30 on epoch=369
03/01/2022 18:47:03 - INFO - __main__ - Step 750 Global step 750 Train loss 1.34 on epoch=374
03/01/2022 18:47:04 - INFO - __main__ - Global step 750 Train loss 1.31 EM 0.0 on epoch=374
03/01/2022 18:47:07 - INFO - __main__ - Step 760 Global step 760 Train loss 1.24 on epoch=379
03/01/2022 18:47:09 - INFO - __main__ - Step 770 Global step 770 Train loss 1.28 on epoch=384
03/01/2022 18:47:11 - INFO - __main__ - Step 780 Global step 780 Train loss 1.26 on epoch=389
03/01/2022 18:47:13 - INFO - __main__ - Step 790 Global step 790 Train loss 1.20 on epoch=394
03/01/2022 18:47:16 - INFO - __main__ - Step 800 Global step 800 Train loss 1.19 on epoch=399
03/01/2022 18:47:17 - INFO - __main__ - Global step 800 Train loss 1.23 EM 0.0 on epoch=399
03/01/2022 18:47:19 - INFO - __main__ - Step 810 Global step 810 Train loss 1.23 on epoch=404
03/01/2022 18:47:22 - INFO - __main__ - Step 820 Global step 820 Train loss 1.18 on epoch=409
03/01/2022 18:47:24 - INFO - __main__ - Step 830 Global step 830 Train loss 1.22 on epoch=414
03/01/2022 18:47:26 - INFO - __main__ - Step 840 Global step 840 Train loss 1.26 on epoch=419
03/01/2022 18:47:28 - INFO - __main__ - Step 850 Global step 850 Train loss 1.13 on epoch=424
03/01/2022 18:47:30 - INFO - __main__ - Global step 850 Train loss 1.20 EM 0.0 on epoch=424
03/01/2022 18:47:32 - INFO - __main__ - Step 860 Global step 860 Train loss 1.20 on epoch=429
03/01/2022 18:47:34 - INFO - __main__ - Step 870 Global step 870 Train loss 1.14 on epoch=434
03/01/2022 18:47:37 - INFO - __main__ - Step 880 Global step 880 Train loss 1.07 on epoch=439
03/01/2022 18:47:39 - INFO - __main__ - Step 890 Global step 890 Train loss 1.08 on epoch=444
03/01/2022 18:47:41 - INFO - __main__ - Step 900 Global step 900 Train loss 1.08 on epoch=449
03/01/2022 18:47:43 - INFO - __main__ - Global step 900 Train loss 1.12 EM 0.0 on epoch=449
03/01/2022 18:47:45 - INFO - __main__ - Step 910 Global step 910 Train loss 1.14 on epoch=454
03/01/2022 18:47:47 - INFO - __main__ - Step 920 Global step 920 Train loss 1.05 on epoch=459
03/01/2022 18:47:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.96 on epoch=464
03/01/2022 18:47:52 - INFO - __main__ - Step 940 Global step 940 Train loss 1.02 on epoch=469
03/01/2022 18:47:54 - INFO - __main__ - Step 950 Global step 950 Train loss 1.10 on epoch=474
03/01/2022 18:47:55 - INFO - __main__ - Global step 950 Train loss 1.05 EM 0.0 on epoch=474
03/01/2022 18:47:58 - INFO - __main__ - Step 960 Global step 960 Train loss 1.05 on epoch=479
03/01/2022 18:48:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.99 on epoch=484
03/01/2022 18:48:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.00 on epoch=489
03/01/2022 18:48:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.99 on epoch=494
03/01/2022 18:48:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.99 on epoch=499
03/01/2022 18:48:08 - INFO - __main__ - Global step 1000 Train loss 1.00 EM 0.0 on epoch=499
03/01/2022 18:48:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.01 on epoch=504
03/01/2022 18:48:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.89 on epoch=509
03/01/2022 18:48:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.89 on epoch=514
03/01/2022 18:48:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.95 on epoch=519
03/01/2022 18:48:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.93 on epoch=524
03/01/2022 18:48:21 - INFO - __main__ - Global step 1050 Train loss 0.93 EM 0.0 on epoch=524
03/01/2022 18:48:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.96 on epoch=529
03/01/2022 18:48:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.01 on epoch=534
03/01/2022 18:48:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.86 on epoch=539
03/01/2022 18:48:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.83 on epoch=544
03/01/2022 18:48:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.93 on epoch=549
03/01/2022 18:48:34 - INFO - __main__ - Global step 1100 Train loss 0.92 EM 0.0 on epoch=549
03/01/2022 18:48:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.90 on epoch=554
03/01/2022 18:48:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=559
03/01/2022 18:48:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=564
03/01/2022 18:48:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.87 on epoch=569
03/01/2022 18:48:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.84 on epoch=574
03/01/2022 18:48:47 - INFO - __main__ - Global step 1150 Train loss 0.86 EM 0.0 on epoch=574
03/01/2022 18:48:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.77 on epoch=579
03/01/2022 18:48:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.77 on epoch=584
03/01/2022 18:48:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=589
03/01/2022 18:48:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.77 on epoch=594
03/01/2022 18:48:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.77 on epoch=599
03/01/2022 18:48:59 - INFO - __main__ - Global step 1200 Train loss 0.77 EM 0.0 on epoch=599
03/01/2022 18:49:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.75 on epoch=604
03/01/2022 18:49:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.69 on epoch=609
03/01/2022 18:49:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.78 on epoch=614
03/01/2022 18:49:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=619
03/01/2022 18:49:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.65 on epoch=624
03/01/2022 18:49:12 - INFO - __main__ - Global step 1250 Train loss 0.72 EM 0.0 on epoch=624
03/01/2022 18:49:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
03/01/2022 18:49:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
03/01/2022 18:49:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.67 on epoch=639
03/01/2022 18:49:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.67 on epoch=644
03/01/2022 18:49:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.64 on epoch=649
03/01/2022 18:49:25 - INFO - __main__ - Global step 1300 Train loss 0.67 EM 0.03125 on epoch=649
03/01/2022 18:49:25 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=649, global_step=1300
03/01/2022 18:49:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.65 on epoch=654
03/01/2022 18:49:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.63 on epoch=659
03/01/2022 18:49:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.63 on epoch=664
03/01/2022 18:49:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.59 on epoch=669
03/01/2022 18:49:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.69 on epoch=674
03/01/2022 18:49:38 - INFO - __main__ - Global step 1350 Train loss 0.64 EM 0.03125 on epoch=674
03/01/2022 18:49:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.64 on epoch=679
03/01/2022 18:49:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.67 on epoch=684
03/01/2022 18:49:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.65 on epoch=689
03/01/2022 18:49:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.57 on epoch=694
03/01/2022 18:49:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.69 on epoch=699
03/01/2022 18:49:50 - INFO - __main__ - Global step 1400 Train loss 0.65 EM 0.0 on epoch=699
03/01/2022 18:49:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.65 on epoch=704
03/01/2022 18:49:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.61 on epoch=709
03/01/2022 18:49:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.66 on epoch=714
03/01/2022 18:50:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.60 on epoch=719
03/01/2022 18:50:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.65 on epoch=724
03/01/2022 18:50:03 - INFO - __main__ - Global step 1450 Train loss 0.63 EM 0.0 on epoch=724
03/01/2022 18:50:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.58 on epoch=729
03/01/2022 18:50:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=734
03/01/2022 18:50:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.54 on epoch=739
03/01/2022 18:50:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.58 on epoch=744
03/01/2022 18:50:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
03/01/2022 18:50:16 - INFO - __main__ - Global step 1500 Train loss 0.58 EM 0.0 on epoch=749
03/01/2022 18:50:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.50 on epoch=754
03/01/2022 18:50:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
03/01/2022 18:50:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.55 on epoch=764
03/01/2022 18:50:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=769
03/01/2022 18:50:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=774
03/01/2022 18:50:35 - INFO - __main__ - Global step 1550 Train loss 0.53 EM 0.0 on epoch=774
03/01/2022 18:50:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=779
03/01/2022 18:50:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=784
03/01/2022 18:50:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.55 on epoch=789
03/01/2022 18:50:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.50 on epoch=794
03/01/2022 18:50:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
03/01/2022 18:50:48 - INFO - __main__ - Global step 1600 Train loss 0.52 EM 0.0 on epoch=799
03/01/2022 18:50:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
03/01/2022 18:50:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=809
03/01/2022 18:50:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
03/01/2022 18:50:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.53 on epoch=819
03/01/2022 18:50:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.52 on epoch=824
03/01/2022 18:51:00 - INFO - __main__ - Global step 1650 Train loss 0.54 EM 0.0 on epoch=824
03/01/2022 18:51:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=829
03/01/2022 18:51:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
03/01/2022 18:51:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=839
03/01/2022 18:51:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=844
03/01/2022 18:51:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=849
03/01/2022 18:51:12 - INFO - __main__ - Global step 1700 Train loss 0.51 EM 0.0 on epoch=849
03/01/2022 18:51:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=854
03/01/2022 18:51:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=859
03/01/2022 18:51:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
03/01/2022 18:51:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
03/01/2022 18:51:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=874
03/01/2022 18:51:25 - INFO - __main__ - Global step 1750 Train loss 0.49 EM 0.0 on epoch=874
03/01/2022 18:51:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
03/01/2022 18:51:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
03/01/2022 18:51:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
03/01/2022 18:51:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
03/01/2022 18:51:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=899
03/01/2022 18:51:37 - INFO - __main__ - Global step 1800 Train loss 0.43 EM 0.0 on epoch=899
03/01/2022 18:51:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=904
03/01/2022 18:51:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=909
03/01/2022 18:51:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=914
03/01/2022 18:51:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
03/01/2022 18:51:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
03/01/2022 18:51:50 - INFO - __main__ - Global step 1850 Train loss 0.45 EM 0.0 on epoch=924
03/01/2022 18:51:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
03/01/2022 18:51:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=934
03/01/2022 18:51:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
03/01/2022 18:51:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
03/01/2022 18:52:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
03/01/2022 18:52:03 - INFO - __main__ - Global step 1900 Train loss 0.42 EM 0.0 on epoch=949
03/01/2022 18:52:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
03/01/2022 18:52:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/01/2022 18:52:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
03/01/2022 18:52:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
03/01/2022 18:52:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
03/01/2022 18:52:15 - INFO - __main__ - Global step 1950 Train loss 0.38 EM 0.0 on epoch=974
03/01/2022 18:52:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=979
03/01/2022 18:52:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=984
03/01/2022 18:52:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=989
03/01/2022 18:52:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
03/01/2022 18:52:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
03/01/2022 18:52:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:52:28 - INFO - __main__ - Printing 3 examples
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 18:52:28 - INFO - __main__ - ['ok computer']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 18:52:28 - INFO - __main__ - ['thursday']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 18:52:28 - INFO - __main__ - ['sigourney weaver']
03/01/2022 18:52:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 18:52:28 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:52:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:52:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:52:28 - INFO - __main__ - Printing 3 examples
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 18:52:28 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 18:52:28 - INFO - __main__ - ['daphne du maurier']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 18:52:28 - INFO - __main__ - ['back to the future']
03/01/2022 18:52:28 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:52:28 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:52:28 - INFO - __main__ - Global step 2000 Train loss 0.40 EM 0.0 on epoch=999
03/01/2022 18:52:28 - INFO - __main__ - save last model!
03/01/2022 18:52:28 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:52:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 18:52:28 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 18:52:28 - INFO - __main__ - Printing 3 examples
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 18:52:28 - INFO - __main__ - ['taming of the shrew']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 18:52:28 - INFO - __main__ - ['henry fonda']
03/01/2022 18:52:28 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 18:52:28 - INFO - __main__ - ['tchaikovsky']
03/01/2022 18:52:28 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:52:30 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:52:34 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 18:52:42 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:52:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:52:43 - INFO - __main__ - Starting training!
03/01/2022 18:55:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_42_0.2_8_predictions.txt
03/01/2022 18:55:21 - INFO - __main__ - EM on test data: 0.0048
03/01/2022 18:55:22 - INFO - __main__ - prefix=freebase_qa_32_42, lr=0.2, bsz=8, dev_performance=0.03125, test_performance=0.004757135703555333
03/01/2022 18:55:22 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.5, bsz=8 ...
03/01/2022 18:55:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:55:23 - INFO - __main__ - Printing 3 examples
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 18:55:23 - INFO - __main__ - ['ok computer']
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 18:55:23 - INFO - __main__ - ['thursday']
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 18:55:23 - INFO - __main__ - ['sigourney weaver']
03/01/2022 18:55:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 18:55:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:55:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 18:55:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 18:55:23 - INFO - __main__ - Printing 3 examples
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 18:55:23 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 18:55:23 - INFO - __main__ - ['daphne du maurier']
03/01/2022 18:55:23 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 18:55:23 - INFO - __main__ - ['back to the future']
03/01/2022 18:55:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 18:55:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 18:55:23 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 18:55:35 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 18:55:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 18:55:36 - INFO - __main__ - Starting training!
03/01/2022 18:55:39 - INFO - __main__ - Step 10 Global step 10 Train loss 3.67 on epoch=4
03/01/2022 18:55:41 - INFO - __main__ - Step 20 Global step 20 Train loss 3.01 on epoch=9
03/01/2022 18:55:43 - INFO - __main__ - Step 30 Global step 30 Train loss 2.89 on epoch=14
03/01/2022 18:55:45 - INFO - __main__ - Step 40 Global step 40 Train loss 2.79 on epoch=19
03/01/2022 18:55:47 - INFO - __main__ - Step 50 Global step 50 Train loss 2.81 on epoch=24
03/01/2022 18:55:50 - INFO - __main__ - Global step 50 Train loss 3.03 EM 0.0 on epoch=24
03/01/2022 18:55:50 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 18:55:52 - INFO - __main__ - Step 60 Global step 60 Train loss 2.66 on epoch=29
03/01/2022 18:55:54 - INFO - __main__ - Step 70 Global step 70 Train loss 2.61 on epoch=34
03/01/2022 18:55:56 - INFO - __main__ - Step 80 Global step 80 Train loss 2.58 on epoch=39
03/01/2022 18:55:59 - INFO - __main__ - Step 90 Global step 90 Train loss 2.50 on epoch=44
03/01/2022 18:56:01 - INFO - __main__ - Step 100 Global step 100 Train loss 2.38 on epoch=49
03/01/2022 18:56:03 - INFO - __main__ - Global step 100 Train loss 2.55 EM 0.0 on epoch=49
03/01/2022 18:56:05 - INFO - __main__ - Step 110 Global step 110 Train loss 2.28 on epoch=54
03/01/2022 18:56:07 - INFO - __main__ - Step 120 Global step 120 Train loss 2.25 on epoch=59
03/01/2022 18:56:09 - INFO - __main__ - Step 130 Global step 130 Train loss 2.07 on epoch=64
03/01/2022 18:56:12 - INFO - __main__ - Step 140 Global step 140 Train loss 2.07 on epoch=69
03/01/2022 18:56:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.97 on epoch=74
03/01/2022 18:56:15 - INFO - __main__ - Global step 150 Train loss 2.13 EM 0.0 on epoch=74
03/01/2022 18:56:17 - INFO - __main__ - Step 160 Global step 160 Train loss 1.97 on epoch=79
03/01/2022 18:56:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.86 on epoch=84
03/01/2022 18:56:21 - INFO - __main__ - Step 180 Global step 180 Train loss 1.66 on epoch=89
03/01/2022 18:56:23 - INFO - __main__ - Step 190 Global step 190 Train loss 1.72 on epoch=94
03/01/2022 18:56:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.64 on epoch=99
03/01/2022 18:56:27 - INFO - __main__ - Global step 200 Train loss 1.77 EM 0.0 on epoch=99
03/01/2022 18:56:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.57 on epoch=104
03/01/2022 18:56:31 - INFO - __main__ - Step 220 Global step 220 Train loss 1.60 on epoch=109
03/01/2022 18:56:33 - INFO - __main__ - Step 230 Global step 230 Train loss 1.54 on epoch=114
03/01/2022 18:56:35 - INFO - __main__ - Step 240 Global step 240 Train loss 1.51 on epoch=119
03/01/2022 18:56:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.38 on epoch=124
03/01/2022 18:56:39 - INFO - __main__ - Global step 250 Train loss 1.52 EM 0.0 on epoch=124
03/01/2022 18:56:41 - INFO - __main__ - Step 260 Global step 260 Train loss 1.37 on epoch=129
03/01/2022 18:56:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.31 on epoch=134
03/01/2022 18:56:45 - INFO - __main__ - Step 280 Global step 280 Train loss 1.35 on epoch=139
03/01/2022 18:56:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.28 on epoch=144
03/01/2022 18:56:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.28 on epoch=149
03/01/2022 18:56:51 - INFO - __main__ - Global step 300 Train loss 1.32 EM 0.0 on epoch=149
03/01/2022 18:56:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.24 on epoch=154
03/01/2022 18:56:55 - INFO - __main__ - Step 320 Global step 320 Train loss 1.14 on epoch=159
03/01/2022 18:56:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.22 on epoch=164
03/01/2022 18:56:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.08 on epoch=169
03/01/2022 18:57:01 - INFO - __main__ - Step 350 Global step 350 Train loss 1.04 on epoch=174
03/01/2022 18:57:03 - INFO - __main__ - Global step 350 Train loss 1.14 EM 0.0 on epoch=174
03/01/2022 18:57:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.05 on epoch=179
03/01/2022 18:57:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.05 on epoch=184
03/01/2022 18:57:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=189
03/01/2022 18:57:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=194
03/01/2022 18:57:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=199
03/01/2022 18:57:15 - INFO - __main__ - Global step 400 Train loss 0.99 EM 0.0 on epoch=199
03/01/2022 18:57:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=204
03/01/2022 18:57:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=209
03/01/2022 18:57:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=214
03/01/2022 18:57:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.74 on epoch=219
03/01/2022 18:57:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=224
03/01/2022 18:57:27 - INFO - __main__ - Global step 450 Train loss 0.80 EM 0.0 on epoch=224
03/01/2022 18:57:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=229
03/01/2022 18:57:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=234
03/01/2022 18:57:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.68 on epoch=239
03/01/2022 18:57:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=244
03/01/2022 18:57:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=249
03/01/2022 18:57:40 - INFO - __main__ - Global step 500 Train loss 0.70 EM 0.0 on epoch=249
03/01/2022 18:57:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=254
03/01/2022 18:57:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=259
03/01/2022 18:57:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=264
03/01/2022 18:57:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=269
03/01/2022 18:57:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.73 on epoch=274
03/01/2022 18:57:52 - INFO - __main__ - Global step 550 Train loss 0.64 EM 0.0 on epoch=274
03/01/2022 18:57:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
03/01/2022 18:57:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=284
03/01/2022 18:57:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=289
03/01/2022 18:58:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=294
03/01/2022 18:58:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=299
03/01/2022 18:58:04 - INFO - __main__ - Global step 600 Train loss 0.58 EM 0.0 on epoch=299
03/01/2022 18:58:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=304
03/01/2022 18:58:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=309
03/01/2022 18:58:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=314
03/01/2022 18:58:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=319
03/01/2022 18:58:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=324
03/01/2022 18:58:16 - INFO - __main__ - Global step 650 Train loss 0.52 EM 0.0 on epoch=324
03/01/2022 18:58:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=329
03/01/2022 18:58:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=334
03/01/2022 18:58:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
03/01/2022 18:58:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/01/2022 18:58:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=349
03/01/2022 18:58:28 - INFO - __main__ - Global step 700 Train loss 0.44 EM 0.0 on epoch=349
03/01/2022 18:58:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
03/01/2022 18:58:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=359
03/01/2022 18:58:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=364
03/01/2022 18:58:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=369
03/01/2022 18:58:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=374
03/01/2022 18:58:41 - INFO - __main__ - Global step 750 Train loss 0.45 EM 0.0 on epoch=374
03/01/2022 18:58:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=379
03/01/2022 18:58:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=384
03/01/2022 18:58:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=389
03/01/2022 18:58:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=394
03/01/2022 18:58:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=399
03/01/2022 18:58:54 - INFO - __main__ - Global step 800 Train loss 0.40 EM 0.0 on epoch=399
03/01/2022 18:58:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=404
03/01/2022 18:58:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
03/01/2022 18:59:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=414
03/01/2022 18:59:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/01/2022 18:59:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/01/2022 18:59:06 - INFO - __main__ - Global step 850 Train loss 0.38 EM 0.0 on epoch=424
03/01/2022 18:59:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=429
03/01/2022 18:59:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=434
03/01/2022 18:59:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
03/01/2022 18:59:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
03/01/2022 18:59:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=449
03/01/2022 18:59:19 - INFO - __main__ - Global step 900 Train loss 0.35 EM 0.0 on epoch=449
03/01/2022 18:59:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/01/2022 18:59:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=459
03/01/2022 18:59:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
03/01/2022 18:59:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/01/2022 18:59:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=474
03/01/2022 18:59:31 - INFO - __main__ - Global step 950 Train loss 0.31 EM 0.0 on epoch=474
03/01/2022 18:59:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=479
03/01/2022 18:59:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=484
03/01/2022 18:59:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=489
03/01/2022 18:59:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/01/2022 18:59:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=499
03/01/2022 18:59:43 - INFO - __main__ - Global step 1000 Train loss 0.28 EM 0.0 on epoch=499
03/01/2022 18:59:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/01/2022 18:59:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
03/01/2022 18:59:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=514
03/01/2022 18:59:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=519
03/01/2022 18:59:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/01/2022 18:59:56 - INFO - __main__ - Global step 1050 Train loss 0.29 EM 0.0 on epoch=524
03/01/2022 18:59:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=529
03/01/2022 19:00:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=534
03/01/2022 19:00:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=539
03/01/2022 19:00:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
03/01/2022 19:00:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=549
03/01/2022 19:00:08 - INFO - __main__ - Global step 1100 Train loss 0.25 EM 0.0 on epoch=549
03/01/2022 19:00:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=554
03/01/2022 19:00:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/01/2022 19:00:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=564
03/01/2022 19:00:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=569
03/01/2022 19:00:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/01/2022 19:00:19 - INFO - __main__ - Global step 1150 Train loss 0.25 EM 0.0 on epoch=574
03/01/2022 19:00:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/01/2022 19:00:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=584
03/01/2022 19:00:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=589
03/01/2022 19:00:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/01/2022 19:00:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/01/2022 19:00:31 - INFO - __main__ - Global step 1200 Train loss 0.22 EM 0.0 on epoch=599
03/01/2022 19:00:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=604
03/01/2022 19:00:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/01/2022 19:00:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
03/01/2022 19:00:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 19:00:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 19:00:43 - INFO - __main__ - Global step 1250 Train loss 0.21 EM 0.0 on epoch=624
03/01/2022 19:00:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=629
03/01/2022 19:00:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/01/2022 19:00:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=639
03/01/2022 19:00:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=644
03/01/2022 19:00:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 19:00:55 - INFO - __main__ - Global step 1300 Train loss 0.21 EM 0.0 on epoch=649
03/01/2022 19:00:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=654
03/01/2022 19:00:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/01/2022 19:01:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/01/2022 19:01:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/01/2022 19:01:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/01/2022 19:01:07 - INFO - __main__ - Global step 1350 Train loss 0.20 EM 0.0 on epoch=674
03/01/2022 19:01:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
03/01/2022 19:01:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=684
03/01/2022 19:01:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=689
03/01/2022 19:01:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
03/01/2022 19:01:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
03/01/2022 19:01:19 - INFO - __main__ - Global step 1400 Train loss 0.17 EM 0.0 on epoch=699
03/01/2022 19:01:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
03/01/2022 19:01:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=709
03/01/2022 19:01:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=714
03/01/2022 19:01:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 19:01:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=724
03/01/2022 19:01:31 - INFO - __main__ - Global step 1450 Train loss 0.16 EM 0.0 on epoch=724
03/01/2022 19:01:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=729
03/01/2022 19:01:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
03/01/2022 19:01:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/01/2022 19:01:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
03/01/2022 19:01:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/01/2022 19:01:43 - INFO - __main__ - Global step 1500 Train loss 0.16 EM 0.0 on epoch=749
03/01/2022 19:01:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=754
03/01/2022 19:01:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=759
03/01/2022 19:01:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=764
03/01/2022 19:01:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 19:01:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
03/01/2022 19:01:54 - INFO - __main__ - Global step 1550 Train loss 0.16 EM 0.0 on epoch=774
03/01/2022 19:01:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=779
03/01/2022 19:01:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=784
03/01/2022 19:02:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/01/2022 19:02:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=794
03/01/2022 19:02:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=799
03/01/2022 19:02:06 - INFO - __main__ - Global step 1600 Train loss 0.18 EM 0.0 on epoch=799
03/01/2022 19:02:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
03/01/2022 19:02:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=809
03/01/2022 19:02:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/01/2022 19:02:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=819
03/01/2022 19:02:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=824
03/01/2022 19:02:18 - INFO - __main__ - Global step 1650 Train loss 0.11 EM 0.0 on epoch=824
03/01/2022 19:02:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=829
03/01/2022 19:02:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=834
03/01/2022 19:02:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
03/01/2022 19:02:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=844
03/01/2022 19:02:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=849
03/01/2022 19:02:30 - INFO - __main__ - Global step 1700 Train loss 0.13 EM 0.0 on epoch=849
03/01/2022 19:02:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=854
03/01/2022 19:02:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
03/01/2022 19:02:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 19:02:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 19:02:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/01/2022 19:02:42 - INFO - __main__ - Global step 1750 Train loss 0.13 EM 0.0 on epoch=874
03/01/2022 19:02:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=879
03/01/2022 19:02:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/01/2022 19:02:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
03/01/2022 19:02:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/01/2022 19:02:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 19:02:54 - INFO - __main__ - Global step 1800 Train loss 0.12 EM 0.0 on epoch=899
03/01/2022 19:02:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=904
03/01/2022 19:02:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/01/2022 19:03:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=914
03/01/2022 19:03:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 19:03:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=924
03/01/2022 19:03:07 - INFO - __main__ - Global step 1850 Train loss 0.12 EM 0.0 on epoch=924
03/01/2022 19:03:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=929
03/01/2022 19:03:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/01/2022 19:03:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=939
03/01/2022 19:03:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=944
03/01/2022 19:03:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/01/2022 19:03:20 - INFO - __main__ - Global step 1900 Train loss 0.11 EM 0.0 on epoch=949
03/01/2022 19:03:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=954
03/01/2022 19:03:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 19:03:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=964
03/01/2022 19:03:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 19:03:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/01/2022 19:03:33 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 19:03:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
03/01/2022 19:03:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
03/01/2022 19:03:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 19:03:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=994
03/01/2022 19:03:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=999
03/01/2022 19:03:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:03:46 - INFO - __main__ - Printing 3 examples
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:03:46 - INFO - __main__ - ['ok computer']
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:03:46 - INFO - __main__ - ['thursday']
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:03:46 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:03:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:03:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:03:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:03:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:03:46 - INFO - __main__ - Printing 3 examples
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:03:46 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:03:46 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:03:46 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:03:46 - INFO - __main__ - ['back to the future']
03/01/2022 19:03:46 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:03:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:03:46 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:03:46 - INFO - __main__ - Global step 2000 Train loss 0.10 EM 0.0 on epoch=999
03/01/2022 19:03:46 - INFO - __main__ - save last model!
03/01/2022 19:03:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:03:47 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:03:47 - INFO - __main__ - Printing 3 examples
03/01/2022 19:03:47 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:03:47 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:03:47 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:03:47 - INFO - __main__ - ['henry fonda']
03/01/2022 19:03:47 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:03:47 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:03:47 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:03:48 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:03:52 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:03:59 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:04:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:04:00 - INFO - __main__ - Starting training!
03/01/2022 19:06:52 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_87_0.5_8_predictions.txt
03/01/2022 19:06:52 - INFO - __main__ - EM on test data: 0.0045
03/01/2022 19:06:53 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.5, bsz=8, dev_performance=0.0, test_performance=0.004506760140210316
03/01/2022 19:06:53 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.4, bsz=8 ...
03/01/2022 19:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:06:54 - INFO - __main__ - Printing 3 examples
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:06:54 - INFO - __main__ - ['ok computer']
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:06:54 - INFO - __main__ - ['thursday']
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:06:54 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:06:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:06:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:06:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:06:54 - INFO - __main__ - Printing 3 examples
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:06:54 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:06:54 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:06:54 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:06:54 - INFO - __main__ - ['back to the future']
03/01/2022 19:06:54 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:06:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:06:54 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:07:06 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:07:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:07:07 - INFO - __main__ - Starting training!
03/01/2022 19:07:10 - INFO - __main__ - Step 10 Global step 10 Train loss 3.73 on epoch=4
03/01/2022 19:07:12 - INFO - __main__ - Step 20 Global step 20 Train loss 3.09 on epoch=9
03/01/2022 19:07:15 - INFO - __main__ - Step 30 Global step 30 Train loss 3.01 on epoch=14
03/01/2022 19:07:17 - INFO - __main__ - Step 40 Global step 40 Train loss 2.88 on epoch=19
03/01/2022 19:07:19 - INFO - __main__ - Step 50 Global step 50 Train loss 2.79 on epoch=24
03/01/2022 19:07:22 - INFO - __main__ - Global step 50 Train loss 3.10 EM 0.0 on epoch=24
03/01/2022 19:07:22 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:07:24 - INFO - __main__ - Step 60 Global step 60 Train loss 2.81 on epoch=29
03/01/2022 19:07:26 - INFO - __main__ - Step 70 Global step 70 Train loss 2.73 on epoch=34
03/01/2022 19:07:28 - INFO - __main__ - Step 80 Global step 80 Train loss 2.71 on epoch=39
03/01/2022 19:07:31 - INFO - __main__ - Step 90 Global step 90 Train loss 2.57 on epoch=44
03/01/2022 19:07:33 - INFO - __main__ - Step 100 Global step 100 Train loss 2.51 on epoch=49
03/01/2022 19:07:35 - INFO - __main__ - Global step 100 Train loss 2.67 EM 0.0 on epoch=49
03/01/2022 19:07:38 - INFO - __main__ - Step 110 Global step 110 Train loss 2.49 on epoch=54
03/01/2022 19:07:40 - INFO - __main__ - Step 120 Global step 120 Train loss 2.33 on epoch=59
03/01/2022 19:07:42 - INFO - __main__ - Step 130 Global step 130 Train loss 2.30 on epoch=64
03/01/2022 19:07:44 - INFO - __main__ - Step 140 Global step 140 Train loss 2.23 on epoch=69
03/01/2022 19:07:46 - INFO - __main__ - Step 150 Global step 150 Train loss 2.21 on epoch=74
03/01/2022 19:07:47 - INFO - __main__ - Global step 150 Train loss 2.31 EM 0.0 on epoch=74
03/01/2022 19:07:49 - INFO - __main__ - Step 160 Global step 160 Train loss 2.15 on epoch=79
03/01/2022 19:07:52 - INFO - __main__ - Step 170 Global step 170 Train loss 2.06 on epoch=84
03/01/2022 19:07:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.88 on epoch=89
03/01/2022 19:07:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.94 on epoch=94
03/01/2022 19:07:58 - INFO - __main__ - Step 200 Global step 200 Train loss 1.84 on epoch=99
03/01/2022 19:08:00 - INFO - __main__ - Global step 200 Train loss 1.97 EM 0.0 on epoch=99
03/01/2022 19:08:03 - INFO - __main__ - Step 210 Global step 210 Train loss 1.78 on epoch=104
03/01/2022 19:08:05 - INFO - __main__ - Step 220 Global step 220 Train loss 1.74 on epoch=109
03/01/2022 19:08:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.68 on epoch=114
03/01/2022 19:08:09 - INFO - __main__ - Step 240 Global step 240 Train loss 1.57 on epoch=119
03/01/2022 19:08:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.54 on epoch=124
03/01/2022 19:08:13 - INFO - __main__ - Global step 250 Train loss 1.66 EM 0.0 on epoch=124
03/01/2022 19:08:15 - INFO - __main__ - Step 260 Global step 260 Train loss 1.58 on epoch=129
03/01/2022 19:08:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.46 on epoch=134
03/01/2022 19:08:19 - INFO - __main__ - Step 280 Global step 280 Train loss 1.37 on epoch=139
03/01/2022 19:08:21 - INFO - __main__ - Step 290 Global step 290 Train loss 1.31 on epoch=144
03/01/2022 19:08:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.34 on epoch=149
03/01/2022 19:08:25 - INFO - __main__ - Global step 300 Train loss 1.41 EM 0.0 on epoch=149
03/01/2022 19:08:27 - INFO - __main__ - Step 310 Global step 310 Train loss 1.26 on epoch=154
03/01/2022 19:08:29 - INFO - __main__ - Step 320 Global step 320 Train loss 1.17 on epoch=159
03/01/2022 19:08:31 - INFO - __main__ - Step 330 Global step 330 Train loss 1.21 on epoch=164
03/01/2022 19:08:33 - INFO - __main__ - Step 340 Global step 340 Train loss 1.22 on epoch=169
03/01/2022 19:08:36 - INFO - __main__ - Step 350 Global step 350 Train loss 1.21 on epoch=174
03/01/2022 19:08:37 - INFO - __main__ - Global step 350 Train loss 1.21 EM 0.0 on epoch=174
03/01/2022 19:08:39 - INFO - __main__ - Step 360 Global step 360 Train loss 1.14 on epoch=179
03/01/2022 19:08:41 - INFO - __main__ - Step 370 Global step 370 Train loss 1.13 on epoch=184
03/01/2022 19:08:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.13 on epoch=189
03/01/2022 19:08:46 - INFO - __main__ - Step 390 Global step 390 Train loss 1.18 on epoch=194
03/01/2022 19:08:48 - INFO - __main__ - Step 400 Global step 400 Train loss 1.00 on epoch=199
03/01/2022 19:08:49 - INFO - __main__ - Global step 400 Train loss 1.12 EM 0.0 on epoch=199
03/01/2022 19:08:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=204
03/01/2022 19:08:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.96 on epoch=209
03/01/2022 19:08:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=214
03/01/2022 19:08:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
03/01/2022 19:09:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
03/01/2022 19:09:01 - INFO - __main__ - Global step 450 Train loss 0.92 EM 0.0 on epoch=224
03/01/2022 19:09:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.89 on epoch=229
03/01/2022 19:09:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.87 on epoch=234
03/01/2022 19:09:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=239
03/01/2022 19:09:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.84 on epoch=244
03/01/2022 19:09:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.76 on epoch=249
03/01/2022 19:09:13 - INFO - __main__ - Global step 500 Train loss 0.83 EM 0.0 on epoch=249
03/01/2022 19:09:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=254
03/01/2022 19:09:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.75 on epoch=259
03/01/2022 19:09:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=264
03/01/2022 19:09:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=269
03/01/2022 19:09:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.66 on epoch=274
03/01/2022 19:09:25 - INFO - __main__ - Global step 550 Train loss 0.73 EM 0.0 on epoch=274
03/01/2022 19:09:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.71 on epoch=279
03/01/2022 19:09:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=284
03/01/2022 19:09:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.64 on epoch=289
03/01/2022 19:09:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.69 on epoch=294
03/01/2022 19:09:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=299
03/01/2022 19:09:37 - INFO - __main__ - Global step 600 Train loss 0.67 EM 0.0 on epoch=299
03/01/2022 19:09:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
03/01/2022 19:09:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.59 on epoch=309
03/01/2022 19:09:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=314
03/01/2022 19:09:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=319
03/01/2022 19:09:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=324
03/01/2022 19:09:48 - INFO - __main__ - Global step 650 Train loss 0.61 EM 0.0 on epoch=324
03/01/2022 19:09:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
03/01/2022 19:09:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=334
03/01/2022 19:09:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
03/01/2022 19:09:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=344
03/01/2022 19:09:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.57 on epoch=349
03/01/2022 19:10:00 - INFO - __main__ - Global step 700 Train loss 0.55 EM 0.0 on epoch=349
03/01/2022 19:10:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=354
03/01/2022 19:10:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=359
03/01/2022 19:10:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=364
03/01/2022 19:10:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=369
03/01/2022 19:10:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
03/01/2022 19:10:12 - INFO - __main__ - Global step 750 Train loss 0.54 EM 0.0 on epoch=374
03/01/2022 19:10:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=379
03/01/2022 19:10:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=384
03/01/2022 19:10:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=389
03/01/2022 19:10:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
03/01/2022 19:10:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=399
03/01/2022 19:10:24 - INFO - __main__ - Global step 800 Train loss 0.49 EM 0.0 on epoch=399
03/01/2022 19:10:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=404
03/01/2022 19:10:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=409
03/01/2022 19:10:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/01/2022 19:10:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/01/2022 19:10:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=424
03/01/2022 19:10:35 - INFO - __main__ - Global step 850 Train loss 0.43 EM 0.0 on epoch=424
03/01/2022 19:10:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=429
03/01/2022 19:10:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=434
03/01/2022 19:10:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/01/2022 19:10:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
03/01/2022 19:10:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=449
03/01/2022 19:10:47 - INFO - __main__ - Global step 900 Train loss 0.42 EM 0.0 on epoch=449
03/01/2022 19:10:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=454
03/01/2022 19:10:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/01/2022 19:10:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
03/01/2022 19:10:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/01/2022 19:10:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 19:10:59 - INFO - __main__ - Global step 950 Train loss 0.39 EM 0.0 on epoch=474
03/01/2022 19:11:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/01/2022 19:11:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=484
03/01/2022 19:11:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/01/2022 19:11:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=494
03/01/2022 19:11:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=499
03/01/2022 19:11:11 - INFO - __main__ - Global step 1000 Train loss 0.34 EM 0.0 on epoch=499
03/01/2022 19:11:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/01/2022 19:11:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=509
03/01/2022 19:11:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=514
03/01/2022 19:11:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/01/2022 19:11:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/01/2022 19:11:22 - INFO - __main__ - Global step 1050 Train loss 0.30 EM 0.0 on epoch=524
03/01/2022 19:11:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=529
03/01/2022 19:11:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=534
03/01/2022 19:11:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=539
03/01/2022 19:11:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=544
03/01/2022 19:11:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=549
03/01/2022 19:11:34 - INFO - __main__ - Global step 1100 Train loss 0.29 EM 0.0 on epoch=549
03/01/2022 19:11:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
03/01/2022 19:11:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/01/2022 19:11:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/01/2022 19:11:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
03/01/2022 19:11:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/01/2022 19:11:46 - INFO - __main__ - Global step 1150 Train loss 0.24 EM 0.0 on epoch=574
03/01/2022 19:11:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=579
03/01/2022 19:11:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/01/2022 19:11:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=589
03/01/2022 19:11:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
03/01/2022 19:11:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/01/2022 19:11:57 - INFO - __main__ - Global step 1200 Train loss 0.26 EM 0.0 on epoch=599
03/01/2022 19:12:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/01/2022 19:12:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/01/2022 19:12:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=614
03/01/2022 19:12:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 19:12:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 19:12:09 - INFO - __main__ - Global step 1250 Train loss 0.24 EM 0.0 on epoch=624
03/01/2022 19:12:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/01/2022 19:12:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=634
03/01/2022 19:12:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/01/2022 19:12:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=644
03/01/2022 19:12:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=649
03/01/2022 19:12:21 - INFO - __main__ - Global step 1300 Train loss 0.23 EM 0.0 on epoch=649
03/01/2022 19:12:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
03/01/2022 19:12:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=659
03/01/2022 19:12:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/01/2022 19:12:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=669
03/01/2022 19:12:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=674
03/01/2022 19:12:33 - INFO - __main__ - Global step 1350 Train loss 0.23 EM 0.0 on epoch=674
03/01/2022 19:12:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/01/2022 19:12:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=684
03/01/2022 19:12:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/01/2022 19:12:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
03/01/2022 19:12:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/01/2022 19:12:45 - INFO - __main__ - Global step 1400 Train loss 0.21 EM 0.0 on epoch=699
03/01/2022 19:12:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 19:12:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
03/01/2022 19:12:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=714
03/01/2022 19:12:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 19:12:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/01/2022 19:12:57 - INFO - __main__ - Global step 1450 Train loss 0.19 EM 0.0 on epoch=724
03/01/2022 19:12:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 19:13:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=734
03/01/2022 19:13:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=739
03/01/2022 19:13:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=744
03/01/2022 19:13:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
03/01/2022 19:13:09 - INFO - __main__ - Global step 1500 Train loss 0.18 EM 0.0 on epoch=749
03/01/2022 19:13:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=754
03/01/2022 19:13:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=759
03/01/2022 19:13:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=764
03/01/2022 19:13:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=769
03/01/2022 19:13:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=774
03/01/2022 19:13:20 - INFO - __main__ - Global step 1550 Train loss 0.18 EM 0.0 on epoch=774
03/01/2022 19:13:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=779
03/01/2022 19:13:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/01/2022 19:13:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=789
03/01/2022 19:13:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=794
03/01/2022 19:13:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=799
03/01/2022 19:13:32 - INFO - __main__ - Global step 1600 Train loss 0.18 EM 0.0 on epoch=799
03/01/2022 19:13:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=804
03/01/2022 19:13:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/01/2022 19:13:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 19:13:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=819
03/01/2022 19:13:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=824
03/01/2022 19:13:44 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 19:13:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
03/01/2022 19:13:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/01/2022 19:13:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=839
03/01/2022 19:13:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=844
03/01/2022 19:13:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=849
03/01/2022 19:13:56 - INFO - __main__ - Global step 1700 Train loss 0.17 EM 0.0 on epoch=849
03/01/2022 19:13:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=854
03/01/2022 19:14:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=859
03/01/2022 19:14:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=864
03/01/2022 19:14:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=869
03/01/2022 19:14:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/01/2022 19:14:08 - INFO - __main__ - Global step 1750 Train loss 0.17 EM 0.0 on epoch=874
03/01/2022 19:14:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/01/2022 19:14:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/01/2022 19:14:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
03/01/2022 19:14:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=894
03/01/2022 19:14:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 19:14:20 - INFO - __main__ - Global step 1800 Train loss 0.13 EM 0.0 on epoch=899
03/01/2022 19:14:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 19:14:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=909
03/01/2022 19:14:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=914
03/01/2022 19:14:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/01/2022 19:14:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=924
03/01/2022 19:14:32 - INFO - __main__ - Global step 1850 Train loss 0.15 EM 0.0 on epoch=924
03/01/2022 19:14:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=929
03/01/2022 19:14:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=934
03/01/2022 19:14:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
03/01/2022 19:14:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/01/2022 19:14:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/01/2022 19:14:45 - INFO - __main__ - Global step 1900 Train loss 0.12 EM 0.0 on epoch=949
03/01/2022 19:14:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/01/2022 19:14:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
03/01/2022 19:14:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=964
03/01/2022 19:14:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/01/2022 19:14:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=974
03/01/2022 19:14:56 - INFO - __main__ - Global step 1950 Train loss 0.14 EM 0.0 on epoch=974
03/01/2022 19:14:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
03/01/2022 19:15:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/01/2022 19:15:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 19:15:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=994
03/01/2022 19:15:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=999
03/01/2022 19:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:15:08 - INFO - __main__ - Printing 3 examples
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:15:08 - INFO - __main__ - ['ok computer']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:15:08 - INFO - __main__ - ['thursday']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:15:08 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:15:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:15:08 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:15:08 - INFO - __main__ - Global step 2000 Train loss 0.16 EM 0.0 on epoch=999
03/01/2022 19:15:08 - INFO - __main__ - save last model!
03/01/2022 19:15:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:15:08 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:15:08 - INFO - __main__ - Loaded 32 examples from train data
03/01/2022 19:15:08 - INFO - __main__ - Printing 3 examples
use DistributedSampler
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:15:08 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:15:08 - INFO - __main__ - ['henry fonda']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:15:08 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:15:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:15:08 - INFO - __main__ - Printing 3 examples
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:15:08 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:15:08 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:15:08 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:15:08 - INFO - __main__ - ['back to the future']
03/01/2022 19:15:08 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:15:08 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:15:08 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:15:08 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:15:10 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:15:14 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:15:23 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:15:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:15:23 - INFO - __main__ - Starting training!
03/01/2022 19:18:13 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_87_0.4_8_predictions.txt
03/01/2022 19:18:13 - INFO - __main__ - EM on test data: 0.0063
03/01/2022 19:18:13 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.006259389083625438
03/01/2022 19:18:13 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.3, bsz=8 ...
03/01/2022 19:18:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:18:14 - INFO - __main__ - Printing 3 examples
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:18:14 - INFO - __main__ - ['ok computer']
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:18:14 - INFO - __main__ - ['thursday']
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:18:14 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:18:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:18:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:18:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:18:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:18:14 - INFO - __main__ - Printing 3 examples
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:18:14 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:18:14 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:18:14 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:18:14 - INFO - __main__ - ['back to the future']
03/01/2022 19:18:14 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:18:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:18:14 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:18:28 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:18:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:18:29 - INFO - __main__ - Starting training!
03/01/2022 19:18:32 - INFO - __main__ - Step 10 Global step 10 Train loss 3.78 on epoch=4
03/01/2022 19:18:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.19 on epoch=9
03/01/2022 19:18:36 - INFO - __main__ - Step 30 Global step 30 Train loss 2.93 on epoch=14
03/01/2022 19:18:38 - INFO - __main__ - Step 40 Global step 40 Train loss 2.94 on epoch=19
03/01/2022 19:18:41 - INFO - __main__ - Step 50 Global step 50 Train loss 2.83 on epoch=24
03/01/2022 19:18:44 - INFO - __main__ - Global step 50 Train loss 3.13 EM 0.0 on epoch=24
03/01/2022 19:18:44 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 19:18:46 - INFO - __main__ - Step 60 Global step 60 Train loss 2.83 on epoch=29
03/01/2022 19:18:48 - INFO - __main__ - Step 70 Global step 70 Train loss 2.76 on epoch=34
03/01/2022 19:18:51 - INFO - __main__ - Step 80 Global step 80 Train loss 2.73 on epoch=39
03/01/2022 19:18:53 - INFO - __main__ - Step 90 Global step 90 Train loss 2.66 on epoch=44
03/01/2022 19:18:55 - INFO - __main__ - Step 100 Global step 100 Train loss 2.53 on epoch=49
03/01/2022 19:18:58 - INFO - __main__ - Global step 100 Train loss 2.70 EM 0.0 on epoch=49
03/01/2022 19:19:00 - INFO - __main__ - Step 110 Global step 110 Train loss 2.55 on epoch=54
03/01/2022 19:19:03 - INFO - __main__ - Step 120 Global step 120 Train loss 2.44 on epoch=59
03/01/2022 19:19:05 - INFO - __main__ - Step 130 Global step 130 Train loss 2.45 on epoch=64
03/01/2022 19:19:07 - INFO - __main__ - Step 140 Global step 140 Train loss 2.31 on epoch=69
03/01/2022 19:19:10 - INFO - __main__ - Step 150 Global step 150 Train loss 2.37 on epoch=74
03/01/2022 19:19:12 - INFO - __main__ - Global step 150 Train loss 2.42 EM 0.0 on epoch=74
03/01/2022 19:19:15 - INFO - __main__ - Step 160 Global step 160 Train loss 2.29 on epoch=79
03/01/2022 19:19:17 - INFO - __main__ - Step 170 Global step 170 Train loss 2.27 on epoch=84
03/01/2022 19:19:19 - INFO - __main__ - Step 180 Global step 180 Train loss 2.10 on epoch=89
03/01/2022 19:19:21 - INFO - __main__ - Step 190 Global step 190 Train loss 2.09 on epoch=94
03/01/2022 19:19:24 - INFO - __main__ - Step 200 Global step 200 Train loss 2.07 on epoch=99
03/01/2022 19:19:25 - INFO - __main__ - Global step 200 Train loss 2.16 EM 0.0 on epoch=99
03/01/2022 19:19:27 - INFO - __main__ - Step 210 Global step 210 Train loss 2.05 on epoch=104
03/01/2022 19:19:30 - INFO - __main__ - Step 220 Global step 220 Train loss 1.97 on epoch=109
03/01/2022 19:19:32 - INFO - __main__ - Step 230 Global step 230 Train loss 1.99 on epoch=114
03/01/2022 19:19:34 - INFO - __main__ - Step 240 Global step 240 Train loss 1.94 on epoch=119
03/01/2022 19:19:37 - INFO - __main__ - Step 250 Global step 250 Train loss 1.85 on epoch=124
03/01/2022 19:19:38 - INFO - __main__ - Global step 250 Train loss 1.96 EM 0.0 on epoch=124
03/01/2022 19:19:40 - INFO - __main__ - Step 260 Global step 260 Train loss 1.86 on epoch=129
03/01/2022 19:19:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.85 on epoch=134
03/01/2022 19:19:45 - INFO - __main__ - Step 280 Global step 280 Train loss 1.69 on epoch=139
03/01/2022 19:19:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.68 on epoch=144
03/01/2022 19:19:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.66 on epoch=149
03/01/2022 19:19:51 - INFO - __main__ - Global step 300 Train loss 1.75 EM 0.0 on epoch=149
03/01/2022 19:19:53 - INFO - __main__ - Step 310 Global step 310 Train loss 1.69 on epoch=154
03/01/2022 19:19:55 - INFO - __main__ - Step 320 Global step 320 Train loss 1.56 on epoch=159
03/01/2022 19:19:58 - INFO - __main__ - Step 330 Global step 330 Train loss 1.50 on epoch=164
03/01/2022 19:20:00 - INFO - __main__ - Step 340 Global step 340 Train loss 1.53 on epoch=169
03/01/2022 19:20:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.54 on epoch=174
03/01/2022 19:20:04 - INFO - __main__ - Global step 350 Train loss 1.56 EM 0.0 on epoch=174
03/01/2022 19:20:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.45 on epoch=179
03/01/2022 19:20:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.48 on epoch=184
03/01/2022 19:20:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.43 on epoch=189
03/01/2022 19:20:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.39 on epoch=194
03/01/2022 19:20:15 - INFO - __main__ - Step 400 Global step 400 Train loss 1.31 on epoch=199
03/01/2022 19:20:16 - INFO - __main__ - Global step 400 Train loss 1.41 EM 0.0 on epoch=199
03/01/2022 19:20:19 - INFO - __main__ - Step 410 Global step 410 Train loss 1.35 on epoch=204
03/01/2022 19:20:21 - INFO - __main__ - Step 420 Global step 420 Train loss 1.37 on epoch=209
03/01/2022 19:20:23 - INFO - __main__ - Step 430 Global step 430 Train loss 1.29 on epoch=214
03/01/2022 19:20:26 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
03/01/2022 19:20:28 - INFO - __main__ - Step 450 Global step 450 Train loss 1.27 on epoch=224
03/01/2022 19:20:29 - INFO - __main__ - Global step 450 Train loss 1.29 EM 0.0 on epoch=224
03/01/2022 19:20:31 - INFO - __main__ - Step 460 Global step 460 Train loss 1.24 on epoch=229
03/01/2022 19:20:34 - INFO - __main__ - Step 470 Global step 470 Train loss 1.24 on epoch=234
03/01/2022 19:20:36 - INFO - __main__ - Step 480 Global step 480 Train loss 1.14 on epoch=239
03/01/2022 19:20:38 - INFO - __main__ - Step 490 Global step 490 Train loss 1.19 on epoch=244
03/01/2022 19:20:40 - INFO - __main__ - Step 500 Global step 500 Train loss 1.21 on epoch=249
03/01/2022 19:20:42 - INFO - __main__ - Global step 500 Train loss 1.20 EM 0.0 on epoch=249
03/01/2022 19:20:44 - INFO - __main__ - Step 510 Global step 510 Train loss 1.16 on epoch=254
03/01/2022 19:20:47 - INFO - __main__ - Step 520 Global step 520 Train loss 1.13 on epoch=259
03/01/2022 19:20:49 - INFO - __main__ - Step 530 Global step 530 Train loss 1.09 on epoch=264
03/01/2022 19:20:51 - INFO - __main__ - Step 540 Global step 540 Train loss 1.09 on epoch=269
03/01/2022 19:20:53 - INFO - __main__ - Step 550 Global step 550 Train loss 1.03 on epoch=274
03/01/2022 19:20:55 - INFO - __main__ - Global step 550 Train loss 1.10 EM 0.0 on epoch=274
03/01/2022 19:20:57 - INFO - __main__ - Step 560 Global step 560 Train loss 1.08 on epoch=279
03/01/2022 19:20:59 - INFO - __main__ - Step 570 Global step 570 Train loss 1.11 on epoch=284
03/01/2022 19:21:02 - INFO - __main__ - Step 580 Global step 580 Train loss 1.06 on epoch=289
03/01/2022 19:21:04 - INFO - __main__ - Step 590 Global step 590 Train loss 1.07 on epoch=294
03/01/2022 19:21:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.99 on epoch=299
03/01/2022 19:21:07 - INFO - __main__ - Global step 600 Train loss 1.06 EM 0.0 on epoch=299
03/01/2022 19:21:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.97 on epoch=304
03/01/2022 19:21:12 - INFO - __main__ - Step 620 Global step 620 Train loss 1.00 on epoch=309
03/01/2022 19:21:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.88 on epoch=314
03/01/2022 19:21:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.85 on epoch=319
03/01/2022 19:21:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.85 on epoch=324
03/01/2022 19:21:20 - INFO - __main__ - Global step 650 Train loss 0.91 EM 0.0 on epoch=324
03/01/2022 19:21:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.82 on epoch=329
03/01/2022 19:21:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.85 on epoch=334
03/01/2022 19:21:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=339
03/01/2022 19:21:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.73 on epoch=344
03/01/2022 19:21:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.77 on epoch=349
03/01/2022 19:21:33 - INFO - __main__ - Global step 700 Train loss 0.79 EM 0.0 on epoch=349
03/01/2022 19:21:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.75 on epoch=354
03/01/2022 19:21:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.77 on epoch=359
03/01/2022 19:21:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=364
03/01/2022 19:21:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.73 on epoch=369
03/01/2022 19:21:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.61 on epoch=374
03/01/2022 19:21:45 - INFO - __main__ - Global step 750 Train loss 0.71 EM 0.0 on epoch=374
03/01/2022 19:21:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.59 on epoch=379
03/01/2022 19:21:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.63 on epoch=384
03/01/2022 19:21:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=389
03/01/2022 19:21:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=394
03/01/2022 19:21:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=399
03/01/2022 19:21:58 - INFO - __main__ - Global step 800 Train loss 0.60 EM 0.0 on epoch=399
03/01/2022 19:22:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.67 on epoch=404
03/01/2022 19:22:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.63 on epoch=409
03/01/2022 19:22:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.58 on epoch=414
03/01/2022 19:22:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.61 on epoch=419
03/01/2022 19:22:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=424
03/01/2022 19:22:11 - INFO - __main__ - Global step 850 Train loss 0.61 EM 0.0 on epoch=424
03/01/2022 19:22:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=429
03/01/2022 19:22:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=434
03/01/2022 19:22:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=439
03/01/2022 19:22:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.57 on epoch=444
03/01/2022 19:22:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
03/01/2022 19:22:24 - INFO - __main__ - Global step 900 Train loss 0.54 EM 0.0 on epoch=449
03/01/2022 19:22:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
03/01/2022 19:22:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
03/01/2022 19:22:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
03/01/2022 19:22:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=469
03/01/2022 19:22:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=474
03/01/2022 19:22:36 - INFO - __main__ - Global step 950 Train loss 0.50 EM 0.0 on epoch=474
03/01/2022 19:22:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=479
03/01/2022 19:22:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
03/01/2022 19:22:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=489
03/01/2022 19:22:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=494
03/01/2022 19:22:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=499
03/01/2022 19:22:49 - INFO - __main__ - Global step 1000 Train loss 0.47 EM 0.0 on epoch=499
03/01/2022 19:22:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
03/01/2022 19:22:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=509
03/01/2022 19:22:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=514
03/01/2022 19:22:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=519
03/01/2022 19:23:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=524
03/01/2022 19:23:02 - INFO - __main__ - Global step 1050 Train loss 0.46 EM 0.0 on epoch=524
03/01/2022 19:23:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=529
03/01/2022 19:23:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=534
03/01/2022 19:23:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=539
03/01/2022 19:23:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=544
03/01/2022 19:23:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=549
03/01/2022 19:23:15 - INFO - __main__ - Global step 1100 Train loss 0.42 EM 0.0 on epoch=549
03/01/2022 19:23:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=554
03/01/2022 19:23:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
03/01/2022 19:23:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
03/01/2022 19:23:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=569
03/01/2022 19:23:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=574
03/01/2022 19:23:27 - INFO - __main__ - Global step 1150 Train loss 0.39 EM 0.0 on epoch=574
03/01/2022 19:23:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=579
03/01/2022 19:23:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
03/01/2022 19:23:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=589
03/01/2022 19:23:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
03/01/2022 19:23:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=599
03/01/2022 19:23:40 - INFO - __main__ - Global step 1200 Train loss 0.35 EM 0.0 on epoch=599
03/01/2022 19:23:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
03/01/2022 19:23:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
03/01/2022 19:23:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=614
03/01/2022 19:23:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=619
03/01/2022 19:23:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=624
03/01/2022 19:23:53 - INFO - __main__ - Global step 1250 Train loss 0.36 EM 0.0 on epoch=624
03/01/2022 19:23:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
03/01/2022 19:23:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=634
03/01/2022 19:24:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=639
03/01/2022 19:24:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=644
03/01/2022 19:24:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=649
03/01/2022 19:24:05 - INFO - __main__ - Global step 1300 Train loss 0.34 EM 0.0 on epoch=649
03/01/2022 19:24:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=654
03/01/2022 19:24:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=659
03/01/2022 19:24:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=664
03/01/2022 19:24:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=669
03/01/2022 19:24:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
03/01/2022 19:24:18 - INFO - __main__ - Global step 1350 Train loss 0.32 EM 0.0 on epoch=674
03/01/2022 19:24:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=679
03/01/2022 19:24:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=684
03/01/2022 19:24:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=689
03/01/2022 19:24:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=694
03/01/2022 19:24:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=699
03/01/2022 19:24:31 - INFO - __main__ - Global step 1400 Train loss 0.31 EM 0.0 on epoch=699
03/01/2022 19:24:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=704
03/01/2022 19:24:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
03/01/2022 19:24:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
03/01/2022 19:24:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=719
03/01/2022 19:24:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=724
03/01/2022 19:24:44 - INFO - __main__ - Global step 1450 Train loss 0.29 EM 0.0 on epoch=724
03/01/2022 19:24:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
03/01/2022 19:24:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=734
03/01/2022 19:24:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=739
03/01/2022 19:24:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=744
03/01/2022 19:24:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=749
03/01/2022 19:24:57 - INFO - __main__ - Global step 1500 Train loss 0.29 EM 0.0 on epoch=749
03/01/2022 19:24:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
03/01/2022 19:25:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=759
03/01/2022 19:25:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/01/2022 19:25:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=769
03/01/2022 19:25:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=774
03/01/2022 19:25:09 - INFO - __main__ - Global step 1550 Train loss 0.28 EM 0.0 on epoch=774
03/01/2022 19:25:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=779
03/01/2022 19:25:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
03/01/2022 19:25:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=789
03/01/2022 19:25:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/01/2022 19:25:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=799
03/01/2022 19:25:22 - INFO - __main__ - Global step 1600 Train loss 0.25 EM 0.0 on epoch=799
03/01/2022 19:25:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
03/01/2022 19:25:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=809
03/01/2022 19:25:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=814
03/01/2022 19:25:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=819
03/01/2022 19:25:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.22 on epoch=824
03/01/2022 19:25:35 - INFO - __main__ - Global step 1650 Train loss 0.23 EM 0.0 on epoch=824
03/01/2022 19:25:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=829
03/01/2022 19:25:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
03/01/2022 19:25:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=839
03/01/2022 19:25:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
03/01/2022 19:25:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=849
03/01/2022 19:25:47 - INFO - __main__ - Global step 1700 Train loss 0.25 EM 0.0 on epoch=849
03/01/2022 19:25:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
03/01/2022 19:25:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=859
03/01/2022 19:25:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/01/2022 19:25:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=869
03/01/2022 19:25:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=874
03/01/2022 19:26:00 - INFO - __main__ - Global step 1750 Train loss 0.23 EM 0.0 on epoch=874
03/01/2022 19:26:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=879
03/01/2022 19:26:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=884
03/01/2022 19:26:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=889
03/01/2022 19:26:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=894
03/01/2022 19:26:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=899
03/01/2022 19:26:13 - INFO - __main__ - Global step 1800 Train loss 0.20 EM 0.0 on epoch=899
03/01/2022 19:26:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=904
03/01/2022 19:26:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
03/01/2022 19:26:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=914
03/01/2022 19:26:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=919
03/01/2022 19:26:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
03/01/2022 19:26:26 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 19:26:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=929
03/01/2022 19:26:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=934
03/01/2022 19:26:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=939
03/01/2022 19:26:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=944
03/01/2022 19:26:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/01/2022 19:26:38 - INFO - __main__ - Global step 1900 Train loss 0.21 EM 0.0 on epoch=949
03/01/2022 19:26:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=954
03/01/2022 19:26:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=959
03/01/2022 19:26:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=964
03/01/2022 19:26:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=969
03/01/2022 19:26:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=974
03/01/2022 19:26:51 - INFO - __main__ - Global step 1950 Train loss 0.18 EM 0.0 on epoch=974
03/01/2022 19:26:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
03/01/2022 19:26:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
03/01/2022 19:26:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=989
03/01/2022 19:27:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=994
03/01/2022 19:27:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=999
03/01/2022 19:27:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:27:04 - INFO - __main__ - Printing 3 examples
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:27:04 - INFO - __main__ - ['ok computer']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:27:04 - INFO - __main__ - ['thursday']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:27:04 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:27:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:27:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:27:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:27:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:27:04 - INFO - __main__ - Printing 3 examples
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:27:04 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:27:04 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:27:04 - INFO - __main__ - ['back to the future']
03/01/2022 19:27:04 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:27:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:27:04 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:27:04 - INFO - __main__ - Global step 2000 Train loss 0.20 EM 0.0 on epoch=999
03/01/2022 19:27:04 - INFO - __main__ - save last model!
03/01/2022 19:27:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:27:04 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:27:04 - INFO - __main__ - Printing 3 examples
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:27:04 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:27:04 - INFO - __main__ - ['henry fonda']
03/01/2022 19:27:04 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:27:04 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:27:04 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:27:06 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:27:09 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:27:18 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:27:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:27:19 - INFO - __main__ - Starting training!
03/01/2022 19:30:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_87_0.3_8_predictions.txt
03/01/2022 19:30:10 - INFO - __main__ - EM on test data: 0.0055
03/01/2022 19:30:10 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.005508262393590386
03/01/2022 19:30:10 - INFO - __main__ - Running ... prefix=freebase_qa_32_87, lr=0.2, bsz=8 ...
03/01/2022 19:30:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:30:11 - INFO - __main__ - Printing 3 examples
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] Which 1997 album, voted the best ever by readers of Q magazine, contains tracks let Down', 'No Surprises' and 'Paranoid Android'?
03/01/2022 19:30:11 - INFO - __main__ - ['ok computer']
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] Which day of the week is named after the Norse god of thunder?
03/01/2022 19:30:11 - INFO - __main__ - ['thursday']
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] Who shaved her head to play the character Lt Ellen Ripley in the 1992 film 'Alien3'?
03/01/2022 19:30:11 - INFO - __main__ - ['sigourney weaver']
03/01/2022 19:30:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:30:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:30:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:30:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:30:11 - INFO - __main__ - Printing 3 examples
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] What type of creature was Lonesome George, who died in 2012 and who gained fame as the rarest creature in the world (hint: he was aged perhaps more than 100 years)?
03/01/2022 19:30:11 - INFO - __main__ - ['pinta island tortoise']
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] Who wrote the story on which Alfred Hitchcock's 1963 film The Birds was based?
03/01/2022 19:30:11 - INFO - __main__ - ['daphne du maurier']
03/01/2022 19:30:11 - INFO - __main__ -  [freebase_qa] Dr Emmett Brown was a character in which series of films?
03/01/2022 19:30:11 - INFO - __main__ - ['back to the future']
03/01/2022 19:30:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:30:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:30:11 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:30:23 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:30:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:30:24 - INFO - __main__ - Starting training!
03/01/2022 19:30:27 - INFO - __main__ - Step 10 Global step 10 Train loss 3.83 on epoch=4
03/01/2022 19:30:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.40 on epoch=9
03/01/2022 19:30:31 - INFO - __main__ - Step 30 Global step 30 Train loss 3.07 on epoch=14
03/01/2022 19:30:33 - INFO - __main__ - Step 40 Global step 40 Train loss 2.95 on epoch=19
03/01/2022 19:30:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.94 on epoch=24
03/01/2022 19:30:38 - INFO - __main__ - Global step 50 Train loss 3.24 EM 0.03125 on epoch=24
03/01/2022 19:30:38 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/01/2022 19:30:40 - INFO - __main__ - Step 60 Global step 60 Train loss 2.80 on epoch=29
03/01/2022 19:30:42 - INFO - __main__ - Step 70 Global step 70 Train loss 2.83 on epoch=34
03/01/2022 19:30:44 - INFO - __main__ - Step 80 Global step 80 Train loss 2.86 on epoch=39
03/01/2022 19:30:46 - INFO - __main__ - Step 90 Global step 90 Train loss 2.80 on epoch=44
03/01/2022 19:30:49 - INFO - __main__ - Step 100 Global step 100 Train loss 2.74 on epoch=49
03/01/2022 19:30:50 - INFO - __main__ - Global step 100 Train loss 2.81 EM 0.0 on epoch=49
03/01/2022 19:30:52 - INFO - __main__ - Step 110 Global step 110 Train loss 2.71 on epoch=54
03/01/2022 19:30:54 - INFO - __main__ - Step 120 Global step 120 Train loss 2.67 on epoch=59
03/01/2022 19:30:56 - INFO - __main__ - Step 130 Global step 130 Train loss 2.65 on epoch=64
03/01/2022 19:30:59 - INFO - __main__ - Step 140 Global step 140 Train loss 2.67 on epoch=69
03/01/2022 19:31:01 - INFO - __main__ - Step 150 Global step 150 Train loss 2.63 on epoch=74
03/01/2022 19:31:03 - INFO - __main__ - Global step 150 Train loss 2.67 EM 0.0 on epoch=74
03/01/2022 19:31:05 - INFO - __main__ - Step 160 Global step 160 Train loss 2.48 on epoch=79
03/01/2022 19:31:07 - INFO - __main__ - Step 170 Global step 170 Train loss 2.55 on epoch=84
03/01/2022 19:31:10 - INFO - __main__ - Step 180 Global step 180 Train loss 2.43 on epoch=89
03/01/2022 19:31:12 - INFO - __main__ - Step 190 Global step 190 Train loss 2.48 on epoch=94
03/01/2022 19:31:14 - INFO - __main__ - Step 200 Global step 200 Train loss 2.42 on epoch=99
03/01/2022 19:31:16 - INFO - __main__ - Global step 200 Train loss 2.47 EM 0.0 on epoch=99
03/01/2022 19:31:18 - INFO - __main__ - Step 210 Global step 210 Train loss 2.34 on epoch=104
03/01/2022 19:31:21 - INFO - __main__ - Step 220 Global step 220 Train loss 2.30 on epoch=109
03/01/2022 19:31:23 - INFO - __main__ - Step 230 Global step 230 Train loss 2.22 on epoch=114
03/01/2022 19:31:25 - INFO - __main__ - Step 240 Global step 240 Train loss 2.18 on epoch=119
03/01/2022 19:31:27 - INFO - __main__ - Step 250 Global step 250 Train loss 2.19 on epoch=124
03/01/2022 19:31:29 - INFO - __main__ - Global step 250 Train loss 2.25 EM 0.0 on epoch=124
03/01/2022 19:31:32 - INFO - __main__ - Step 260 Global step 260 Train loss 2.13 on epoch=129
03/01/2022 19:31:34 - INFO - __main__ - Step 270 Global step 270 Train loss 2.12 on epoch=134
03/01/2022 19:31:36 - INFO - __main__ - Step 280 Global step 280 Train loss 2.09 on epoch=139
03/01/2022 19:31:38 - INFO - __main__ - Step 290 Global step 290 Train loss 2.03 on epoch=144
03/01/2022 19:31:40 - INFO - __main__ - Step 300 Global step 300 Train loss 1.92 on epoch=149
03/01/2022 19:31:41 - INFO - __main__ - Global step 300 Train loss 2.06 EM 0.0 on epoch=149
03/01/2022 19:31:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.95 on epoch=154
03/01/2022 19:31:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.89 on epoch=159
03/01/2022 19:31:47 - INFO - __main__ - Step 330 Global step 330 Train loss 1.84 on epoch=164
03/01/2022 19:31:50 - INFO - __main__ - Step 340 Global step 340 Train loss 1.82 on epoch=169
03/01/2022 19:31:52 - INFO - __main__ - Step 350 Global step 350 Train loss 1.82 on epoch=174
03/01/2022 19:31:53 - INFO - __main__ - Global step 350 Train loss 1.87 EM 0.0 on epoch=174
03/01/2022 19:31:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.82 on epoch=179
03/01/2022 19:31:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.82 on epoch=184
03/01/2022 19:31:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.75 on epoch=189
03/01/2022 19:32:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.64 on epoch=194
03/01/2022 19:32:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.64 on epoch=199
03/01/2022 19:32:05 - INFO - __main__ - Global step 400 Train loss 1.73 EM 0.0 on epoch=199
03/01/2022 19:32:07 - INFO - __main__ - Step 410 Global step 410 Train loss 1.68 on epoch=204
03/01/2022 19:32:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.66 on epoch=209
03/01/2022 19:32:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.64 on epoch=214
03/01/2022 19:32:13 - INFO - __main__ - Step 440 Global step 440 Train loss 1.69 on epoch=219
03/01/2022 19:32:15 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
03/01/2022 19:32:17 - INFO - __main__ - Global step 450 Train loss 1.64 EM 0.0 on epoch=224
03/01/2022 19:32:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.59 on epoch=229
03/01/2022 19:32:21 - INFO - __main__ - Step 470 Global step 470 Train loss 1.55 on epoch=234
03/01/2022 19:32:23 - INFO - __main__ - Step 480 Global step 480 Train loss 1.51 on epoch=239
03/01/2022 19:32:25 - INFO - __main__ - Step 490 Global step 490 Train loss 1.47 on epoch=244
03/01/2022 19:32:27 - INFO - __main__ - Step 500 Global step 500 Train loss 1.47 on epoch=249
03/01/2022 19:32:29 - INFO - __main__ - Global step 500 Train loss 1.52 EM 0.0 on epoch=249
03/01/2022 19:32:31 - INFO - __main__ - Step 510 Global step 510 Train loss 1.54 on epoch=254
03/01/2022 19:32:33 - INFO - __main__ - Step 520 Global step 520 Train loss 1.41 on epoch=259
03/01/2022 19:32:35 - INFO - __main__ - Step 530 Global step 530 Train loss 1.34 on epoch=264
03/01/2022 19:32:37 - INFO - __main__ - Step 540 Global step 540 Train loss 1.38 on epoch=269
03/01/2022 19:32:39 - INFO - __main__ - Step 550 Global step 550 Train loss 1.38 on epoch=274
03/01/2022 19:32:41 - INFO - __main__ - Global step 550 Train loss 1.41 EM 0.0 on epoch=274
03/01/2022 19:32:43 - INFO - __main__ - Step 560 Global step 560 Train loss 1.32 on epoch=279
03/01/2022 19:32:45 - INFO - __main__ - Step 570 Global step 570 Train loss 1.34 on epoch=284
03/01/2022 19:32:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.39 on epoch=289
03/01/2022 19:32:49 - INFO - __main__ - Step 590 Global step 590 Train loss 1.32 on epoch=294
03/01/2022 19:32:51 - INFO - __main__ - Step 600 Global step 600 Train loss 1.24 on epoch=299
03/01/2022 19:32:53 - INFO - __main__ - Global step 600 Train loss 1.32 EM 0.0 on epoch=299
03/01/2022 19:32:55 - INFO - __main__ - Step 610 Global step 610 Train loss 1.25 on epoch=304
03/01/2022 19:32:57 - INFO - __main__ - Step 620 Global step 620 Train loss 1.29 on epoch=309
03/01/2022 19:32:59 - INFO - __main__ - Step 630 Global step 630 Train loss 1.30 on epoch=314
03/01/2022 19:33:01 - INFO - __main__ - Step 640 Global step 640 Train loss 1.24 on epoch=319
03/01/2022 19:33:03 - INFO - __main__ - Step 650 Global step 650 Train loss 1.24 on epoch=324
03/01/2022 19:33:05 - INFO - __main__ - Global step 650 Train loss 1.26 EM 0.0 on epoch=324
03/01/2022 19:33:07 - INFO - __main__ - Step 660 Global step 660 Train loss 1.26 on epoch=329
03/01/2022 19:33:09 - INFO - __main__ - Step 670 Global step 670 Train loss 1.15 on epoch=334
03/01/2022 19:33:11 - INFO - __main__ - Step 680 Global step 680 Train loss 1.20 on epoch=339
03/01/2022 19:33:13 - INFO - __main__ - Step 690 Global step 690 Train loss 1.25 on epoch=344
03/01/2022 19:33:15 - INFO - __main__ - Step 700 Global step 700 Train loss 1.13 on epoch=349
03/01/2022 19:33:17 - INFO - __main__ - Global step 700 Train loss 1.20 EM 0.0 on epoch=349
03/01/2022 19:33:19 - INFO - __main__ - Step 710 Global step 710 Train loss 1.10 on epoch=354
03/01/2022 19:33:21 - INFO - __main__ - Step 720 Global step 720 Train loss 1.08 on epoch=359
03/01/2022 19:33:23 - INFO - __main__ - Step 730 Global step 730 Train loss 1.05 on epoch=364
03/01/2022 19:33:25 - INFO - __main__ - Step 740 Global step 740 Train loss 1.01 on epoch=369
03/01/2022 19:33:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.96 on epoch=374
03/01/2022 19:33:29 - INFO - __main__ - Global step 750 Train loss 1.04 EM 0.0 on epoch=374
03/01/2022 19:33:31 - INFO - __main__ - Step 760 Global step 760 Train loss 1.06 on epoch=379
03/01/2022 19:33:33 - INFO - __main__ - Step 770 Global step 770 Train loss 1.00 on epoch=384
03/01/2022 19:33:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.99 on epoch=389
03/01/2022 19:33:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.98 on epoch=394
03/01/2022 19:33:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.98 on epoch=399
03/01/2022 19:33:41 - INFO - __main__ - Global step 800 Train loss 1.00 EM 0.0 on epoch=399
03/01/2022 19:33:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.93 on epoch=404
03/01/2022 19:33:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.89 on epoch=409
03/01/2022 19:33:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.91 on epoch=414
03/01/2022 19:33:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.95 on epoch=419
03/01/2022 19:33:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.91 on epoch=424
03/01/2022 19:33:52 - INFO - __main__ - Global step 850 Train loss 0.92 EM 0.0 on epoch=424
03/01/2022 19:33:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=429
03/01/2022 19:33:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.95 on epoch=434
03/01/2022 19:33:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=439
03/01/2022 19:34:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.90 on epoch=444
03/01/2022 19:34:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.86 on epoch=449
03/01/2022 19:34:04 - INFO - __main__ - Global step 900 Train loss 0.90 EM 0.0 on epoch=449
03/01/2022 19:34:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=454
03/01/2022 19:34:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.88 on epoch=459
03/01/2022 19:34:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=464
03/01/2022 19:34:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=469
03/01/2022 19:34:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=474
03/01/2022 19:34:16 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 19:34:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.86 on epoch=479
03/01/2022 19:34:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.83 on epoch=484
03/01/2022 19:34:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.81 on epoch=489
03/01/2022 19:34:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.81 on epoch=494
03/01/2022 19:34:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.75 on epoch=499
03/01/2022 19:34:28 - INFO - __main__ - Global step 1000 Train loss 0.82 EM 0.0 on epoch=499
03/01/2022 19:34:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.82 on epoch=504
03/01/2022 19:34:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=509
03/01/2022 19:34:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.73 on epoch=514
03/01/2022 19:34:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.72 on epoch=519
03/01/2022 19:34:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.66 on epoch=524
03/01/2022 19:34:41 - INFO - __main__ - Global step 1050 Train loss 0.72 EM 0.0 on epoch=524
03/01/2022 19:34:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.73 on epoch=529
03/01/2022 19:34:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.71 on epoch=534
03/01/2022 19:34:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.76 on epoch=539
03/01/2022 19:34:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
03/01/2022 19:34:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.61 on epoch=549
03/01/2022 19:34:53 - INFO - __main__ - Global step 1100 Train loss 0.71 EM 0.0 on epoch=549
03/01/2022 19:34:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
03/01/2022 19:34:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.68 on epoch=559
03/01/2022 19:35:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.66 on epoch=564
03/01/2022 19:35:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.61 on epoch=569
03/01/2022 19:35:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.76 on epoch=574
03/01/2022 19:35:05 - INFO - __main__ - Global step 1150 Train loss 0.68 EM 0.0 on epoch=574
03/01/2022 19:35:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.63 on epoch=579
03/01/2022 19:35:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.56 on epoch=584
03/01/2022 19:35:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.66 on epoch=589
03/01/2022 19:35:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.59 on epoch=594
03/01/2022 19:35:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
03/01/2022 19:35:17 - INFO - __main__ - Global step 1200 Train loss 0.60 EM 0.0 on epoch=599
03/01/2022 19:35:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.55 on epoch=604
03/01/2022 19:35:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=609
03/01/2022 19:35:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.55 on epoch=614
03/01/2022 19:35:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.61 on epoch=619
03/01/2022 19:35:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.58 on epoch=624
03/01/2022 19:35:30 - INFO - __main__ - Global step 1250 Train loss 0.58 EM 0.0 on epoch=624
03/01/2022 19:35:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=629
03/01/2022 19:35:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=634
03/01/2022 19:35:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.56 on epoch=639
03/01/2022 19:35:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=644
03/01/2022 19:35:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=649
03/01/2022 19:35:42 - INFO - __main__ - Global step 1300 Train loss 0.55 EM 0.0 on epoch=649
03/01/2022 19:35:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.55 on epoch=654
03/01/2022 19:35:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
03/01/2022 19:35:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=664
03/01/2022 19:35:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
03/01/2022 19:35:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=674
03/01/2022 19:35:54 - INFO - __main__ - Global step 1350 Train loss 0.53 EM 0.0 on epoch=674
03/01/2022 19:35:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=679
03/01/2022 19:35:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
03/01/2022 19:36:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=689
03/01/2022 19:36:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=694
03/01/2022 19:36:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=699
03/01/2022 19:36:06 - INFO - __main__ - Global step 1400 Train loss 0.49 EM 0.0 on epoch=699
03/01/2022 19:36:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=704
03/01/2022 19:36:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.51 on epoch=709
03/01/2022 19:36:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
03/01/2022 19:36:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
03/01/2022 19:36:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
03/01/2022 19:36:18 - INFO - __main__ - Global step 1450 Train loss 0.47 EM 0.0 on epoch=724
03/01/2022 19:36:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=729
03/01/2022 19:36:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.46 on epoch=734
03/01/2022 19:36:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=739
03/01/2022 19:36:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
03/01/2022 19:36:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
03/01/2022 19:36:30 - INFO - __main__ - Global step 1500 Train loss 0.44 EM 0.0 on epoch=749
03/01/2022 19:36:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=754
03/01/2022 19:36:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=759
03/01/2022 19:36:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=764
03/01/2022 19:36:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
03/01/2022 19:36:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=774
03/01/2022 19:36:43 - INFO - __main__ - Global step 1550 Train loss 0.44 EM 0.0 on epoch=774
03/01/2022 19:36:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
03/01/2022 19:36:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=784
03/01/2022 19:36:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=789
03/01/2022 19:36:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
03/01/2022 19:36:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=799
03/01/2022 19:36:56 - INFO - __main__ - Global step 1600 Train loss 0.41 EM 0.0 on epoch=799
03/01/2022 19:36:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
03/01/2022 19:37:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/01/2022 19:37:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=814
03/01/2022 19:37:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=819
03/01/2022 19:37:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
03/01/2022 19:37:09 - INFO - __main__ - Global step 1650 Train loss 0.39 EM 0.0 on epoch=824
03/01/2022 19:37:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=829
03/01/2022 19:37:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=834
03/01/2022 19:37:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
03/01/2022 19:37:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=844
03/01/2022 19:37:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=849
03/01/2022 19:37:22 - INFO - __main__ - Global step 1700 Train loss 0.37 EM 0.0 on epoch=849
03/01/2022 19:37:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=854
03/01/2022 19:37:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
03/01/2022 19:37:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
03/01/2022 19:37:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
03/01/2022 19:37:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=874
03/01/2022 19:37:35 - INFO - __main__ - Global step 1750 Train loss 0.33 EM 0.0 on epoch=874
03/01/2022 19:37:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=879
03/01/2022 19:37:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
03/01/2022 19:37:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=889
03/01/2022 19:37:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
03/01/2022 19:37:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
03/01/2022 19:37:48 - INFO - __main__ - Global step 1800 Train loss 0.31 EM 0.0 on epoch=899
03/01/2022 19:37:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=904
03/01/2022 19:37:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
03/01/2022 19:37:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
03/01/2022 19:37:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=919
03/01/2022 19:37:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=924
03/01/2022 19:38:01 - INFO - __main__ - Global step 1850 Train loss 0.32 EM 0.0 on epoch=924
03/01/2022 19:38:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=929
03/01/2022 19:38:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=934
03/01/2022 19:38:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=939
03/01/2022 19:38:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=944
03/01/2022 19:38:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
03/01/2022 19:38:13 - INFO - __main__ - Global step 1900 Train loss 0.33 EM 0.0 on epoch=949
03/01/2022 19:38:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=954
03/01/2022 19:38:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
03/01/2022 19:38:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
03/01/2022 19:38:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
03/01/2022 19:38:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
03/01/2022 19:38:26 - INFO - __main__ - Global step 1950 Train loss 0.30 EM 0.0 on epoch=974
03/01/2022 19:38:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
03/01/2022 19:38:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
03/01/2022 19:38:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
03/01/2022 19:38:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=994
03/01/2022 19:38:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.29 on epoch=999
03/01/2022 19:38:39 - INFO - __main__ - Global step 2000 Train loss 0.29 EM 0.0 on epoch=999
03/01/2022 19:38:39 - INFO - __main__ - save last model!
03/01/2022 19:38:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:38:39 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 19:38:39 - INFO - __main__ - Printing 3 examples
03/01/2022 19:38:39 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 19:38:39 - INFO - __main__ - ['taming of the shrew']
03/01/2022 19:38:39 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 19:38:39 - INFO - __main__ - ['henry fonda']
03/01/2022 19:38:39 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 19:38:39 - INFO - __main__ - ['tchaikovsky']
03/01/2022 19:38:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:38:41 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:38:45 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 19:41:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_87_0.2_8_predictions.txt
03/01/2022 19:41:37 - INFO - __main__ - EM on test data: 0.0078
03/01/2022 19:41:37 - INFO - __main__ - prefix=freebase_qa_32_87, lr=0.2, bsz=8, dev_performance=0.03125, test_performance=0.007761642463695543
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003807544708251953 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2168", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 14009, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2169", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 14009, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 14009, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2451): No such process
Task: glue-qnli, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_zl94xjhy/none_p07622m8
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_0/1/error.json
03/01/2022 19:41:43 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 19:41:43 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
Output directory () already exists and is not empty.
03/01/2022 19:41:43 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 19:41:43 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 19:41:43 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 19:41:43 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 19:41:43 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 19:41:43 - INFO - __main__ - args.device: cuda:0
03/01/2022 19:41:43 - INFO - __main__ - Using 2 gpus
03/01/2022 19:41:43 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 19:41:43 - INFO - __main__ - args.device: cuda:1
03/01/2022 19:41:43 - INFO - __main__ - Using 2 gpus
03/01/2022 19:41:43 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
03/01/2022 19:41:43 - INFO - __main__ - Fine-tuning the following samples: ['glue-qnli_16_100', 'glue-qnli_16_13', 'glue-qnli_16_21', 'glue-qnli_16_42', 'glue-qnli_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 19:41:52 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.5, bsz=8 ...
03/01/2022 19:41:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:41:53 - INFO - __main__ - Printing 3 examples
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:41:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:41:53 - INFO - __main__ - Printing 3 examples
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:41:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:41:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:41:53 - INFO - __main__ - Printing 3 examples
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:41:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:41:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:41:53 - INFO - __main__ - Printing 3 examples
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 19:41:53 - INFO - __main__ - ['entailment']
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:41:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:41:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:41:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:42:08 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:42:08 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:42:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:42:09 - INFO - __main__ - Starting training!
03/01/2022 19:42:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:42:14 - INFO - __main__ - Starting training!
03/01/2022 19:42:17 - INFO - __main__ - Step 10 Global step 10 Train loss 0.60 on epoch=4
03/01/2022 19:42:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.24 on epoch=9
03/01/2022 19:42:21 - INFO - __main__ - Step 30 Global step 30 Train loss 0.17 on epoch=14
03/01/2022 19:42:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.17 on epoch=19
03/01/2022 19:42:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.19 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 19:42:27 - INFO - __main__ - Global step 50 Train loss 0.27 ACC 0.625 on epoch=24
03/01/2022 19:42:27 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.625 on epoch=24, global_step=50
03/01/2022 19:42:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 19:42:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.16 on epoch=34
03/01/2022 19:42:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 19:42:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 19:42:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 19:42:39 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5 on epoch=49
03/01/2022 19:42:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.13 on epoch=54
03/01/2022 19:42:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 19:42:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 19:42:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 19:42:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.14 on epoch=74
03/01/2022 19:42:51 - INFO - __main__ - Global step 150 Train loss 0.15 ACC 0.65625 on epoch=74
03/01/2022 19:42:51 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=74, global_step=150
03/01/2022 19:42:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.13 on epoch=79
03/01/2022 19:42:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
03/01/2022 19:42:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
03/01/2022 19:43:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
03/01/2022 19:43:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.12 on epoch=99
03/01/2022 19:43:03 - INFO - __main__ - Global step 200 Train loss 0.13 ACC 0.5625 on epoch=99
03/01/2022 19:43:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.12 on epoch=104
03/01/2022 19:43:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.10 on epoch=109
03/01/2022 19:43:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.10 on epoch=114
03/01/2022 19:43:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
03/01/2022 19:43:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.07 on epoch=124
03/01/2022 19:43:15 - INFO - __main__ - Global step 250 Train loss 0.10 ACC 0.59375 on epoch=124
03/01/2022 19:43:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.09 on epoch=129
03/01/2022 19:43:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
03/01/2022 19:43:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.05 on epoch=139
03/01/2022 19:43:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.07 on epoch=144
03/01/2022 19:43:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.04 on epoch=149
03/01/2022 19:43:27 - INFO - __main__ - Global step 300 Train loss 0.07 ACC 0.65625 on epoch=149
03/01/2022 19:43:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.04 on epoch=154
03/01/2022 19:43:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.05 on epoch=159
03/01/2022 19:43:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.02 on epoch=164
03/01/2022 19:43:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
03/01/2022 19:43:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.03 on epoch=174
03/01/2022 19:43:39 - INFO - __main__ - Global step 350 Train loss 0.03 ACC 0.5625 on epoch=174
03/01/2022 19:43:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.02 on epoch=179
03/01/2022 19:43:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.03 on epoch=184
03/01/2022 19:43:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/01/2022 19:43:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/01/2022 19:43:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/01/2022 19:43:52 - INFO - __main__ - Global step 400 Train loss 0.02 ACC 0.6875 on epoch=199
03/01/2022 19:43:52 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=199, global_step=400
03/01/2022 19:43:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/01/2022 19:43:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/01/2022 19:43:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/01/2022 19:44:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/01/2022 19:44:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/01/2022 19:44:04 - INFO - __main__ - Global step 450 Train loss 0.00 ACC 0.71875 on epoch=224
03/01/2022 19:44:04 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=224, global_step=450
03/01/2022 19:44:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/01/2022 19:44:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/01/2022 19:44:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/01/2022 19:44:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/01/2022 19:44:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/01/2022 19:44:16 - INFO - __main__ - Global step 500 Train loss 0.01 ACC 0.75 on epoch=249
03/01/2022 19:44:16 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=249, global_step=500
03/01/2022 19:44:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/01/2022 19:44:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/01/2022 19:44:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/01/2022 19:44:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/01/2022 19:44:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/01/2022 19:44:28 - INFO - __main__ - Global step 550 Train loss 0.00 ACC 0.71875 on epoch=274
03/01/2022 19:44:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/01/2022 19:44:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 19:44:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
03/01/2022 19:44:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/01/2022 19:44:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/01/2022 19:44:40 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.71875 on epoch=299
03/01/2022 19:44:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 19:44:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/01/2022 19:44:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/01/2022 19:44:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/01/2022 19:44:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 19:44:52 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.6875 on epoch=324
03/01/2022 19:44:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/01/2022 19:44:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/01/2022 19:44:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 19:45:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 19:45:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 19:45:04 - INFO - __main__ - Global step 700 Train loss 0.00 ACC 0.6875 on epoch=349
03/01/2022 19:45:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 19:45:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 19:45:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 19:45:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 19:45:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
03/01/2022 19:45:16 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.6875 on epoch=374
03/01/2022 19:45:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 19:45:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 19:45:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 19:45:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 19:45:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 19:45:28 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.6875 on epoch=399
03/01/2022 19:45:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 19:45:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 19:45:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 19:45:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 19:45:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 19:45:40 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.6875 on epoch=424
03/01/2022 19:45:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 19:45:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 19:45:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 19:45:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 19:45:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 19:45:52 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.71875 on epoch=449
03/01/2022 19:45:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/01/2022 19:45:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 19:45:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 19:46:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 19:46:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 19:46:04 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.65625 on epoch=474
03/01/2022 19:46:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/01/2022 19:46:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 19:46:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 19:46:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 19:46:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 19:46:16 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.65625 on epoch=499
03/01/2022 19:46:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 19:46:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 19:46:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 19:46:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 19:46:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 19:46:28 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.6875 on epoch=524
03/01/2022 19:46:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 19:46:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 19:46:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 19:46:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 19:46:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 19:46:40 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.6875 on epoch=549
03/01/2022 19:46:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 19:46:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 19:46:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 19:46:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 19:46:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 19:46:52 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.71875 on epoch=574
03/01/2022 19:46:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 19:46:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 19:46:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 19:47:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 19:47:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 19:47:04 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.71875 on epoch=599
03/01/2022 19:47:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 19:47:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 19:47:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/01/2022 19:47:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 19:47:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 19:47:16 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.6875 on epoch=624
03/01/2022 19:47:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 19:47:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 19:47:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 19:47:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 19:47:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 19:47:28 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.71875 on epoch=649
03/01/2022 19:47:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 19:47:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/01/2022 19:47:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 19:47:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 19:47:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 19:47:40 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.75 on epoch=674
03/01/2022 19:47:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 19:47:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 19:47:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 19:47:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 19:47:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 19:47:52 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.75 on epoch=699
03/01/2022 19:47:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 19:47:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 19:47:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 19:48:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 19:48:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 19:48:04 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.71875 on epoch=724
03/01/2022 19:48:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 19:48:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 19:48:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 19:48:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 19:48:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 19:48:17 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.71875 on epoch=749
03/01/2022 19:48:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 19:48:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 19:48:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 19:48:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 19:48:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 19:48:29 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.71875 on epoch=774
03/01/2022 19:48:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 19:48:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 19:48:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 19:48:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 19:48:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 19:48:41 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.75 on epoch=799
03/01/2022 19:48:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 19:48:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 19:48:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 19:48:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 19:48:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 19:48:53 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.75 on epoch=824
03/01/2022 19:48:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 19:48:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 19:48:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 19:49:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 19:49:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 19:49:05 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.8125 on epoch=849
03/01/2022 19:49:05 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.8125 on epoch=849, global_step=1700
03/01/2022 19:49:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 19:49:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 19:49:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 19:49:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 19:49:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 19:49:17 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.78125 on epoch=874
03/01/2022 19:49:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 19:49:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 19:49:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 19:49:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 19:49:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 19:49:29 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.78125 on epoch=899
03/01/2022 19:49:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 19:49:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 19:49:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 19:49:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 19:49:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 19:49:41 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.78125 on epoch=924
03/01/2022 19:49:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
03/01/2022 19:49:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 19:49:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 19:49:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 19:49:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 19:49:53 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.78125 on epoch=949
03/01/2022 19:49:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 19:49:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 19:50:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 19:50:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 19:50:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 19:50:05 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.75 on epoch=974
03/01/2022 19:50:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 19:50:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 19:50:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
03/01/2022 19:50:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 19:50:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/01/2022 19:50:17 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.6875 on epoch=999
03/01/2022 19:50:17 - INFO - __main__ - save last model!
03/01/2022 19:50:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 19:50:17 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 19:50:17 - INFO - __main__ - Printing 3 examples
03/01/2022 19:50:17 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 19:50:17 - INFO - __main__ - ['entailment']
03/01/2022 19:50:17 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 19:50:17 - INFO - __main__ - ['not_entailment']
03/01/2022 19:50:17 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 19:50:17 - INFO - __main__ - ['not_entailment']
03/01/2022 19:50:17 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:50:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:50:18 - INFO - __main__ - Printing 3 examples
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:50:18 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:50:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:50:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:50:18 - INFO - __main__ - Printing 3 examples
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 19:50:18 - INFO - __main__ - ['entailment']
03/01/2022 19:50:18 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:50:18 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:50:18 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:50:20 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:50:26 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 19:50:31 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:50:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:50:31 - INFO - __main__ - Starting training!
03/01/2022 19:53:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_100_0.5_8_predictions.txt
03/01/2022 19:53:16 - INFO - __main__ - ACC on test data: 0.5409
03/01/2022 19:53:16 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.5, bsz=8, dev_performance=0.8125, test_performance=0.5409115870400879
03/01/2022 19:53:16 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.4, bsz=8 ...
03/01/2022 19:53:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:53:17 - INFO - __main__ - Printing 3 examples
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 19:53:17 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:53:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 19:53:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 19:53:17 - INFO - __main__ - Printing 3 examples
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 19:53:17 - INFO - __main__ - ['entailment']
03/01/2022 19:53:17 - INFO - __main__ - Tokenizing Input ...
03/01/2022 19:53:17 - INFO - __main__ - Tokenizing Output ...
03/01/2022 19:53:17 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 19:53:31 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 19:53:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 19:53:32 - INFO - __main__ - Starting training!
03/01/2022 19:53:35 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=4
03/01/2022 19:53:37 - INFO - __main__ - Step 20 Global step 20 Train loss 0.24 on epoch=9
03/01/2022 19:53:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.21 on epoch=14
03/01/2022 19:53:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.16 on epoch=19
03/01/2022 19:53:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.18 on epoch=24
03/01/2022 19:53:45 - INFO - __main__ - Global step 50 Train loss 0.30 ACC 0.5 on epoch=24
03/01/2022 19:53:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 19:53:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.16 on epoch=29
03/01/2022 19:53:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.16 on epoch=34
03/01/2022 19:53:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 19:53:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.18 on epoch=44
03/01/2022 19:53:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 19:53:58 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5625 on epoch=49
03/01/2022 19:53:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=49, global_step=100
03/01/2022 19:54:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.16 on epoch=54
03/01/2022 19:54:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 19:54:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 19:54:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 19:54:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.14 on epoch=74
03/01/2022 19:54:11 - INFO - __main__ - Global step 150 Train loss 0.15 ACC 0.5 on epoch=74
03/01/2022 19:54:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 19:54:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 19:54:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
03/01/2022 19:54:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.17 on epoch=94
03/01/2022 19:54:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
03/01/2022 19:54:23 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.625 on epoch=99
03/01/2022 19:54:23 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=99, global_step=200
03/01/2022 19:54:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/01/2022 19:54:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/01/2022 19:54:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
03/01/2022 19:54:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
03/01/2022 19:54:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.13 on epoch=124
03/01/2022 19:54:36 - INFO - __main__ - Global step 250 Train loss 0.13 ACC 0.59375 on epoch=124
03/01/2022 19:54:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
03/01/2022 19:54:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/01/2022 19:54:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/01/2022 19:54:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
03/01/2022 19:54:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.11 on epoch=149
03/01/2022 19:54:49 - INFO - __main__ - Global step 300 Train loss 0.11 ACC 0.65625 on epoch=149
03/01/2022 19:54:49 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=149, global_step=300
03/01/2022 19:54:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.08 on epoch=154
03/01/2022 19:54:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
03/01/2022 19:54:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.08 on epoch=164
03/01/2022 19:54:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
03/01/2022 19:55:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
03/01/2022 19:55:02 - INFO - __main__ - Global step 350 Train loss 0.08 ACC 0.71875 on epoch=174
03/01/2022 19:55:02 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.71875 on epoch=174, global_step=350
03/01/2022 19:55:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
03/01/2022 19:55:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.09 on epoch=184
03/01/2022 19:55:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.03 on epoch=189
03/01/2022 19:55:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.04 on epoch=194
03/01/2022 19:55:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
03/01/2022 19:55:14 - INFO - __main__ - Global step 400 Train loss 0.05 ACC 0.71875 on epoch=199
03/01/2022 19:55:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.02 on epoch=204
03/01/2022 19:55:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/01/2022 19:55:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
03/01/2022 19:55:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
03/01/2022 19:55:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
03/01/2022 19:55:26 - INFO - __main__ - Global step 450 Train loss 0.03 ACC 0.75 on epoch=224
03/01/2022 19:55:26 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=224, global_step=450
03/01/2022 19:55:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
03/01/2022 19:55:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.02 on epoch=234
03/01/2022 19:55:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
03/01/2022 19:55:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/01/2022 19:55:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/01/2022 19:55:39 - INFO - __main__ - Global step 500 Train loss 0.01 ACC 0.6875 on epoch=249
03/01/2022 19:55:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/01/2022 19:55:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/01/2022 19:55:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/01/2022 19:55:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/01/2022 19:55:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/01/2022 19:55:51 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.6875 on epoch=274
03/01/2022 19:55:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 19:55:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 19:55:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 19:56:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 19:56:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/01/2022 19:56:03 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.59375 on epoch=299
03/01/2022 19:56:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 19:56:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/01/2022 19:56:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/01/2022 19:56:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/01/2022 19:56:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 19:56:15 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.75 on epoch=324
03/01/2022 19:56:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/01/2022 19:56:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/01/2022 19:56:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 19:56:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 19:56:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 19:56:28 - INFO - __main__ - Global step 700 Train loss 0.00 ACC 0.71875 on epoch=349
03/01/2022 19:56:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 19:56:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 19:56:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 19:56:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 19:56:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 19:56:40 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.78125 on epoch=374
03/01/2022 19:56:40 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=374, global_step=750
03/01/2022 19:56:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 19:56:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 19:56:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 19:56:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 19:56:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 19:56:52 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.6875 on epoch=399
03/01/2022 19:56:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 19:56:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 19:56:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 19:57:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 19:57:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 19:57:05 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.71875 on epoch=424
03/01/2022 19:57:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/01/2022 19:57:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 19:57:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 19:57:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 19:57:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 19:57:17 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.8125 on epoch=449
03/01/2022 19:57:17 - INFO - __main__ - Saving model with best ACC: 0.78125 -> 0.8125 on epoch=449, global_step=900
03/01/2022 19:57:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/01/2022 19:57:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 19:57:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 19:57:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 19:57:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 19:57:29 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.8125 on epoch=474
03/01/2022 19:57:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 19:57:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/01/2022 19:57:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 19:57:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/01/2022 19:57:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/01/2022 19:57:41 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
03/01/2022 19:57:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 19:57:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 19:57:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 19:57:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 19:57:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 19:57:54 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.75 on epoch=524
03/01/2022 19:57:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 19:57:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 19:58:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 19:58:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 19:58:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 19:58:06 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.8125 on epoch=549
03/01/2022 19:58:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 19:58:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 19:58:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 19:58:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 19:58:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 19:58:18 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.75 on epoch=574
03/01/2022 19:58:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 19:58:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 19:58:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 19:58:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 19:58:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/01/2022 19:58:31 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.625 on epoch=599
03/01/2022 19:58:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 19:58:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 19:58:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/01/2022 19:58:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 19:58:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 19:58:43 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.71875 on epoch=624
03/01/2022 19:58:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
03/01/2022 19:58:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 19:58:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 19:58:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 19:58:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 19:58:56 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.78125 on epoch=649
03/01/2022 19:58:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 19:59:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
03/01/2022 19:59:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 19:59:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 19:59:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 19:59:08 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.6875 on epoch=674
03/01/2022 19:59:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 19:59:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 19:59:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 19:59:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 19:59:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 19:59:20 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.84375 on epoch=699
03/01/2022 19:59:20 - INFO - __main__ - Saving model with best ACC: 0.8125 -> 0.84375 on epoch=699, global_step=1400
03/01/2022 19:59:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 19:59:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 19:59:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 19:59:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 19:59:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 19:59:32 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.84375 on epoch=724
03/01/2022 19:59:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 19:59:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 19:59:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 19:59:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 19:59:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 19:59:45 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.84375 on epoch=749
03/01/2022 19:59:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 19:59:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 19:59:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 19:59:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 19:59:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 19:59:57 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.8125 on epoch=774
03/01/2022 19:59:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:00:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:00:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:00:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:00:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 20:00:09 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.8125 on epoch=799
03/01/2022 20:00:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:00:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:00:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:00:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/01/2022 20:00:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 20:00:21 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.875 on epoch=824
03/01/2022 20:00:21 - INFO - __main__ - Saving model with best ACC: 0.84375 -> 0.875 on epoch=824, global_step=1650
03/01/2022 20:00:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:00:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 20:00:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:00:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 20:00:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:00:34 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.8125 on epoch=849
03/01/2022 20:00:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:00:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 20:00:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 20:00:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:00:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:00:46 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
03/01/2022 20:00:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 20:00:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 20:00:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 20:00:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:00:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 20:00:58 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5625 on epoch=899
03/01/2022 20:01:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:01:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 20:01:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:01:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:01:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 20:01:11 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.5625 on epoch=924
03/01/2022 20:01:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 20:01:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 20:01:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/01/2022 20:01:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 20:01:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 20:01:23 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.65625 on epoch=949
03/01/2022 20:01:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 20:01:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 20:01:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 20:01:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 20:01:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 20:01:35 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.78125 on epoch=974
03/01/2022 20:01:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 20:01:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 20:01:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 20:01:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 20:01:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 20:01:47 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.8125 on epoch=999
03/01/2022 20:01:47 - INFO - __main__ - save last model!
03/01/2022 20:01:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:01:47 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 20:01:47 - INFO - __main__ - Printing 3 examples
03/01/2022 20:01:47 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 20:01:47 - INFO - __main__ - ['entailment']
03/01/2022 20:01:47 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 20:01:47 - INFO - __main__ - ['not_entailment']
03/01/2022 20:01:47 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 20:01:47 - INFO - __main__ - ['not_entailment']
03/01/2022 20:01:47 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:01:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:01:48 - INFO - __main__ - Printing 3 examples
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:01:48 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:01:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:01:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:01:48 - INFO - __main__ - Printing 3 examples
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 20:01:48 - INFO - __main__ - ['entailment']
03/01/2022 20:01:48 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:01:48 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:01:48 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:01:50 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:01:56 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 20:02:02 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:02:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:02:03 - INFO - __main__ - Starting training!
03/01/2022 20:04:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_100_0.4_8_predictions.txt
03/01/2022 20:04:47 - INFO - __main__ - ACC on test data: 0.5404
03/01/2022 20:04:50 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.4, bsz=8, dev_performance=0.875, test_performance=0.5403624382207578
03/01/2022 20:04:50 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.3, bsz=8 ...
03/01/2022 20:04:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:04:51 - INFO - __main__ - Printing 3 examples
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:04:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:04:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:04:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:04:51 - INFO - __main__ - Printing 3 examples
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 20:04:51 - INFO - __main__ - ['entailment']
03/01/2022 20:04:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:04:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:04:51 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:05:03 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:05:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:05:03 - INFO - __main__ - Starting training!
03/01/2022 20:05:06 - INFO - __main__ - Step 10 Global step 10 Train loss 0.77 on epoch=4
03/01/2022 20:05:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.29 on epoch=9
03/01/2022 20:05:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.23 on epoch=14
03/01/2022 20:05:13 - INFO - __main__ - Step 40 Global step 40 Train loss 0.22 on epoch=19
03/01/2022 20:05:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.18 on epoch=24
03/01/2022 20:05:16 - INFO - __main__ - Global step 50 Train loss 0.34 ACC 0.5 on epoch=24
03/01/2022 20:05:16 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 20:05:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 20:05:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 20:05:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.16 on epoch=39
03/01/2022 20:05:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 20:05:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.20 on epoch=49
03/01/2022 20:05:29 - INFO - __main__ - Global step 100 Train loss 0.18 ACC 0.5 on epoch=49
03/01/2022 20:05:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 20:05:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
03/01/2022 20:05:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 20:05:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 20:05:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
03/01/2022 20:05:41 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.65625 on epoch=74
03/01/2022 20:05:41 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=74, global_step=150
03/01/2022 20:05:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.14 on epoch=79
03/01/2022 20:05:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 20:05:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 20:05:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 20:05:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 20:05:53 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 20:05:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
03/01/2022 20:05:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.12 on epoch=109
03/01/2022 20:06:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
03/01/2022 20:06:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
03/01/2022 20:06:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
03/01/2022 20:06:06 - INFO - __main__ - Global step 250 Train loss 0.13 ACC 0.53125 on epoch=124
03/01/2022 20:06:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.12 on epoch=129
03/01/2022 20:06:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.12 on epoch=134
03/01/2022 20:06:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.13 on epoch=139
03/01/2022 20:06:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
03/01/2022 20:06:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.09 on epoch=149
03/01/2022 20:06:18 - INFO - __main__ - Global step 300 Train loss 0.12 ACC 0.625 on epoch=149
03/01/2022 20:06:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.08 on epoch=154
03/01/2022 20:06:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
03/01/2022 20:06:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
03/01/2022 20:06:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.06 on epoch=169
03/01/2022 20:06:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.08 on epoch=174
03/01/2022 20:06:31 - INFO - __main__ - Global step 350 Train loss 0.07 ACC 0.6875 on epoch=174
03/01/2022 20:06:31 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=174, global_step=350
03/01/2022 20:06:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
03/01/2022 20:06:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
03/01/2022 20:06:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.03 on epoch=189
03/01/2022 20:06:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
03/01/2022 20:06:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
03/01/2022 20:06:43 - INFO - __main__ - Global step 400 Train loss 0.06 ACC 0.8125 on epoch=199
03/01/2022 20:06:43 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.8125 on epoch=199, global_step=400
03/01/2022 20:06:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/01/2022 20:06:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
03/01/2022 20:06:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
03/01/2022 20:06:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
03/01/2022 20:06:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/01/2022 20:06:55 - INFO - __main__ - Global step 450 Train loss 0.03 ACC 0.6875 on epoch=224
03/01/2022 20:06:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
03/01/2022 20:07:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
03/01/2022 20:07:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
03/01/2022 20:07:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/01/2022 20:07:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/01/2022 20:07:08 - INFO - __main__ - Global step 500 Train loss 0.03 ACC 0.71875 on epoch=249
03/01/2022 20:07:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/01/2022 20:07:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/01/2022 20:07:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/01/2022 20:07:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/01/2022 20:07:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
03/01/2022 20:07:20 - INFO - __main__ - Global step 550 Train loss 0.02 ACC 0.75 on epoch=274
03/01/2022 20:07:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 20:07:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/01/2022 20:07:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.03 on epoch=289
03/01/2022 20:07:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/01/2022 20:07:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/01/2022 20:07:33 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.71875 on epoch=299
03/01/2022 20:07:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/01/2022 20:07:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 20:07:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/01/2022 20:07:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 20:07:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 20:07:45 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.8125 on epoch=324
03/01/2022 20:07:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
03/01/2022 20:07:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 20:07:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/01/2022 20:07:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 20:07:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 20:07:58 - INFO - __main__ - Global step 700 Train loss 0.02 ACC 0.78125 on epoch=349
03/01/2022 20:08:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 20:08:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 20:08:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 20:08:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 20:08:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 20:08:10 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.75 on epoch=374
03/01/2022 20:08:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 20:08:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 20:08:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 20:08:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 20:08:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 20:08:23 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.78125 on epoch=399
03/01/2022 20:08:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 20:08:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 20:08:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 20:08:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 20:08:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 20:08:35 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.8125 on epoch=424
03/01/2022 20:08:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 20:08:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 20:08:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 20:08:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 20:08:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 20:08:48 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.8125 on epoch=449
03/01/2022 20:08:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 20:08:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 20:08:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 20:08:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 20:08:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 20:09:00 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.75 on epoch=474
03/01/2022 20:09:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 20:09:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 20:09:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 20:09:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 20:09:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 20:09:12 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.71875 on epoch=499
03/01/2022 20:09:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 20:09:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 20:09:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 20:09:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 20:09:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 20:09:25 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.75 on epoch=524
03/01/2022 20:09:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 20:09:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/01/2022 20:09:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/01/2022 20:09:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 20:09:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 20:09:37 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.84375 on epoch=549
03/01/2022 20:09:37 - INFO - __main__ - Saving model with best ACC: 0.8125 -> 0.84375 on epoch=549, global_step=1100
03/01/2022 20:09:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 20:09:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 20:09:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 20:09:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 20:09:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 20:09:50 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.84375 on epoch=574
03/01/2022 20:09:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 20:09:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 20:09:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 20:09:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 20:10:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/01/2022 20:10:02 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.8125 on epoch=599
03/01/2022 20:10:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/01/2022 20:10:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 20:10:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 20:10:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 20:10:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 20:10:15 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.84375 on epoch=624
03/01/2022 20:10:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 20:10:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 20:10:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 20:10:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 20:10:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 20:10:27 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.75 on epoch=649
03/01/2022 20:10:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 20:10:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 20:10:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 20:10:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 20:10:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 20:10:40 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.75 on epoch=674
03/01/2022 20:10:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 20:10:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 20:10:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 20:10:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 20:10:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 20:10:52 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.71875 on epoch=699
03/01/2022 20:10:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/01/2022 20:10:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 20:10:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 20:11:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/01/2022 20:11:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 20:11:04 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.84375 on epoch=724
03/01/2022 20:11:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 20:11:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 20:11:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/01/2022 20:11:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/01/2022 20:11:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/01/2022 20:11:17 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.65625 on epoch=749
03/01/2022 20:11:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 20:11:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 20:11:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 20:11:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 20:11:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 20:11:29 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.8125 on epoch=774
03/01/2022 20:11:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:11:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:11:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:11:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:11:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 20:11:42 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.8125 on epoch=799
03/01/2022 20:11:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:11:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:11:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:11:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 20:11:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 20:11:54 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.84375 on epoch=824
03/01/2022 20:11:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:11:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 20:12:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:12:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 20:12:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:12:07 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.84375 on epoch=849
03/01/2022 20:12:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:12:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 20:12:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 20:12:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:12:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:12:19 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
03/01/2022 20:12:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 20:12:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/01/2022 20:12:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 20:12:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:12:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 20:12:32 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=899
03/01/2022 20:12:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:12:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 20:12:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:12:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:12:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 20:12:44 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.8125 on epoch=924
03/01/2022 20:12:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 20:12:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 20:12:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 20:12:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 20:12:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 20:12:56 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.8125 on epoch=949
03/01/2022 20:12:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 20:13:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 20:13:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 20:13:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 20:13:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 20:13:09 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.8125 on epoch=974
03/01/2022 20:13:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 20:13:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 20:13:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 20:13:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 20:13:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 20:13:21 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.71875 on epoch=999
03/01/2022 20:13:21 - INFO - __main__ - save last model!
03/01/2022 20:13:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:13:21 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 20:13:21 - INFO - __main__ - Printing 3 examples
03/01/2022 20:13:21 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 20:13:21 - INFO - __main__ - ['entailment']
03/01/2022 20:13:21 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 20:13:21 - INFO - __main__ - ['not_entailment']
03/01/2022 20:13:21 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 20:13:21 - INFO - __main__ - ['not_entailment']
03/01/2022 20:13:21 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:13:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:13:22 - INFO - __main__ - Printing 3 examples
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:13:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:13:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:13:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:13:22 - INFO - __main__ - Printing 3 examples
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 20:13:22 - INFO - __main__ - ['entailment']
03/01/2022 20:13:22 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:13:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:13:22 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:13:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:13:29 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 20:13:36 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:13:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:13:37 - INFO - __main__ - Starting training!
03/01/2022 20:16:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_100_0.3_8_predictions.txt
03/01/2022 20:16:19 - INFO - __main__ - ACC on test data: 0.5310
03/01/2022 20:16:21 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.3, bsz=8, dev_performance=0.84375, test_performance=0.5310269082921472
03/01/2022 20:16:21 - INFO - __main__ - Running ... prefix=glue-qnli_16_100, lr=0.2, bsz=8 ...
03/01/2022 20:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:16:22 - INFO - __main__ - Printing 3 examples
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: What law enforces digital rights management systems? [SEP] sentence: Laws such as the Digital Millennium Copyright Act have been enacted, that use criminal law to prevent any circumvention of software used to enforce digital rights management systems.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: What type of government does Tajikistan have? [SEP] sentence: Tajikistan is officially a republic, and holds elections for the presidency and parliament, operating under a presidential system.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: What is the street address of The Fitzroy Tavern? [SEP] sentence: The Fitzroy Tavern is a pub situated at 16 Charlotte Street in the Fitzrovia district, to which it gives its name.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:16:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:16:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:16:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:16:22 - INFO - __main__ - Printing 3 examples
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: What was distinctly made by Nintendo, Konami, and Acclaim? [SEP] sentence: All licensed US cartridges were made by Nintendo, Konami and Acclaim.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: In what year was the Sciences Academy of Lisbon founded? [SEP] sentence: One of the oldest learned societies of Portugal is the Sciences Academy of Lisbon, founded in 1779.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ -  [glue-qnli] question: Afrotheria,Xenartha, and Boreoeutheria deprives from which two lineages? [SEP] sentence: Boreoeutheria in turn contains two major lineages- Euarchontoglires and Laurasiatheria.
03/01/2022 20:16:22 - INFO - __main__ - ['entailment']
03/01/2022 20:16:22 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:16:22 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:16:22 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:16:34 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:16:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:16:35 - INFO - __main__ - Starting training!
03/01/2022 20:16:38 - INFO - __main__ - Step 10 Global step 10 Train loss 0.90 on epoch=4
03/01/2022 20:16:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.35 on epoch=9
03/01/2022 20:16:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.25 on epoch=14
03/01/2022 20:16:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.20 on epoch=19
03/01/2022 20:16:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.21 on epoch=24
03/01/2022 20:16:49 - INFO - __main__ - Global step 50 Train loss 0.38 ACC 0.5 on epoch=24
03/01/2022 20:16:49 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 20:16:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.21 on epoch=29
03/01/2022 20:16:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.19 on epoch=34
03/01/2022 20:16:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.19 on epoch=39
03/01/2022 20:16:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.20 on epoch=44
03/01/2022 20:17:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 20:17:01 - INFO - __main__ - Global step 100 Train loss 0.19 ACC 0.46875 on epoch=49
03/01/2022 20:17:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.18 on epoch=54
03/01/2022 20:17:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.18 on epoch=59
03/01/2022 20:17:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.17 on epoch=64
03/01/2022 20:17:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 20:17:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 20:17:14 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.46875 on epoch=74
03/01/2022 20:17:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.18 on epoch=79
03/01/2022 20:17:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 20:17:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 20:17:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 20:17:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 20:17:26 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 20:17:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/01/2022 20:17:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.16 on epoch=109
03/01/2022 20:17:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
03/01/2022 20:17:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
03/01/2022 20:17:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.14 on epoch=124
03/01/2022 20:17:38 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5625 on epoch=124
03/01/2022 20:17:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=124, global_step=250
03/01/2022 20:17:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
03/01/2022 20:17:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
03/01/2022 20:17:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/01/2022 20:17:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
03/01/2022 20:17:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
03/01/2022 20:17:51 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.5 on epoch=149
03/01/2022 20:17:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
03/01/2022 20:17:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.12 on epoch=159
03/01/2022 20:17:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
03/01/2022 20:18:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
03/01/2022 20:18:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.12 on epoch=174
03/01/2022 20:18:03 - INFO - __main__ - Global step 350 Train loss 0.12 ACC 0.65625 on epoch=174
03/01/2022 20:18:03 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=174, global_step=350
03/01/2022 20:18:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
03/01/2022 20:18:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
03/01/2022 20:18:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
03/01/2022 20:18:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
03/01/2022 20:18:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
03/01/2022 20:18:16 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.71875 on epoch=199
03/01/2022 20:18:16 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.71875 on epoch=199, global_step=400
03/01/2022 20:18:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.06 on epoch=204
03/01/2022 20:18:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=209
03/01/2022 20:18:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=214
03/01/2022 20:18:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
03/01/2022 20:18:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
03/01/2022 20:18:28 - INFO - __main__ - Global step 450 Train loss 0.08 ACC 0.65625 on epoch=224
03/01/2022 20:18:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
03/01/2022 20:18:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
03/01/2022 20:18:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
03/01/2022 20:18:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
03/01/2022 20:18:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/01/2022 20:18:41 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.71875 on epoch=249
03/01/2022 20:18:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
03/01/2022 20:18:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
03/01/2022 20:18:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
03/01/2022 20:18:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
03/01/2022 20:18:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
03/01/2022 20:18:53 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.6875 on epoch=274
03/01/2022 20:18:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
03/01/2022 20:18:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/01/2022 20:19:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
03/01/2022 20:19:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
03/01/2022 20:19:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/01/2022 20:19:05 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.6875 on epoch=299
03/01/2022 20:19:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
03/01/2022 20:19:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 20:19:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/01/2022 20:19:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 20:19:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/01/2022 20:19:18 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.6875 on epoch=324
03/01/2022 20:19:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
03/01/2022 20:19:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
03/01/2022 20:19:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
03/01/2022 20:19:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 20:19:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/01/2022 20:19:30 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.71875 on epoch=349
03/01/2022 20:19:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/01/2022 20:19:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/01/2022 20:19:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 20:19:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 20:19:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/01/2022 20:19:43 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.6875 on epoch=374
03/01/2022 20:19:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 20:19:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 20:19:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 20:19:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 20:19:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
03/01/2022 20:19:55 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.71875 on epoch=399
03/01/2022 20:19:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 20:20:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/01/2022 20:20:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 20:20:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/01/2022 20:20:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 20:20:08 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.75 on epoch=424
03/01/2022 20:20:08 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=424, global_step=850
03/01/2022 20:20:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/01/2022 20:20:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 20:20:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
03/01/2022 20:20:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 20:20:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 20:20:20 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.71875 on epoch=449
03/01/2022 20:20:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/01/2022 20:20:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/01/2022 20:20:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
03/01/2022 20:20:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/01/2022 20:20:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/01/2022 20:20:32 - INFO - __main__ - Global step 950 Train loss 0.02 ACC 0.6875 on epoch=474
03/01/2022 20:20:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 20:20:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/01/2022 20:20:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/01/2022 20:20:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/01/2022 20:20:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/01/2022 20:20:45 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
03/01/2022 20:20:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 20:20:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/01/2022 20:20:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 20:20:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/01/2022 20:20:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/01/2022 20:20:57 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.78125 on epoch=524
03/01/2022 20:20:57 - INFO - __main__ - Saving model with best ACC: 0.75 -> 0.78125 on epoch=524, global_step=1050
03/01/2022 20:21:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/01/2022 20:21:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 20:21:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/01/2022 20:21:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/01/2022 20:21:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 20:21:10 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.6875 on epoch=549
03/01/2022 20:21:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 20:21:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/01/2022 20:21:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 20:21:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/01/2022 20:21:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/01/2022 20:21:22 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.6875 on epoch=574
03/01/2022 20:21:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 20:21:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 20:21:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 20:21:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 20:21:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 20:21:35 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.75 on epoch=599
03/01/2022 20:21:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 20:21:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 20:21:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 20:21:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 20:21:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/01/2022 20:21:47 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.8125 on epoch=624
03/01/2022 20:21:47 - INFO - __main__ - Saving model with best ACC: 0.78125 -> 0.8125 on epoch=624, global_step=1250
03/01/2022 20:21:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 20:21:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/01/2022 20:21:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 20:21:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/01/2022 20:21:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 20:22:00 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.75 on epoch=649
03/01/2022 20:22:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 20:22:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 20:22:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 20:22:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 20:22:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 20:22:12 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.6875 on epoch=674
03/01/2022 20:22:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 20:22:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 20:22:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 20:22:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 20:22:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 20:22:24 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.8125 on epoch=699
03/01/2022 20:22:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 20:22:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/01/2022 20:22:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 20:22:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/01/2022 20:22:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 20:22:37 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.78125 on epoch=724
03/01/2022 20:22:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 20:22:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 20:22:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 20:22:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 20:22:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 20:22:49 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.75 on epoch=749
03/01/2022 20:22:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 20:22:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 20:22:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 20:22:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 20:23:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 20:23:02 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.6875 on epoch=774
03/01/2022 20:23:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:23:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:23:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:23:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:23:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/01/2022 20:23:14 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.75 on epoch=799
03/01/2022 20:23:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:23:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:23:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:23:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 20:23:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 20:23:26 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.71875 on epoch=824
03/01/2022 20:23:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:23:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 20:23:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:23:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/01/2022 20:23:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:23:39 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.75 on epoch=849
03/01/2022 20:23:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:23:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 20:23:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 20:23:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:23:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:23:51 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.75 on epoch=874
03/01/2022 20:23:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 20:23:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 20:23:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/01/2022 20:24:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:24:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/01/2022 20:24:04 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.8125 on epoch=899
03/01/2022 20:24:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:24:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/01/2022 20:24:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:24:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:24:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 20:24:16 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.78125 on epoch=924
03/01/2022 20:24:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 20:24:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 20:24:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 20:24:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 20:24:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 20:24:29 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.78125 on epoch=949
03/01/2022 20:24:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 20:24:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 20:24:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 20:24:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 20:24:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 20:24:41 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.65625 on epoch=974
03/01/2022 20:24:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 20:24:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 20:24:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 20:24:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 20:24:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
03/01/2022 20:24:53 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.78125 on epoch=999
03/01/2022 20:24:53 - INFO - __main__ - save last model!
03/01/2022 20:24:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:24:54 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 20:24:54 - INFO - __main__ - Printing 3 examples
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 20:24:54 - INFO - __main__ - ['not_entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 20:24:54 - INFO - __main__ - ['not_entailment']
03/01/2022 20:24:54 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:24:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:24:54 - INFO - __main__ - Printing 3 examples
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:24:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:24:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:24:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:24:54 - INFO - __main__ - Printing 3 examples
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:24:54 - INFO - __main__ - ['entailment']
03/01/2022 20:24:54 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:24:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:24:54 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:24:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:25:02 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 20:25:06 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:25:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:25:07 - INFO - __main__ - Starting training!
03/01/2022 20:27:52 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_100_0.2_8_predictions.txt
03/01/2022 20:27:52 - INFO - __main__ - ACC on test data: 0.5502
03/01/2022 20:27:52 - INFO - __main__ - prefix=glue-qnli_16_100, lr=0.2, bsz=8, dev_performance=0.8125, test_performance=0.5502471169686985
03/01/2022 20:27:52 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.5, bsz=8 ...
03/01/2022 20:27:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:27:53 - INFO - __main__ - Printing 3 examples
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:27:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:27:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:27:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:27:53 - INFO - __main__ - Printing 3 examples
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:27:53 - INFO - __main__ - ['entailment']
03/01/2022 20:27:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:27:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:27:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:28:07 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:28:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:28:08 - INFO - __main__ - Starting training!
03/01/2022 20:28:11 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=4
03/01/2022 20:28:14 - INFO - __main__ - Step 20 Global step 20 Train loss 0.21 on epoch=9
03/01/2022 20:28:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.18 on epoch=14
03/01/2022 20:28:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.18 on epoch=19
03/01/2022 20:28:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.18 on epoch=24
03/01/2022 20:28:22 - INFO - __main__ - Global step 50 Train loss 0.26 ACC 0.5 on epoch=24
03/01/2022 20:28:22 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 20:28:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.17 on epoch=29
03/01/2022 20:28:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.16 on epoch=34
03/01/2022 20:28:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.16 on epoch=39
03/01/2022 20:28:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.18 on epoch=44
03/01/2022 20:28:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 20:28:35 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5 on epoch=49
03/01/2022 20:28:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.20 on epoch=54
03/01/2022 20:28:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.20 on epoch=59
03/01/2022 20:28:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.17 on epoch=64
03/01/2022 20:28:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
03/01/2022 20:28:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.15 on epoch=74
03/01/2022 20:28:48 - INFO - __main__ - Global step 150 Train loss 0.18 ACC 0.5 on epoch=74
03/01/2022 20:28:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.14 on epoch=79
03/01/2022 20:28:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
03/01/2022 20:28:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 20:28:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 20:29:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
03/01/2022 20:29:01 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 20:29:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
03/01/2022 20:29:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/01/2022 20:29:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
03/01/2022 20:29:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/01/2022 20:29:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/01/2022 20:29:14 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 20:29:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
03/01/2022 20:29:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/01/2022 20:29:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/01/2022 20:29:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.15 on epoch=144
03/01/2022 20:29:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
03/01/2022 20:29:27 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.5 on epoch=149
03/01/2022 20:29:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.14 on epoch=154
03/01/2022 20:29:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
03/01/2022 20:29:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 20:29:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
03/01/2022 20:29:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/01/2022 20:29:40 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.5 on epoch=174
03/01/2022 20:29:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
03/01/2022 20:29:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=184
03/01/2022 20:29:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
03/01/2022 20:29:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
03/01/2022 20:29:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.16 on epoch=199
03/01/2022 20:29:53 - INFO - __main__ - Global step 400 Train loss 0.15 ACC 0.53125 on epoch=199
03/01/2022 20:29:53 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=199, global_step=400
03/01/2022 20:29:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=204
03/01/2022 20:29:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
03/01/2022 20:30:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=214
03/01/2022 20:30:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
03/01/2022 20:30:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=224
03/01/2022 20:30:06 - INFO - __main__ - Global step 450 Train loss 0.14 ACC 0.53125 on epoch=224
03/01/2022 20:30:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/01/2022 20:30:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
03/01/2022 20:30:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=239
03/01/2022 20:30:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
03/01/2022 20:30:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.09 on epoch=249
03/01/2022 20:30:19 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.5625 on epoch=249
03/01/2022 20:30:19 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=249, global_step=500
03/01/2022 20:30:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
03/01/2022 20:30:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.08 on epoch=259
03/01/2022 20:30:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
03/01/2022 20:30:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
03/01/2022 20:30:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/01/2022 20:30:32 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.59375 on epoch=274
03/01/2022 20:30:32 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=274, global_step=550
03/01/2022 20:30:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
03/01/2022 20:30:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
03/01/2022 20:30:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
03/01/2022 20:30:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/01/2022 20:30:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/01/2022 20:30:45 - INFO - __main__ - Global step 600 Train loss 0.04 ACC 0.625 on epoch=299
03/01/2022 20:30:45 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=299, global_step=600
03/01/2022 20:30:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 20:30:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
03/01/2022 20:30:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/01/2022 20:30:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 20:30:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 20:30:58 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.6875 on epoch=324
03/01/2022 20:30:58 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=324, global_step=650
03/01/2022 20:31:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
03/01/2022 20:31:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
03/01/2022 20:31:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
03/01/2022 20:31:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 20:31:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/01/2022 20:31:11 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.59375 on epoch=349
03/01/2022 20:31:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/01/2022 20:31:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 20:31:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/01/2022 20:31:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 20:31:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/01/2022 20:31:24 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.59375 on epoch=374
03/01/2022 20:31:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 20:31:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 20:31:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 20:31:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 20:31:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 20:31:37 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.65625 on epoch=399
03/01/2022 20:31:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 20:31:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 20:31:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 20:31:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 20:31:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 20:31:50 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.71875 on epoch=424
03/01/2022 20:31:50 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=424, global_step=850
03/01/2022 20:31:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 20:31:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 20:31:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 20:32:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 20:32:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
03/01/2022 20:32:04 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.6875 on epoch=449
03/01/2022 20:32:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/01/2022 20:32:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 20:32:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/01/2022 20:32:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 20:32:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 20:32:17 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.65625 on epoch=474
03/01/2022 20:32:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 20:32:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/01/2022 20:32:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 20:32:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 20:32:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 20:32:30 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
03/01/2022 20:32:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 20:32:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 20:32:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 20:32:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 20:32:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
03/01/2022 20:32:43 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.6875 on epoch=524
03/01/2022 20:32:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 20:32:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 20:32:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/01/2022 20:32:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 20:32:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 20:32:56 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.6875 on epoch=549
03/01/2022 20:32:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 20:33:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/01/2022 20:33:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/01/2022 20:33:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 20:33:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 20:33:09 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.65625 on epoch=574
03/01/2022 20:33:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 20:33:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 20:33:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 20:33:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 20:33:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 20:33:22 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.53125 on epoch=599
03/01/2022 20:33:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 20:33:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 20:33:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 20:33:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 20:33:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 20:33:35 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.6875 on epoch=624
03/01/2022 20:33:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 20:33:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 20:33:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 20:33:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 20:33:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 20:33:48 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.6875 on epoch=649
03/01/2022 20:33:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 20:33:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 20:33:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 20:33:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 20:34:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 20:34:02 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
03/01/2022 20:34:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/01/2022 20:34:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 20:34:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 20:34:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 20:34:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 20:34:15 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.625 on epoch=699
03/01/2022 20:34:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 20:34:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 20:34:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 20:34:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 20:34:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 20:34:28 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.625 on epoch=724
03/01/2022 20:34:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/01/2022 20:34:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 20:34:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 20:34:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 20:34:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 20:34:41 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.65625 on epoch=749
03/01/2022 20:34:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 20:34:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
03/01/2022 20:34:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 20:34:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 20:34:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 20:34:54 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.65625 on epoch=774
03/01/2022 20:34:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:34:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:35:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:35:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:35:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 20:35:07 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.625 on epoch=799
03/01/2022 20:35:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:35:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:35:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:35:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 20:35:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 20:35:20 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
03/01/2022 20:35:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:35:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 20:35:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:35:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 20:35:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:35:33 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.65625 on epoch=849
03/01/2022 20:35:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:35:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/01/2022 20:35:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 20:35:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:35:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:35:46 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
03/01/2022 20:35:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
03/01/2022 20:35:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 20:35:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 20:35:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:35:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 20:35:59 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=899
03/01/2022 20:36:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:36:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 20:36:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:36:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:36:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 20:36:12 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.6875 on epoch=924
03/01/2022 20:36:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 20:36:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 20:36:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 20:36:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 20:36:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 20:36:25 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.6875 on epoch=949
03/01/2022 20:36:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 20:36:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 20:36:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 20:36:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 20:36:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 20:36:38 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=974
03/01/2022 20:36:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 20:36:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 20:36:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 20:36:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 20:36:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 20:36:51 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.6875 on epoch=999
03/01/2022 20:36:51 - INFO - __main__ - save last model!
03/01/2022 20:36:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:36:51 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 20:36:51 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:51 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 20:36:51 - INFO - __main__ - ['entailment']
03/01/2022 20:36:51 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 20:36:51 - INFO - __main__ - ['not_entailment']
03/01/2022 20:36:51 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 20:36:51 - INFO - __main__ - ['not_entailment']
03/01/2022 20:36:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:36:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:36:53 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:36:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:36:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:36:53 - INFO - __main__ - Printing 3 examples
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:36:53 - INFO - __main__ - ['entailment']
03/01/2022 20:36:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:36:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:36:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:36:59 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 20:37:05 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:37:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:37:06 - INFO - __main__ - Starting training!
03/01/2022 20:39:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_13_0.5_8_predictions.txt
03/01/2022 20:39:50 - INFO - __main__ - ACC on test data: 0.5557
03/01/2022 20:39:51 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.5, bsz=8, dev_performance=0.71875, test_performance=0.5557386051619989
03/01/2022 20:39:51 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.4, bsz=8 ...
03/01/2022 20:39:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:39:52 - INFO - __main__ - Printing 3 examples
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:39:52 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:39:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:39:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:39:52 - INFO - __main__ - Printing 3 examples
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:39:52 - INFO - __main__ - ['entailment']
03/01/2022 20:39:52 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:39:52 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:39:52 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:40:06 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:40:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:40:07 - INFO - __main__ - Starting training!
03/01/2022 20:40:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=4
03/01/2022 20:40:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.25 on epoch=9
03/01/2022 20:40:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.19 on epoch=14
03/01/2022 20:40:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.20 on epoch=19
03/01/2022 20:40:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.20 on epoch=24
03/01/2022 20:40:21 - INFO - __main__ - Global step 50 Train loss 0.30 ACC 0.5 on epoch=24
03/01/2022 20:40:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 20:40:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.16 on epoch=29
03/01/2022 20:40:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.16 on epoch=34
03/01/2022 20:40:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 20:40:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 20:40:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 20:40:34 - INFO - __main__ - Global step 100 Train loss 0.16 ACC 0.5 on epoch=49
03/01/2022 20:40:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.15 on epoch=54
03/01/2022 20:40:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 20:40:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 20:40:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.13 on epoch=69
03/01/2022 20:40:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.14 on epoch=74
03/01/2022 20:40:47 - INFO - __main__ - Global step 150 Train loss 0.15 ACC 0.5 on epoch=74
03/01/2022 20:40:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.14 on epoch=79
03/01/2022 20:40:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
03/01/2022 20:40:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 20:40:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 20:40:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 20:41:00 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 20:41:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/01/2022 20:41:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
03/01/2022 20:41:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
03/01/2022 20:41:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/01/2022 20:41:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/01/2022 20:41:13 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 20:41:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
03/01/2022 20:41:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
03/01/2022 20:41:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/01/2022 20:41:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/01/2022 20:41:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
03/01/2022 20:41:26 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.5 on epoch=149
03/01/2022 20:41:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.13 on epoch=154
03/01/2022 20:41:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
03/01/2022 20:41:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 20:41:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
03/01/2022 20:41:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
03/01/2022 20:41:39 - INFO - __main__ - Global step 350 Train loss 0.13 ACC 0.59375 on epoch=174
03/01/2022 20:41:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=174, global_step=350
03/01/2022 20:41:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
03/01/2022 20:41:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.12 on epoch=184
03/01/2022 20:41:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.11 on epoch=189
03/01/2022 20:41:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
03/01/2022 20:41:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=199
03/01/2022 20:41:52 - INFO - __main__ - Global step 400 Train loss 0.11 ACC 0.59375 on epoch=199
03/01/2022 20:41:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
03/01/2022 20:41:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
03/01/2022 20:41:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.05 on epoch=214
03/01/2022 20:42:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
03/01/2022 20:42:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
03/01/2022 20:42:05 - INFO - __main__ - Global step 450 Train loss 0.08 ACC 0.625 on epoch=224
03/01/2022 20:42:05 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=224, global_step=450
03/01/2022 20:42:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
03/01/2022 20:42:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
03/01/2022 20:42:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
03/01/2022 20:42:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
03/01/2022 20:42:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
03/01/2022 20:42:18 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.59375 on epoch=249
03/01/2022 20:42:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.03 on epoch=254
03/01/2022 20:42:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/01/2022 20:42:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
03/01/2022 20:42:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
03/01/2022 20:42:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/01/2022 20:42:31 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.625 on epoch=274
03/01/2022 20:42:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
03/01/2022 20:42:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/01/2022 20:42:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
03/01/2022 20:42:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 20:42:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/01/2022 20:42:44 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.6875 on epoch=299
03/01/2022 20:42:44 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=299, global_step=600
03/01/2022 20:42:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/01/2022 20:42:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 20:42:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/01/2022 20:42:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 20:42:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 20:42:57 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.59375 on epoch=324
03/01/2022 20:42:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 20:43:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 20:43:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
03/01/2022 20:43:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/01/2022 20:43:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 20:43:10 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.59375 on epoch=349
03/01/2022 20:43:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 20:43:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 20:43:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 20:43:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 20:43:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 20:43:23 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.625 on epoch=374
03/01/2022 20:43:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/01/2022 20:43:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 20:43:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 20:43:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 20:43:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 20:43:36 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.65625 on epoch=399
03/01/2022 20:43:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 20:43:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 20:43:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 20:43:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 20:43:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 20:43:49 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.625 on epoch=424
03/01/2022 20:43:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 20:43:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 20:43:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 20:43:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 20:44:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 20:44:02 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.59375 on epoch=449
03/01/2022 20:44:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 20:44:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 20:44:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 20:44:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 20:44:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 20:44:15 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
03/01/2022 20:44:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 20:44:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 20:44:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 20:44:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 20:44:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 20:44:28 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.65625 on epoch=499
03/01/2022 20:44:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 20:44:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
03/01/2022 20:44:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
03/01/2022 20:44:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 20:44:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 20:44:40 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.65625 on epoch=524
03/01/2022 20:44:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 20:44:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 20:44:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 20:44:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 20:44:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 20:44:53 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.65625 on epoch=549
03/01/2022 20:44:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 20:44:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 20:45:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
03/01/2022 20:45:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 20:45:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 20:45:06 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.6875 on epoch=574
03/01/2022 20:45:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 20:45:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 20:45:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 20:45:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 20:45:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 20:45:19 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.65625 on epoch=599
03/01/2022 20:45:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 20:45:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 20:45:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 20:45:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/01/2022 20:45:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 20:45:32 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
03/01/2022 20:45:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 20:45:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 20:45:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 20:45:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 20:45:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
03/01/2022 20:45:44 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.65625 on epoch=649
03/01/2022 20:45:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 20:45:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 20:45:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 20:45:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 20:45:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 20:45:57 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.71875 on epoch=674
03/01/2022 20:45:57 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=674, global_step=1350
03/01/2022 20:46:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 20:46:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 20:46:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 20:46:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 20:46:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 20:46:10 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
03/01/2022 20:46:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 20:46:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/01/2022 20:46:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 20:46:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 20:46:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 20:46:23 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.65625 on epoch=724
03/01/2022 20:46:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 20:46:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 20:46:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 20:46:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 20:46:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 20:46:36 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.65625 on epoch=749
03/01/2022 20:46:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/01/2022 20:46:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 20:46:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 20:46:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 20:46:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 20:46:49 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.59375 on epoch=774
03/01/2022 20:46:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:46:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:46:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:46:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:47:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 20:47:02 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.65625 on epoch=799
03/01/2022 20:47:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:47:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:47:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:47:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 20:47:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/01/2022 20:47:15 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.65625 on epoch=824
03/01/2022 20:47:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:47:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 20:47:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:47:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 20:47:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:47:27 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.65625 on epoch=849
03/01/2022 20:47:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:47:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 20:47:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 20:47:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:47:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:47:40 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.71875 on epoch=874
03/01/2022 20:47:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 20:47:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 20:47:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 20:47:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:47:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 20:47:53 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
03/01/2022 20:47:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:47:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 20:48:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:48:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:48:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/01/2022 20:48:06 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.6875 on epoch=924
03/01/2022 20:48:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 20:48:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 20:48:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 20:48:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 20:48:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 20:48:19 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.71875 on epoch=949
03/01/2022 20:48:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 20:48:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 20:48:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 20:48:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 20:48:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/01/2022 20:48:32 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.71875 on epoch=974
03/01/2022 20:48:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 20:48:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 20:48:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 20:48:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 20:48:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 20:48:44 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.71875 on epoch=999
03/01/2022 20:48:44 - INFO - __main__ - save last model!
03/01/2022 20:48:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 20:48:45 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 20:48:45 - INFO - __main__ - Printing 3 examples
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 20:48:45 - INFO - __main__ - ['not_entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 20:48:45 - INFO - __main__ - ['not_entailment']
03/01/2022 20:48:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:48:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:48:45 - INFO - __main__ - Printing 3 examples
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 20:48:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:48:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:48:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:48:45 - INFO - __main__ - Printing 3 examples
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:48:45 - INFO - __main__ - ['entailment']
03/01/2022 20:48:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:48:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:48:45 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:48:47 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:48:53 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 20:48:57 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:48:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:48:58 - INFO - __main__ - Starting training!
03/01/2022 20:51:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_13_0.4_8_predictions.txt
03/01/2022 20:51:44 - INFO - __main__ - ACC on test data: 0.5751
03/01/2022 20:51:44 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.4, bsz=8, dev_performance=0.71875, test_performance=0.5751418634449936
03/01/2022 20:51:44 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.3, bsz=8 ...
03/01/2022 20:51:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:51:45 - INFO - __main__ - Printing 3 examples
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 20:51:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:51:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 20:51:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 20:51:45 - INFO - __main__ - Printing 3 examples
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 20:51:45 - INFO - __main__ - ['entailment']
03/01/2022 20:51:45 - INFO - __main__ - Tokenizing Input ...
03/01/2022 20:51:45 - INFO - __main__ - Tokenizing Output ...
03/01/2022 20:51:45 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 20:51:59 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 20:52:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 20:52:00 - INFO - __main__ - Starting training!
03/01/2022 20:52:03 - INFO - __main__ - Step 10 Global step 10 Train loss 0.75 on epoch=4
03/01/2022 20:52:06 - INFO - __main__ - Step 20 Global step 20 Train loss 0.28 on epoch=9
03/01/2022 20:52:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.27 on epoch=14
03/01/2022 20:52:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.21 on epoch=19
03/01/2022 20:52:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.19 on epoch=24
03/01/2022 20:52:14 - INFO - __main__ - Global step 50 Train loss 0.34 ACC 0.5 on epoch=24
03/01/2022 20:52:14 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 20:52:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.20 on epoch=29
03/01/2022 20:52:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.17 on epoch=34
03/01/2022 20:52:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.19 on epoch=39
03/01/2022 20:52:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 20:52:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 20:52:27 - INFO - __main__ - Global step 100 Train loss 0.18 ACC 0.5 on epoch=49
03/01/2022 20:52:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 20:52:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 20:52:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.17 on epoch=64
03/01/2022 20:52:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.14 on epoch=69
03/01/2022 20:52:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 20:52:40 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.5 on epoch=74
03/01/2022 20:52:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
03/01/2022 20:52:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
03/01/2022 20:52:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.12 on epoch=89
03/01/2022 20:52:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
03/01/2022 20:52:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 20:52:52 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.5 on epoch=99
03/01/2022 20:52:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
03/01/2022 20:52:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.17 on epoch=109
03/01/2022 20:52:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.16 on epoch=114
03/01/2022 20:53:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
03/01/2022 20:53:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
03/01/2022 20:53:05 - INFO - __main__ - Global step 250 Train loss 0.16 ACC 0.5 on epoch=124
03/01/2022 20:53:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
03/01/2022 20:53:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
03/01/2022 20:53:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
03/01/2022 20:53:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/01/2022 20:53:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.15 on epoch=149
03/01/2022 20:53:18 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.53125 on epoch=149
03/01/2022 20:53:18 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
03/01/2022 20:53:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.16 on epoch=154
03/01/2022 20:53:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.14 on epoch=159
03/01/2022 20:53:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.12 on epoch=164
03/01/2022 20:53:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.15 on epoch=169
03/01/2022 20:53:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.12 on epoch=174
03/01/2022 20:53:31 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.53125 on epoch=174
03/01/2022 20:53:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
03/01/2022 20:53:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
03/01/2022 20:53:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.12 on epoch=189
03/01/2022 20:53:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=194
03/01/2022 20:53:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
03/01/2022 20:53:43 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.5625 on epoch=199
03/01/2022 20:53:43 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
03/01/2022 20:53:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
03/01/2022 20:53:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
03/01/2022 20:53:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
03/01/2022 20:53:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
03/01/2022 20:53:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
03/01/2022 20:53:56 - INFO - __main__ - Global step 450 Train loss 0.10 ACC 0.59375 on epoch=224
03/01/2022 20:53:56 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=224, global_step=450
03/01/2022 20:53:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/01/2022 20:54:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.06 on epoch=234
03/01/2022 20:54:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
03/01/2022 20:54:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
03/01/2022 20:54:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
03/01/2022 20:54:09 - INFO - __main__ - Global step 500 Train loss 0.06 ACC 0.5625 on epoch=249
03/01/2022 20:54:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
03/01/2022 20:54:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
03/01/2022 20:54:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/01/2022 20:54:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.04 on epoch=269
03/01/2022 20:54:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/01/2022 20:54:22 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.5625 on epoch=274
03/01/2022 20:54:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
03/01/2022 20:54:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/01/2022 20:54:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/01/2022 20:54:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
03/01/2022 20:54:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/01/2022 20:54:35 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.5625 on epoch=299
03/01/2022 20:54:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 20:54:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
03/01/2022 20:54:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/01/2022 20:54:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 20:54:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 20:54:48 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.65625 on epoch=324
03/01/2022 20:54:48 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=324, global_step=650
03/01/2022 20:54:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 20:54:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/01/2022 20:54:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/01/2022 20:54:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/01/2022 20:55:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/01/2022 20:55:01 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.59375 on epoch=349
03/01/2022 20:55:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 20:55:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 20:55:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 20:55:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/01/2022 20:55:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 20:55:15 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.59375 on epoch=374
03/01/2022 20:55:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 20:55:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/01/2022 20:55:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 20:55:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 20:55:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 20:55:28 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.5625 on epoch=399
03/01/2022 20:55:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 20:55:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 20:55:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 20:55:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 20:55:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/01/2022 20:55:41 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.53125 on epoch=424
03/01/2022 20:55:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 20:55:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 20:55:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 20:55:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
03/01/2022 20:55:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
03/01/2022 20:55:54 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.5625 on epoch=449
03/01/2022 20:55:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 20:55:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 20:56:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/01/2022 20:56:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 20:56:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 20:56:06 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.59375 on epoch=474
03/01/2022 20:56:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 20:56:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 20:56:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/01/2022 20:56:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 20:56:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 20:56:19 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.53125 on epoch=499
03/01/2022 20:56:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 20:56:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 20:56:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 20:56:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 20:56:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 20:56:32 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.5625 on epoch=524
03/01/2022 20:56:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 20:56:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/01/2022 20:56:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 20:56:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 20:56:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 20:56:45 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.625 on epoch=549
03/01/2022 20:56:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/01/2022 20:56:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 20:56:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/01/2022 20:56:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 20:56:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 20:56:58 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.65625 on epoch=574
03/01/2022 20:57:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 20:57:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 20:57:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 20:57:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 20:57:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/01/2022 20:57:11 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.5625 on epoch=599
03/01/2022 20:57:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 20:57:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 20:57:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 20:57:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 20:57:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 20:57:24 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
03/01/2022 20:57:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 20:57:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 20:57:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 20:57:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 20:57:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 20:57:36 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.5625 on epoch=649
03/01/2022 20:57:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 20:57:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 20:57:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 20:57:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 20:57:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 20:57:49 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.5625 on epoch=674
03/01/2022 20:57:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 20:57:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 20:57:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 20:57:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 20:58:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/01/2022 20:58:02 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.5625 on epoch=699
03/01/2022 20:58:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 20:58:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 20:58:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 20:58:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 20:58:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 20:58:15 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.5625 on epoch=724
03/01/2022 20:58:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/01/2022 20:58:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 20:58:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 20:58:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 20:58:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 20:58:28 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.625 on epoch=749
03/01/2022 20:58:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 20:58:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 20:58:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/01/2022 20:58:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 20:58:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/01/2022 20:58:41 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5625 on epoch=774
03/01/2022 20:58:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 20:58:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 20:58:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 20:58:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 20:58:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 20:58:53 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.53125 on epoch=799
03/01/2022 20:58:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 20:58:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 20:59:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 20:59:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 20:59:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 20:59:06 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
03/01/2022 20:59:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 20:59:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/01/2022 20:59:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 20:59:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 20:59:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 20:59:19 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5625 on epoch=849
03/01/2022 20:59:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 20:59:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 20:59:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 20:59:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 20:59:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 20:59:32 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.5625 on epoch=874
03/01/2022 20:59:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 20:59:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
03/01/2022 20:59:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 20:59:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 20:59:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 20:59:45 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.625 on epoch=899
03/01/2022 20:59:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 20:59:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 20:59:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 20:59:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 20:59:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 20:59:58 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.5625 on epoch=924
03/01/2022 21:00:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/01/2022 21:00:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:00:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:00:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:00:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:00:10 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.5625 on epoch=949
03/01/2022 21:00:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:00:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:00:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:00:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
03/01/2022 21:00:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:00:23 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=974
03/01/2022 21:00:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:00:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:00:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:00:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:00:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:00:36 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.59375 on epoch=999
03/01/2022 21:00:36 - INFO - __main__ - save last model!
03/01/2022 21:00:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:00:36 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:00:36 - INFO - __main__ - Printing 3 examples
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:00:36 - INFO - __main__ - ['entailment']
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:00:36 - INFO - __main__ - ['not_entailment']
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:00:36 - INFO - __main__ - ['not_entailment']
03/01/2022 21:00:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:00:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:00:36 - INFO - __main__ - Printing 3 examples
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 21:00:36 - INFO - __main__ - ['entailment']
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 21:00:36 - INFO - __main__ - ['entailment']
03/01/2022 21:00:36 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 21:00:36 - INFO - __main__ - ['entailment']
03/01/2022 21:00:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:00:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:00:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:00:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:00:37 - INFO - __main__ - Printing 3 examples
03/01/2022 21:00:37 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 21:00:37 - INFO - __main__ - ['entailment']
03/01/2022 21:00:37 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 21:00:37 - INFO - __main__ - ['entailment']
03/01/2022 21:00:37 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 21:00:37 - INFO - __main__ - ['entailment']
03/01/2022 21:00:37 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:00:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:00:37 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:00:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:00:44 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:00:49 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:00:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:00:50 - INFO - __main__ - Starting training!
03/01/2022 21:03:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_13_0.3_8_predictions.txt
03/01/2022 21:03:36 - INFO - __main__ - ACC on test data: 0.5686
03/01/2022 21:03:36 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.5685520776130332
03/01/2022 21:03:36 - INFO - __main__ - Running ... prefix=glue-qnli_16_13, lr=0.2, bsz=8 ...
03/01/2022 21:03:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:03:37 - INFO - __main__ - Printing 3 examples
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: Sanskrit is the primary sacred language of which religion? [SEP] sentence: Sanskrit (/ˈsænskrɪt/; Sanskrit: saṃskṛtam [səmskr̩t̪əm] or saṃskṛta, originally saṃskṛtā vāk, "refined speech") is the primary sacred language of Hinduism, a philosophical language in Buddhism, Hinduism, Sikhism and Jainism, and a literary language that was in use as a lingua franca in Greater India.
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: Bees can detect what kind of light? [SEP] sentence: Some insects such as bees can perceive ultraviolet wavelengths, or detect polarized light, while the antennae of male moths can detect the pheromones of female moths over distances of many kilometers.
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: Which Indie band said Beyoncé was an inspiration for one of hteir albums? [SEP] sentence: American indie rock band White Rabbits also cited her an inspiration for their third album Milk Famous (2012), friend Gwyneth Paltrow studied Beyoncé at her live concerts while learning to become a musical performer for the 2010 film Country Strong.
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:03:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:03:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:03:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:03:37 - INFO - __main__ - Printing 3 examples
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: What year was Cyprus supposed to host the international art festival Manifesta? [SEP] sentence: Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture over the location of some of the Manifesta events in the Turkish sector of the capital Nicosia.
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: How long was Jesus said to be in the tomb? [SEP] sentence: Some have argued that Jesus was crucified on Wednesday, not Friday, on the grounds of the mention of "three days and three nights" in Matthew before his resurrection, celebrated on Sunday.
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ -  [glue-qnli] question: What was the name of the non-subscription Xbox online gaming service? [SEP] sentence: When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold).
03/01/2022 21:03:37 - INFO - __main__ - ['entailment']
03/01/2022 21:03:37 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:03:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:03:37 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:03:51 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:03:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:03:52 - INFO - __main__ - Starting training!
03/01/2022 21:03:55 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=4
03/01/2022 21:03:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.32 on epoch=9
03/01/2022 21:04:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.26 on epoch=14
03/01/2022 21:04:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.21 on epoch=19
03/01/2022 21:04:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.21 on epoch=24
03/01/2022 21:04:06 - INFO - __main__ - Global step 50 Train loss 0.36 ACC 0.5 on epoch=24
03/01/2022 21:04:06 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 21:04:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 21:04:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.20 on epoch=34
03/01/2022 21:04:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.19 on epoch=39
03/01/2022 21:04:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.19 on epoch=44
03/01/2022 21:04:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 21:04:19 - INFO - __main__ - Global step 100 Train loss 0.19 ACC 0.53125 on epoch=49
03/01/2022 21:04:19 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=49, global_step=100
03/01/2022 21:04:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 21:04:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.18 on epoch=59
03/01/2022 21:04:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 21:04:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.14 on epoch=69
03/01/2022 21:04:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.16 on epoch=74
03/01/2022 21:04:32 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.5 on epoch=74
03/01/2022 21:04:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 21:04:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 21:04:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 21:04:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 21:04:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 21:04:45 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.5 on epoch=99
03/01/2022 21:04:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 21:04:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
03/01/2022 21:04:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.17 on epoch=114
03/01/2022 21:04:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
03/01/2022 21:04:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/01/2022 21:04:58 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.5 on epoch=124
03/01/2022 21:05:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
03/01/2022 21:05:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/01/2022 21:05:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
03/01/2022 21:05:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/01/2022 21:05:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
03/01/2022 21:05:11 - INFO - __main__ - Global step 300 Train loss 0.15 ACC 0.5 on epoch=149
03/01/2022 21:05:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
03/01/2022 21:05:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
03/01/2022 21:05:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.16 on epoch=164
03/01/2022 21:05:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.16 on epoch=169
03/01/2022 21:05:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.15 on epoch=174
03/01/2022 21:05:24 - INFO - __main__ - Global step 350 Train loss 0.15 ACC 0.5 on epoch=174
03/01/2022 21:05:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
03/01/2022 21:05:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=184
03/01/2022 21:05:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
03/01/2022 21:05:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.15 on epoch=194
03/01/2022 21:05:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.13 on epoch=199
03/01/2022 21:05:36 - INFO - __main__ - Global step 400 Train loss 0.14 ACC 0.5625 on epoch=199
03/01/2022 21:05:36 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=199, global_step=400
03/01/2022 21:05:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
03/01/2022 21:05:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
03/01/2022 21:05:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
03/01/2022 21:05:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
03/01/2022 21:05:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=224
03/01/2022 21:05:49 - INFO - __main__ - Global step 450 Train loss 0.13 ACC 0.5625 on epoch=224
03/01/2022 21:05:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
03/01/2022 21:05:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=234
03/01/2022 21:05:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.12 on epoch=239
03/01/2022 21:05:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
03/01/2022 21:06:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.11 on epoch=249
03/01/2022 21:06:02 - INFO - __main__ - Global step 500 Train loss 0.11 ACC 0.53125 on epoch=249
03/01/2022 21:06:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
03/01/2022 21:06:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
03/01/2022 21:06:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
03/01/2022 21:06:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=269
03/01/2022 21:06:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
03/01/2022 21:06:15 - INFO - __main__ - Global step 550 Train loss 0.11 ACC 0.5625 on epoch=274
03/01/2022 21:06:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
03/01/2022 21:06:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=284
03/01/2022 21:06:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.08 on epoch=289
03/01/2022 21:06:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
03/01/2022 21:06:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/01/2022 21:06:28 - INFO - __main__ - Global step 600 Train loss 0.06 ACC 0.65625 on epoch=299
03/01/2022 21:06:28 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=299, global_step=600
03/01/2022 21:06:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
03/01/2022 21:06:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
03/01/2022 21:06:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
03/01/2022 21:06:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
03/01/2022 21:06:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=324
03/01/2022 21:06:41 - INFO - __main__ - Global step 650 Train loss 0.05 ACC 0.65625 on epoch=324
03/01/2022 21:06:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
03/01/2022 21:06:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
03/01/2022 21:06:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
03/01/2022 21:06:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/01/2022 21:06:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/01/2022 21:06:54 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.65625 on epoch=349
03/01/2022 21:06:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=354
03/01/2022 21:06:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
03/01/2022 21:07:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/01/2022 21:07:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/01/2022 21:07:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 21:07:07 - INFO - __main__ - Global step 750 Train loss 0.03 ACC 0.625 on epoch=374
03/01/2022 21:07:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 21:07:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 21:07:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/01/2022 21:07:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 21:07:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 21:07:20 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.5625 on epoch=399
03/01/2022 21:07:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 21:07:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/01/2022 21:07:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 21:07:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 21:07:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/01/2022 21:07:33 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.53125 on epoch=424
03/01/2022 21:07:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/01/2022 21:07:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:07:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 21:07:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 21:07:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 21:07:46 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.5625 on epoch=449
03/01/2022 21:07:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:07:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/01/2022 21:07:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/01/2022 21:07:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:07:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/01/2022 21:07:59 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.625 on epoch=474
03/01/2022 21:08:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:08:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:08:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/01/2022 21:08:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:08:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:08:12 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.59375 on epoch=499
03/01/2022 21:08:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:08:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 21:08:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/01/2022 21:08:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/01/2022 21:08:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:08:25 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
03/01/2022 21:08:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:08:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:08:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/01/2022 21:08:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/01/2022 21:08:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:08:38 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
03/01/2022 21:08:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:08:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
03/01/2022 21:08:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:08:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:08:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:08:51 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.59375 on epoch=574
03/01/2022 21:08:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:08:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 21:08:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
03/01/2022 21:09:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:09:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:09:04 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.59375 on epoch=599
03/01/2022 21:09:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:09:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 21:09:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:09:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:09:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/01/2022 21:09:17 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.53125 on epoch=624
03/01/2022 21:09:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 21:09:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:09:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:09:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:09:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:09:30 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
03/01/2022 21:09:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/01/2022 21:09:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:09:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/01/2022 21:09:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/01/2022 21:09:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:09:43 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.59375 on epoch=674
03/01/2022 21:09:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:09:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:09:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:09:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/01/2022 21:09:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:09:55 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.59375 on epoch=699
03/01/2022 21:09:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:10:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/01/2022 21:10:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:10:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:10:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
03/01/2022 21:10:08 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.59375 on epoch=724
03/01/2022 21:10:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:10:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 21:10:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/01/2022 21:10:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
03/01/2022 21:10:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:10:21 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.625 on epoch=749
03/01/2022 21:10:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:10:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/01/2022 21:10:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:10:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:10:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:10:34 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.625 on epoch=774
03/01/2022 21:10:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:10:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:10:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:10:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:10:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/01/2022 21:10:47 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.53125 on epoch=799
03/01/2022 21:10:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/01/2022 21:10:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:10:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:10:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
03/01/2022 21:10:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:11:00 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.59375 on epoch=824
03/01/2022 21:11:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:11:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:11:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:11:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:11:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:11:13 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.59375 on epoch=849
03/01/2022 21:11:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:11:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:11:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:11:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:11:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:11:26 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
03/01/2022 21:11:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:11:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/01/2022 21:11:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:11:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:11:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:11:39 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
03/01/2022 21:11:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:11:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:11:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/01/2022 21:11:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:11:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:11:51 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
03/01/2022 21:11:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:11:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:11:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:12:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:12:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:12:04 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.53125 on epoch=949
03/01/2022 21:12:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:12:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:12:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/01/2022 21:12:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:12:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:12:17 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
03/01/2022 21:12:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:12:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:12:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:12:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:12:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:12:30 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
03/01/2022 21:12:30 - INFO - __main__ - save last model!
03/01/2022 21:12:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:12:30 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:12:30 - INFO - __main__ - Printing 3 examples
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:12:30 - INFO - __main__ - ['entailment']
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:12:30 - INFO - __main__ - ['not_entailment']
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:12:30 - INFO - __main__ - ['not_entailment']
03/01/2022 21:12:30 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:12:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:12:30 - INFO - __main__ - Printing 3 examples
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:12:30 - INFO - __main__ - ['entailment']
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:12:30 - INFO - __main__ - ['entailment']
03/01/2022 21:12:30 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:12:30 - INFO - __main__ - ['entailment']
03/01/2022 21:12:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:12:30 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:12:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:12:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:12:31 - INFO - __main__ - Printing 3 examples
03/01/2022 21:12:31 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:12:31 - INFO - __main__ - ['entailment']
03/01/2022 21:12:31 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:12:31 - INFO - __main__ - ['entailment']
03/01/2022 21:12:31 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:12:31 - INFO - __main__ - ['entailment']
03/01/2022 21:12:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:12:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:12:31 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:12:33 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:12:38 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:12:43 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:12:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:12:44 - INFO - __main__ - Starting training!
03/01/2022 21:15:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_13_0.2_8_predictions.txt
03/01/2022 21:15:28 - INFO - __main__ - ACC on test data: 0.5748
03/01/2022 21:15:28 - INFO - __main__ - prefix=glue-qnli_16_13, lr=0.2, bsz=8, dev_performance=0.65625, test_performance=0.5747757642321069
03/01/2022 21:15:28 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.5, bsz=8 ...
03/01/2022 21:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:15:29 - INFO - __main__ - Printing 3 examples
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:15:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:15:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:15:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:15:29 - INFO - __main__ - Printing 3 examples
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:15:29 - INFO - __main__ - ['entailment']
03/01/2022 21:15:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:15:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:15:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:15:42 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:15:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:15:42 - INFO - __main__ - Starting training!
03/01/2022 21:15:45 - INFO - __main__ - Step 10 Global step 10 Train loss 0.63 on epoch=4
03/01/2022 21:15:48 - INFO - __main__ - Step 20 Global step 20 Train loss 0.22 on epoch=9
03/01/2022 21:15:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.19 on epoch=14
03/01/2022 21:15:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.18 on epoch=19
03/01/2022 21:15:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.18 on epoch=24
03/01/2022 21:15:55 - INFO - __main__ - Global step 50 Train loss 0.28 ACC 0.5 on epoch=24
03/01/2022 21:15:55 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 21:15:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.20 on epoch=29
03/01/2022 21:16:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.17 on epoch=34
03/01/2022 21:16:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 21:16:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.16 on epoch=44
03/01/2022 21:16:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 21:16:07 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5 on epoch=49
03/01/2022 21:16:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.16 on epoch=54
03/01/2022 21:16:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 21:16:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.16 on epoch=64
03/01/2022 21:16:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
03/01/2022 21:16:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.15 on epoch=74
03/01/2022 21:16:20 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.5 on epoch=74
03/01/2022 21:16:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
03/01/2022 21:16:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/01/2022 21:16:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.17 on epoch=89
03/01/2022 21:16:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.15 on epoch=94
03/01/2022 21:16:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 21:16:32 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.53125 on epoch=99
03/01/2022 21:16:32 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=99, global_step=200
03/01/2022 21:16:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.15 on epoch=104
03/01/2022 21:16:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
03/01/2022 21:16:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.14 on epoch=114
03/01/2022 21:16:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
03/01/2022 21:16:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.12 on epoch=124
03/01/2022 21:16:44 - INFO - __main__ - Global step 250 Train loss 0.13 ACC 0.53125 on epoch=124
03/01/2022 21:16:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
03/01/2022 21:16:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
03/01/2022 21:16:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/01/2022 21:16:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
03/01/2022 21:16:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.05 on epoch=149
03/01/2022 21:16:57 - INFO - __main__ - Global step 300 Train loss 0.09 ACC 0.59375 on epoch=149
03/01/2022 21:16:57 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=149, global_step=300
03/01/2022 21:16:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.07 on epoch=154
03/01/2022 21:17:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.06 on epoch=159
03/01/2022 21:17:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
03/01/2022 21:17:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.04 on epoch=169
03/01/2022 21:17:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.06 on epoch=174
03/01/2022 21:17:09 - INFO - __main__ - Global step 350 Train loss 0.06 ACC 0.59375 on epoch=174
03/01/2022 21:17:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.07 on epoch=179
03/01/2022 21:17:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.02 on epoch=184
03/01/2022 21:17:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.02 on epoch=189
03/01/2022 21:17:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.02 on epoch=194
03/01/2022 21:17:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/01/2022 21:17:22 - INFO - __main__ - Global step 400 Train loss 0.03 ACC 0.625 on epoch=199
03/01/2022 21:17:22 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=199, global_step=400
03/01/2022 21:17:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/01/2022 21:17:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/01/2022 21:17:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.02 on epoch=214
03/01/2022 21:17:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/01/2022 21:17:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
03/01/2022 21:17:34 - INFO - __main__ - Global step 450 Train loss 0.02 ACC 0.59375 on epoch=224
03/01/2022 21:17:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/01/2022 21:17:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.01 on epoch=234
03/01/2022 21:17:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/01/2022 21:17:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/01/2022 21:17:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/01/2022 21:17:46 - INFO - __main__ - Global step 500 Train loss 0.01 ACC 0.625 on epoch=249
03/01/2022 21:17:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/01/2022 21:17:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/01/2022 21:17:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/01/2022 21:17:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/01/2022 21:17:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
03/01/2022 21:17:58 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.71875 on epoch=274
03/01/2022 21:17:58 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.71875 on epoch=274, global_step=550
03/01/2022 21:18:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/01/2022 21:18:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/01/2022 21:18:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/01/2022 21:18:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/01/2022 21:18:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/01/2022 21:18:10 - INFO - __main__ - Global step 600 Train loss 0.00 ACC 0.71875 on epoch=299
03/01/2022 21:18:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/01/2022 21:18:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/01/2022 21:18:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/01/2022 21:18:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 21:18:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 21:18:22 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.5625 on epoch=324
03/01/2022 21:18:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/01/2022 21:18:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/01/2022 21:18:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 21:18:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 21:18:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 21:18:34 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.6875 on epoch=349
03/01/2022 21:18:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 21:18:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 21:18:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 21:18:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 21:18:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 21:18:47 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.65625 on epoch=374
03/01/2022 21:18:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 21:18:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 21:18:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 21:18:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 21:18:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 21:18:59 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.65625 on epoch=399
03/01/2022 21:19:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 21:19:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:19:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 21:19:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 21:19:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 21:19:11 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.59375 on epoch=424
03/01/2022 21:19:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 21:19:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:19:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 21:19:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:19:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 21:19:23 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.75 on epoch=449
03/01/2022 21:19:23 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.75 on epoch=449, global_step=900
03/01/2022 21:19:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/01/2022 21:19:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:19:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 21:19:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:19:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:19:36 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.71875 on epoch=474
03/01/2022 21:19:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:19:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:19:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 21:19:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:19:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:19:48 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.625 on epoch=499
03/01/2022 21:19:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:19:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:19:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:19:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:19:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:20:00 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.65625 on epoch=524
03/01/2022 21:20:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
03/01/2022 21:20:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:20:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 21:20:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 21:20:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:20:12 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.6875 on epoch=549
03/01/2022 21:20:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:20:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:20:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:20:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:20:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/01/2022 21:20:24 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.75 on epoch=574
03/01/2022 21:20:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:20:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/01/2022 21:20:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:20:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
03/01/2022 21:20:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:20:36 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.71875 on epoch=599
03/01/2022 21:20:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:20:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 21:20:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:20:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:20:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:20:48 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.75 on epoch=624
03/01/2022 21:20:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:20:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:20:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:20:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:20:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:21:00 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.71875 on epoch=649
03/01/2022 21:21:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:21:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:21:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:21:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:21:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:21:12 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.6875 on epoch=674
03/01/2022 21:21:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:21:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:21:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:21:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:21:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:21:24 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.71875 on epoch=699
03/01/2022 21:21:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:21:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:21:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:21:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:21:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:21:36 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.6875 on epoch=724
03/01/2022 21:21:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/01/2022 21:21:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:21:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:21:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/01/2022 21:21:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:21:48 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
03/01/2022 21:21:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:21:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:21:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:21:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:22:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:22:01 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.6875 on epoch=774
03/01/2022 21:22:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:22:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:22:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:22:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:22:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:22:13 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.71875 on epoch=799
03/01/2022 21:22:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:22:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:22:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:22:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:22:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:22:26 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.6875 on epoch=824
03/01/2022 21:22:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:22:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:22:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:22:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:22:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:22:38 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.59375 on epoch=849
03/01/2022 21:22:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:22:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:22:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:22:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:22:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:22:50 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.6875 on epoch=874
03/01/2022 21:22:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:22:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:22:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:22:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:23:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:23:02 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
03/01/2022 21:23:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:23:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:23:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:23:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:23:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:23:15 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.6875 on epoch=924
03/01/2022 21:23:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:23:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:23:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:23:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:23:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:23:27 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.71875 on epoch=949
03/01/2022 21:23:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:23:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:23:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:23:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:23:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:23:39 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=974
03/01/2022 21:23:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:23:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:23:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:23:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:23:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:23:51 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.625 on epoch=999
03/01/2022 21:23:51 - INFO - __main__ - save last model!
03/01/2022 21:23:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:23:52 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:23:52 - INFO - __main__ - Printing 3 examples
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:23:52 - INFO - __main__ - ['not_entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:23:52 - INFO - __main__ - ['not_entailment']
03/01/2022 21:23:52 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:23:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:23:52 - INFO - __main__ - Printing 3 examples
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:23:52 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:23:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:23:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:23:52 - INFO - __main__ - Printing 3 examples
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:23:52 - INFO - __main__ - ['entailment']
03/01/2022 21:23:52 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:23:52 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:23:52 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:23:54 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:24:00 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:24:05 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:24:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:24:05 - INFO - __main__ - Starting training!
03/01/2022 21:26:51 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_21_0.5_8_predictions.txt
03/01/2022 21:26:51 - INFO - __main__ - ACC on test data: 0.5193
03/01/2022 21:26:52 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.5, bsz=8, dev_performance=0.75, test_performance=0.519311733479773
03/01/2022 21:26:52 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.4, bsz=8 ...
03/01/2022 21:26:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:26:53 - INFO - __main__ - Printing 3 examples
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:26:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:26:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:26:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:26:53 - INFO - __main__ - Printing 3 examples
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:26:53 - INFO - __main__ - ['entailment']
03/01/2022 21:26:53 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:26:53 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:26:53 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:27:07 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:27:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:27:07 - INFO - __main__ - Starting training!
03/01/2022 21:27:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=4
03/01/2022 21:27:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.24 on epoch=9
03/01/2022 21:27:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.20 on epoch=14
03/01/2022 21:27:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.17 on epoch=19
03/01/2022 21:27:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.16 on epoch=24
03/01/2022 21:27:21 - INFO - __main__ - Global step 50 Train loss 0.29 ACC 0.4375 on epoch=24
03/01/2022 21:27:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.4375 on epoch=24, global_step=50
03/01/2022 21:27:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.17 on epoch=29
03/01/2022 21:27:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 21:27:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.21 on epoch=39
03/01/2022 21:27:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.14 on epoch=44
03/01/2022 21:27:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.19 on epoch=49
03/01/2022 21:27:33 - INFO - __main__ - Global step 100 Train loss 0.18 ACC 0.40625 on epoch=49
03/01/2022 21:27:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 21:27:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
03/01/2022 21:27:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.13 on epoch=64
03/01/2022 21:27:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 21:27:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.14 on epoch=74
03/01/2022 21:27:46 - INFO - __main__ - Global step 150 Train loss 0.15 ACC 0.5 on epoch=74
03/01/2022 21:27:46 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=74, global_step=150
03/01/2022 21:27:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
03/01/2022 21:27:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
03/01/2022 21:27:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 21:27:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
03/01/2022 21:27:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.13 on epoch=99
03/01/2022 21:27:58 - INFO - __main__ - Global step 200 Train loss 0.14 ACC 0.53125 on epoch=99
03/01/2022 21:27:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=99, global_step=200
03/01/2022 21:28:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.12 on epoch=104
03/01/2022 21:28:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
03/01/2022 21:28:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.11 on epoch=114
03/01/2022 21:28:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.09 on epoch=119
03/01/2022 21:28:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.10 on epoch=124
03/01/2022 21:28:11 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.5 on epoch=124
03/01/2022 21:28:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
03/01/2022 21:28:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.08 on epoch=134
03/01/2022 21:28:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/01/2022 21:28:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.08 on epoch=144
03/01/2022 21:28:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.07 on epoch=149
03/01/2022 21:28:23 - INFO - __main__ - Global step 300 Train loss 0.09 ACC 0.625 on epoch=149
03/01/2022 21:28:23 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=149, global_step=300
03/01/2022 21:28:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
03/01/2022 21:28:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.05 on epoch=159
03/01/2022 21:28:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.05 on epoch=164
03/01/2022 21:28:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.06 on epoch=169
03/01/2022 21:28:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.03 on epoch=174
03/01/2022 21:28:36 - INFO - __main__ - Global step 350 Train loss 0.04 ACC 0.59375 on epoch=174
03/01/2022 21:28:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
03/01/2022 21:28:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.02 on epoch=184
03/01/2022 21:28:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.02 on epoch=189
03/01/2022 21:28:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.04 on epoch=194
03/01/2022 21:28:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/01/2022 21:28:48 - INFO - __main__ - Global step 400 Train loss 0.03 ACC 0.625 on epoch=199
03/01/2022 21:28:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.05 on epoch=204
03/01/2022 21:28:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.02 on epoch=209
03/01/2022 21:28:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/01/2022 21:28:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
03/01/2022 21:29:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
03/01/2022 21:29:01 - INFO - __main__ - Global step 450 Train loss 0.02 ACC 0.6875 on epoch=224
03/01/2022 21:29:01 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=224, global_step=450
03/01/2022 21:29:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/01/2022 21:29:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.01 on epoch=234
03/01/2022 21:29:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/01/2022 21:29:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
03/01/2022 21:29:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/01/2022 21:29:13 - INFO - __main__ - Global step 500 Train loss 0.02 ACC 0.6875 on epoch=249
03/01/2022 21:29:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/01/2022 21:29:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/01/2022 21:29:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/01/2022 21:29:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/01/2022 21:29:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/01/2022 21:29:26 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.71875 on epoch=274
03/01/2022 21:29:26 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=274, global_step=550
03/01/2022 21:29:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 21:29:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 21:29:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 21:29:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/01/2022 21:29:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/01/2022 21:29:38 - INFO - __main__ - Global step 600 Train loss 0.00 ACC 0.65625 on epoch=299
03/01/2022 21:29:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/01/2022 21:29:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/01/2022 21:29:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/01/2022 21:29:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 21:29:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 21:29:51 - INFO - __main__ - Global step 650 Train loss 0.00 ACC 0.6875 on epoch=324
03/01/2022 21:29:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 21:29:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 21:29:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 21:30:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 21:30:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 21:30:03 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.78125 on epoch=349
03/01/2022 21:30:03 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.78125 on epoch=349, global_step=700
03/01/2022 21:30:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 21:30:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 21:30:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 21:30:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 21:30:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 21:30:15 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.75 on epoch=374
03/01/2022 21:30:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 21:30:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/01/2022 21:30:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 21:30:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 21:30:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/01/2022 21:30:28 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.6875 on epoch=399
03/01/2022 21:30:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 21:30:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:30:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 21:30:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 21:30:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 21:30:40 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.6875 on epoch=424
03/01/2022 21:30:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 21:30:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:30:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 21:30:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 21:30:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 21:30:53 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.625 on epoch=449
03/01/2022 21:30:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:30:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:30:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 21:31:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:31:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:31:05 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.6875 on epoch=474
03/01/2022 21:31:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:31:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:31:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/01/2022 21:31:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:31:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:31:17 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.6875 on epoch=499
03/01/2022 21:31:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:31:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:31:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:31:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:31:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/01/2022 21:31:30 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.75 on epoch=524
03/01/2022 21:31:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:31:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:31:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/01/2022 21:31:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 21:31:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:31:42 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.71875 on epoch=549
03/01/2022 21:31:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 21:31:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:31:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:31:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:31:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:31:54 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.6875 on epoch=574
03/01/2022 21:31:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:31:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 21:32:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:32:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:32:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:32:07 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.6875 on epoch=599
03/01/2022 21:32:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:32:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 21:32:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:32:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:32:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:32:19 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.6875 on epoch=624
03/01/2022 21:32:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:32:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:32:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:32:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:32:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:32:31 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.6875 on epoch=649
03/01/2022 21:32:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:32:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:32:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:32:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:32:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:32:43 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.71875 on epoch=674
03/01/2022 21:32:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:32:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:32:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
03/01/2022 21:32:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:32:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:32:56 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.625 on epoch=699
03/01/2022 21:32:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:33:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:33:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:33:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/01/2022 21:33:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:33:08 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.65625 on epoch=724
03/01/2022 21:33:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/01/2022 21:33:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:33:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:33:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 21:33:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:33:20 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.71875 on epoch=749
03/01/2022 21:33:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:33:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:33:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:33:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:33:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/01/2022 21:33:33 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.65625 on epoch=774
03/01/2022 21:33:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:33:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:33:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:33:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:33:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:33:45 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.6875 on epoch=799
03/01/2022 21:33:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:33:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:33:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:33:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:33:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:33:57 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.71875 on epoch=824
03/01/2022 21:34:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:34:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:34:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:34:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/01/2022 21:34:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:34:10 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.75 on epoch=849
03/01/2022 21:34:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:34:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:34:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:34:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:34:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:34:22 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.71875 on epoch=874
03/01/2022 21:34:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:34:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:34:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:34:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:34:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:34:35 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.6875 on epoch=899
03/01/2022 21:34:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:34:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:34:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:34:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:34:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:34:47 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.65625 on epoch=924
03/01/2022 21:34:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/01/2022 21:34:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:34:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:34:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:34:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:34:59 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.71875 on epoch=949
03/01/2022 21:35:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:35:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:35:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:35:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:35:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:35:11 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=974
03/01/2022 21:35:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:35:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:35:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:35:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:35:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:35:24 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.6875 on epoch=999
03/01/2022 21:35:24 - INFO - __main__ - save last model!
03/01/2022 21:35:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:35:24 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:35:24 - INFO - __main__ - Printing 3 examples
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:35:24 - INFO - __main__ - ['not_entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:35:24 - INFO - __main__ - ['not_entailment']
03/01/2022 21:35:24 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:35:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:35:24 - INFO - __main__ - Printing 3 examples
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:35:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:35:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:35:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:35:24 - INFO - __main__ - Printing 3 examples
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:35:24 - INFO - __main__ - ['entailment']
03/01/2022 21:35:24 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:35:24 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:35:24 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:35:27 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:35:32 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:35:38 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:35:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:35:39 - INFO - __main__ - Starting training!
03/01/2022 21:38:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_21_0.4_8_predictions.txt
03/01/2022 21:38:24 - INFO - __main__ - ACC on test data: 0.5129
03/01/2022 21:38:25 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.4, bsz=8, dev_performance=0.78125, test_performance=0.5129049972542559
03/01/2022 21:38:25 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.3, bsz=8 ...
03/01/2022 21:38:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:38:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:38:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:38:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:38:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:38:26 - INFO - __main__ - Printing 3 examples
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:38:26 - INFO - __main__ - ['entailment']
03/01/2022 21:38:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:38:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:38:26 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:38:40 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:38:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:38:41 - INFO - __main__ - Starting training!
03/01/2022 21:38:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.77 on epoch=4
03/01/2022 21:38:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.28 on epoch=9
03/01/2022 21:38:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.22 on epoch=14
03/01/2022 21:38:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.20 on epoch=19
03/01/2022 21:38:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.17 on epoch=24
03/01/2022 21:38:54 - INFO - __main__ - Global step 50 Train loss 0.33 ACC 0.375 on epoch=24
03/01/2022 21:38:54 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.375 on epoch=24, global_step=50
03/01/2022 21:38:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 21:38:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 21:39:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.16 on epoch=39
03/01/2022 21:39:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.18 on epoch=44
03/01/2022 21:39:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 21:39:06 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5 on epoch=49
03/01/2022 21:39:06 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=49, global_step=100
03/01/2022 21:39:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.18 on epoch=54
03/01/2022 21:39:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
03/01/2022 21:39:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 21:39:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.18 on epoch=69
03/01/2022 21:39:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 21:39:18 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.5 on epoch=74
03/01/2022 21:39:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.16 on epoch=79
03/01/2022 21:39:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 21:39:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
03/01/2022 21:39:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 21:39:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.14 on epoch=99
03/01/2022 21:39:31 - INFO - __main__ - Global step 200 Train loss 0.15 ACC 0.65625 on epoch=99
03/01/2022 21:39:31 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=99, global_step=200
03/01/2022 21:39:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.13 on epoch=104
03/01/2022 21:39:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/01/2022 21:39:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
03/01/2022 21:39:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.12 on epoch=119
03/01/2022 21:39:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.14 on epoch=124
03/01/2022 21:39:44 - INFO - __main__ - Global step 250 Train loss 0.13 ACC 0.53125 on epoch=124
03/01/2022 21:39:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.13 on epoch=129
03/01/2022 21:39:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
03/01/2022 21:39:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/01/2022 21:39:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
03/01/2022 21:39:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.06 on epoch=149
03/01/2022 21:39:56 - INFO - __main__ - Global step 300 Train loss 0.10 ACC 0.53125 on epoch=149
03/01/2022 21:39:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
03/01/2022 21:40:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.08 on epoch=159
03/01/2022 21:40:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
03/01/2022 21:40:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.05 on epoch=169
03/01/2022 21:40:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.06 on epoch=174
03/01/2022 21:40:09 - INFO - __main__ - Global step 350 Train loss 0.07 ACC 0.6875 on epoch=174
03/01/2022 21:40:09 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=174, global_step=350
03/01/2022 21:40:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.05 on epoch=179
03/01/2022 21:40:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.03 on epoch=184
03/01/2022 21:40:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
03/01/2022 21:40:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
03/01/2022 21:40:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
03/01/2022 21:40:21 - INFO - __main__ - Global step 400 Train loss 0.04 ACC 0.71875 on epoch=199
03/01/2022 21:40:21 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=199, global_step=400
03/01/2022 21:40:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/01/2022 21:40:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
03/01/2022 21:40:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
03/01/2022 21:40:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.03 on epoch=219
03/01/2022 21:40:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
03/01/2022 21:40:34 - INFO - __main__ - Global step 450 Train loss 0.03 ACC 0.5625 on epoch=224
03/01/2022 21:40:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
03/01/2022 21:40:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.02 on epoch=234
03/01/2022 21:40:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/01/2022 21:40:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/01/2022 21:40:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
03/01/2022 21:40:47 - INFO - __main__ - Global step 500 Train loss 0.03 ACC 0.53125 on epoch=249
03/01/2022 21:40:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/01/2022 21:40:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.01 on epoch=259
03/01/2022 21:40:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/01/2022 21:40:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/01/2022 21:40:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/01/2022 21:40:59 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.625 on epoch=274
03/01/2022 21:41:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 21:41:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 21:41:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 21:41:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 21:41:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/01/2022 21:41:12 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.5625 on epoch=299
03/01/2022 21:41:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/01/2022 21:41:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/01/2022 21:41:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/01/2022 21:41:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 21:41:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 21:41:24 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.59375 on epoch=324
03/01/2022 21:41:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/01/2022 21:41:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 21:41:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 21:41:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 21:41:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/01/2022 21:41:37 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.625 on epoch=349
03/01/2022 21:41:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 21:41:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 21:41:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 21:41:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 21:41:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 21:41:50 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.65625 on epoch=374
03/01/2022 21:41:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 21:41:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 21:41:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 21:41:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 21:42:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 21:42:03 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.59375 on epoch=399
03/01/2022 21:42:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 21:42:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
03/01/2022 21:42:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 21:42:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 21:42:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 21:42:15 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.6875 on epoch=424
03/01/2022 21:42:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 21:42:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 21:42:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 21:42:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:42:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 21:42:28 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.625 on epoch=449
03/01/2022 21:42:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:42:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 21:42:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/01/2022 21:42:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:42:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 21:42:41 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.625 on epoch=474
03/01/2022 21:42:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 21:42:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:42:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
03/01/2022 21:42:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:42:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 21:42:53 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.59375 on epoch=499
03/01/2022 21:42:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 21:42:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 21:43:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 21:43:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 21:43:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 21:43:06 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.625 on epoch=524
03/01/2022 21:43:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:43:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/01/2022 21:43:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 21:43:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/01/2022 21:43:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:43:18 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.65625 on epoch=549
03/01/2022 21:43:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/01/2022 21:43:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:43:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 21:43:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:43:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:43:31 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.6875 on epoch=574
03/01/2022 21:43:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 21:43:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 21:43:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:43:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 21:43:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:43:44 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.625 on epoch=599
03/01/2022 21:43:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/01/2022 21:43:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 21:43:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/01/2022 21:43:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 21:43:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:43:56 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.5625 on epoch=624
03/01/2022 21:43:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 21:44:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:44:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
03/01/2022 21:44:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:44:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:44:09 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.59375 on epoch=649
03/01/2022 21:44:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/01/2022 21:44:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:44:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:44:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:44:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:44:21 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.59375 on epoch=674
03/01/2022 21:44:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 21:44:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:44:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:44:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:44:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:44:34 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.625 on epoch=699
03/01/2022 21:44:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:44:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 21:44:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 21:44:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:44:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 21:44:46 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.59375 on epoch=724
03/01/2022 21:44:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:44:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
03/01/2022 21:44:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:44:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 21:44:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 21:44:59 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.59375 on epoch=749
03/01/2022 21:45:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:45:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:45:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:45:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:45:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:45:12 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.59375 on epoch=774
03/01/2022 21:45:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 21:45:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:45:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:45:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:45:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:45:24 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.625 on epoch=799
03/01/2022 21:45:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/01/2022 21:45:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:45:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:45:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:45:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:45:37 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
03/01/2022 21:45:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:45:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 21:45:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 21:45:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/01/2022 21:45:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:45:49 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.625 on epoch=849
03/01/2022 21:45:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:45:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:45:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 21:45:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:46:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:46:01 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.65625 on epoch=874
03/01/2022 21:46:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 21:46:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 21:46:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:46:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:46:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:46:13 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.65625 on epoch=899
03/01/2022 21:46:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 21:46:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:46:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:46:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:46:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:46:25 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.65625 on epoch=924
03/01/2022 21:46:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:46:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:46:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:46:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:46:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:46:37 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.625 on epoch=949
03/01/2022 21:46:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:46:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:46:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:46:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:46:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:46:50 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
03/01/2022 21:46:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:46:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:46:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 21:46:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:47:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:47:02 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.65625 on epoch=999
03/01/2022 21:47:02 - INFO - __main__ - save last model!
03/01/2022 21:47:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:47:02 - INFO - __main__ - Printing 3 examples
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:47:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:47:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:47:02 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:47:02 - INFO - __main__ - Printing 3 examples
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:47:02 - INFO - __main__ - ['not_entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:47:02 - INFO - __main__ - ['not_entailment']
03/01/2022 21:47:02 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:47:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:47:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:47:02 - INFO - __main__ - Printing 3 examples
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:47:02 - INFO - __main__ - ['entailment']
03/01/2022 21:47:02 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:47:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:47:02 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:47:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:47:10 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:47:15 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:47:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:47:16 - INFO - __main__ - Starting training!
03/01/2022 21:50:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_21_0.3_8_predictions.txt
03/01/2022 21:50:08 - INFO - __main__ - ACC on test data: 0.4939
03/01/2022 21:50:08 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.3, bsz=8, dev_performance=0.71875, test_performance=0.4938678381841479
03/01/2022 21:50:08 - INFO - __main__ - Running ... prefix=glue-qnli_16_21, lr=0.2, bsz=8 ...
03/01/2022 21:50:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:50:09 - INFO - __main__ - Printing 3 examples
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: How many conductors are present in the bulb's base? [SEP] sentence: Contact wires and a base with two (or more) conductors provide electrical connections to the filament.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: Who raised Victoria? [SEP] sentence: Both the Duke of Kent and King George III died in 1820, and Victoria was raised under close supervision by her German-born mother Princess Victoria of Saxe-Coburg-Saalfeld.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: How long are all the public beaches together in miles? [SEP] sentence: New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 21:50:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:50:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:50:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:50:09 - INFO - __main__ - Printing 3 examples
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: The Tallgrass Prairie Preserve is the largest protected tallgrass prairie in what area? [SEP] sentence: With 39,000 acres (158 km2), the Tallgrass Prairie Preserve in north-central Oklahoma is the largest protected area of tallgrass prairie in the world and is part of an ecosystem that encompasses only 10 percent of its former land area, once covering 14 states.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: Who stopped attending Cabinet with the passage of the Ministry of Defence Act of 1946? [SEP] sentence: The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ -  [glue-qnli] question: On what wall of a church was the Last Judgment typically painted? [SEP] sentence: Large illuminated bibles and psalters were the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with a Last Judgement on the west wall, a Christ in Majesty at the east end, and narrative biblical scenes down the nave, or in the best surviving example, at Saint-Savin-sur-Gartempe, on the barrel-vaulted roof.
03/01/2022 21:50:09 - INFO - __main__ - ['entailment']
03/01/2022 21:50:09 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:50:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:50:09 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:50:21 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:50:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:50:22 - INFO - __main__ - Starting training!
03/01/2022 21:50:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.86 on epoch=4
03/01/2022 21:50:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.39 on epoch=9
03/01/2022 21:50:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.27 on epoch=14
03/01/2022 21:50:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.23 on epoch=19
03/01/2022 21:50:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.20 on epoch=24
03/01/2022 21:50:35 - INFO - __main__ - Global step 50 Train loss 0.39 ACC 0.5 on epoch=24
03/01/2022 21:50:35 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 21:50:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.17 on epoch=29
03/01/2022 21:50:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 21:50:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 21:50:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.19 on epoch=44
03/01/2022 21:50:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 21:50:47 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.375 on epoch=49
03/01/2022 21:50:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=54
03/01/2022 21:50:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 21:50:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=64
03/01/2022 21:50:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.17 on epoch=69
03/01/2022 21:50:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.15 on epoch=74
03/01/2022 21:50:59 - INFO - __main__ - Global step 150 Train loss 0.17 ACC 0.5 on epoch=74
03/01/2022 21:51:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 21:51:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.14 on epoch=84
03/01/2022 21:51:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.17 on epoch=89
03/01/2022 21:51:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 21:51:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
03/01/2022 21:51:11 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.5 on epoch=99
03/01/2022 21:51:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 21:51:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
03/01/2022 21:51:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
03/01/2022 21:51:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
03/01/2022 21:51:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
03/01/2022 21:51:23 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.46875 on epoch=124
03/01/2022 21:51:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.12 on epoch=129
03/01/2022 21:51:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/01/2022 21:51:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.12 on epoch=139
03/01/2022 21:51:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/01/2022 21:51:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.12 on epoch=149
03/01/2022 21:51:35 - INFO - __main__ - Global step 300 Train loss 0.13 ACC 0.53125 on epoch=149
03/01/2022 21:51:35 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
03/01/2022 21:51:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.12 on epoch=154
03/01/2022 21:51:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
03/01/2022 21:51:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
03/01/2022 21:51:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.09 on epoch=169
03/01/2022 21:51:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
03/01/2022 21:51:47 - INFO - __main__ - Global step 350 Train loss 0.10 ACC 0.53125 on epoch=174
03/01/2022 21:51:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
03/01/2022 21:51:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
03/01/2022 21:51:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
03/01/2022 21:51:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
03/01/2022 21:51:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.09 on epoch=199
03/01/2022 21:51:59 - INFO - __main__ - Global step 400 Train loss 0.09 ACC 0.5 on epoch=199
03/01/2022 21:52:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
03/01/2022 21:52:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
03/01/2022 21:52:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.06 on epoch=214
03/01/2022 21:52:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/01/2022 21:52:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
03/01/2022 21:52:11 - INFO - __main__ - Global step 450 Train loss 0.07 ACC 0.5625 on epoch=224
03/01/2022 21:52:11 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=224, global_step=450
03/01/2022 21:52:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.05 on epoch=229
03/01/2022 21:52:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/01/2022 21:52:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
03/01/2022 21:52:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
03/01/2022 21:52:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
03/01/2022 21:52:23 - INFO - __main__ - Global step 500 Train loss 0.05 ACC 0.5625 on epoch=249
03/01/2022 21:52:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
03/01/2022 21:52:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/01/2022 21:52:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/01/2022 21:52:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/01/2022 21:52:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/01/2022 21:52:35 - INFO - __main__ - Global step 550 Train loss 0.04 ACC 0.5625 on epoch=274
03/01/2022 21:52:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/01/2022 21:52:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/01/2022 21:52:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
03/01/2022 21:52:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 21:52:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/01/2022 21:52:47 - INFO - __main__ - Global step 600 Train loss 0.03 ACC 0.65625 on epoch=299
03/01/2022 21:52:47 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=299, global_step=600
03/01/2022 21:52:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/01/2022 21:52:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/01/2022 21:52:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/01/2022 21:52:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/01/2022 21:52:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 21:53:00 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.625 on epoch=324
03/01/2022 21:53:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
03/01/2022 21:53:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/01/2022 21:53:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/01/2022 21:53:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
03/01/2022 21:53:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/01/2022 21:53:12 - INFO - __main__ - Global step 700 Train loss 0.03 ACC 0.59375 on epoch=349
03/01/2022 21:53:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/01/2022 21:53:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 21:53:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/01/2022 21:53:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 21:53:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 21:53:25 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.59375 on epoch=374
03/01/2022 21:53:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 21:53:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 21:53:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/01/2022 21:53:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/01/2022 21:53:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 21:53:37 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.59375 on epoch=399
03/01/2022 21:53:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 21:53:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 21:53:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 21:53:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/01/2022 21:53:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 21:53:50 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.59375 on epoch=424
03/01/2022 21:53:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/01/2022 21:53:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 21:53:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 21:53:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 21:54:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 21:54:02 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.625 on epoch=449
03/01/2022 21:54:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 21:54:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/01/2022 21:54:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/01/2022 21:54:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 21:54:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/01/2022 21:54:15 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.5625 on epoch=474
03/01/2022 21:54:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 21:54:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 21:54:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 21:54:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 21:54:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/01/2022 21:54:27 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.53125 on epoch=499
03/01/2022 21:54:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 21:54:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 21:54:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/01/2022 21:54:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/01/2022 21:54:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/01/2022 21:54:40 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
03/01/2022 21:54:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 21:54:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 21:54:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/01/2022 21:54:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/01/2022 21:54:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 21:54:52 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.65625 on epoch=549
03/01/2022 21:54:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/01/2022 21:54:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 21:54:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/01/2022 21:55:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 21:55:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 21:55:05 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.6875 on epoch=574
03/01/2022 21:55:05 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=574, global_step=1150
03/01/2022 21:55:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 21:55:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/01/2022 21:55:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 21:55:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/01/2022 21:55:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 21:55:17 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.53125 on epoch=599
03/01/2022 21:55:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 21:55:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 21:55:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 21:55:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 21:55:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 21:55:30 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.59375 on epoch=624
03/01/2022 21:55:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 21:55:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 21:55:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 21:55:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 21:55:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 21:55:42 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.59375 on epoch=649
03/01/2022 21:55:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 21:55:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 21:55:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 21:55:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 21:55:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 21:55:55 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.65625 on epoch=674
03/01/2022 21:55:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/01/2022 21:56:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 21:56:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 21:56:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 21:56:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 21:56:08 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.65625 on epoch=699
03/01/2022 21:56:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 21:56:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
03/01/2022 21:56:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/01/2022 21:56:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 21:56:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/01/2022 21:56:20 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.625 on epoch=724
03/01/2022 21:56:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 21:56:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 21:56:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 21:56:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/01/2022 21:56:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/01/2022 21:56:33 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.6875 on epoch=749
03/01/2022 21:56:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 21:56:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 21:56:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 21:56:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 21:56:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 21:56:45 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.625 on epoch=774
03/01/2022 21:56:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
03/01/2022 21:56:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 21:56:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 21:56:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 21:56:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 21:56:58 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.625 on epoch=799
03/01/2022 21:57:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 21:57:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 21:57:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 21:57:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 21:57:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 21:57:10 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.625 on epoch=824
03/01/2022 21:57:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 21:57:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/01/2022 21:57:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/01/2022 21:57:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 21:57:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 21:57:23 - INFO - __main__ - Global step 1700 Train loss 0.01 ACC 0.59375 on epoch=849
03/01/2022 21:57:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 21:57:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 21:57:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 21:57:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 21:57:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 21:57:35 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.59375 on epoch=874
03/01/2022 21:57:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/01/2022 21:57:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/01/2022 21:57:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 21:57:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 21:57:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 21:57:48 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.59375 on epoch=899
03/01/2022 21:57:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/01/2022 21:57:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 21:57:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 21:57:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 21:57:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 21:58:00 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.625 on epoch=924
03/01/2022 21:58:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 21:58:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 21:58:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 21:58:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 21:58:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 21:58:13 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.59375 on epoch=949
03/01/2022 21:58:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 21:58:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 21:58:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 21:58:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 21:58:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 21:58:25 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.6875 on epoch=974
03/01/2022 21:58:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 21:58:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 21:58:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
03/01/2022 21:58:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 21:58:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 21:58:38 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.6875 on epoch=999
03/01/2022 21:58:38 - INFO - __main__ - save last model!
03/01/2022 21:58:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 21:58:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:58:38 - INFO - __main__ - Printing 3 examples
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 21:58:38 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 21:58:38 - INFO - __main__ - Printing 3 examples
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 21:58:38 - INFO - __main__ - ['entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:58:38 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:58:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 21:58:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 21:58:38 - INFO - __main__ - Printing 3 examples
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 21:58:38 - INFO - __main__ - ['not_entailment']
03/01/2022 21:58:38 - INFO - __main__ - Tokenizing Input ...
03/01/2022 21:58:38 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:58:38 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 21:58:41 - INFO - __main__ - Tokenizing Output ...
03/01/2022 21:58:46 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 21:58:50 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 21:58:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 21:58:51 - INFO - __main__ - Starting training!
03/01/2022 22:01:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_21_0.2_8_predictions.txt
03/01/2022 22:01:53 - INFO - __main__ - ACC on test data: 0.5045
03/01/2022 22:01:54 - INFO - __main__ - prefix=glue-qnli_16_21, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.504484715357862
03/01/2022 22:01:54 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.5, bsz=8 ...
03/01/2022 22:01:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:01:55 - INFO - __main__ - Printing 3 examples
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:01:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:01:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:01:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:01:55 - INFO - __main__ - Printing 3 examples
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:01:55 - INFO - __main__ - ['not_entailment']
03/01/2022 22:01:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:01:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:01:55 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:02:09 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:02:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:02:10 - INFO - __main__ - Starting training!
03/01/2022 22:02:13 - INFO - __main__ - Step 10 Global step 10 Train loss 0.58 on epoch=4
03/01/2022 22:02:15 - INFO - __main__ - Step 20 Global step 20 Train loss 0.21 on epoch=9
03/01/2022 22:02:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.17 on epoch=14
03/01/2022 22:02:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.19 on epoch=19
03/01/2022 22:02:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.19 on epoch=24
03/01/2022 22:02:23 - INFO - __main__ - Global step 50 Train loss 0.27 ACC 0.5 on epoch=24
03/01/2022 22:02:23 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:02:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 22:02:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 22:02:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.14 on epoch=39
03/01/2022 22:02:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.14 on epoch=44
03/01/2022 22:02:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.13 on epoch=49
03/01/2022 22:02:35 - INFO - __main__ - Global step 100 Train loss 0.15 ACC 0.5 on epoch=49
03/01/2022 22:02:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.17 on epoch=54
03/01/2022 22:02:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.12 on epoch=59
03/01/2022 22:02:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.14 on epoch=64
03/01/2022 22:02:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.13 on epoch=69
03/01/2022 22:02:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.15 on epoch=74
03/01/2022 22:02:47 - INFO - __main__ - Global step 150 Train loss 0.14 ACC 0.46875 on epoch=74
03/01/2022 22:02:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.14 on epoch=79
03/01/2022 22:02:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.13 on epoch=84
03/01/2022 22:02:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.12 on epoch=89
03/01/2022 22:02:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
03/01/2022 22:02:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.12 on epoch=99
03/01/2022 22:02:59 - INFO - __main__ - Global step 200 Train loss 0.13 ACC 0.46875 on epoch=99
03/01/2022 22:03:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.09 on epoch=104
03/01/2022 22:03:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.09 on epoch=109
03/01/2022 22:03:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.09 on epoch=114
03/01/2022 22:03:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
03/01/2022 22:03:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.07 on epoch=124
03/01/2022 22:03:11 - INFO - __main__ - Global step 250 Train loss 0.09 ACC 0.4375 on epoch=124
03/01/2022 22:03:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.07 on epoch=129
03/01/2022 22:03:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.10 on epoch=134
03/01/2022 22:03:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.08 on epoch=139
03/01/2022 22:03:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.06 on epoch=144
03/01/2022 22:03:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.05 on epoch=149
03/01/2022 22:03:23 - INFO - __main__ - Global step 300 Train loss 0.07 ACC 0.46875 on epoch=149
03/01/2022 22:03:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
03/01/2022 22:03:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.03 on epoch=159
03/01/2022 22:03:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.04 on epoch=164
03/01/2022 22:03:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.05 on epoch=169
03/01/2022 22:03:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.02 on epoch=174
03/01/2022 22:03:35 - INFO - __main__ - Global step 350 Train loss 0.04 ACC 0.5 on epoch=174
03/01/2022 22:03:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.03 on epoch=179
03/01/2022 22:03:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.04 on epoch=184
03/01/2022 22:03:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.03 on epoch=189
03/01/2022 22:03:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
03/01/2022 22:03:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/01/2022 22:03:48 - INFO - __main__ - Global step 400 Train loss 0.03 ACC 0.4375 on epoch=199
03/01/2022 22:03:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
03/01/2022 22:03:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.04 on epoch=209
03/01/2022 22:03:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/01/2022 22:03:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/01/2022 22:03:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
03/01/2022 22:04:00 - INFO - __main__ - Global step 450 Train loss 0.03 ACC 0.4375 on epoch=224
03/01/2022 22:04:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
03/01/2022 22:04:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.01 on epoch=234
03/01/2022 22:04:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/01/2022 22:04:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/01/2022 22:04:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/01/2022 22:04:12 - INFO - __main__ - Global step 500 Train loss 0.02 ACC 0.53125 on epoch=249
03/01/2022 22:04:12 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=249, global_step=500
03/01/2022 22:04:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/01/2022 22:04:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/01/2022 22:04:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/01/2022 22:04:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/01/2022 22:04:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/01/2022 22:04:24 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.4375 on epoch=274
03/01/2022 22:04:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/01/2022 22:04:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 22:04:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 22:04:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/01/2022 22:04:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/01/2022 22:04:36 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.5 on epoch=299
03/01/2022 22:04:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/01/2022 22:04:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
03/01/2022 22:04:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/01/2022 22:04:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/01/2022 22:04:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 22:04:49 - INFO - __main__ - Global step 650 Train loss 0.01 ACC 0.4375 on epoch=324
03/01/2022 22:04:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 22:04:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/01/2022 22:04:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 22:04:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 22:05:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/01/2022 22:05:01 - INFO - __main__ - Global step 700 Train loss 0.00 ACC 0.46875 on epoch=349
03/01/2022 22:05:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 22:05:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 22:05:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 22:05:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/01/2022 22:05:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 22:05:13 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.40625 on epoch=374
03/01/2022 22:05:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 22:05:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 22:05:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 22:05:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 22:05:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 22:05:25 - INFO - __main__ - Global step 800 Train loss 0.00 ACC 0.46875 on epoch=399
03/01/2022 22:05:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 22:05:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 22:05:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 22:05:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 22:05:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 22:05:37 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.46875 on epoch=424
03/01/2022 22:05:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 22:05:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 22:05:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/01/2022 22:05:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 22:05:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 22:05:49 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.53125 on epoch=449
03/01/2022 22:05:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 22:05:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/01/2022 22:05:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 22:05:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 22:06:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 22:06:02 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.5 on epoch=474
03/01/2022 22:06:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 22:06:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 22:06:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:06:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 22:06:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 22:06:14 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.4375 on epoch=499
03/01/2022 22:06:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 22:06:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 22:06:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:06:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:06:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 22:06:26 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.4375 on epoch=524
03/01/2022 22:06:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:06:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 22:06:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:06:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 22:06:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 22:06:38 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.4375 on epoch=549
03/01/2022 22:06:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:06:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
03/01/2022 22:06:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:06:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 22:06:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:06:50 - INFO - __main__ - Global step 1150 Train loss 0.01 ACC 0.4375 on epoch=574
03/01/2022 22:06:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 22:06:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 22:06:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:06:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 22:07:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:07:02 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.40625 on epoch=599
03/01/2022 22:07:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 22:07:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 22:07:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:07:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 22:07:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:07:13 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.46875 on epoch=624
03/01/2022 22:07:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 22:07:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 22:07:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:07:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:07:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:07:25 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.4375 on epoch=649
03/01/2022 22:07:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:07:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:07:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:07:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:07:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:07:37 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.4375 on epoch=674
03/01/2022 22:07:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:07:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:07:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:07:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:07:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:07:49 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.4375 on epoch=699
03/01/2022 22:07:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 22:07:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:07:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:07:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:08:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:08:01 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.375 on epoch=724
03/01/2022 22:08:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:08:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:08:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:08:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:08:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:08:13 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.4375 on epoch=749
03/01/2022 22:08:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:08:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:08:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:08:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:08:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:08:25 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.46875 on epoch=774
03/01/2022 22:08:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:08:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/01/2022 22:08:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/01/2022 22:08:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:08:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:08:37 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.46875 on epoch=799
03/01/2022 22:08:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:08:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:08:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:08:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:08:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:08:49 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.40625 on epoch=824
03/01/2022 22:08:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:08:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:08:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:08:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:09:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/01/2022 22:09:01 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.46875 on epoch=849
03/01/2022 22:09:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:09:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:09:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 22:09:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:09:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:09:13 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.40625 on epoch=874
03/01/2022 22:09:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:09:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:09:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:09:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:09:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:09:25 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.4375 on epoch=899
03/01/2022 22:09:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:09:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:09:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:09:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:09:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:09:37 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.4375 on epoch=924
03/01/2022 22:09:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:09:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
03/01/2022 22:09:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:09:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:09:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:09:49 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.4375 on epoch=949
03/01/2022 22:09:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:09:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:09:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:09:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:10:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:10:01 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.4375 on epoch=974
03/01/2022 22:10:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:10:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:10:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:10:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:10:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:10:13 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.40625 on epoch=999
03/01/2022 22:10:13 - INFO - __main__ - save last model!
03/01/2022 22:10:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:10:13 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:10:13 - INFO - __main__ - Printing 3 examples
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:10:13 - INFO - __main__ - ['entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:10:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:10:13 - INFO - __main__ - Printing 3 examples
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:10:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:10:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:10:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:10:13 - INFO - __main__ - Printing 3 examples
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:10:13 - INFO - __main__ - ['not_entailment']
03/01/2022 22:10:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:10:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:10:13 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:10:16 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:10:21 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:10:28 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:10:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:10:28 - INFO - __main__ - Starting training!
03/01/2022 22:13:12 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_42_0.5_8_predictions.txt
03/01/2022 22:13:12 - INFO - __main__ - ACC on test data: 0.5113
03/01/2022 22:13:13 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.5112575507962658
03/01/2022 22:13:13 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.4, bsz=8 ...
03/01/2022 22:13:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:13:14 - INFO - __main__ - Printing 3 examples
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:13:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:13:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:13:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:13:14 - INFO - __main__ - Printing 3 examples
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:13:14 - INFO - __main__ - ['not_entailment']
03/01/2022 22:13:14 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:13:14 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:13:14 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:13:28 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:13:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:13:28 - INFO - __main__ - Starting training!
03/01/2022 22:13:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.67 on epoch=4
03/01/2022 22:13:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.25 on epoch=9
03/01/2022 22:13:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.19 on epoch=14
03/01/2022 22:13:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.22 on epoch=19
03/01/2022 22:13:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.17 on epoch=24
03/01/2022 22:13:42 - INFO - __main__ - Global step 50 Train loss 0.30 ACC 0.5 on epoch=24
03/01/2022 22:13:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:13:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.17 on epoch=29
03/01/2022 22:13:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.16 on epoch=34
03/01/2022 22:13:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.16 on epoch=39
03/01/2022 22:13:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.16 on epoch=44
03/01/2022 22:13:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 22:13:53 - INFO - __main__ - Global step 100 Train loss 0.16 ACC 0.53125 on epoch=49
03/01/2022 22:13:53 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=49, global_step=100
03/01/2022 22:13:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.15 on epoch=54
03/01/2022 22:13:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
03/01/2022 22:14:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.13 on epoch=64
03/01/2022 22:14:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.15 on epoch=69
03/01/2022 22:14:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.13 on epoch=74
03/01/2022 22:14:05 - INFO - __main__ - Global step 150 Train loss 0.14 ACC 0.5 on epoch=74
03/01/2022 22:14:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
03/01/2022 22:14:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.12 on epoch=84
03/01/2022 22:14:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.13 on epoch=89
03/01/2022 22:14:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.13 on epoch=94
03/01/2022 22:14:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.12 on epoch=99
03/01/2022 22:14:17 - INFO - __main__ - Global step 200 Train loss 0.13 ACC 0.4375 on epoch=99
03/01/2022 22:14:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.11 on epoch=104
03/01/2022 22:14:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.13 on epoch=109
03/01/2022 22:14:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.09 on epoch=114
03/01/2022 22:14:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.10 on epoch=119
03/01/2022 22:14:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.06 on epoch=124
03/01/2022 22:14:29 - INFO - __main__ - Global step 250 Train loss 0.10 ACC 0.46875 on epoch=124
03/01/2022 22:14:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.08 on epoch=129
03/01/2022 22:14:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.08 on epoch=134
03/01/2022 22:14:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.05 on epoch=139
03/01/2022 22:14:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.05 on epoch=144
03/01/2022 22:14:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.04 on epoch=149
03/01/2022 22:14:41 - INFO - __main__ - Global step 300 Train loss 0.06 ACC 0.375 on epoch=149
03/01/2022 22:14:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.03 on epoch=154
03/01/2022 22:14:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.03 on epoch=159
03/01/2022 22:14:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
03/01/2022 22:14:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
03/01/2022 22:14:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.03 on epoch=174
03/01/2022 22:14:52 - INFO - __main__ - Global step 350 Train loss 0.04 ACC 0.4375 on epoch=174
03/01/2022 22:14:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.05 on epoch=179
03/01/2022 22:14:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.02 on epoch=184
03/01/2022 22:14:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/01/2022 22:15:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.02 on epoch=194
03/01/2022 22:15:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.06 on epoch=199
03/01/2022 22:15:04 - INFO - __main__ - Global step 400 Train loss 0.03 ACC 0.4375 on epoch=199
03/01/2022 22:15:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.02 on epoch=204
03/01/2022 22:15:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.02 on epoch=209
03/01/2022 22:15:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/01/2022 22:15:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/01/2022 22:15:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/01/2022 22:15:16 - INFO - __main__ - Global step 450 Train loss 0.02 ACC 0.34375 on epoch=224
03/01/2022 22:15:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
03/01/2022 22:15:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.01 on epoch=234
03/01/2022 22:15:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/01/2022 22:15:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/01/2022 22:15:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/01/2022 22:15:28 - INFO - __main__ - Global step 500 Train loss 0.01 ACC 0.53125 on epoch=249
03/01/2022 22:15:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/01/2022 22:15:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/01/2022 22:15:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/01/2022 22:15:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
03/01/2022 22:15:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/01/2022 22:15:40 - INFO - __main__ - Global step 550 Train loss 0.01 ACC 0.4375 on epoch=274
03/01/2022 22:15:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
03/01/2022 22:15:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/01/2022 22:15:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 22:15:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 22:15:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/01/2022 22:15:51 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.40625 on epoch=299
03/01/2022 22:15:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/01/2022 22:15:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/01/2022 22:15:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/01/2022 22:16:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 22:16:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 22:16:03 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.46875 on epoch=324
03/01/2022 22:16:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/01/2022 22:16:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 22:16:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/01/2022 22:16:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/01/2022 22:16:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/01/2022 22:16:15 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.4375 on epoch=349
03/01/2022 22:16:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 22:16:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 22:16:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 22:16:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 22:16:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/01/2022 22:16:27 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.4375 on epoch=374
03/01/2022 22:16:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 22:16:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 22:16:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 22:16:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/01/2022 22:16:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
03/01/2022 22:16:39 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.46875 on epoch=399
03/01/2022 22:16:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 22:16:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
03/01/2022 22:16:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 22:16:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 22:16:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 22:16:51 - INFO - __main__ - Global step 850 Train loss 0.01 ACC 0.375 on epoch=424
03/01/2022 22:16:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/01/2022 22:16:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 22:16:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 22:17:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/01/2022 22:17:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 22:17:03 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.4375 on epoch=449
03/01/2022 22:17:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/01/2022 22:17:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 22:17:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 22:17:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/01/2022 22:17:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 22:17:15 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.375 on epoch=474
03/01/2022 22:17:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 22:17:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 22:17:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:17:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/01/2022 22:17:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 22:17:27 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.4375 on epoch=499
03/01/2022 22:17:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 22:17:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 22:17:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:17:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:17:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 22:17:40 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.4375 on epoch=524
03/01/2022 22:17:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:17:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 22:17:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:17:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 22:17:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 22:17:52 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.4375 on epoch=549
03/01/2022 22:17:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:17:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 22:17:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:18:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 22:18:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:18:04 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.46875 on epoch=574
03/01/2022 22:18:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 22:18:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 22:18:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:18:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 22:18:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:18:16 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 0.4375 on epoch=599
03/01/2022 22:18:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 22:18:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 22:18:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:18:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:18:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:18:29 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.5 on epoch=624
03/01/2022 22:18:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 22:18:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
03/01/2022 22:18:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:18:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/01/2022 22:18:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:18:41 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.4375 on epoch=649
03/01/2022 22:18:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:18:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:18:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:18:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:18:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:18:53 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.375 on epoch=674
03/01/2022 22:18:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:18:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:19:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:19:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:19:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:19:05 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.375 on epoch=699
03/01/2022 22:19:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/01/2022 22:19:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:19:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:19:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:19:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:19:18 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.4375 on epoch=724
03/01/2022 22:19:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:19:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 22:19:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:19:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:19:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:19:30 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.4375 on epoch=749
03/01/2022 22:19:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:19:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:19:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:19:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/01/2022 22:19:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:19:42 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.46875 on epoch=774
03/01/2022 22:19:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:19:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:19:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 22:19:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:19:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:19:54 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.46875 on epoch=799
03/01/2022 22:19:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:19:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:20:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:20:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:20:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:20:06 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.4375 on epoch=824
03/01/2022 22:20:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:20:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:20:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:20:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:20:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:20:18 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5 on epoch=849
03/01/2022 22:20:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/01/2022 22:20:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
03/01/2022 22:20:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:20:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:20:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:20:31 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.46875 on epoch=874
03/01/2022 22:20:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:20:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:20:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:20:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:20:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/01/2022 22:20:43 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.5 on epoch=899
03/01/2022 22:20:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:20:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:20:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:20:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:20:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:20:55 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.40625 on epoch=924
03/01/2022 22:20:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:21:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:21:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:21:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:21:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:21:08 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.40625 on epoch=949
03/01/2022 22:21:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:21:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:21:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:21:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:21:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:21:20 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.4375 on epoch=974
03/01/2022 22:21:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:21:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:21:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:21:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:21:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:21:32 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.4375 on epoch=999
03/01/2022 22:21:32 - INFO - __main__ - save last model!
03/01/2022 22:21:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:21:32 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:21:32 - INFO - __main__ - Printing 3 examples
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:21:32 - INFO - __main__ - ['entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:21:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:21:32 - INFO - __main__ - Printing 3 examples
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:21:32 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:21:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:21:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:21:32 - INFO - __main__ - Printing 3 examples
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:21:32 - INFO - __main__ - ['not_entailment']
03/01/2022 22:21:32 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:21:32 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:21:32 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:21:35 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:21:40 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:21:47 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:21:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:21:47 - INFO - __main__ - Starting training!
03/01/2022 22:24:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_42_0.4_8_predictions.txt
03/01/2022 22:24:29 - INFO - __main__ - ACC on test data: 0.5041
03/01/2022 22:24:30 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.5041186161449753
03/01/2022 22:24:30 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.3, bsz=8 ...
03/01/2022 22:24:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:24:31 - INFO - __main__ - Printing 3 examples
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:24:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:24:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:24:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:24:31 - INFO - __main__ - Printing 3 examples
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:24:31 - INFO - __main__ - ['not_entailment']
03/01/2022 22:24:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:24:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:24:31 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:24:45 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:24:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:24:46 - INFO - __main__ - Starting training!
03/01/2022 22:24:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.71 on epoch=4
03/01/2022 22:24:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.26 on epoch=9
03/01/2022 22:24:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.21 on epoch=14
03/01/2022 22:24:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.19 on epoch=19
03/01/2022 22:24:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.19 on epoch=24
03/01/2022 22:24:58 - INFO - __main__ - Global step 50 Train loss 0.31 ACC 0.5 on epoch=24
03/01/2022 22:24:58 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:25:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.19 on epoch=29
03/01/2022 22:25:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.20 on epoch=34
03/01/2022 22:25:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.15 on epoch=39
03/01/2022 22:25:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.16 on epoch=44
03/01/2022 22:25:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 22:25:11 - INFO - __main__ - Global step 100 Train loss 0.17 ACC 0.5 on epoch=49
03/01/2022 22:25:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.16 on epoch=54
03/01/2022 22:25:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.15 on epoch=59
03/01/2022 22:25:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 22:25:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 22:25:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 22:25:23 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.5 on epoch=74
03/01/2022 22:25:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.17 on epoch=79
03/01/2022 22:25:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/01/2022 22:25:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 22:25:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 22:25:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.16 on epoch=99
03/01/2022 22:25:35 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.46875 on epoch=99
03/01/2022 22:25:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/01/2022 22:25:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.12 on epoch=109
03/01/2022 22:25:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
03/01/2022 22:25:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.11 on epoch=119
03/01/2022 22:25:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.11 on epoch=124
03/01/2022 22:25:48 - INFO - __main__ - Global step 250 Train loss 0.13 ACC 0.53125 on epoch=124
03/01/2022 22:25:48 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=124, global_step=250
03/01/2022 22:25:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
03/01/2022 22:25:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.09 on epoch=134
03/01/2022 22:25:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/01/2022 22:25:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
03/01/2022 22:25:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.10 on epoch=149
03/01/2022 22:26:00 - INFO - __main__ - Global step 300 Train loss 0.10 ACC 0.46875 on epoch=149
03/01/2022 22:26:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.08 on epoch=154
03/01/2022 22:26:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
03/01/2022 22:26:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.06 on epoch=164
03/01/2022 22:26:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.06 on epoch=169
03/01/2022 22:26:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
03/01/2022 22:26:13 - INFO - __main__ - Global step 350 Train loss 0.08 ACC 0.46875 on epoch=174
03/01/2022 22:26:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.04 on epoch=179
03/01/2022 22:26:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
03/01/2022 22:26:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
03/01/2022 22:26:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.06 on epoch=194
03/01/2022 22:26:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
03/01/2022 22:26:25 - INFO - __main__ - Global step 400 Train loss 0.05 ACC 0.5 on epoch=199
03/01/2022 22:26:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.05 on epoch=204
03/01/2022 22:26:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.04 on epoch=209
03/01/2022 22:26:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
03/01/2022 22:26:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.04 on epoch=219
03/01/2022 22:26:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.05 on epoch=224
03/01/2022 22:26:38 - INFO - __main__ - Global step 450 Train loss 0.04 ACC 0.4375 on epoch=224
03/01/2022 22:26:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.04 on epoch=229
03/01/2022 22:26:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
03/01/2022 22:26:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/01/2022 22:26:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
03/01/2022 22:26:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/01/2022 22:26:50 - INFO - __main__ - Global step 500 Train loss 0.04 ACC 0.5 on epoch=249
03/01/2022 22:26:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.03 on epoch=254
03/01/2022 22:26:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/01/2022 22:26:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
03/01/2022 22:26:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/01/2022 22:27:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/01/2022 22:27:02 - INFO - __main__ - Global step 550 Train loss 0.03 ACC 0.5 on epoch=274
03/01/2022 22:27:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/01/2022 22:27:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/01/2022 22:27:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
03/01/2022 22:27:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/01/2022 22:27:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/01/2022 22:27:15 - INFO - __main__ - Global step 600 Train loss 0.02 ACC 0.5 on epoch=299
03/01/2022 22:27:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/01/2022 22:27:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 22:27:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/01/2022 22:27:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/01/2022 22:27:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/01/2022 22:27:27 - INFO - __main__ - Global step 650 Train loss 0.02 ACC 0.5 on epoch=324
03/01/2022 22:27:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/01/2022 22:27:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/01/2022 22:27:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/01/2022 22:27:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/01/2022 22:27:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/01/2022 22:27:40 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.5 on epoch=349
03/01/2022 22:27:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/01/2022 22:27:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/01/2022 22:27:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 22:27:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/01/2022 22:27:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/01/2022 22:27:52 - INFO - __main__ - Global step 750 Train loss 0.01 ACC 0.46875 on epoch=374
03/01/2022 22:27:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/01/2022 22:27:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/01/2022 22:27:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 22:28:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/01/2022 22:28:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/01/2022 22:28:05 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.4375 on epoch=399
03/01/2022 22:28:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/01/2022 22:28:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/01/2022 22:28:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
03/01/2022 22:28:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/01/2022 22:28:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/01/2022 22:28:17 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.5 on epoch=424
03/01/2022 22:28:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/01/2022 22:28:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 22:28:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 22:28:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 22:28:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 22:28:30 - INFO - __main__ - Global step 900 Train loss 0.00 ACC 0.5 on epoch=449
03/01/2022 22:28:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/01/2022 22:28:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/01/2022 22:28:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/01/2022 22:28:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/01/2022 22:28:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/01/2022 22:28:42 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.46875 on epoch=474
03/01/2022 22:28:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/01/2022 22:28:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/01/2022 22:28:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/01/2022 22:28:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/01/2022 22:28:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 22:28:55 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 0.5 on epoch=499
03/01/2022 22:28:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 22:28:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/01/2022 22:29:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:29:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:29:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 22:29:07 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5 on epoch=524
03/01/2022 22:29:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:29:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 22:29:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:29:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/01/2022 22:29:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/01/2022 22:29:20 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5 on epoch=549
03/01/2022 22:29:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:29:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/01/2022 22:29:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:29:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 22:29:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:29:32 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.5625 on epoch=574
03/01/2022 22:29:32 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=574, global_step=1150
03/01/2022 22:29:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 22:29:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 22:29:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:29:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/01/2022 22:29:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:29:44 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.53125 on epoch=599
03/01/2022 22:29:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/01/2022 22:29:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 22:29:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/01/2022 22:29:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:29:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:29:57 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.5625 on epoch=624
03/01/2022 22:29:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/01/2022 22:30:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/01/2022 22:30:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:30:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:30:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:30:09 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.46875 on epoch=649
03/01/2022 22:30:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:30:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:30:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:30:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:30:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:30:21 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.53125 on epoch=674
03/01/2022 22:30:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=679
03/01/2022 22:30:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:30:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:30:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:30:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/01/2022 22:30:33 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
03/01/2022 22:30:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/01/2022 22:30:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:30:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:30:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:30:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:30:46 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.46875 on epoch=724
03/01/2022 22:30:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:30:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:30:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:30:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:30:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:30:58 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.4375 on epoch=749
03/01/2022 22:31:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:31:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
03/01/2022 22:31:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/01/2022 22:31:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:31:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:31:10 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 0.5 on epoch=774
03/01/2022 22:31:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:31:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:31:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/01/2022 22:31:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:31:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:31:23 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.5 on epoch=799
03/01/2022 22:31:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/01/2022 22:31:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:31:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:31:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/01/2022 22:31:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:31:35 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 0.5 on epoch=824
03/01/2022 22:31:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:31:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/01/2022 22:31:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:31:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/01/2022 22:31:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:31:47 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.46875 on epoch=849
03/01/2022 22:31:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:31:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/01/2022 22:31:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/01/2022 22:31:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:31:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:31:59 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
03/01/2022 22:32:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:32:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:32:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:32:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:32:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:32:12 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.53125 on epoch=899
03/01/2022 22:32:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:32:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:32:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:32:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:32:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/01/2022 22:32:24 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.5625 on epoch=924
03/01/2022 22:32:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:32:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:32:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:32:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:32:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:32:36 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.53125 on epoch=949
03/01/2022 22:32:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
03/01/2022 22:32:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
03/01/2022 22:32:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/01/2022 22:32:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:32:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:32:49 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.5625 on epoch=974
03/01/2022 22:32:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:32:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:32:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:32:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:33:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:33:01 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.5 on epoch=999
03/01/2022 22:33:01 - INFO - __main__ - save last model!
03/01/2022 22:33:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:33:01 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:33:01 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:33:01 - INFO - __main__ - ['entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:33:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:33:01 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:33:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:33:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:33:01 - INFO - __main__ - Printing 3 examples
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:33:01 - INFO - __main__ - ['not_entailment']
03/01/2022 22:33:01 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:33:01 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:01 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:33:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:33:09 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:33:14 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:33:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:33:15 - INFO - __main__ - Starting training!
03/01/2022 22:36:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_42_0.3_8_predictions.txt
03/01/2022 22:36:01 - INFO - __main__ - ACC on test data: 0.5030
03/01/2022 22:36:01 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.5030203185063152
03/01/2022 22:36:01 - INFO - __main__ - Running ... prefix=glue-qnli_16_42, lr=0.2, bsz=8 ...
03/01/2022 22:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:36:02 - INFO - __main__ - Printing 3 examples
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: In what year did Robert Louis Stevenson die? [SEP] sentence: Mission work in Samoa had begun in late 1830 by John Williams, of the London Missionary Society arriving in Sapapali'i from The Cook Islands and Tahiti.
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: Who was the author of Conversations on the Plurality of Worlds (1686)? [SEP] sentence: Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions.
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: What are some courses Eton offers in the summer months? [SEP] sentence: These comparatively new developments will run alongside long-established courses that Eton has provided for pupils from state schools, most of them in the summer holidays (July and August).
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:36:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:36:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:36:02 - INFO - __main__ - Printing 3 examples
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: In what year were the French defeated in Southern Germany by the Archduke Charles? [SEP] sentence: In the first notable encounter between the two commanders, Napoleon pushed back his opponent and advanced deep into Austrian territory after winning at the Battle of Tarvis in March 1797.
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: Which former contestant replaced Keith Urban for auditions in New York City? [SEP] sentence: Randy Jackson did not return as the in-house mentor for this season.
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ -  [glue-qnli] question: What happened to the Sands Atlantic City a year after it closed? [SEP] sentence: The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City.
03/01/2022 22:36:02 - INFO - __main__ - ['not_entailment']
03/01/2022 22:36:02 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:36:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:36:02 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:36:16 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:36:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:36:17 - INFO - __main__ - Starting training!
03/01/2022 22:36:23 - INFO - __main__ - Step 10 Global step 10 Train loss 0.90 on epoch=4
03/01/2022 22:36:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.39 on epoch=9
03/01/2022 22:36:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.25 on epoch=14
03/01/2022 22:36:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.21 on epoch=19
03/01/2022 22:36:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.22 on epoch=24
03/01/2022 22:36:33 - INFO - __main__ - Global step 50 Train loss 0.39 ACC 0.5 on epoch=24
03/01/2022 22:36:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/01/2022 22:36:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.18 on epoch=29
03/01/2022 22:36:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.21 on epoch=34
03/01/2022 22:36:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.21 on epoch=39
03/01/2022 22:36:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.17 on epoch=44
03/01/2022 22:36:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.16 on epoch=49
03/01/2022 22:36:45 - INFO - __main__ - Global step 100 Train loss 0.19 ACC 0.5 on epoch=49
03/01/2022 22:36:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.18 on epoch=54
03/01/2022 22:36:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
03/01/2022 22:36:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 22:36:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.13 on epoch=69
03/01/2022 22:36:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 22:36:57 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.5 on epoch=74
03/01/2022 22:36:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.18 on epoch=79
03/01/2022 22:37:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.15 on epoch=84
03/01/2022 22:37:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
03/01/2022 22:37:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/01/2022 22:37:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.15 on epoch=99
03/01/2022 22:37:09 - INFO - __main__ - Global step 200 Train loss 0.16 ACC 0.5 on epoch=99
03/01/2022 22:37:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/01/2022 22:37:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/01/2022 22:37:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
03/01/2022 22:37:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/01/2022 22:37:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/01/2022 22:37:21 - INFO - __main__ - Global step 250 Train loss 0.15 ACC 0.40625 on epoch=124
03/01/2022 22:37:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.14 on epoch=129
03/01/2022 22:37:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/01/2022 22:37:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.16 on epoch=139
03/01/2022 22:37:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.12 on epoch=144
03/01/2022 22:37:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.13 on epoch=149
03/01/2022 22:37:34 - INFO - __main__ - Global step 300 Train loss 0.14 ACC 0.40625 on epoch=149
03/01/2022 22:37:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.15 on epoch=154
03/01/2022 22:37:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.13 on epoch=159
03/01/2022 22:37:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/01/2022 22:37:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
03/01/2022 22:37:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=174
03/01/2022 22:37:47 - INFO - __main__ - Global step 350 Train loss 0.14 ACC 0.4375 on epoch=174
03/01/2022 22:37:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.12 on epoch=179
03/01/2022 22:37:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.13 on epoch=184
03/01/2022 22:37:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.11 on epoch=189
03/01/2022 22:37:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.10 on epoch=194
03/01/2022 22:37:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
03/01/2022 22:37:59 - INFO - __main__ - Global step 400 Train loss 0.12 ACC 0.40625 on epoch=199
03/01/2022 22:38:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.09 on epoch=204
03/01/2022 22:38:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=209
03/01/2022 22:38:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
03/01/2022 22:38:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.12 on epoch=219
03/01/2022 22:38:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
03/01/2022 22:38:11 - INFO - __main__ - Global step 450 Train loss 0.12 ACC 0.40625 on epoch=224
03/01/2022 22:38:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
03/01/2022 22:38:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=234
03/01/2022 22:38:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
03/01/2022 22:38:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=244
03/01/2022 22:38:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/01/2022 22:38:23 - INFO - __main__ - Global step 500 Train loss 0.09 ACC 0.4375 on epoch=249
03/01/2022 22:38:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
03/01/2022 22:38:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
03/01/2022 22:38:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=264
03/01/2022 22:38:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
03/01/2022 22:38:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
03/01/2022 22:38:35 - INFO - __main__ - Global step 550 Train loss 0.08 ACC 0.40625 on epoch=274
03/01/2022 22:38:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=279
03/01/2022 22:38:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
03/01/2022 22:38:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.07 on epoch=289
03/01/2022 22:38:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
03/01/2022 22:38:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=299
03/01/2022 22:38:47 - INFO - __main__ - Global step 600 Train loss 0.07 ACC 0.375 on epoch=299
03/01/2022 22:38:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
03/01/2022 22:38:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.05 on epoch=309
03/01/2022 22:38:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/01/2022 22:38:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/01/2022 22:38:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/01/2022 22:38:59 - INFO - __main__ - Global step 650 Train loss 0.03 ACC 0.46875 on epoch=324
03/01/2022 22:39:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
03/01/2022 22:39:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/01/2022 22:39:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
03/01/2022 22:39:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/01/2022 22:39:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
03/01/2022 22:39:11 - INFO - __main__ - Global step 700 Train loss 0.04 ACC 0.53125 on epoch=349
03/01/2022 22:39:11 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=349, global_step=700
03/01/2022 22:39:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/01/2022 22:39:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/01/2022 22:39:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/01/2022 22:39:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/01/2022 22:39:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/01/2022 22:39:23 - INFO - __main__ - Global step 750 Train loss 0.02 ACC 0.375 on epoch=374
03/01/2022 22:39:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/01/2022 22:39:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/01/2022 22:39:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/01/2022 22:39:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
03/01/2022 22:39:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/01/2022 22:39:35 - INFO - __main__ - Global step 800 Train loss 0.02 ACC 0.40625 on epoch=399
03/01/2022 22:39:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
03/01/2022 22:39:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/01/2022 22:39:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/01/2022 22:39:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/01/2022 22:39:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/01/2022 22:39:47 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 0.46875 on epoch=424
03/01/2022 22:39:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/01/2022 22:39:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/01/2022 22:39:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
03/01/2022 22:39:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/01/2022 22:39:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/01/2022 22:39:59 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 0.5 on epoch=449
03/01/2022 22:40:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 22:40:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 22:40:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 22:40:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
03/01/2022 22:40:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 22:40:11 - INFO - __main__ - Global step 950 Train loss 0.01 ACC 0.59375 on epoch=474
03/01/2022 22:40:11 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=474, global_step=950
03/01/2022 22:40:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 22:40:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/01/2022 22:40:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:40:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/01/2022 22:40:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/01/2022 22:40:23 - INFO - __main__ - Global step 1000 Train loss 0.02 ACC 0.5 on epoch=499
03/01/2022 22:40:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/01/2022 22:40:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 22:40:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/01/2022 22:40:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/01/2022 22:40:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/01/2022 22:40:35 - INFO - __main__ - Global step 1050 Train loss 0.01 ACC 0.5625 on epoch=524
03/01/2022 22:40:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:40:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 22:40:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/01/2022 22:40:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/01/2022 22:40:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/01/2022 22:40:47 - INFO - __main__ - Global step 1100 Train loss 0.01 ACC 0.5625 on epoch=549
03/01/2022 22:40:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:40:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
03/01/2022 22:40:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:40:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
03/01/2022 22:40:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:40:59 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.375 on epoch=574
03/01/2022 22:41:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/01/2022 22:41:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/01/2022 22:41:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/01/2022 22:41:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/01/2022 22:41:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/01/2022 22:41:11 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.4375 on epoch=599
03/01/2022 22:41:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
03/01/2022 22:41:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/01/2022 22:41:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/01/2022 22:41:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/01/2022 22:41:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/01/2022 22:41:23 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.4375 on epoch=624
03/01/2022 22:41:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
03/01/2022 22:41:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 22:41:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/01/2022 22:41:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/01/2022 22:41:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/01/2022 22:41:35 - INFO - __main__ - Global step 1300 Train loss 0.01 ACC 0.5 on epoch=649
03/01/2022 22:41:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:41:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:41:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:41:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:41:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/01/2022 22:41:47 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.5 on epoch=674
03/01/2022 22:41:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:41:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
03/01/2022 22:41:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:41:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/01/2022 22:41:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/01/2022 22:41:59 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.5 on epoch=699
03/01/2022 22:42:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/01/2022 22:42:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:42:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:42:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/01/2022 22:42:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:42:11 - INFO - __main__ - Global step 1450 Train loss 0.01 ACC 0.5625 on epoch=724
03/01/2022 22:42:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/01/2022 22:42:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/01/2022 22:42:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
03/01/2022 22:42:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:42:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:42:23 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 0.46875 on epoch=749
03/01/2022 22:42:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:42:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:42:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:42:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:42:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:42:35 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.53125 on epoch=774
03/01/2022 22:42:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:42:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
03/01/2022 22:42:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/01/2022 22:42:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:42:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:42:47 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.5625 on epoch=799
03/01/2022 22:42:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:42:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:42:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:42:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:42:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/01/2022 22:42:59 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.53125 on epoch=824
03/01/2022 22:43:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:43:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/01/2022 22:43:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:43:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:43:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:43:11 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.5625 on epoch=849
03/01/2022 22:43:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:43:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:43:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/01/2022 22:43:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
03/01/2022 22:43:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:43:23 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.5 on epoch=874
03/01/2022 22:43:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
03/01/2022 22:43:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
03/01/2022 22:43:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:43:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:43:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:43:35 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.5625 on epoch=899
03/01/2022 22:43:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:43:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=909
03/01/2022 22:43:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/01/2022 22:43:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:43:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:43:47 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.59375 on epoch=924
03/01/2022 22:43:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/01/2022 22:43:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:43:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/01/2022 22:43:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
03/01/2022 22:43:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:43:59 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.5625 on epoch=949
03/01/2022 22:44:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/01/2022 22:44:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/01/2022 22:44:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:44:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:44:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:44:11 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.625 on epoch=974
03/01/2022 22:44:11 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=974, global_step=1950
03/01/2022 22:44:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:44:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:44:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/01/2022 22:44:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:44:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:44:23 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.5625 on epoch=999
03/01/2022 22:44:23 - INFO - __main__ - save last model!
03/01/2022 22:44:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:44:23 - INFO - __main__ - Printing 3 examples
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:44:23 - INFO - __main__ - ['not_entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:44:23 - INFO - __main__ - ['not_entailment']
03/01/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:44:23 - INFO - __main__ - Printing 3 examples
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 22:44:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:44:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:44:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:44:23 - INFO - __main__ - Printing 3 examples
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/01/2022 22:44:23 - INFO - __main__ - ['entailment']
03/01/2022 22:44:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:44:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:44:23 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:44:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:44:31 - INFO - __main__ - Loaded 5463 examples from test data
03/01/2022 22:44:37 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:44:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:44:38 - INFO - __main__ - Starting training!
03/01/2022 22:47:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli/glue-qnli_16_42_0.2_8_predictions.txt
03/01/2022 22:47:21 - INFO - __main__ - ACC on test data: 0.5248
03/01/2022 22:47:22 - INFO - __main__ - prefix=glue-qnli_16_42, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.5248032216730734
03/01/2022 22:47:22 - INFO - __main__ - Running ... prefix=glue-qnli_16_87, lr=0.5, bsz=8 ...
03/01/2022 22:47:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:47:23 - INFO - __main__ - Printing 3 examples
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: Where does the Rhine originate? [SEP] sentence: The Rhine originates in a 30 square kilometre area in Switzerland and represents almost 60 percent of water exported from the country.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: What is the children's agency of the United Nations Organization? [SEP] sentence: The United Nations Organization and its children's agency UNICEF withdrew their staff, saying that it wasn't sure the event would help its mission of raising awareness of conditions for children and amid concerns that the relay would be used as a propaganda stunt.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: In what year did Paul VI die? [SEP] sentence: This became the last of Paul VI's consistories before his death in August 1978.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 22:47:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:47:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 22:47:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 22:47:23 - INFO - __main__ - Printing 3 examples
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: What movement came out of the French Revolution? [SEP] sentence: Nationalism during the 19th century threatened the old aristocratic regimes.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: What is not possible unless the reactants surmount an energy barrier known as the activation energy? [SEP] sentence: Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ -  [glue-qnli] question: What percent of sales are contributed by hunters? [SEP] sentence: Although non-hunters buy a significant number of Duck Stamps, eighty-seven percent of their sales are contributed by hunters, which is logical, as hunters are required to purchase them.
03/01/2022 22:47:23 - INFO - __main__ - ['entailment']
03/01/2022 22:47:23 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:47:23 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:47:23 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 22:47:37 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 22:47:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 22:47:38 - INFO - __main__ - Starting training!
03/01/2022 22:47:41 - INFO - __main__ - Step 10 Global step 10 Train loss 0.59 on epoch=4
03/01/2022 22:47:43 - INFO - __main__ - Step 20 Global step 20 Train loss 0.25 on epoch=9
03/01/2022 22:47:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.18 on epoch=14
03/01/2022 22:47:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.19 on epoch=19
03/01/2022 22:47:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.18 on epoch=24
03/01/2022 22:47:51 - INFO - __main__ - Global step 50 Train loss 0.28 ACC 0.34375 on epoch=24
03/01/2022 22:47:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.34375 on epoch=24, global_step=50
03/01/2022 22:47:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.17 on epoch=29
03/01/2022 22:47:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.18 on epoch=34
03/01/2022 22:47:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.17 on epoch=39
03/01/2022 22:48:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.18 on epoch=44
03/01/2022 22:48:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.17 on epoch=49
03/01/2022 22:48:05 - INFO - __main__ - Global step 100 Train loss 0.18 ACC 0.46875 on epoch=49
03/01/2022 22:48:05 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.46875 on epoch=49, global_step=100
03/01/2022 22:48:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.15 on epoch=54
03/01/2022 22:48:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.16 on epoch=59
03/01/2022 22:48:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.15 on epoch=64
03/01/2022 22:48:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.16 on epoch=69
03/01/2022 22:48:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.17 on epoch=74
03/01/2022 22:48:18 - INFO - __main__ - Global step 150 Train loss 0.16 ACC 0.34375 on epoch=74
03/01/2022 22:48:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.15 on epoch=79
03/01/2022 22:48:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.12 on epoch=84
03/01/2022 22:48:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.15 on epoch=89
03/01/2022 22:48:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.14 on epoch=94
03/01/2022 22:48:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.12 on epoch=99
03/01/2022 22:48:31 - INFO - __main__ - Global step 200 Train loss 0.14 ACC 0.46875 on epoch=99
03/01/2022 22:48:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/01/2022 22:48:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.11 on epoch=109
03/01/2022 22:48:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.10 on epoch=114
03/01/2022 22:48:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.10 on epoch=119
03/01/2022 22:48:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.10 on epoch=124
03/01/2022 22:48:44 - INFO - __main__ - Global step 250 Train loss 0.11 ACC 0.5 on epoch=124
03/01/2022 22:48:44 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=124, global_step=250
03/01/2022 22:48:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.07 on epoch=129
03/01/2022 22:48:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.07 on epoch=134
03/01/2022 22:48:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.06 on epoch=139
03/01/2022 22:48:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.04 on epoch=144
03/01/2022 22:48:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.06 on epoch=149
03/01/2022 22:48:57 - INFO - __main__ - Global step 300 Train loss 0.06 ACC 0.4375 on epoch=149
03/01/2022 22:49:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.05 on epoch=154
03/01/2022 22:49:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.04 on epoch=159
03/01/2022 22:49:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.03 on epoch=164
03/01/2022 22:49:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.03 on epoch=169
03/01/2022 22:49:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.04 on epoch=174
03/01/2022 22:49:10 - INFO - __main__ - Global step 350 Train loss 0.04 ACC 0.375 on epoch=174
03/01/2022 22:49:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.01 on epoch=179
03/01/2022 22:49:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/01/2022 22:49:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/01/2022 22:49:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/01/2022 22:49:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/01/2022 22:49:23 - INFO - __main__ - Global step 400 Train loss 0.01 ACC 0.34375 on epoch=199
03/01/2022 22:49:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.02 on epoch=204
03/01/2022 22:49:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
03/01/2022 22:49:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/01/2022 22:49:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/01/2022 22:49:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/01/2022 22:49:36 - INFO - __main__ - Global step 450 Train loss 0.01 ACC 0.46875 on epoch=224
03/01/2022 22:49:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/01/2022 22:49:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/01/2022 22:49:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/01/2022 22:49:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/01/2022 22:49:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/01/2022 22:49:49 - INFO - __main__ - Global step 500 Train loss 0.01 ACC 0.40625 on epoch=249
03/01/2022 22:49:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/01/2022 22:49:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/01/2022 22:49:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/01/2022 22:49:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/01/2022 22:50:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/01/2022 22:50:02 - INFO - __main__ - Global step 550 Train loss 0.00 ACC 0.375 on epoch=274
03/01/2022 22:50:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/01/2022 22:50:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/01/2022 22:50:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/01/2022 22:50:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/01/2022 22:50:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/01/2022 22:50:15 - INFO - __main__ - Global step 600 Train loss 0.01 ACC 0.4375 on epoch=299
03/01/2022 22:50:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/01/2022 22:50:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/01/2022 22:50:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/01/2022 22:50:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/01/2022 22:50:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/01/2022 22:50:28 - INFO - __main__ - Global step 650 Train loss 0.00 ACC 0.375 on epoch=324
03/01/2022 22:50:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/01/2022 22:50:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/01/2022 22:50:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
03/01/2022 22:50:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/01/2022 22:50:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/01/2022 22:50:41 - INFO - __main__ - Global step 700 Train loss 0.01 ACC 0.4375 on epoch=349
03/01/2022 22:50:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/01/2022 22:50:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/01/2022 22:50:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/01/2022 22:50:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/01/2022 22:50:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/01/2022 22:50:54 - INFO - __main__ - Global step 750 Train loss 0.00 ACC 0.375 on epoch=374
03/01/2022 22:50:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/01/2022 22:50:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/01/2022 22:51:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/01/2022 22:51:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/01/2022 22:51:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/01/2022 22:51:07 - INFO - __main__ - Global step 800 Train loss 0.01 ACC 0.375 on epoch=399
03/01/2022 22:51:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/01/2022 22:51:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/01/2022 22:51:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/01/2022 22:51:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/01/2022 22:51:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/01/2022 22:51:20 - INFO - __main__ - Global step 850 Train loss 0.00 ACC 0.375 on epoch=424
03/01/2022 22:51:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/01/2022 22:51:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/01/2022 22:51:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/01/2022 22:51:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/01/2022 22:51:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/01/2022 22:51:33 - INFO - __main__ - Global step 900 Train loss 0.01 ACC 0.34375 on epoch=449
03/01/2022 22:51:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/01/2022 22:51:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/01/2022 22:51:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/01/2022 22:51:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/01/2022 22:51:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/01/2022 22:51:46 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 0.40625 on epoch=474
03/01/2022 22:51:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/01/2022 22:51:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/01/2022 22:51:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/01/2022 22:51:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/01/2022 22:51:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/01/2022 22:51:59 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 0.375 on epoch=499
03/01/2022 22:52:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/01/2022 22:52:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/01/2022 22:52:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/01/2022 22:52:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/01/2022 22:52:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/01/2022 22:52:12 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 0.375 on epoch=524
03/01/2022 22:52:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/01/2022 22:52:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/01/2022 22:52:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/01/2022 22:52:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/01/2022 22:52:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/01/2022 22:52:24 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 0.375 on epoch=549
03/01/2022 22:52:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/01/2022 22:52:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/01/2022 22:52:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/01/2022 22:52:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/01/2022 22:52:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/01/2022 22:52:37 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 0.40625 on epoch=574
03/01/2022 22:52:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/01/2022 22:52:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
03/01/2022 22:52:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/01/2022 22:52:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/01/2022 22:52:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/01/2022 22:52:50 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.34375 on epoch=599
03/01/2022 22:52:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/01/2022 22:52:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/01/2022 22:52:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/01/2022 22:53:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/01/2022 22:53:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/01/2022 22:53:03 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 0.375 on epoch=624
03/01/2022 22:53:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/01/2022 22:53:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/01/2022 22:53:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/01/2022 22:53:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/01/2022 22:53:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/01/2022 22:53:16 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 0.34375 on epoch=649
03/01/2022 22:53:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/01/2022 22:53:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/01/2022 22:53:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/01/2022 22:53:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/01/2022 22:53:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/01/2022 22:53:28 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 0.34375 on epoch=674
03/01/2022 22:53:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/01/2022 22:53:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/01/2022 22:53:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/01/2022 22:53:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/01/2022 22:53:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/01/2022 22:53:40 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 0.34375 on epoch=699
03/01/2022 22:53:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/01/2022 22:53:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/01/2022 22:53:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/01/2022 22:53:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/01/2022 22:53:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/01/2022 22:53:53 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.375 on epoch=724
03/01/2022 22:53:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/01/2022 22:53:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/01/2022 22:54:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/01/2022 22:54:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/01/2022 22:54:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/01/2022 22:54:05 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.375 on epoch=749
03/01/2022 22:54:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/01/2022 22:54:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/01/2022 22:54:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/01/2022 22:54:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/01/2022 22:54:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/01/2022 22:54:18 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.28125 on epoch=774
03/01/2022 22:54:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/01/2022 22:54:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/01/2022 22:54:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/01/2022 22:54:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/01/2022 22:54:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/01/2022 22:54:30 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.375 on epoch=799
03/01/2022 22:54:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/01/2022 22:54:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/01/2022 22:54:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/01/2022 22:54:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/01/2022 22:54:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
03/01/2022 22:54:43 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.40625 on epoch=824
03/01/2022 22:54:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/01/2022 22:54:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/01/2022 22:54:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/01/2022 22:54:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/01/2022 22:54:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/01/2022 22:54:55 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.34375 on epoch=849
03/01/2022 22:54:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/01/2022 22:55:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/01/2022 22:55:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
03/01/2022 22:55:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/01/2022 22:55:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/01/2022 22:55:08 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 0.34375 on epoch=874
03/01/2022 22:55:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/01/2022 22:55:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/01/2022 22:55:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/01/2022 22:55:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/01/2022 22:55:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/01/2022 22:55:20 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.40625 on epoch=899
03/01/2022 22:55:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/01/2022 22:55:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/01/2022 22:55:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/01/2022 22:55:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/01/2022 22:55:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/01/2022 22:55:32 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.34375 on epoch=924
03/01/2022 22:55:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/01/2022 22:55:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/01/2022 22:55:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/01/2022 22:55:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/01/2022 22:55:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/01/2022 22:55:45 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.34375 on epoch=949
03/01/2022 22:55:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/01/2022 22:55:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/01/2022 22:55:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/01/2022 22:55:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/01/2022 22:55:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/01/2022 22:55:57 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.40625 on epoch=974
03/01/2022 22:56:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/01/2022 22:56:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/01/2022 22:56:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/01/2022 22:56:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/01/2022 22:56:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/01/2022 22:56:10 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.375 on epoch=999
03/01/2022 22:56:10 - INFO - __main__ - save last model!
03/01/2022 22:56:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 22:56:10 - INFO - __main__ - Start tokenizing ... 5463 instances
03/01/2022 22:56:10 - INFO - __main__ - Printing 3 examples
03/01/2022 22:56:10 - INFO - __main__ -  [glue-qnli] question: What came into force after the new constitution was herald? [SEP] sentence: As of that day, the new constitution heralding the Second Republic came into force.
03/01/2022 22:56:10 - INFO - __main__ - ['entailment']
03/01/2022 22:56:10 - INFO - __main__ -  [glue-qnli] question: What is the first major city in the stream of the Rhine? [SEP] sentence: The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
03/01/2022 22:56:10 - INFO - __main__ - ['not_entailment']
03/01/2022 22:56:10 - INFO - __main__ -  [glue-qnli] question: What is the minimum required if you want to teach in Canada? [SEP] sentence: In most provinces a second Bachelor's Degree such as a Bachelor of Education is required to become a qualified teacher.
03/01/2022 22:56:10 - INFO - __main__ - ['not_entailment']
03/01/2022 22:56:10 - INFO - __main__ - Tokenizing Input ...
03/01/2022 22:56:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 22:56:18 - INFO - __main__ - Loaded 5463 examples from test data
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1552, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2459) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_1/1/error.json
03/01/2022 22:56:36 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 22:56:36 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
Output directory () already exists and is not empty.
03/01/2022 22:56:36 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 22:56:36 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 22:56:36 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 22:56:36 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 22:56:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:56:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:56:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:56:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:57:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:58:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 22:59:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:00:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:01:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:02:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:03:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:04:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:05:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:06:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:07:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:08:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:09:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:10:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:11:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:12:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:13:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:14:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:15:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:16:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:17:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:18:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:19:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:20:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:21:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:22:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:23:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:24:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:25:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 23:26:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2593) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_2/1/error.json
Output directory () already exists and is not empty.
03/01/2022 23:26:42 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 23:26:42 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 23:26:42 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 23:26:42 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 23:26:43 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 23:26:43 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 23:26:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:26:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:27:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:28:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:29:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:30:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:31:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:32:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:33:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:34:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:35:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:36:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:37:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:38:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:39:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:40:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:41:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:42:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:43:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:44:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:45:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:46:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:47:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:48:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:49:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:50:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:51:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:52:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:53:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:54:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:55:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 23:56:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2641) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_zl94xjhy/none_p07622m8/attempt_3/1/error.json
Output directory () already exists and is not empty.
03/01/2022 23:56:49 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 23:56:49 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 23:56:49 - INFO - __main__ - Namespace(task_dir='data/glue-qnli/', task_name='glue-qnli', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 23:56:49 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-glue-qnli
03/01/2022 23:56:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 23:56:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 23:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 23:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:04:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:05:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:06:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:07:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:08:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:09:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:10:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:11:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:12:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:13:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:14:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:15:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:16:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:17:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:18:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:19:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:20:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:21:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:22:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:23:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:24:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:25:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 00:26:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
        main()main()

  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2689) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00033926963806152344 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2689", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 17113, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2690", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 17113, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 17113, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2689 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     singletask_from_meta.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-03-02_00:26:54
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2689)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-03-02_00:26:54
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2690)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2758): No such process
Task: hatexplain, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_qu76xg7v/none_xzo5er7q
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qu76xg7v/none_xzo5er7q/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qu76xg7v/none_xzo5er7q/attempt_0/1/error.json
03/02/2022 00:26:57 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 00:26:57 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain
Output directory () already exists and is not empty.
03/02/2022 00:26:57 - INFO - __main__ - Namespace(task_dir='data/hatexplain/', task_name='hatexplain', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 00:26:57 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain
03/02/2022 00:26:58 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 00:26:58 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 00:26:58 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/02/2022 00:26:58 - INFO - __main__ - args.device: cuda:0
03/02/2022 00:26:58 - INFO - __main__ - Using 2 gpus
03/02/2022 00:26:58 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/02/2022 00:26:58 - INFO - __main__ - args.device: cuda:1
03/02/2022 00:26:58 - INFO - __main__ - Using 2 gpus
03/02/2022 00:26:58 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
03/02/2022 00:26:58 - INFO - __main__ - Fine-tuning the following samples: ['hatexplain_16_100', 'hatexplain_16_13', 'hatexplain_16_21', 'hatexplain_16_42', 'hatexplain_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/02/2022 00:27:03 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.5, bsz=8 ...
03/02/2022 00:27:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:04 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:04 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:27:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:04 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:04 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:27:04 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:27:04 - INFO - __main__ - Printing 3 examples
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:27:04 - INFO - __main__ - ['offensive']
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:27:04 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:27:04 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:27:19 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:27:19 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:27:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:27:22 - INFO - __main__ - Starting training!
03/02/2022 00:27:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:27:27 - INFO - __main__ - Starting training!
03/02/2022 00:27:30 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=3
03/02/2022 00:27:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.96 on epoch=6
03/02/2022 00:27:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.74 on epoch=9
03/02/2022 00:27:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=13
03/02/2022 00:27:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/02/2022 00:27:39 - INFO - __main__ - Global step 50 Train loss 1.13 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 00:27:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 00:27:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
03/02/2022 00:27:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
03/02/2022 00:27:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
03/02/2022 00:27:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
03/02/2022 00:27:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
03/02/2022 00:27:51 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16129032258064516 on epoch=33
03/02/2022 00:27:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
03/02/2022 00:27:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
03/02/2022 00:27:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
03/02/2022 00:28:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/02/2022 00:28:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
03/02/2022 00:28:04 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 00:28:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
03/02/2022 00:28:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
03/02/2022 00:28:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
03/02/2022 00:28:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
03/02/2022 00:28:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
03/02/2022 00:28:16 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.23301985370950887 on epoch=66
03/02/2022 00:28:16 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.23301985370950887 on epoch=66, global_step=200
03/02/2022 00:28:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
03/02/2022 00:28:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
03/02/2022 00:28:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
03/02/2022 00:28:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 00:28:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=83
03/02/2022 00:28:28 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.22222222222222224 on epoch=83
03/02/2022 00:28:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
03/02/2022 00:28:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
03/02/2022 00:28:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
03/02/2022 00:28:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
03/02/2022 00:28:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
03/02/2022 00:28:40 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.2119004250151791 on epoch=99
03/02/2022 00:28:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
03/02/2022 00:28:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
03/02/2022 00:28:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 00:28:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
03/02/2022 00:28:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
03/02/2022 00:28:52 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.326395173453997 on epoch=116
03/02/2022 00:28:52 - INFO - __main__ - Saving model with best Classification-F1: 0.23301985370950887 -> 0.326395173453997 on epoch=116, global_step=350
03/02/2022 00:28:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
03/02/2022 00:28:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
03/02/2022 00:28:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
03/02/2022 00:29:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=129
03/02/2022 00:29:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=133
03/02/2022 00:29:04 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.35136579175849864 on epoch=133
03/02/2022 00:29:04 - INFO - __main__ - Saving model with best Classification-F1: 0.326395173453997 -> 0.35136579175849864 on epoch=133, global_step=400
03/02/2022 00:29:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
03/02/2022 00:29:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=139
03/02/2022 00:29:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=143
03/02/2022 00:29:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=146
03/02/2022 00:29:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=149
03/02/2022 00:29:17 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.28619528619528617 on epoch=149
03/02/2022 00:29:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=153
03/02/2022 00:29:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
03/02/2022 00:29:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
03/02/2022 00:29:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
03/02/2022 00:29:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
03/02/2022 00:29:29 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.2678062678062678 on epoch=166
03/02/2022 00:29:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=169
03/02/2022 00:29:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
03/02/2022 00:29:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
03/02/2022 00:29:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
03/02/2022 00:29:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=183
03/02/2022 00:29:41 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.3574869875682884 on epoch=183
03/02/2022 00:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.35136579175849864 -> 0.3574869875682884 on epoch=183, global_step=550
03/02/2022 00:29:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=186
03/02/2022 00:29:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=189
03/02/2022 00:29:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=193
03/02/2022 00:29:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=196
03/02/2022 00:29:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=199
03/02/2022 00:29:53 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.24678534193149787 on epoch=199
03/02/2022 00:29:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=203
03/02/2022 00:29:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=206
03/02/2022 00:30:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
03/02/2022 00:30:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
03/02/2022 00:30:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=216
03/02/2022 00:30:05 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.19695652173913045 on epoch=216
03/02/2022 00:30:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
03/02/2022 00:30:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=223
03/02/2022 00:30:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=226
03/02/2022 00:30:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=229
03/02/2022 00:30:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
03/02/2022 00:30:17 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.2349624060150376 on epoch=233
03/02/2022 00:30:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=236
03/02/2022 00:30:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=239
03/02/2022 00:30:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=243
03/02/2022 00:30:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=246
03/02/2022 00:30:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=249
03/02/2022 00:30:30 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.20608108108108109 on epoch=249
03/02/2022 00:30:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
03/02/2022 00:30:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=256
03/02/2022 00:30:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
03/02/2022 00:30:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=263
03/02/2022 00:30:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
03/02/2022 00:30:42 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.2134259259259259 on epoch=266
03/02/2022 00:30:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
03/02/2022 00:30:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=273
03/02/2022 00:30:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
03/02/2022 00:30:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
03/02/2022 00:30:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=283
03/02/2022 00:30:54 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.22885261435093462 on epoch=283
03/02/2022 00:30:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
03/02/2022 00:30:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
03/02/2022 00:31:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
03/02/2022 00:31:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=296
03/02/2022 00:31:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=299
03/02/2022 00:31:07 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.32499999999999996 on epoch=299
03/02/2022 00:31:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
03/02/2022 00:31:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=306
03/02/2022 00:31:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
03/02/2022 00:31:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
03/02/2022 00:31:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
03/02/2022 00:31:19 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.258968850698174 on epoch=316
03/02/2022 00:31:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
03/02/2022 00:31:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
03/02/2022 00:31:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
03/02/2022 00:31:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
03/02/2022 00:31:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
03/02/2022 00:31:31 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.288003663003663 on epoch=333
03/02/2022 00:31:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 00:31:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
03/02/2022 00:31:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
03/02/2022 00:31:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
03/02/2022 00:31:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
03/02/2022 00:31:43 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.28412485065710874 on epoch=349
03/02/2022 00:31:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
03/02/2022 00:31:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
03/02/2022 00:31:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=359
03/02/2022 00:31:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
03/02/2022 00:31:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
03/02/2022 00:31:55 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.12941176470588237 on epoch=366
03/02/2022 00:31:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
03/02/2022 00:32:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
03/02/2022 00:32:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
03/02/2022 00:32:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
03/02/2022 00:32:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
03/02/2022 00:32:07 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.23877068557919617 on epoch=383
03/02/2022 00:32:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
03/02/2022 00:32:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 00:32:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
03/02/2022 00:32:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
03/02/2022 00:32:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
03/02/2022 00:32:20 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.23877068557919617 on epoch=399
03/02/2022 00:32:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 00:32:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
03/02/2022 00:32:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
03/02/2022 00:32:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 00:32:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 00:32:32 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.18442668442668442 on epoch=416
03/02/2022 00:32:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 00:32:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
03/02/2022 00:32:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
03/02/2022 00:32:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 00:32:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
03/02/2022 00:32:44 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.1502895861448493 on epoch=433
03/02/2022 00:32:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
03/02/2022 00:32:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 00:32:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 00:32:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
03/02/2022 00:32:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
03/02/2022 00:32:56 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.19256282930897237 on epoch=449
03/02/2022 00:32:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 00:33:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 00:33:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
03/02/2022 00:33:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 00:33:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
03/02/2022 00:33:08 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.23003105590062106 on epoch=466
03/02/2022 00:33:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 00:33:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
03/02/2022 00:33:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 00:33:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 00:33:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
03/02/2022 00:33:21 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.21142857142857144 on epoch=483
03/02/2022 00:33:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 00:33:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 00:33:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 00:33:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 00:33:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 00:33:33 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.2923688531674013 on epoch=499
03/02/2022 00:33:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 00:33:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
03/02/2022 00:33:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 00:33:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
03/02/2022 00:33:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
03/02/2022 00:33:45 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.20304436304436307 on epoch=516
03/02/2022 00:33:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 00:33:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 00:33:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 00:33:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 00:33:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 00:33:57 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.19845779220779222 on epoch=533
03/02/2022 00:33:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 00:34:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 00:34:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 00:34:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 00:34:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 00:34:10 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2932932932932933 on epoch=549
03/02/2022 00:34:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 00:34:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/02/2022 00:34:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 00:34:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 00:34:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 00:34:22 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.32414529914529916 on epoch=566
03/02/2022 00:34:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 00:34:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
03/02/2022 00:34:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 00:34:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 00:34:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 00:34:34 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3193857271906052 on epoch=583
03/02/2022 00:34:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 00:34:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
03/02/2022 00:34:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
03/02/2022 00:34:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 00:34:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 00:34:46 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.21700680272108844 on epoch=599
03/02/2022 00:34:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 00:34:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 00:34:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 00:34:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 00:34:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 00:34:59 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3016304347826087 on epoch=616
03/02/2022 00:35:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 00:35:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 00:35:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 00:35:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 00:35:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 00:35:11 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.27048405253283303 on epoch=633
03/02/2022 00:35:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 00:35:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 00:35:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 00:35:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/02/2022 00:35:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 00:35:23 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.28075396825396826 on epoch=649
03/02/2022 00:35:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 00:35:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
03/02/2022 00:35:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 00:35:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 00:35:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 00:35:36 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.25584541062801935 on epoch=666
03/02/2022 00:35:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 00:35:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 00:35:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 00:35:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 00:35:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 00:35:48 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.2262997871693524 on epoch=683
03/02/2022 00:35:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
03/02/2022 00:35:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 00:35:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 00:35:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 00:35:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/02/2022 00:36:00 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.23416666666666663 on epoch=699
03/02/2022 00:36:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 00:36:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 00:36:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 00:36:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 00:36:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 00:36:13 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.3015742397137746 on epoch=716
03/02/2022 00:36:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 00:36:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 00:36:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 00:36:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 00:36:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 00:36:25 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.29483267386493195 on epoch=733
03/02/2022 00:36:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 00:36:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 00:36:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 00:36:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 00:36:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 00:36:37 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.22532204637467795 on epoch=749
03/02/2022 00:36:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 00:36:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=756
03/02/2022 00:36:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 00:36:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 00:36:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 00:36:48 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.21792582417582418 on epoch=766
03/02/2022 00:36:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 00:36:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
03/02/2022 00:36:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 00:36:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 00:36:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 00:37:00 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.30385385385385383 on epoch=783
03/02/2022 00:37:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 00:37:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 00:37:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 00:37:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 00:37:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 00:37:12 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.2688370188370188 on epoch=799
03/02/2022 00:37:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
03/02/2022 00:37:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 00:37:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=809
03/02/2022 00:37:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 00:37:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 00:37:24 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.16159344229519668 on epoch=816
03/02/2022 00:37:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 00:37:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 00:37:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 00:37:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 00:37:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 00:37:36 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.19730963480963482 on epoch=833
03/02/2022 00:37:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 00:37:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 00:37:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 00:37:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 00:37:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 00:37:47 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.19730963480963482 on epoch=849
03/02/2022 00:37:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 00:37:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 00:37:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 00:37:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 00:37:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 00:37:59 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.22064922064922063 on epoch=866
03/02/2022 00:38:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
03/02/2022 00:38:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 00:38:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 00:38:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
03/02/2022 00:38:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
03/02/2022 00:38:11 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.17544621845697114 on epoch=883
03/02/2022 00:38:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 00:38:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 00:38:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 00:38:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 00:38:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 00:38:23 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.18209876543209877 on epoch=899
03/02/2022 00:38:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 00:38:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 00:38:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 00:38:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 00:38:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 00:38:35 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.1642026642026642 on epoch=916
03/02/2022 00:38:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 00:38:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
03/02/2022 00:38:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=926
03/02/2022 00:38:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 00:38:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 00:38:46 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.1852266507438921 on epoch=933
03/02/2022 00:38:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 00:38:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
03/02/2022 00:38:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 00:38:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 00:38:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 00:38:58 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.16346153846153846 on epoch=949
03/02/2022 00:39:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 00:39:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 00:39:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 00:39:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 00:39:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 00:39:10 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.19917417417417416 on epoch=966
03/02/2022 00:39:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 00:39:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 00:39:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 00:39:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 00:39:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 00:39:22 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.16753968253968254 on epoch=983
03/02/2022 00:39:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 00:39:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 00:39:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 00:39:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 00:39:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 00:39:34 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.22626641651031892 on epoch=999
03/02/2022 00:39:34 - INFO - __main__ - save last model!
03/02/2022 00:39:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 00:39:34 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 00:39:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 00:39:34 - INFO - __main__ - ['normal']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 00:39:34 - INFO - __main__ - ['normal']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 00:39:34 - INFO - __main__ - ['normal']
03/02/2022 00:39:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:39:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:39:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 00:39:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:39:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:39:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:39:34 - INFO - __main__ - Printing 3 examples
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:39:34 - INFO - __main__ - ['offensive']
03/02/2022 00:39:34 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:39:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:39:34 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:39:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:39:37 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 00:39:48 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:39:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:39:49 - INFO - __main__ - Starting training!
03/02/2022 00:40:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_100_0.5_8_predictions.txt
03/02/2022 00:40:25 - INFO - __main__ - Classification-F1 on test data: 0.1517
03/02/2022 00:40:25 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.5, bsz=8, dev_performance=0.3574869875682884, test_performance=0.15171903231719377
03/02/2022 00:40:25 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.4, bsz=8 ...
03/02/2022 00:40:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:40:26 - INFO - __main__ - Printing 3 examples
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:40:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:40:26 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:40:26 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:40:26 - INFO - __main__ - Printing 3 examples
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:40:26 - INFO - __main__ - ['offensive']
03/02/2022 00:40:26 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:40:26 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:40:26 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:40:40 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:40:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:40:41 - INFO - __main__ - Starting training!
03/02/2022 00:40:44 - INFO - __main__ - Step 10 Global step 10 Train loss 3.08 on epoch=3
03/02/2022 00:40:46 - INFO - __main__ - Step 20 Global step 20 Train loss 1.23 on epoch=6
03/02/2022 00:40:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.65 on epoch=9
03/02/2022 00:40:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=13
03/02/2022 00:40:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
03/02/2022 00:40:54 - INFO - __main__ - Global step 50 Train loss 1.23 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 00:40:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 00:40:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
03/02/2022 00:40:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
03/02/2022 00:41:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=26
03/02/2022 00:41:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
03/02/2022 00:41:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
03/02/2022 00:41:06 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 00:41:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
03/02/2022 00:41:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 00:41:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=43
03/02/2022 00:41:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
03/02/2022 00:41:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=49
03/02/2022 00:41:18 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 00:41:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
03/02/2022 00:41:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
03/02/2022 00:41:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
03/02/2022 00:41:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/02/2022 00:41:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
03/02/2022 00:41:30 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.25350749879051765 on epoch=66
03/02/2022 00:41:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.25350749879051765 on epoch=66, global_step=200
03/02/2022 00:41:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/02/2022 00:41:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
03/02/2022 00:41:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 00:41:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
03/02/2022 00:41:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
03/02/2022 00:41:43 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.3341617479548514 on epoch=83
03/02/2022 00:41:43 - INFO - __main__ - Saving model with best Classification-F1: 0.25350749879051765 -> 0.3341617479548514 on epoch=83, global_step=250
03/02/2022 00:41:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
03/02/2022 00:41:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
03/02/2022 00:41:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=93
03/02/2022 00:41:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
03/02/2022 00:41:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
03/02/2022 00:41:55 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.19293478260869565 on epoch=99
03/02/2022 00:41:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=103
03/02/2022 00:41:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=106
03/02/2022 00:42:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
03/02/2022 00:42:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
03/02/2022 00:42:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
03/02/2022 00:42:07 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.22758620689655173 on epoch=116
03/02/2022 00:42:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
03/02/2022 00:42:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
03/02/2022 00:42:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
03/02/2022 00:42:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/02/2022 00:42:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
03/02/2022 00:42:19 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.23181154888471958 on epoch=133
03/02/2022 00:42:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=136
03/02/2022 00:42:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
03/02/2022 00:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 00:42:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/02/2022 00:42:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
03/02/2022 00:42:33 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.25481481481481477 on epoch=149
03/02/2022 00:42:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=153
03/02/2022 00:42:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
03/02/2022 00:42:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=159
03/02/2022 00:42:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=163
03/02/2022 00:42:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=166
03/02/2022 00:42:45 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.29647565240785584 on epoch=166
03/02/2022 00:42:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=169
03/02/2022 00:42:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
03/02/2022 00:42:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=176
03/02/2022 00:42:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=179
03/02/2022 00:42:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=183
03/02/2022 00:42:57 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.43788359788359793 on epoch=183
03/02/2022 00:42:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3341617479548514 -> 0.43788359788359793 on epoch=183, global_step=550
03/02/2022 00:42:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=186
03/02/2022 00:43:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
03/02/2022 00:43:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
03/02/2022 00:43:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=196
03/02/2022 00:43:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=199
03/02/2022 00:43:10 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.26666666666666666 on epoch=199
03/02/2022 00:43:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=203
03/02/2022 00:43:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
03/02/2022 00:43:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=209
03/02/2022 00:43:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
03/02/2022 00:43:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=216
03/02/2022 00:43:22 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.32036613272311215 on epoch=216
03/02/2022 00:43:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
03/02/2022 00:43:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
03/02/2022 00:43:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
03/02/2022 00:43:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=229
03/02/2022 00:43:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=233
03/02/2022 00:43:34 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.22478991596638653 on epoch=233
03/02/2022 00:43:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
03/02/2022 00:43:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=239
03/02/2022 00:43:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=243
03/02/2022 00:43:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
03/02/2022 00:43:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=249
03/02/2022 00:43:47 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.1548964218455744 on epoch=249
03/02/2022 00:43:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=253
03/02/2022 00:43:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=256
03/02/2022 00:43:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=259
03/02/2022 00:43:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
03/02/2022 00:43:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=266
03/02/2022 00:43:59 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.2667958656330749 on epoch=266
03/02/2022 00:44:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=269
03/02/2022 00:44:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=273
03/02/2022 00:44:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
03/02/2022 00:44:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=279
03/02/2022 00:44:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
03/02/2022 00:44:11 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.2246498599439776 on epoch=283
03/02/2022 00:44:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
03/02/2022 00:44:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
03/02/2022 00:44:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
03/02/2022 00:44:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=296
03/02/2022 00:44:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=299
03/02/2022 00:44:24 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.2765151515151515 on epoch=299
03/02/2022 00:44:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
03/02/2022 00:44:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=306
03/02/2022 00:44:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
03/02/2022 00:44:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=313
03/02/2022 00:44:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
03/02/2022 00:44:36 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.3131269592476489 on epoch=316
03/02/2022 00:44:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
03/02/2022 00:44:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=323
03/02/2022 00:44:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=326
03/02/2022 00:44:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
03/02/2022 00:44:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
03/02/2022 00:44:48 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.36129032258064514 on epoch=333
03/02/2022 00:44:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
03/02/2022 00:44:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
03/02/2022 00:44:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=343
03/02/2022 00:44:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
03/02/2022 00:45:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
03/02/2022 00:45:01 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.2511244377811095 on epoch=349
03/02/2022 00:45:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 00:45:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
03/02/2022 00:45:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
03/02/2022 00:45:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
03/02/2022 00:45:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 00:45:13 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.2298743961352657 on epoch=366
03/02/2022 00:45:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
03/02/2022 00:45:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
03/02/2022 00:45:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
03/02/2022 00:45:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
03/02/2022 00:45:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 00:45:26 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.2993734335839599 on epoch=383
03/02/2022 00:45:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
03/02/2022 00:45:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=389
03/02/2022 00:45:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
03/02/2022 00:45:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 00:45:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
03/02/2022 00:45:38 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.2823426573426573 on epoch=399
03/02/2022 00:45:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 00:45:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
03/02/2022 00:45:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
03/02/2022 00:45:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
03/02/2022 00:45:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
03/02/2022 00:45:50 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.2504310344827586 on epoch=416
03/02/2022 00:45:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 00:45:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
03/02/2022 00:45:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
03/02/2022 00:45:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 00:46:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
03/02/2022 00:46:02 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.25462962962962965 on epoch=433
03/02/2022 00:46:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
03/02/2022 00:46:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 00:46:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
03/02/2022 00:46:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
03/02/2022 00:46:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
03/02/2022 00:46:15 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.26641414141414144 on epoch=449
03/02/2022 00:46:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 00:46:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
03/02/2022 00:46:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
03/02/2022 00:46:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 00:46:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 00:46:27 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.26011904761904764 on epoch=466
03/02/2022 00:46:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 00:46:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
03/02/2022 00:46:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/02/2022 00:46:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
03/02/2022 00:46:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
03/02/2022 00:46:40 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.22704511531609609 on epoch=483
03/02/2022 00:46:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
03/02/2022 00:46:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 00:46:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 00:46:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 00:46:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 00:46:52 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.28964803312629395 on epoch=499
03/02/2022 00:46:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 00:46:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 00:46:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 00:47:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 00:47:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
03/02/2022 00:47:05 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.33888888888888885 on epoch=516
03/02/2022 00:47:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
03/02/2022 00:47:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 00:47:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
03/02/2022 00:47:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
03/02/2022 00:47:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
03/02/2022 00:47:17 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2934782608695652 on epoch=533
03/02/2022 00:47:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
03/02/2022 00:47:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 00:47:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 00:47:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 00:47:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 00:47:30 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.21885748854895862 on epoch=549
03/02/2022 00:47:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
03/02/2022 00:47:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/02/2022 00:47:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 00:47:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 00:47:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
03/02/2022 00:47:42 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.30185728250244376 on epoch=566
03/02/2022 00:47:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 00:47:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 00:47:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 00:47:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 00:47:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 00:47:55 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.23524094189691622 on epoch=583
03/02/2022 00:47:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/02/2022 00:47:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=589
03/02/2022 00:48:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 00:48:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=596
03/02/2022 00:48:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
03/02/2022 00:48:07 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.20975515712357815 on epoch=599
03/02/2022 00:48:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
03/02/2022 00:48:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 00:48:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 00:48:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 00:48:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 00:48:19 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2485958485958486 on epoch=616
03/02/2022 00:48:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 00:48:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 00:48:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
03/02/2022 00:48:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 00:48:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 00:48:31 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.31648284313725494 on epoch=633
03/02/2022 00:48:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/02/2022 00:48:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 00:48:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 00:48:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 00:48:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=649
03/02/2022 00:48:43 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.32045042072265306 on epoch=649
03/02/2022 00:48:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=653
03/02/2022 00:48:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 00:48:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=659
03/02/2022 00:48:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 00:48:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 00:48:56 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.280395433027012 on epoch=666
03/02/2022 00:48:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 00:49:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 00:49:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 00:49:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
03/02/2022 00:49:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 00:49:08 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.23986427838712224 on epoch=683
03/02/2022 00:49:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 00:49:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 00:49:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 00:49:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 00:49:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=699
03/02/2022 00:49:20 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.23717948717948723 on epoch=699
03/02/2022 00:49:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 00:49:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 00:49:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 00:49:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 00:49:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 00:49:33 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.21566181566181564 on epoch=716
03/02/2022 00:49:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 00:49:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 00:49:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 00:49:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 00:49:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
03/02/2022 00:49:45 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.3179347826086956 on epoch=733
03/02/2022 00:49:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 00:49:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 00:49:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 00:49:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
03/02/2022 00:49:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 00:49:57 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.22762762762762764 on epoch=749
03/02/2022 00:50:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 00:50:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 00:50:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 00:50:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 00:50:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 00:50:10 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.317911877394636 on epoch=766
03/02/2022 00:50:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 00:50:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
03/02/2022 00:50:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 00:50:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/02/2022 00:50:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 00:50:22 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3312937062937063 on epoch=783
03/02/2022 00:50:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 00:50:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 00:50:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 00:50:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 00:50:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 00:50:34 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.2951839826839827 on epoch=799
03/02/2022 00:50:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 00:50:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 00:50:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 00:50:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 00:50:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 00:50:46 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.30214940098661025 on epoch=816
03/02/2022 00:50:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
03/02/2022 00:50:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 00:50:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 00:50:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 00:50:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 00:50:59 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.25217391304347825 on epoch=833
03/02/2022 00:51:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 00:51:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 00:51:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 00:51:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 00:51:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 00:51:11 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.22990925589836658 on epoch=849
03/02/2022 00:51:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
03/02/2022 00:51:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 00:51:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 00:51:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 00:51:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 00:51:23 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2621324621324621 on epoch=866
03/02/2022 00:51:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/02/2022 00:51:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 00:51:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 00:51:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 00:51:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 00:51:35 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.205018543359899 on epoch=883
03/02/2022 00:51:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
03/02/2022 00:51:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 00:51:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 00:51:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 00:51:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 00:51:47 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.25059229269755584 on epoch=899
03/02/2022 00:51:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 00:51:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 00:51:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
03/02/2022 00:51:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 00:51:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/02/2022 00:51:59 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.22523809523809524 on epoch=916
03/02/2022 00:52:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 00:52:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 00:52:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 00:52:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 00:52:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 00:52:11 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.27914015553199006 on epoch=933
03/02/2022 00:52:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 00:52:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
03/02/2022 00:52:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 00:52:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 00:52:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 00:52:23 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.34615384615384615 on epoch=949
03/02/2022 00:52:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
03/02/2022 00:52:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 00:52:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 00:52:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 00:52:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 00:52:36 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3129032258064516 on epoch=966
03/02/2022 00:52:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=969
03/02/2022 00:52:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 00:52:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 00:52:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 00:52:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 00:52:48 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.25028824833702884 on epoch=983
03/02/2022 00:52:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 00:52:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 00:52:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 00:52:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 00:52:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
03/02/2022 00:53:00 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.163049853372434 on epoch=999
03/02/2022 00:53:00 - INFO - __main__ - save last model!
03/02/2022 00:53:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 00:53:00 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 00:53:00 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 00:53:00 - INFO - __main__ - ['normal']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 00:53:00 - INFO - __main__ - ['normal']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 00:53:00 - INFO - __main__ - ['normal']
03/02/2022 00:53:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:53:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:00 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 00:53:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:00 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:53:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:00 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:53:00 - INFO - __main__ - ['offensive']
03/02/2022 00:53:00 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:53:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:01 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:53:01 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:03 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 00:53:13 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:53:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:53:14 - INFO - __main__ - Starting training!
03/02/2022 00:53:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_100_0.4_8_predictions.txt
03/02/2022 00:53:43 - INFO - __main__ - Classification-F1 on test data: 0.1359
03/02/2022 00:53:44 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.4, bsz=8, dev_performance=0.43788359788359793, test_performance=0.13590046942318348
03/02/2022 00:53:44 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.3, bsz=8 ...
03/02/2022 00:53:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:45 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 00:53:45 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:45 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 00:53:45 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 00:53:45 - INFO - __main__ - Printing 3 examples
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 00:53:45 - INFO - __main__ - ['offensive']
03/02/2022 00:53:45 - INFO - __main__ - Tokenizing Input ...
03/02/2022 00:53:45 - INFO - __main__ - Tokenizing Output ...
03/02/2022 00:53:45 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 00:53:57 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 00:53:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 00:53:58 - INFO - __main__ - Starting training!
03/02/2022 00:54:01 - INFO - __main__ - Step 10 Global step 10 Train loss 3.43 on epoch=3
03/02/2022 00:54:03 - INFO - __main__ - Step 20 Global step 20 Train loss 1.56 on epoch=6
03/02/2022 00:54:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.84 on epoch=9
03/02/2022 00:54:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=13
03/02/2022 00:54:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=16
03/02/2022 00:54:11 - INFO - __main__ - Global step 50 Train loss 1.43 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 00:54:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 00:54:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.63 on epoch=19
03/02/2022 00:54:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=23
03/02/2022 00:54:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
03/02/2022 00:54:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=29
03/02/2022 00:54:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=33
03/02/2022 00:54:23 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 00:54:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
03/02/2022 00:54:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 00:54:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
03/02/2022 00:54:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=46
03/02/2022 00:54:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
03/02/2022 00:54:35 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.19486504279131012 on epoch=49
03/02/2022 00:54:35 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19486504279131012 on epoch=49, global_step=150
03/02/2022 00:54:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
03/02/2022 00:54:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=56
03/02/2022 00:54:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
03/02/2022 00:54:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
03/02/2022 00:54:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
03/02/2022 00:54:48 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.1693121693121693 on epoch=66
03/02/2022 00:54:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
03/02/2022 00:54:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=73
03/02/2022 00:54:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 00:54:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=79
03/02/2022 00:54:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
03/02/2022 00:55:00 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.2927536231884058 on epoch=83
03/02/2022 00:55:00 - INFO - __main__ - Saving model with best Classification-F1: 0.19486504279131012 -> 0.2927536231884058 on epoch=83, global_step=250
03/02/2022 00:55:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
03/02/2022 00:55:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
03/02/2022 00:55:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
03/02/2022 00:55:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
03/02/2022 00:55:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 00:55:12 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.2657952069716776 on epoch=99
03/02/2022 00:55:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
03/02/2022 00:55:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 00:55:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 00:55:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 00:55:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 00:55:25 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.25925925925925924 on epoch=116
03/02/2022 00:55:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
03/02/2022 00:55:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
03/02/2022 00:55:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
03/02/2022 00:55:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
03/02/2022 00:55:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
03/02/2022 00:55:37 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.29996215409960153 on epoch=133
03/02/2022 00:55:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2927536231884058 -> 0.29996215409960153 on epoch=133, global_step=400
03/02/2022 00:55:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
03/02/2022 00:55:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
03/02/2022 00:55:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 00:55:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/02/2022 00:55:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
03/02/2022 00:55:50 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.31710144927536227 on epoch=149
03/02/2022 00:55:50 - INFO - __main__ - Saving model with best Classification-F1: 0.29996215409960153 -> 0.31710144927536227 on epoch=149, global_step=450
03/02/2022 00:55:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
03/02/2022 00:55:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
03/02/2022 00:55:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
03/02/2022 00:55:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
03/02/2022 00:56:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
03/02/2022 00:56:02 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.27673806304703624 on epoch=166
03/02/2022 00:56:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 00:56:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
03/02/2022 00:56:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=176
03/02/2022 00:56:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=179
03/02/2022 00:56:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=183
03/02/2022 00:56:14 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.2166199813258637 on epoch=183
03/02/2022 00:56:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=186
03/02/2022 00:56:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=189
03/02/2022 00:56:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=193
03/02/2022 00:56:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=196
03/02/2022 00:56:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=199
03/02/2022 00:56:27 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.1775878442545109 on epoch=199
03/02/2022 00:56:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=203
03/02/2022 00:56:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
03/02/2022 00:56:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
03/02/2022 00:56:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=213
03/02/2022 00:56:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=216
03/02/2022 00:56:39 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.3075747508305648 on epoch=216
03/02/2022 00:56:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=219
03/02/2022 00:56:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=223
03/02/2022 00:56:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=226
03/02/2022 00:56:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=229
03/02/2022 00:56:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=233
03/02/2022 00:56:51 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.25536049316537124 on epoch=233
03/02/2022 00:56:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
03/02/2022 00:56:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=239
03/02/2022 00:56:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=243
03/02/2022 00:57:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
03/02/2022 00:57:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=249
03/02/2022 00:57:04 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.22321428571428575 on epoch=249
03/02/2022 00:57:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=253
03/02/2022 00:57:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
03/02/2022 00:57:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=259
03/02/2022 00:57:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=263
03/02/2022 00:57:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=266
03/02/2022 00:57:16 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.3642857142857143 on epoch=266
03/02/2022 00:57:16 - INFO - __main__ - Saving model with best Classification-F1: 0.31710144927536227 -> 0.3642857142857143 on epoch=266, global_step=800
03/02/2022 00:57:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
03/02/2022 00:57:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
03/02/2022 00:57:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=276
03/02/2022 00:57:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=279
03/02/2022 00:57:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
03/02/2022 00:57:28 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.2851258581235698 on epoch=283
03/02/2022 00:57:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=286
03/02/2022 00:57:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=289
03/02/2022 00:57:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=293
03/02/2022 00:57:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
03/02/2022 00:57:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=299
03/02/2022 00:57:41 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.33069820520800913 on epoch=299
03/02/2022 00:57:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
03/02/2022 00:57:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=306
03/02/2022 00:57:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=309
03/02/2022 00:57:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=313
03/02/2022 00:57:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=316
03/02/2022 00:57:53 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.355230352303523 on epoch=316
03/02/2022 00:57:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=319
03/02/2022 00:57:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=323
03/02/2022 00:58:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=326
03/02/2022 00:58:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=329
03/02/2022 00:58:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=333
03/02/2022 00:58:05 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.3174825174825175 on epoch=333
03/02/2022 00:58:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=336
03/02/2022 00:58:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=339
03/02/2022 00:58:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=343
03/02/2022 00:58:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=346
03/02/2022 00:58:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=349
03/02/2022 00:58:17 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.39498207885304654 on epoch=349
03/02/2022 00:58:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3642857142857143 -> 0.39498207885304654 on epoch=349, global_step=1050
03/02/2022 00:58:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
03/02/2022 00:58:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
03/02/2022 00:58:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=359
03/02/2022 00:58:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
03/02/2022 00:58:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=366
03/02/2022 00:58:30 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.21602977667493797 on epoch=366
03/02/2022 00:58:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=369
03/02/2022 00:58:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=373
03/02/2022 00:58:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
03/02/2022 00:58:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=379
03/02/2022 00:58:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
03/02/2022 00:58:42 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.3516420361247947 on epoch=383
03/02/2022 00:58:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=386
03/02/2022 00:58:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=389
03/02/2022 00:58:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=393
03/02/2022 00:58:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=396
03/02/2022 00:58:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
03/02/2022 00:58:54 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.2731113075940662 on epoch=399
03/02/2022 00:58:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=403
03/02/2022 00:58:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=406
03/02/2022 00:59:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=409
03/02/2022 00:59:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
03/02/2022 00:59:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
03/02/2022 00:59:07 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.2874434389140272 on epoch=416
03/02/2022 00:59:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=419
03/02/2022 00:59:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
03/02/2022 00:59:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=426
03/02/2022 00:59:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
03/02/2022 00:59:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=433
03/02/2022 00:59:19 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.22190985485103135 on epoch=433
03/02/2022 00:59:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
03/02/2022 00:59:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
03/02/2022 00:59:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
03/02/2022 00:59:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=446
03/02/2022 00:59:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
03/02/2022 00:59:31 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.33413461538461536 on epoch=449
03/02/2022 00:59:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
03/02/2022 00:59:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=456
03/02/2022 00:59:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
03/02/2022 00:59:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
03/02/2022 00:59:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
03/02/2022 00:59:44 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.32344682344682346 on epoch=466
03/02/2022 00:59:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/02/2022 00:59:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 00:59:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 00:59:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
03/02/2022 00:59:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
03/02/2022 00:59:56 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.35430788879064745 on epoch=483
03/02/2022 00:59:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
03/02/2022 01:00:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 01:00:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
03/02/2022 01:00:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=496
03/02/2022 01:00:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 01:00:08 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.36995120628896716 on epoch=499
03/02/2022 01:00:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
03/02/2022 01:00:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 01:00:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
03/02/2022 01:00:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 01:00:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
03/02/2022 01:00:21 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2851351351351351 on epoch=516
03/02/2022 01:00:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/02/2022 01:00:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
03/02/2022 01:00:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
03/02/2022 01:00:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
03/02/2022 01:00:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
03/02/2022 01:00:33 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.2915542521994135 on epoch=533
03/02/2022 01:00:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=536
03/02/2022 01:00:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
03/02/2022 01:00:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
03/02/2022 01:00:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 01:00:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
03/02/2022 01:00:46 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.2944734345351044 on epoch=549
03/02/2022 01:00:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
03/02/2022 01:00:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 01:00:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/02/2022 01:00:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
03/02/2022 01:00:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
03/02/2022 01:00:58 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.1474910394265233 on epoch=566
03/02/2022 01:01:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/02/2022 01:01:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
03/02/2022 01:01:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 01:01:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 01:01:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/02/2022 01:01:10 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.24193939393939398 on epoch=583
03/02/2022 01:01:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=586
03/02/2022 01:01:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
03/02/2022 01:01:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
03/02/2022 01:01:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
03/02/2022 01:01:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
03/02/2022 01:01:23 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.24663147605083088 on epoch=599
03/02/2022 01:01:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/02/2022 01:01:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
03/02/2022 01:01:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
03/02/2022 01:01:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
03/02/2022 01:01:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 01:01:35 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.2121164021164021 on epoch=616
03/02/2022 01:01:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=619
03/02/2022 01:01:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 01:01:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 01:01:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
03/02/2022 01:01:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 01:01:48 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.2043956043956044 on epoch=633
03/02/2022 01:01:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
03/02/2022 01:01:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 01:01:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 01:01:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 01:01:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 01:02:00 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.22626050420168067 on epoch=649
03/02/2022 01:02:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
03/02/2022 01:02:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 01:02:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 01:02:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 01:02:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
03/02/2022 01:02:12 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.17951734539969835 on epoch=666
03/02/2022 01:02:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 01:02:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 01:02:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 01:02:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 01:02:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 01:02:25 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.314375 on epoch=683
03/02/2022 01:02:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 01:02:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
03/02/2022 01:02:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 01:02:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 01:02:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/02/2022 01:02:37 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2785714285714285 on epoch=699
03/02/2022 01:02:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 01:02:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 01:02:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 01:02:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:02:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 01:02:49 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.21590909090909088 on epoch=716
03/02/2022 01:02:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 01:02:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
03/02/2022 01:02:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 01:02:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 01:03:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:03:02 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2165925925925926 on epoch=733
03/02/2022 01:03:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 01:03:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 01:03:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:03:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 01:03:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/02/2022 01:03:14 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.24777348777348776 on epoch=749
03/02/2022 01:03:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
03/02/2022 01:03:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 01:03:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 01:03:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
03/02/2022 01:03:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
03/02/2022 01:03:26 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.15132083081266384 on epoch=766
03/02/2022 01:03:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 01:03:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
03/02/2022 01:03:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:03:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:03:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:03:39 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.2130952380952381 on epoch=783
03/02/2022 01:03:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:03:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:03:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:03:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 01:03:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 01:03:51 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.21101097178683387 on epoch=799
03/02/2022 01:03:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 01:03:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 01:03:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 01:04:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 01:04:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:04:03 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.1973109243697479 on epoch=816
03/02/2022 01:04:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
03/02/2022 01:04:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
03/02/2022 01:04:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 01:04:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
03/02/2022 01:04:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:04:16 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.20484848484848484 on epoch=833
03/02/2022 01:04:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
03/02/2022 01:04:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 01:04:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 01:04:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 01:04:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=849
03/02/2022 01:04:28 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.18376623376623377 on epoch=849
03/02/2022 01:04:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
03/02/2022 01:04:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:04:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 01:04:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
03/02/2022 01:04:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:04:40 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.13862433862433862 on epoch=866
03/02/2022 01:04:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 01:04:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 01:04:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 01:04:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:04:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:04:53 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.1508789722785666 on epoch=883
03/02/2022 01:04:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:04:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 01:04:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:05:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:05:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 01:05:05 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.20172413793103447 on epoch=899
03/02/2022 01:05:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 01:05:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:05:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 01:05:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:05:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=916
03/02/2022 01:05:17 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2033333333333333 on epoch=916
03/02/2022 01:05:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:05:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:05:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:05:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:05:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:05:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.15437580437580437 on epoch=933
03/02/2022 01:05:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=936
03/02/2022 01:05:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
03/02/2022 01:05:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:05:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 01:05:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 01:05:42 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.27378857701438347 on epoch=949
03/02/2022 01:05:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:05:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
03/02/2022 01:05:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:05:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:05:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:05:54 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.28835978835978837 on epoch=966
03/02/2022 01:05:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:05:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 01:06:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:06:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 01:06:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 01:06:07 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.17077497665732957 on epoch=983
03/02/2022 01:06:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:06:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 01:06:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:06:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/02/2022 01:06:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:06:19 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2308054298642534 on epoch=999
03/02/2022 01:06:19 - INFO - __main__ - save last model!
03/02/2022 01:06:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:06:19 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:06:19 - INFO - __main__ - Printing 3 examples
03/02/2022 01:06:19 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:06:19 - INFO - __main__ - ['normal']
03/02/2022 01:06:19 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:06:19 - INFO - __main__ - ['normal']
03/02/2022 01:06:19 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:06:19 - INFO - __main__ - ['normal']
03/02/2022 01:06:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:06:20 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:06:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:06:21 - INFO - __main__ - Printing 3 examples
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:06:21 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:06:21 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:06:21 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:06:21 - INFO - __main__ - Printing 3 examples
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 01:06:21 - INFO - __main__ - ['offensive']
03/02/2022 01:06:21 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:06:21 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:06:21 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:06:22 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:06:33 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:06:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:06:34 - INFO - __main__ - Starting training!
03/02/2022 01:07:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_100_0.3_8_predictions.txt
03/02/2022 01:07:08 - INFO - __main__ - Classification-F1 on test data: 0.1518
03/02/2022 01:07:09 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.3, bsz=8, dev_performance=0.39498207885304654, test_performance=0.1518287470929424
03/02/2022 01:07:09 - INFO - __main__ - Running ... prefix=hatexplain_16_100, lr=0.2, bsz=8 ...
03/02/2022 01:07:09 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:07:09 - INFO - __main__ - Printing 3 examples
03/02/2022 01:07:09 - INFO - __main__ -  [hatexplain] 🆘 ‼ 😎 💪 germany the resistance begins young german women start the campaign <number> decibels the campaign is directed against the imported migrants violence against sexual harassment abuse and rape of women in germany 1 2 0 db twitter com 1 2 0 dezibel 1 2 0 db info
03/02/2022 01:07:09 - INFO - __main__ - ['offensive']
03/02/2022 01:07:09 - INFO - __main__ -  [hatexplain] <user> you did hear about the police officer getting killed in california by illegal immigrant you need to build the wall
03/02/2022 01:07:09 - INFO - __main__ - ['offensive']
03/02/2022 01:07:09 - INFO - __main__ -  [hatexplain] lol and what was right wing leader vox gay doing about this with his movement oh yeah trolling grannies out of book reviews on amazon great optics bro
03/02/2022 01:07:09 - INFO - __main__ - ['offensive']
03/02/2022 01:07:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:07:09 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:07:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:07:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:07:10 - INFO - __main__ - Printing 3 examples
03/02/2022 01:07:10 - INFO - __main__ -  [hatexplain] getting ass raped by a muzzie sargon have you even read john locke
03/02/2022 01:07:10 - INFO - __main__ - ['offensive']
03/02/2022 01:07:10 - INFO - __main__ -  [hatexplain] fuck you all hoes
03/02/2022 01:07:10 - INFO - __main__ - ['offensive']
03/02/2022 01:07:10 - INFO - __main__ -  [hatexplain] <user> <user> nigga behave 😂
03/02/2022 01:07:10 - INFO - __main__ - ['offensive']
03/02/2022 01:07:10 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:07:10 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:07:10 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:07:22 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:07:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:07:23 - INFO - __main__ - Starting training!
03/02/2022 01:07:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.70 on epoch=3
03/02/2022 01:07:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.43 on epoch=6
03/02/2022 01:07:30 - INFO - __main__ - Step 30 Global step 30 Train loss 1.44 on epoch=9
03/02/2022 01:07:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=13
03/02/2022 01:07:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=16
03/02/2022 01:07:36 - INFO - __main__ - Global step 50 Train loss 1.85 Classification-F1 0.16733925208501477 on epoch=16
03/02/2022 01:07:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16733925208501477 on epoch=16, global_step=50
03/02/2022 01:07:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.76 on epoch=19
03/02/2022 01:07:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=23
03/02/2022 01:07:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=26
03/02/2022 01:07:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=29
03/02/2022 01:07:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=33
03/02/2022 01:07:48 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.2909803921568627 on epoch=33
03/02/2022 01:07:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16733925208501477 -> 0.2909803921568627 on epoch=33, global_step=100
03/02/2022 01:07:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=36
03/02/2022 01:07:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 01:07:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=43
03/02/2022 01:07:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/02/2022 01:07:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=49
03/02/2022 01:08:00 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 01:08:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
03/02/2022 01:08:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
03/02/2022 01:08:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 01:08:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
03/02/2022 01:08:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=66
03/02/2022 01:08:13 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 01:08:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
03/02/2022 01:08:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=73
03/02/2022 01:08:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
03/02/2022 01:08:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
03/02/2022 01:08:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=83
03/02/2022 01:08:25 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.17212121212121212 on epoch=83
03/02/2022 01:08:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
03/02/2022 01:08:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=89
03/02/2022 01:08:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 01:08:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
03/02/2022 01:08:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
03/02/2022 01:08:38 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.26508667983322365 on epoch=99
03/02/2022 01:08:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
03/02/2022 01:08:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
03/02/2022 01:08:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 01:08:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=113
03/02/2022 01:08:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
03/02/2022 01:08:50 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.3244444444444445 on epoch=116
03/02/2022 01:08:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2909803921568627 -> 0.3244444444444445 on epoch=116, global_step=350
03/02/2022 01:08:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
03/02/2022 01:08:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
03/02/2022 01:08:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
03/02/2022 01:08:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
03/02/2022 01:09:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=133
03/02/2022 01:09:02 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.34491516146688567 on epoch=133
03/02/2022 01:09:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3244444444444445 -> 0.34491516146688567 on epoch=133, global_step=400
03/02/2022 01:09:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=136
03/02/2022 01:09:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
03/02/2022 01:09:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 01:09:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=146
03/02/2022 01:09:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
03/02/2022 01:09:15 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.25745950554134694 on epoch=149
03/02/2022 01:09:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=153
03/02/2022 01:09:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
03/02/2022 01:09:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=159
03/02/2022 01:09:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=163
03/02/2022 01:09:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=166
03/02/2022 01:09:27 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.29317738791423004 on epoch=166
03/02/2022 01:09:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
03/02/2022 01:09:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=173
03/02/2022 01:09:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=176
03/02/2022 01:09:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=179
03/02/2022 01:09:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=183
03/02/2022 01:09:40 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.2736842105263158 on epoch=183
03/02/2022 01:09:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=186
03/02/2022 01:09:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
03/02/2022 01:09:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=193
03/02/2022 01:09:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=196
03/02/2022 01:09:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=199
03/02/2022 01:09:52 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.2724867724867725 on epoch=199
03/02/2022 01:09:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=203
03/02/2022 01:09:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
03/02/2022 01:09:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
03/02/2022 01:10:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=213
03/02/2022 01:10:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=216
03/02/2022 01:10:05 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.2885915607689801 on epoch=216
03/02/2022 01:10:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=219
03/02/2022 01:10:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=223
03/02/2022 01:10:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=226
03/02/2022 01:10:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=229
03/02/2022 01:10:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=233
03/02/2022 01:10:17 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.3055555555555555 on epoch=233
03/02/2022 01:10:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=236
03/02/2022 01:10:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=239
03/02/2022 01:10:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=243
03/02/2022 01:10:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=246
03/02/2022 01:10:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=249
03/02/2022 01:10:30 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.39607771260997066 on epoch=249
03/02/2022 01:10:30 - INFO - __main__ - Saving model with best Classification-F1: 0.34491516146688567 -> 0.39607771260997066 on epoch=249, global_step=750
03/02/2022 01:10:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=253
03/02/2022 01:10:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=256
03/02/2022 01:10:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=259
03/02/2022 01:10:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=263
03/02/2022 01:10:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=266
03/02/2022 01:10:42 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.36387096774193556 on epoch=266
03/02/2022 01:10:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=269
03/02/2022 01:10:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=273
03/02/2022 01:10:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=276
03/02/2022 01:10:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
03/02/2022 01:10:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=283
03/02/2022 01:10:54 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.4560404807084124 on epoch=283
03/02/2022 01:10:54 - INFO - __main__ - Saving model with best Classification-F1: 0.39607771260997066 -> 0.4560404807084124 on epoch=283, global_step=850
03/02/2022 01:10:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
03/02/2022 01:10:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=289
03/02/2022 01:11:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=293
03/02/2022 01:11:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=296
03/02/2022 01:11:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=299
03/02/2022 01:11:06 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.3952380952380952 on epoch=299
03/02/2022 01:11:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=303
03/02/2022 01:11:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=306
03/02/2022 01:11:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=309
03/02/2022 01:11:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=313
03/02/2022 01:11:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=316
03/02/2022 01:11:19 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.3988699833947941 on epoch=316
03/02/2022 01:11:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.28 on epoch=319
03/02/2022 01:11:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=323
03/02/2022 01:11:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=326
03/02/2022 01:11:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=329
03/02/2022 01:11:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=333
03/02/2022 01:11:31 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.49898989898989904 on epoch=333
03/02/2022 01:11:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4560404807084124 -> 0.49898989898989904 on epoch=333, global_step=1000
03/02/2022 01:11:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=336
03/02/2022 01:11:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=339
03/02/2022 01:11:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=343
03/02/2022 01:11:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=346
03/02/2022 01:11:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=349
03/02/2022 01:11:43 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.40391534391534395 on epoch=349
03/02/2022 01:11:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=353
03/02/2022 01:11:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=356
03/02/2022 01:11:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=359
03/02/2022 01:11:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=363
03/02/2022 01:11:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=366
03/02/2022 01:11:55 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.2310483870967742 on epoch=366
03/02/2022 01:11:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=369
03/02/2022 01:12:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
03/02/2022 01:12:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=376
03/02/2022 01:12:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=379
03/02/2022 01:12:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=383
03/02/2022 01:12:07 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.3930183056619838 on epoch=383
03/02/2022 01:12:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=386
03/02/2022 01:12:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=389
03/02/2022 01:12:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=393
03/02/2022 01:12:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=396
03/02/2022 01:12:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=399
03/02/2022 01:12:19 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.31117845117845117 on epoch=399
03/02/2022 01:12:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=403
03/02/2022 01:12:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=406
03/02/2022 01:12:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=409
03/02/2022 01:12:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
03/02/2022 01:12:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=416
03/02/2022 01:12:32 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.45360691702155115 on epoch=416
03/02/2022 01:12:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=419
03/02/2022 01:12:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=423
03/02/2022 01:12:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=426
03/02/2022 01:12:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=429
03/02/2022 01:12:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=433
03/02/2022 01:12:44 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.24674279022105106 on epoch=433
03/02/2022 01:12:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=436
03/02/2022 01:12:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=439
03/02/2022 01:12:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=443
03/02/2022 01:12:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=446
03/02/2022 01:12:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=449
03/02/2022 01:12:56 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.15166666666666664 on epoch=449
03/02/2022 01:12:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=453
03/02/2022 01:13:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
03/02/2022 01:13:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=459
03/02/2022 01:13:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=463
03/02/2022 01:13:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=466
03/02/2022 01:13:08 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.23939393939393938 on epoch=466
03/02/2022 01:13:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=469
03/02/2022 01:13:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=473
03/02/2022 01:13:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=476
03/02/2022 01:13:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=479
03/02/2022 01:13:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=483
03/02/2022 01:13:20 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.1344893029675638 on epoch=483
03/02/2022 01:13:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=486
03/02/2022 01:13:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
03/02/2022 01:13:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=493
03/02/2022 01:13:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
03/02/2022 01:13:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=499
03/02/2022 01:13:32 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.19645169645169647 on epoch=499
03/02/2022 01:13:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=503
03/02/2022 01:13:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=506
03/02/2022 01:13:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=509
03/02/2022 01:13:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
03/02/2022 01:13:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
03/02/2022 01:13:44 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.1853146853146853 on epoch=516
03/02/2022 01:13:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/02/2022 01:13:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=523
03/02/2022 01:13:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=526
03/02/2022 01:13:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
03/02/2022 01:13:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=533
03/02/2022 01:13:56 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.16514041514041514 on epoch=533
03/02/2022 01:13:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
03/02/2022 01:14:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
03/02/2022 01:14:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=543
03/02/2022 01:14:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=546
03/02/2022 01:14:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
03/02/2022 01:14:08 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.2143859649122807 on epoch=549
03/02/2022 01:14:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=553
03/02/2022 01:14:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=556
03/02/2022 01:14:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=559
03/02/2022 01:14:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=563
03/02/2022 01:14:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=566
03/02/2022 01:14:20 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.21655172413793103 on epoch=566
03/02/2022 01:14:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=569
03/02/2022 01:14:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=573
03/02/2022 01:14:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
03/02/2022 01:14:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=579
03/02/2022 01:14:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=583
03/02/2022 01:14:32 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.1707375478927203 on epoch=583
03/02/2022 01:14:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=586
03/02/2022 01:14:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
03/02/2022 01:14:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=593
03/02/2022 01:14:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/02/2022 01:14:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=599
03/02/2022 01:14:44 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.2559905567578202 on epoch=599
03/02/2022 01:14:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
03/02/2022 01:14:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
03/02/2022 01:14:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=609
03/02/2022 01:14:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
03/02/2022 01:14:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
03/02/2022 01:14:56 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.22333333333333333 on epoch=616
03/02/2022 01:14:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=619
03/02/2022 01:15:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=623
03/02/2022 01:15:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=626
03/02/2022 01:15:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
03/02/2022 01:15:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=633
03/02/2022 01:15:08 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.2753846153846154 on epoch=633
03/02/2022 01:15:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
03/02/2022 01:15:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
03/02/2022 01:15:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
03/02/2022 01:15:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=646
03/02/2022 01:15:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=649
03/02/2022 01:15:20 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.2604444444444445 on epoch=649
03/02/2022 01:15:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
03/02/2022 01:15:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=656
03/02/2022 01:15:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 01:15:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=663
03/02/2022 01:15:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=666
03/02/2022 01:15:31 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.26367791713899147 on epoch=666
03/02/2022 01:15:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
03/02/2022 01:15:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 01:15:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
03/02/2022 01:15:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=679
03/02/2022 01:15:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=683
03/02/2022 01:15:44 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.1794747709381856 on epoch=683
03/02/2022 01:15:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=686
03/02/2022 01:15:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
03/02/2022 01:15:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
03/02/2022 01:15:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
03/02/2022 01:15:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/02/2022 01:15:56 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.2337549857549858 on epoch=699
03/02/2022 01:15:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=703
03/02/2022 01:16:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=706
03/02/2022 01:16:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=709
03/02/2022 01:16:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
03/02/2022 01:16:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=716
03/02/2022 01:16:08 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.30676538908246226 on epoch=716
03/02/2022 01:16:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 01:16:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
03/02/2022 01:16:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=726
03/02/2022 01:16:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/02/2022 01:16:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=733
03/02/2022 01:16:21 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.23463463463463463 on epoch=733
03/02/2022 01:16:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
03/02/2022 01:16:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
03/02/2022 01:16:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=743
03/02/2022 01:16:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=746
03/02/2022 01:16:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
03/02/2022 01:16:33 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.27818775623653674 on epoch=749
03/02/2022 01:16:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 01:16:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
03/02/2022 01:16:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=759
03/02/2022 01:16:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
03/02/2022 01:16:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
03/02/2022 01:16:45 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.22602070393374746 on epoch=766
03/02/2022 01:16:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/02/2022 01:16:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
03/02/2022 01:16:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 01:16:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
03/02/2022 01:16:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=783
03/02/2022 01:16:57 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.21338383838383837 on epoch=783
03/02/2022 01:17:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
03/02/2022 01:17:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
03/02/2022 01:17:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
03/02/2022 01:17:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
03/02/2022 01:17:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=799
03/02/2022 01:17:10 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.22477522477522482 on epoch=799
03/02/2022 01:17:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 01:17:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
03/02/2022 01:17:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
03/02/2022 01:17:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=813
03/02/2022 01:17:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 01:17:22 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.24302325581395348 on epoch=816
03/02/2022 01:17:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
03/02/2022 01:17:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
03/02/2022 01:17:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 01:17:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
03/02/2022 01:17:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
03/02/2022 01:17:34 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.3144230769230769 on epoch=833
03/02/2022 01:17:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
03/02/2022 01:17:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=839
03/02/2022 01:17:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=843
03/02/2022 01:17:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=846
03/02/2022 01:17:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
03/02/2022 01:17:46 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.26644144144144144 on epoch=849
03/02/2022 01:17:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
03/02/2022 01:17:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 01:17:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
03/02/2022 01:17:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 01:17:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
03/02/2022 01:17:58 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.329029029029029 on epoch=866
03/02/2022 01:18:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
03/02/2022 01:18:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 01:18:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
03/02/2022 01:18:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 01:18:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 01:18:10 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.21068352905751284 on epoch=883
03/02/2022 01:18:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
03/02/2022 01:18:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 01:18:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 01:18:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 01:18:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 01:18:22 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3143018018018018 on epoch=899
03/02/2022 01:18:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
03/02/2022 01:18:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
03/02/2022 01:18:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=909
03/02/2022 01:18:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 01:18:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 01:18:35 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.25508166969147006 on epoch=916
03/02/2022 01:18:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 01:18:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/02/2022 01:18:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
03/02/2022 01:18:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 01:18:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
03/02/2022 01:18:47 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.28875448028673834 on epoch=933
03/02/2022 01:18:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:18:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
03/02/2022 01:18:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:18:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
03/02/2022 01:18:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
03/02/2022 01:18:59 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3648717948717949 on epoch=949
03/02/2022 01:19:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 01:19:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/02/2022 01:19:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
03/02/2022 01:19:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=963
03/02/2022 01:19:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
03/02/2022 01:19:11 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.27353670996934754 on epoch=966
03/02/2022 01:19:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/02/2022 01:19:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 01:19:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
03/02/2022 01:19:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
03/02/2022 01:19:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
03/02/2022 01:19:23 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3632478632478633 on epoch=983
03/02/2022 01:19:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=986
03/02/2022 01:19:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
03/02/2022 01:19:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=993
03/02/2022 01:19:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/02/2022 01:19:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
03/02/2022 01:19:35 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.21049239983321327 on epoch=999
03/02/2022 01:19:35 - INFO - __main__ - save last model!
03/02/2022 01:19:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:19:35 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:19:35 - INFO - __main__ - Printing 3 examples
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:19:35 - INFO - __main__ - ['normal']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:19:35 - INFO - __main__ - ['normal']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:19:35 - INFO - __main__ - ['normal']
03/02/2022 01:19:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:19:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:19:35 - INFO - __main__ - Printing 3 examples
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:19:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:19:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:19:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:19:35 - INFO - __main__ - Printing 3 examples
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:19:35 - INFO - __main__ - ['hatespeech']
03/02/2022 01:19:35 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:19:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:19:35 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:19:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:19:38 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:19:50 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:19:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:19:50 - INFO - __main__ - Starting training!
03/02/2022 01:20:23 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_100_0.2_8_predictions.txt
03/02/2022 01:20:23 - INFO - __main__ - Classification-F1 on test data: 0.1754
03/02/2022 01:20:23 - INFO - __main__ - prefix=hatexplain_16_100, lr=0.2, bsz=8, dev_performance=0.49898989898989904, test_performance=0.17536954517848996
03/02/2022 01:20:23 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.5, bsz=8 ...
03/02/2022 01:20:24 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:20:24 - INFO - __main__ - Printing 3 examples
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:20:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:20:24 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:20:24 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:20:24 - INFO - __main__ - Printing 3 examples
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:20:24 - INFO - __main__ - ['hatespeech']
03/02/2022 01:20:24 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:20:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:20:24 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:20:36 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:20:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:20:37 - INFO - __main__ - Starting training!
03/02/2022 01:20:40 - INFO - __main__ - Step 10 Global step 10 Train loss 2.78 on epoch=3
03/02/2022 01:20:42 - INFO - __main__ - Step 20 Global step 20 Train loss 1.04 on epoch=6
03/02/2022 01:20:44 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=9
03/02/2022 01:20:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.72 on epoch=13
03/02/2022 01:20:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=16
03/02/2022 01:20:50 - INFO - __main__ - Global step 50 Train loss 1.16 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 01:20:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 01:20:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=19
03/02/2022 01:20:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
03/02/2022 01:20:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
03/02/2022 01:20:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
03/02/2022 01:21:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/02/2022 01:21:03 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.18666666666666668 on epoch=33
03/02/2022 01:21:03 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18666666666666668 on epoch=33, global_step=100
03/02/2022 01:21:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 01:21:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
03/02/2022 01:21:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
03/02/2022 01:21:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
03/02/2022 01:21:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
03/02/2022 01:21:15 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.1990221455277538 on epoch=49
03/02/2022 01:21:15 - INFO - __main__ - Saving model with best Classification-F1: 0.18666666666666668 -> 0.1990221455277538 on epoch=49, global_step=150
03/02/2022 01:21:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
03/02/2022 01:21:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
03/02/2022 01:21:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
03/02/2022 01:21:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/02/2022 01:21:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
03/02/2022 01:21:27 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.1693121693121693 on epoch=66
03/02/2022 01:21:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
03/02/2022 01:21:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
03/02/2022 01:21:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=76
03/02/2022 01:21:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
03/02/2022 01:21:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=83
03/02/2022 01:21:39 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.3077750874361044 on epoch=83
03/02/2022 01:21:39 - INFO - __main__ - Saving model with best Classification-F1: 0.1990221455277538 -> 0.3077750874361044 on epoch=83, global_step=250
03/02/2022 01:21:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
03/02/2022 01:21:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
03/02/2022 01:21:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 01:21:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
03/02/2022 01:21:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
03/02/2022 01:21:51 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.22265205243928646 on epoch=99
03/02/2022 01:21:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
03/02/2022 01:21:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/02/2022 01:21:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 01:22:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=113
03/02/2022 01:22:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 01:22:04 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.32668566001899335 on epoch=116
03/02/2022 01:22:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3077750874361044 -> 0.32668566001899335 on epoch=116, global_step=350
03/02/2022 01:22:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=119
03/02/2022 01:22:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
03/02/2022 01:22:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
03/02/2022 01:22:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=129
03/02/2022 01:22:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=133
03/02/2022 01:22:16 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.36621534493874924 on epoch=133
03/02/2022 01:22:16 - INFO - __main__ - Saving model with best Classification-F1: 0.32668566001899335 -> 0.36621534493874924 on epoch=133, global_step=400
03/02/2022 01:22:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=136
03/02/2022 01:22:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=139
03/02/2022 01:22:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
03/02/2022 01:22:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=146
03/02/2022 01:22:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
03/02/2022 01:22:28 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.346013324736729 on epoch=149
03/02/2022 01:22:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=153
03/02/2022 01:22:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
03/02/2022 01:22:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
03/02/2022 01:22:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=163
03/02/2022 01:22:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
03/02/2022 01:22:40 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.3680555555555555 on epoch=166
03/02/2022 01:22:40 - INFO - __main__ - Saving model with best Classification-F1: 0.36621534493874924 -> 0.3680555555555555 on epoch=166, global_step=500
03/02/2022 01:22:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=169
03/02/2022 01:22:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=173
03/02/2022 01:22:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=176
03/02/2022 01:22:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=179
03/02/2022 01:22:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
03/02/2022 01:22:52 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3368794326241134 on epoch=183
03/02/2022 01:22:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=186
03/02/2022 01:22:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=189
03/02/2022 01:22:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
03/02/2022 01:23:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.12 on epoch=196
03/02/2022 01:23:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=199
03/02/2022 01:23:04 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.328494623655914 on epoch=199
03/02/2022 01:23:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=203
03/02/2022 01:23:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=206
03/02/2022 01:23:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=209
03/02/2022 01:23:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=213
03/02/2022 01:23:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
03/02/2022 01:23:17 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.2595542564852028 on epoch=216
03/02/2022 01:23:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
03/02/2022 01:23:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=223
03/02/2022 01:23:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=226
03/02/2022 01:23:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=229
03/02/2022 01:23:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=233
03/02/2022 01:23:29 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.273202614379085 on epoch=233
03/02/2022 01:23:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=236
03/02/2022 01:23:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=239
03/02/2022 01:23:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=243
03/02/2022 01:23:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
03/02/2022 01:23:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=249
03/02/2022 01:23:41 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.3042929292929293 on epoch=249
03/02/2022 01:23:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=253
03/02/2022 01:23:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=256
03/02/2022 01:23:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
03/02/2022 01:23:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=263
03/02/2022 01:23:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=266
03/02/2022 01:23:53 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.26752136752136757 on epoch=266
03/02/2022 01:23:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
03/02/2022 01:23:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=273
03/02/2022 01:24:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=276
03/02/2022 01:24:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=279
03/02/2022 01:24:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
03/02/2022 01:24:05 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.24883040935672515 on epoch=283
03/02/2022 01:24:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
03/02/2022 01:24:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=289
03/02/2022 01:24:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
03/02/2022 01:24:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=296
03/02/2022 01:24:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
03/02/2022 01:24:17 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.34615384615384615 on epoch=299
03/02/2022 01:24:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
03/02/2022 01:24:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
03/02/2022 01:24:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=309
03/02/2022 01:24:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
03/02/2022 01:24:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=316
03/02/2022 01:24:29 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.3765714285714286 on epoch=316
03/02/2022 01:24:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3680555555555555 -> 0.3765714285714286 on epoch=316, global_step=950
03/02/2022 01:24:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
03/02/2022 01:24:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=323
03/02/2022 01:24:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
03/02/2022 01:24:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
03/02/2022 01:24:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
03/02/2022 01:24:41 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.3328205128205129 on epoch=333
03/02/2022 01:24:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=336
03/02/2022 01:24:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=339
03/02/2022 01:24:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
03/02/2022 01:24:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=346
03/02/2022 01:24:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
03/02/2022 01:24:53 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.2864309764309764 on epoch=349
03/02/2022 01:24:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
03/02/2022 01:24:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=356
03/02/2022 01:25:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
03/02/2022 01:25:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=363
03/02/2022 01:25:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 01:25:05 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.38638519924098674 on epoch=366
03/02/2022 01:25:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3765714285714286 -> 0.38638519924098674 on epoch=366, global_step=1100
03/02/2022 01:25:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
03/02/2022 01:25:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 01:25:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
03/02/2022 01:25:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
03/02/2022 01:25:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
03/02/2022 01:25:17 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.34947089947089943 on epoch=383
03/02/2022 01:25:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=386
03/02/2022 01:25:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 01:25:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=393
03/02/2022 01:25:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
03/02/2022 01:25:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
03/02/2022 01:25:29 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.35747585747585753 on epoch=399
03/02/2022 01:25:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
03/02/2022 01:25:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
03/02/2022 01:25:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
03/02/2022 01:25:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
03/02/2022 01:25:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 01:25:41 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4330290736984449 on epoch=416
03/02/2022 01:25:41 - INFO - __main__ - Saving model with best Classification-F1: 0.38638519924098674 -> 0.4330290736984449 on epoch=416, global_step=1250
03/02/2022 01:25:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 01:25:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
03/02/2022 01:25:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
03/02/2022 01:25:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
03/02/2022 01:25:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
03/02/2022 01:25:53 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.44795019157088123 on epoch=433
03/02/2022 01:25:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4330290736984449 -> 0.44795019157088123 on epoch=433, global_step=1300
03/02/2022 01:25:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 01:25:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
03/02/2022 01:26:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 01:26:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
03/02/2022 01:26:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
03/02/2022 01:26:05 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4314121814121814 on epoch=449
03/02/2022 01:26:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 01:26:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
03/02/2022 01:26:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
03/02/2022 01:26:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 01:26:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 01:26:17 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.42439153439153443 on epoch=466
03/02/2022 01:26:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
03/02/2022 01:26:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 01:26:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
03/02/2022 01:26:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 01:26:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
03/02/2022 01:26:29 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4113414634146342 on epoch=483
03/02/2022 01:26:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
03/02/2022 01:26:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 01:26:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 01:26:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 01:26:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
03/02/2022 01:26:41 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.46334310850439875 on epoch=499
03/02/2022 01:26:41 - INFO - __main__ - Saving model with best Classification-F1: 0.44795019157088123 -> 0.46334310850439875 on epoch=499, global_step=1500
03/02/2022 01:26:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
03/02/2022 01:26:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
03/02/2022 01:26:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 01:26:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 01:26:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 01:26:53 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.399527665317139 on epoch=516
03/02/2022 01:26:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 01:26:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 01:27:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 01:27:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 01:27:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 01:27:05 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.41884615384615387 on epoch=533
03/02/2022 01:27:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 01:27:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
03/02/2022 01:27:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 01:27:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 01:27:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 01:27:17 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4413001181293864 on epoch=549
03/02/2022 01:27:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 01:27:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 01:27:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 01:27:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 01:27:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 01:27:29 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3986486486486487 on epoch=566
03/02/2022 01:27:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 01:27:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 01:27:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 01:27:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 01:27:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 01:27:41 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.3824148824148824 on epoch=583
03/02/2022 01:27:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 01:27:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 01:27:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 01:27:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 01:27:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
03/02/2022 01:27:53 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.22592592592592595 on epoch=599
03/02/2022 01:27:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 01:27:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 01:28:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 01:28:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 01:28:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 01:28:05 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.26886274509803926 on epoch=616
03/02/2022 01:28:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 01:28:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 01:28:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
03/02/2022 01:28:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
03/02/2022 01:28:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 01:28:17 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2944356120826709 on epoch=633
03/02/2022 01:28:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 01:28:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 01:28:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 01:28:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 01:28:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 01:28:30 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.353661932609301 on epoch=649
03/02/2022 01:28:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 01:28:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 01:28:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 01:28:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
03/02/2022 01:28:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:28:42 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.43509615384615385 on epoch=666
03/02/2022 01:28:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 01:28:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 01:28:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 01:28:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 01:28:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 01:28:54 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.44526353276353275 on epoch=683
03/02/2022 01:28:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 01:28:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 01:29:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 01:29:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 01:29:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/02/2022 01:29:06 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.3157894736842105 on epoch=699
03/02/2022 01:29:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 01:29:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 01:29:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 01:29:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:29:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 01:29:18 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.2654413702239789 on epoch=716
03/02/2022 01:29:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 01:29:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 01:29:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 01:29:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 01:29:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:29:30 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.2883367839889579 on epoch=733
03/02/2022 01:29:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 01:29:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 01:29:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
03/02/2022 01:29:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 01:29:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:29:42 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.4155152388962313 on epoch=749
03/02/2022 01:29:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
03/02/2022 01:29:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 01:29:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
03/02/2022 01:29:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 01:29:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 01:29:54 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.47729863858896115 on epoch=766
03/02/2022 01:29:54 - INFO - __main__ - Saving model with best Classification-F1: 0.46334310850439875 -> 0.47729863858896115 on epoch=766, global_step=2300
03/02/2022 01:29:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 01:29:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 01:30:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:30:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:30:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:30:06 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.3397306397306397 on epoch=783
03/02/2022 01:30:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:30:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:30:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:30:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 01:30:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 01:30:19 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.2910731244064577 on epoch=799
03/02/2022 01:30:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:30:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 01:30:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 01:30:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
03/02/2022 01:30:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:30:31 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.4576724137931034 on epoch=816
03/02/2022 01:30:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 01:30:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
03/02/2022 01:30:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 01:30:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 01:30:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:30:43 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.36792114695340505 on epoch=833
03/02/2022 01:30:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 01:30:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 01:30:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 01:30:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 01:30:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 01:30:55 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.4277777777777778 on epoch=849
03/02/2022 01:30:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 01:30:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:31:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 01:31:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 01:31:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:31:07 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.406612685560054 on epoch=866
03/02/2022 01:31:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/02/2022 01:31:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 01:31:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 01:31:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:31:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
03/02/2022 01:31:19 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.4191919191919192 on epoch=883
03/02/2022 01:31:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:31:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 01:31:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:31:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:31:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 01:31:31 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.35182795698924724 on epoch=899
03/02/2022 01:31:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 01:31:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:31:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:31:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:31:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 01:31:43 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.41507936507936505 on epoch=916
03/02/2022 01:31:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:31:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:31:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:31:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:31:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 01:31:55 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.4197619047619048 on epoch=933
03/02/2022 01:31:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:32:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 01:32:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:32:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 01:32:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/02/2022 01:32:07 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.4723429541595926 on epoch=949
03/02/2022 01:32:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:32:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:32:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:32:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:32:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:32:19 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.4143664717348928 on epoch=966
03/02/2022 01:32:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:32:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 01:32:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 01:32:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 01:32:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:32:31 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.4579936192839419 on epoch=983
03/02/2022 01:32:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:32:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 01:32:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:32:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 01:32:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:32:43 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.425324196875921 on epoch=999
03/02/2022 01:32:43 - INFO - __main__ - save last model!
03/02/2022 01:32:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:32:43 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:32:43 - INFO - __main__ - Printing 3 examples
03/02/2022 01:32:43 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:32:43 - INFO - __main__ - ['normal']
03/02/2022 01:32:43 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:32:43 - INFO - __main__ - ['normal']
03/02/2022 01:32:43 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:32:43 - INFO - __main__ - ['normal']
03/02/2022 01:32:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:32:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:32:44 - INFO - __main__ - Printing 3 examples
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:32:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:32:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:32:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:32:44 - INFO - __main__ - Printing 3 examples
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:32:44 - INFO - __main__ - ['hatespeech']
03/02/2022 01:32:44 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:32:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:32:44 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:32:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:32:46 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:32:56 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:32:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:32:57 - INFO - __main__ - Starting training!
03/02/2022 01:33:31 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_13_0.5_8_predictions.txt
03/02/2022 01:33:31 - INFO - __main__ - Classification-F1 on test data: 0.2029
03/02/2022 01:33:32 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.5, bsz=8, dev_performance=0.47729863858896115, test_performance=0.20289964674404382
03/02/2022 01:33:32 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.4, bsz=8 ...
03/02/2022 01:33:32 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:33:32 - INFO - __main__ - Printing 3 examples
03/02/2022 01:33:32 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:33:32 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:32 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:33:32 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:32 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:33:32 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:33:32 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:33:33 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:33:33 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:33:33 - INFO - __main__ - Printing 3 examples
03/02/2022 01:33:33 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:33:33 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:33 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:33:33 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:33 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:33:33 - INFO - __main__ - ['hatespeech']
03/02/2022 01:33:33 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:33:33 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:33:33 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:33:45 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:33:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:33:46 - INFO - __main__ - Starting training!
03/02/2022 01:33:50 - INFO - __main__ - Step 10 Global step 10 Train loss 3.09 on epoch=3
03/02/2022 01:33:52 - INFO - __main__ - Step 20 Global step 20 Train loss 1.38 on epoch=6
03/02/2022 01:33:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.80 on epoch=9
03/02/2022 01:33:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.74 on epoch=13
03/02/2022 01:33:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=16
03/02/2022 01:34:00 - INFO - __main__ - Global step 50 Train loss 1.32 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 01:34:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 01:34:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
03/02/2022 01:34:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.64 on epoch=23
03/02/2022 01:34:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=26
03/02/2022 01:34:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=29
03/02/2022 01:34:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
03/02/2022 01:34:12 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 01:34:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
03/02/2022 01:34:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
03/02/2022 01:34:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=43
03/02/2022 01:34:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
03/02/2022 01:34:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=49
03/02/2022 01:34:24 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.22847399829497017 on epoch=49
03/02/2022 01:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22847399829497017 on epoch=49, global_step=150
03/02/2022 01:34:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=53
03/02/2022 01:34:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=56
03/02/2022 01:34:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
03/02/2022 01:34:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/02/2022 01:34:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=66
03/02/2022 01:34:36 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.1983273596176822 on epoch=66
03/02/2022 01:34:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=69
03/02/2022 01:34:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=73
03/02/2022 01:34:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
03/02/2022 01:34:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
03/02/2022 01:34:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=83
03/02/2022 01:34:48 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.24611708482676223 on epoch=83
03/02/2022 01:34:48 - INFO - __main__ - Saving model with best Classification-F1: 0.22847399829497017 -> 0.24611708482676223 on epoch=83, global_step=250
03/02/2022 01:34:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=86
03/02/2022 01:34:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
03/02/2022 01:34:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
03/02/2022 01:34:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
03/02/2022 01:34:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
03/02/2022 01:35:00 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.24296675191815856 on epoch=99
03/02/2022 01:35:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
03/02/2022 01:35:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 01:35:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
03/02/2022 01:35:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
03/02/2022 01:35:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
03/02/2022 01:35:12 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.3754645010458964 on epoch=116
03/02/2022 01:35:12 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.3754645010458964 on epoch=116, global_step=350
03/02/2022 01:35:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
03/02/2022 01:35:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
03/02/2022 01:35:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
03/02/2022 01:35:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=129
03/02/2022 01:35:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=133
03/02/2022 01:35:24 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.4508427604379628 on epoch=133
03/02/2022 01:35:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3754645010458964 -> 0.4508427604379628 on epoch=133, global_step=400
03/02/2022 01:35:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=136
03/02/2022 01:35:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=139
03/02/2022 01:35:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=143
03/02/2022 01:35:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=146
03/02/2022 01:35:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=149
03/02/2022 01:35:36 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.49217237259506447 on epoch=149
03/02/2022 01:35:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4508427604379628 -> 0.49217237259506447 on epoch=149, global_step=450
03/02/2022 01:35:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=153
03/02/2022 01:35:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=156
03/02/2022 01:35:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=159
03/02/2022 01:35:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=163
03/02/2022 01:35:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=166
03/02/2022 01:35:48 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.4511920827710301 on epoch=166
03/02/2022 01:35:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
03/02/2022 01:35:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=173
03/02/2022 01:35:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=176
03/02/2022 01:35:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=179
03/02/2022 01:35:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
03/02/2022 01:36:00 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.4466861598440546 on epoch=183
03/02/2022 01:36:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=186
03/02/2022 01:36:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=189
03/02/2022 01:36:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=193
03/02/2022 01:36:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=196
03/02/2022 01:36:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=199
03/02/2022 01:36:12 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.523415292042743 on epoch=199
03/02/2022 01:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.49217237259506447 -> 0.523415292042743 on epoch=199, global_step=600
03/02/2022 01:36:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=203
03/02/2022 01:36:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=206
03/02/2022 01:36:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=209
03/02/2022 01:36:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=213
03/02/2022 01:36:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
03/02/2022 01:36:24 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.39748677248677255 on epoch=216
03/02/2022 01:36:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
03/02/2022 01:36:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=223
03/02/2022 01:36:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=226
03/02/2022 01:36:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
03/02/2022 01:36:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
03/02/2022 01:36:36 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.5784313725490197 on epoch=233
03/02/2022 01:36:36 - INFO - __main__ - Saving model with best Classification-F1: 0.523415292042743 -> 0.5784313725490197 on epoch=233, global_step=700
03/02/2022 01:36:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=236
03/02/2022 01:36:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=239
03/02/2022 01:36:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=243
03/02/2022 01:36:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=246
03/02/2022 01:36:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
03/02/2022 01:36:48 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.4158494208494209 on epoch=249
03/02/2022 01:36:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
03/02/2022 01:36:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
03/02/2022 01:36:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=259
03/02/2022 01:36:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=263
03/02/2022 01:36:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=266
03/02/2022 01:37:00 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.5812776723592662 on epoch=266
03/02/2022 01:37:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5784313725490197 -> 0.5812776723592662 on epoch=266, global_step=800
03/02/2022 01:37:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
03/02/2022 01:37:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
03/02/2022 01:37:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
03/02/2022 01:37:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
03/02/2022 01:37:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
03/02/2022 01:37:13 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.43345069207138176 on epoch=283
03/02/2022 01:37:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
03/02/2022 01:37:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
03/02/2022 01:37:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
03/02/2022 01:37:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 01:37:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
03/02/2022 01:37:25 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.36313131313131314 on epoch=299
03/02/2022 01:37:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=303
03/02/2022 01:37:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=306
03/02/2022 01:37:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
03/02/2022 01:37:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
03/02/2022 01:37:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=316
03/02/2022 01:37:37 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.4111150070126227 on epoch=316
03/02/2022 01:37:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
03/02/2022 01:37:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
03/02/2022 01:37:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
03/02/2022 01:37:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
03/02/2022 01:37:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
03/02/2022 01:37:49 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.47849462365591394 on epoch=333
03/02/2022 01:37:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
03/02/2022 01:37:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
03/02/2022 01:37:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
03/02/2022 01:37:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
03/02/2022 01:38:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
03/02/2022 01:38:02 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.4503103103103103 on epoch=349
03/02/2022 01:38:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
03/02/2022 01:38:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
03/02/2022 01:38:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
03/02/2022 01:38:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
03/02/2022 01:38:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 01:38:14 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.45952380952380956 on epoch=366
03/02/2022 01:38:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 01:38:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 01:38:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
03/02/2022 01:38:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
03/02/2022 01:38:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
03/02/2022 01:38:27 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.38504187344913154 on epoch=383
03/02/2022 01:38:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
03/02/2022 01:38:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
03/02/2022 01:38:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
03/02/2022 01:38:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
03/02/2022 01:38:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
03/02/2022 01:38:39 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4158385093167702 on epoch=399
03/02/2022 01:38:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=403
03/02/2022 01:38:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
03/02/2022 01:38:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 01:38:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 01:38:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 01:38:51 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4327389300741334 on epoch=416
03/02/2022 01:38:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
03/02/2022 01:38:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
03/02/2022 01:38:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=426
03/02/2022 01:39:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
03/02/2022 01:39:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
03/02/2022 01:39:04 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.48370411568409344 on epoch=433
03/02/2022 01:39:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
03/02/2022 01:39:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 01:39:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 01:39:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
03/02/2022 01:39:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 01:39:16 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.2882372598162072 on epoch=449
03/02/2022 01:39:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
03/02/2022 01:39:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 01:39:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
03/02/2022 01:39:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 01:39:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
03/02/2022 01:39:28 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.39901960784313717 on epoch=466
03/02/2022 01:39:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
03/02/2022 01:39:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
03/02/2022 01:39:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
03/02/2022 01:39:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
03/02/2022 01:39:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 01:39:40 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.43538324420677355 on epoch=483
03/02/2022 01:39:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 01:39:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 01:39:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
03/02/2022 01:39:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 01:39:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 01:39:53 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.2963984674329502 on epoch=499
03/02/2022 01:39:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
03/02/2022 01:39:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
03/02/2022 01:39:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 01:40:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 01:40:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
03/02/2022 01:40:05 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.33183023872679046 on epoch=516
03/02/2022 01:40:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
03/02/2022 01:40:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 01:40:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/02/2022 01:40:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 01:40:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
03/02/2022 01:40:17 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4241071428571429 on epoch=533
03/02/2022 01:40:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
03/02/2022 01:40:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 01:40:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 01:40:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 01:40:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 01:40:30 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3399918864097363 on epoch=549
03/02/2022 01:40:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 01:40:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 01:40:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 01:40:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 01:40:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 01:40:42 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3551693404634581 on epoch=566
03/02/2022 01:40:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
03/02/2022 01:40:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
03/02/2022 01:40:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
03/02/2022 01:40:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 01:40:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 01:40:54 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.3413848631239936 on epoch=583
03/02/2022 01:40:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 01:40:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 01:41:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 01:41:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 01:41:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 01:41:06 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.3383559232529484 on epoch=599
03/02/2022 01:41:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 01:41:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
03/02/2022 01:41:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
03/02/2022 01:41:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=613
03/02/2022 01:41:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
03/02/2022 01:41:18 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.2888888888888889 on epoch=616
03/02/2022 01:41:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 01:41:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 01:41:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
03/02/2022 01:41:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 01:41:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 01:41:30 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.35196078431372546 on epoch=633
03/02/2022 01:41:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 01:41:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 01:41:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 01:41:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 01:41:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/02/2022 01:41:42 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3088954056695992 on epoch=649
03/02/2022 01:41:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 01:41:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 01:41:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 01:41:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 01:41:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:41:54 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4256535947712418 on epoch=666
03/02/2022 01:41:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 01:41:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 01:42:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 01:42:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=679
03/02/2022 01:42:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/02/2022 01:42:06 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.3453333333333334 on epoch=683
03/02/2022 01:42:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 01:42:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 01:42:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 01:42:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 01:42:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 01:42:18 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.4027777777777778 on epoch=699
03/02/2022 01:42:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 01:42:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 01:42:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 01:42:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:42:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 01:42:30 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.32192592592592595 on epoch=716
03/02/2022 01:42:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 01:42:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 01:42:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 01:42:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 01:42:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:42:42 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.33422222222222225 on epoch=733
03/02/2022 01:42:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 01:42:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 01:42:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
03/02/2022 01:42:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 01:42:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:42:54 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.4665948275862069 on epoch=749
03/02/2022 01:42:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 01:42:58 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 01:43:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 01:43:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/02/2022 01:43:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
03/02/2022 01:43:06 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.34904761904761905 on epoch=766
03/02/2022 01:43:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=769
03/02/2022 01:43:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 01:43:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:43:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
03/02/2022 01:43:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 01:43:18 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.4626696832579186 on epoch=783
03/02/2022 01:43:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:43:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:43:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:43:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
03/02/2022 01:43:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 01:43:30 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.33090909090909093 on epoch=799
03/02/2022 01:43:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:43:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 01:43:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 01:43:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 01:43:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=816
03/02/2022 01:43:42 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.38671212121212123 on epoch=816
03/02/2022 01:43:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
03/02/2022 01:43:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 01:43:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 01:43:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 01:43:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:43:54 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.44 on epoch=833
03/02/2022 01:43:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 01:43:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 01:44:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 01:44:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 01:44:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
03/02/2022 01:44:06 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.35132275132275137 on epoch=849
03/02/2022 01:44:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
03/02/2022 01:44:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:44:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 01:44:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
03/02/2022 01:44:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:44:18 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3903318903318903 on epoch=866
03/02/2022 01:44:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 01:44:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 01:44:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 01:44:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:44:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:44:30 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.42873677248677244 on epoch=883
03/02/2022 01:44:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:44:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
03/02/2022 01:44:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:44:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 01:44:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 01:44:42 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.4275003748687959 on epoch=899
03/02/2022 01:44:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
03/02/2022 01:44:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
03/02/2022 01:44:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:44:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:44:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=916
03/02/2022 01:44:54 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.42374727668845313 on epoch=916
03/02/2022 01:44:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:44:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=923
03/02/2022 01:45:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 01:45:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:45:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=933
03/02/2022 01:45:06 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.3569327731092437 on epoch=933
03/02/2022 01:45:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 01:45:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 01:45:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 01:45:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 01:45:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 01:45:18 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.4246671786994368 on epoch=949
03/02/2022 01:45:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
03/02/2022 01:45:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:45:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 01:45:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:45:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:45:30 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3576612903225806 on epoch=966
03/02/2022 01:45:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:45:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 01:45:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 01:45:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 01:45:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:45:42 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5820066793697626 on epoch=983
03/02/2022 01:45:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5812776723592662 -> 0.5820066793697626 on epoch=983, global_step=2950
03/02/2022 01:45:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:45:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 01:45:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:45:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/02/2022 01:45:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:45:54 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.44644334160463184 on epoch=999
03/02/2022 01:45:54 - INFO - __main__ - save last model!
03/02/2022 01:45:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:45:54 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:45:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:45:54 - INFO - __main__ - ['normal']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:45:54 - INFO - __main__ - ['normal']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:45:54 - INFO - __main__ - ['normal']
03/02/2022 01:45:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:45:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:45:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:45:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:45:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:45:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:45:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:45:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:45:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:45:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:45:54 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:45:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:45:57 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:46:09 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:46:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:46:09 - INFO - __main__ - Starting training!
03/02/2022 01:46:42 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_13_0.4_8_predictions.txt
03/02/2022 01:46:42 - INFO - __main__ - Classification-F1 on test data: 0.2610
03/02/2022 01:46:42 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.4, bsz=8, dev_performance=0.5820066793697626, test_performance=0.261041701777325
03/02/2022 01:46:42 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.3, bsz=8 ...
03/02/2022 01:46:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:46:43 - INFO - __main__ - Printing 3 examples
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:46:43 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:46:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:46:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:46:43 - INFO - __main__ - Printing 3 examples
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:46:43 - INFO - __main__ - ['hatespeech']
03/02/2022 01:46:43 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:46:43 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:46:44 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:46:57 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:46:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:46:58 - INFO - __main__ - Starting training!
03/02/2022 01:47:01 - INFO - __main__ - Step 10 Global step 10 Train loss 3.39 on epoch=3
03/02/2022 01:47:04 - INFO - __main__ - Step 20 Global step 20 Train loss 1.76 on epoch=6
03/02/2022 01:47:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.89 on epoch=9
03/02/2022 01:47:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.71 on epoch=13
03/02/2022 01:47:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.69 on epoch=16
03/02/2022 01:47:11 - INFO - __main__ - Global step 50 Train loss 1.49 Classification-F1 0.1693121693121693 on epoch=16
03/02/2022 01:47:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1693121693121693 on epoch=16, global_step=50
03/02/2022 01:47:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.69 on epoch=19
03/02/2022 01:47:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
03/02/2022 01:47:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
03/02/2022 01:47:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=29
03/02/2022 01:47:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=33
03/02/2022 01:47:23 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 01:47:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=36
03/02/2022 01:47:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 01:47:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=43
03/02/2022 01:47:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/02/2022 01:47:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
03/02/2022 01:47:35 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 01:47:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=53
03/02/2022 01:47:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
03/02/2022 01:47:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
03/02/2022 01:47:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
03/02/2022 01:47:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
03/02/2022 01:47:48 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.2569230769230769 on epoch=66
03/02/2022 01:47:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.2569230769230769 on epoch=66, global_step=200
03/02/2022 01:47:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
03/02/2022 01:47:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=73
03/02/2022 01:47:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=76
03/02/2022 01:47:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
03/02/2022 01:47:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=83
03/02/2022 01:48:00 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=83
03/02/2022 01:48:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=86
03/02/2022 01:48:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=89
03/02/2022 01:48:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=93
03/02/2022 01:48:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=96
03/02/2022 01:48:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=99
03/02/2022 01:48:12 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 01:48:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=103
03/02/2022 01:48:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 01:48:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 01:48:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 01:48:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/02/2022 01:48:24 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.42777777777777776 on epoch=116
03/02/2022 01:48:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2569230769230769 -> 0.42777777777777776 on epoch=116, global_step=350
03/02/2022 01:48:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=119
03/02/2022 01:48:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
03/02/2022 01:48:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
03/02/2022 01:48:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=129
03/02/2022 01:48:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
03/02/2022 01:48:36 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.3138029405310511 on epoch=133
03/02/2022 01:48:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
03/02/2022 01:48:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
03/02/2022 01:48:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 01:48:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
03/02/2022 01:48:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
03/02/2022 01:48:49 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.35232668566001896 on epoch=149
03/02/2022 01:48:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=153
03/02/2022 01:48:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=156
03/02/2022 01:48:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=159
03/02/2022 01:48:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
03/02/2022 01:49:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
03/02/2022 01:49:01 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.42540792540792544 on epoch=166
03/02/2022 01:49:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 01:49:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=173
03/02/2022 01:49:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=176
03/02/2022 01:49:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
03/02/2022 01:49:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
03/02/2022 01:49:13 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.42529724655819773 on epoch=183
03/02/2022 01:49:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
03/02/2022 01:49:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=189
03/02/2022 01:49:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=193
03/02/2022 01:49:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
03/02/2022 01:49:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=199
03/02/2022 01:49:25 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3097572362278245 on epoch=199
03/02/2022 01:49:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=203
03/02/2022 01:49:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
03/02/2022 01:49:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=209
03/02/2022 01:49:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=213
03/02/2022 01:49:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
03/02/2022 01:49:37 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.26973684210526316 on epoch=216
03/02/2022 01:49:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=219
03/02/2022 01:49:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=223
03/02/2022 01:49:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=226
03/02/2022 01:49:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
03/02/2022 01:49:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=233
03/02/2022 01:49:49 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.23736263736263735 on epoch=233
03/02/2022 01:49:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=236
03/02/2022 01:49:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=239
03/02/2022 01:49:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=243
03/02/2022 01:49:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=246
03/02/2022 01:50:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
03/02/2022 01:50:01 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.2698607698607699 on epoch=249
03/02/2022 01:50:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
03/02/2022 01:50:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=256
03/02/2022 01:50:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
03/02/2022 01:50:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=263
03/02/2022 01:50:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=266
03/02/2022 01:50:13 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.18410852713178294 on epoch=266
03/02/2022 01:50:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
03/02/2022 01:50:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
03/02/2022 01:50:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
03/02/2022 01:50:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=279
03/02/2022 01:50:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
03/02/2022 01:50:25 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.2467552182163188 on epoch=283
03/02/2022 01:50:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
03/02/2022 01:50:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
03/02/2022 01:50:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
03/02/2022 01:50:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 01:50:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
03/02/2022 01:50:36 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.1957983193277311 on epoch=299
03/02/2022 01:50:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
03/02/2022 01:50:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=306
03/02/2022 01:50:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
03/02/2022 01:50:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
03/02/2022 01:50:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=316
03/02/2022 01:50:48 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.2074829931972789 on epoch=316
03/02/2022 01:50:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
03/02/2022 01:50:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
03/02/2022 01:50:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
03/02/2022 01:50:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=329
03/02/2022 01:50:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
03/02/2022 01:51:01 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.2481616334689798 on epoch=333
03/02/2022 01:51:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 01:51:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
03/02/2022 01:51:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
03/02/2022 01:51:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
03/02/2022 01:51:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
03/02/2022 01:51:12 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.20285714285714285 on epoch=349
03/02/2022 01:51:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
03/02/2022 01:51:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
03/02/2022 01:51:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
03/02/2022 01:51:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=363
03/02/2022 01:51:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
03/02/2022 01:51:24 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.19275862068965516 on epoch=366
03/02/2022 01:51:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
03/02/2022 01:51:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
03/02/2022 01:51:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
03/02/2022 01:51:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=379
03/02/2022 01:51:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 01:51:36 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.16085946573751453 on epoch=383
03/02/2022 01:51:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
03/02/2022 01:51:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
03/02/2022 01:51:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
03/02/2022 01:51:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
03/02/2022 01:51:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
03/02/2022 01:51:48 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.16545674531155474 on epoch=399
03/02/2022 01:51:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 01:51:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
03/02/2022 01:51:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 01:51:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
03/02/2022 01:51:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 01:52:00 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.19484593837535016 on epoch=416
03/02/2022 01:52:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 01:52:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
03/02/2022 01:52:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
03/02/2022 01:52:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 01:52:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 01:52:12 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.1571258199165176 on epoch=433
03/02/2022 01:52:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 01:52:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
03/02/2022 01:52:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 01:52:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
03/02/2022 01:52:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
03/02/2022 01:52:24 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.23771337126600284 on epoch=449
03/02/2022 01:52:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 01:52:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 01:52:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 01:52:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 01:52:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 01:52:36 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.27922077922077926 on epoch=466
03/02/2022 01:52:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
03/02/2022 01:52:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
03/02/2022 01:52:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/02/2022 01:52:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 01:52:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=483
03/02/2022 01:52:47 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.24358974358974356 on epoch=483
03/02/2022 01:52:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 01:52:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 01:52:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 01:52:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 01:52:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
03/02/2022 01:53:00 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.18104409333794638 on epoch=499
03/02/2022 01:53:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
03/02/2022 01:53:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
03/02/2022 01:53:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 01:53:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 01:53:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 01:53:12 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.20044662659263676 on epoch=516
03/02/2022 01:53:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
03/02/2022 01:53:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 01:53:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 01:53:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 01:53:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 01:53:24 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.23216374269005846 on epoch=533
03/02/2022 01:53:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 01:53:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 01:53:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 01:53:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 01:53:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 01:53:36 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.26031746031746034 on epoch=549
03/02/2022 01:53:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 01:53:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 01:53:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 01:53:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/02/2022 01:53:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
03/02/2022 01:53:48 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.21572681704260652 on epoch=566
03/02/2022 01:53:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 01:53:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 01:53:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
03/02/2022 01:53:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 01:53:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 01:54:00 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.20013227513227513 on epoch=583
03/02/2022 01:54:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 01:54:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 01:54:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 01:54:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=596
03/02/2022 01:54:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
03/02/2022 01:54:13 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.1780042546875773 on epoch=599
03/02/2022 01:54:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 01:54:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 01:54:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 01:54:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
03/02/2022 01:54:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 01:54:25 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.20397252090800477 on epoch=616
03/02/2022 01:54:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 01:54:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 01:54:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 01:54:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 01:54:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
03/02/2022 01:54:37 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2384615384615385 on epoch=633
03/02/2022 01:54:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 01:54:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 01:54:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
03/02/2022 01:54:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/02/2022 01:54:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 01:54:49 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.19576719576719578 on epoch=649
03/02/2022 01:54:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 01:54:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 01:54:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 01:54:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 01:55:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 01:55:01 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.2624358974358974 on epoch=666
03/02/2022 01:55:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 01:55:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 01:55:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 01:55:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 01:55:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 01:55:13 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.2742307692307692 on epoch=683
03/02/2022 01:55:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 01:55:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 01:55:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 01:55:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 01:55:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
03/02/2022 01:55:25 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.27592613949435546 on epoch=699
03/02/2022 01:55:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 01:55:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 01:55:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
03/02/2022 01:55:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 01:55:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 01:55:37 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.2647451963241437 on epoch=716
03/02/2022 01:55:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 01:55:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 01:55:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 01:55:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 01:55:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 01:55:50 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.21903965599617772 on epoch=733
03/02/2022 01:55:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 01:55:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 01:55:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 01:55:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 01:56:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 01:56:02 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.24523809523809526 on epoch=749
03/02/2022 01:56:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 01:56:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/02/2022 01:56:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/02/2022 01:56:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 01:56:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 01:56:14 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.23879310344827587 on epoch=766
03/02/2022 01:56:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 01:56:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 01:56:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 01:56:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 01:56:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 01:56:26 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.2190704032809296 on epoch=783
03/02/2022 01:56:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 01:56:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 01:56:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 01:56:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 01:56:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 01:56:38 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.252046783625731 on epoch=799
03/02/2022 01:56:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 01:56:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 01:56:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 01:56:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 01:56:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 01:56:50 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.340137299771167 on epoch=816
03/02/2022 01:56:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 01:56:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 01:56:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
03/02/2022 01:56:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
03/02/2022 01:57:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 01:57:03 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3319355992844365 on epoch=833
03/02/2022 01:57:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 01:57:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 01:57:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 01:57:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=846
03/02/2022 01:57:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 01:57:15 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3353170731707317 on epoch=849
03/02/2022 01:57:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 01:57:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 01:57:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 01:57:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=863
03/02/2022 01:57:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 01:57:27 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.25121693121693117 on epoch=866
03/02/2022 01:57:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 01:57:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 01:57:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=876
03/02/2022 01:57:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 01:57:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 01:57:39 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.4403153153153153 on epoch=883
03/02/2022 01:57:40 - INFO - __main__ - Saving model with best Classification-F1: 0.42777777777777776 -> 0.4403153153153153 on epoch=883, global_step=2650
03/02/2022 01:57:42 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 01:57:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 01:57:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 01:57:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 01:57:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 01:57:52 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.27991452991453 on epoch=899
03/02/2022 01:57:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 01:57:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 01:57:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 01:58:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 01:58:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 01:58:04 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.2816666666666667 on epoch=916
03/02/2022 01:58:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 01:58:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 01:58:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=926
03/02/2022 01:58:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 01:58:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
03/02/2022 01:58:16 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.4313186813186813 on epoch=933
03/02/2022 01:58:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=936
03/02/2022 01:58:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
03/02/2022 01:58:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 01:58:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 01:58:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 01:58:28 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.35946763460375075 on epoch=949
03/02/2022 01:58:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 01:58:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 01:58:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
03/02/2022 01:58:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 01:58:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 01:58:41 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3427141963727329 on epoch=966
03/02/2022 01:58:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 01:58:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 01:58:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 01:58:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 01:58:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 01:58:53 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.4461869313482217 on epoch=983
03/02/2022 01:58:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4403153153153153 -> 0.4461869313482217 on epoch=983, global_step=2950
03/02/2022 01:58:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 01:58:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
03/02/2022 01:59:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 01:59:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 01:59:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 01:59:05 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.3338501291989664 on epoch=999
03/02/2022 01:59:05 - INFO - __main__ - save last model!
03/02/2022 01:59:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 01:59:05 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 01:59:05 - INFO - __main__ - Printing 3 examples
03/02/2022 01:59:05 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 01:59:05 - INFO - __main__ - ['normal']
03/02/2022 01:59:05 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 01:59:05 - INFO - __main__ - ['normal']
03/02/2022 01:59:05 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 01:59:05 - INFO - __main__ - ['normal']
03/02/2022 01:59:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:59:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:59:06 - INFO - __main__ - Printing 3 examples
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 01:59:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:59:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:59:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:59:06 - INFO - __main__ - Printing 3 examples
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:59:06 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:59:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:59:06 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 01:59:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:59:08 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 01:59:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 01:59:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 01:59:19 - INFO - __main__ - Starting training!
03/02/2022 01:59:52 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_13_0.3_8_predictions.txt
03/02/2022 01:59:52 - INFO - __main__ - Classification-F1 on test data: 0.1705
03/02/2022 01:59:53 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.3, bsz=8, dev_performance=0.4461869313482217, test_performance=0.17049174223612978
03/02/2022 01:59:53 - INFO - __main__ - Running ... prefix=hatexplain_16_13, lr=0.2, bsz=8 ...
03/02/2022 01:59:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:59:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] i tole sum brillohaided nigger bich off last nite cuz it try to get uppity wit me
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] no islam is another anti racialist universalism promoting endless retard level violence jews just use you sand niggers as they use jungle niggers to destroy what they actually fear the white race
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] remember to support the merchant right is not civic nationalism awesome based nigger loving kike faggot and his crew of fellow scam artists are here to save western civilization
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 01:59:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:59:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 01:59:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 01:59:54 - INFO - __main__ - Printing 3 examples
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] disgusting like we need more sand niggers here
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] fuck you are a faggot lol that really the best you got
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ -  [hatexplain] <user> i will call my friends and we go fuck up that nigger
03/02/2022 01:59:54 - INFO - __main__ - ['hatespeech']
03/02/2022 01:59:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 01:59:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 01:59:54 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:00:06 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:00:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:00:07 - INFO - __main__ - Starting training!
03/02/2022 02:00:10 - INFO - __main__ - Step 10 Global step 10 Train loss 3.65 on epoch=3
03/02/2022 02:00:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.34 on epoch=6
03/02/2022 02:00:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=9
03/02/2022 02:00:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=13
03/02/2022 02:00:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=16
03/02/2022 02:00:20 - INFO - __main__ - Global step 50 Train loss 1.86 Classification-F1 0.15873015873015875 on epoch=16
03/02/2022 02:00:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15873015873015875 on epoch=16, global_step=50
03/02/2022 02:00:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.71 on epoch=19
03/02/2022 02:00:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=23
03/02/2022 02:00:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=26
03/02/2022 02:00:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=29
03/02/2022 02:00:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=33
03/02/2022 02:00:32 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 02:00:32 - INFO - __main__ - Saving model with best Classification-F1: 0.15873015873015875 -> 0.16666666666666666 on epoch=33, global_step=100
03/02/2022 02:00:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=36
03/02/2022 02:00:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=39
03/02/2022 02:00:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
03/02/2022 02:00:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=46
03/02/2022 02:00:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=49
03/02/2022 02:00:44 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 02:00:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=53
03/02/2022 02:00:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=56
03/02/2022 02:00:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=59
03/02/2022 02:00:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/02/2022 02:00:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=66
03/02/2022 02:00:56 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16129032258064516 on epoch=66
03/02/2022 02:00:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=69
03/02/2022 02:01:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
03/02/2022 02:01:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=76
03/02/2022 02:01:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=79
03/02/2022 02:01:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=83
03/02/2022 02:01:08 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=83
03/02/2022 02:01:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
03/02/2022 02:01:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=89
03/02/2022 02:01:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=93
03/02/2022 02:01:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
03/02/2022 02:01:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=99
03/02/2022 02:01:19 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 02:01:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=103
03/02/2022 02:01:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=106
03/02/2022 02:01:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/02/2022 02:01:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
03/02/2022 02:01:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=116
03/02/2022 02:01:31 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=116
03/02/2022 02:01:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=119
03/02/2022 02:01:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=123
03/02/2022 02:01:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=126
03/02/2022 02:01:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
03/02/2022 02:01:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=133
03/02/2022 02:01:43 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=133
03/02/2022 02:01:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=136
03/02/2022 02:01:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=139
03/02/2022 02:01:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 02:01:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=146
03/02/2022 02:01:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=149
03/02/2022 02:01:55 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=149
03/02/2022 02:01:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=153
03/02/2022 02:01:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=156
03/02/2022 02:02:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=159
03/02/2022 02:02:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
03/02/2022 02:02:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=166
03/02/2022 02:02:06 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.28869895536562207 on epoch=166
03/02/2022 02:02:06 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.28869895536562207 on epoch=166, global_step=500
03/02/2022 02:02:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=169
03/02/2022 02:02:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=173
03/02/2022 02:02:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
03/02/2022 02:02:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=179
03/02/2022 02:02:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
03/02/2022 02:02:18 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.3146867798030588 on epoch=183
03/02/2022 02:02:18 - INFO - __main__ - Saving model with best Classification-F1: 0.28869895536562207 -> 0.3146867798030588 on epoch=183, global_step=550
03/02/2022 02:02:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=186
03/02/2022 02:02:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=189
03/02/2022 02:02:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=193
03/02/2022 02:02:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=196
03/02/2022 02:02:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=199
03/02/2022 02:02:30 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.36717251002965284 on epoch=199
03/02/2022 02:02:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3146867798030588 -> 0.36717251002965284 on epoch=199, global_step=600
03/02/2022 02:02:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
03/02/2022 02:02:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=206
03/02/2022 02:02:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=209
03/02/2022 02:02:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=213
03/02/2022 02:02:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=216
03/02/2022 02:02:42 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.48044680525498945 on epoch=216
03/02/2022 02:02:42 - INFO - __main__ - Saving model with best Classification-F1: 0.36717251002965284 -> 0.48044680525498945 on epoch=216, global_step=650
03/02/2022 02:02:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=219
03/02/2022 02:02:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=223
03/02/2022 02:02:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=226
03/02/2022 02:02:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
03/02/2022 02:02:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=233
03/02/2022 02:02:54 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.3690865780627351 on epoch=233
03/02/2022 02:02:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=236
03/02/2022 02:02:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=239
03/02/2022 02:03:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=243
03/02/2022 02:03:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=246
03/02/2022 02:03:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=249
03/02/2022 02:03:06 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.5147993768683424 on epoch=249
03/02/2022 02:03:06 - INFO - __main__ - Saving model with best Classification-F1: 0.48044680525498945 -> 0.5147993768683424 on epoch=249, global_step=750
03/02/2022 02:03:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=253
03/02/2022 02:03:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=256
03/02/2022 02:03:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=259
03/02/2022 02:03:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
03/02/2022 02:03:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=266
03/02/2022 02:03:18 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.5446428571428572 on epoch=266
03/02/2022 02:03:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5147993768683424 -> 0.5446428571428572 on epoch=266, global_step=800
03/02/2022 02:03:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
03/02/2022 02:03:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=273
03/02/2022 02:03:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=276
03/02/2022 02:03:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=279
03/02/2022 02:03:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=283
03/02/2022 02:03:30 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3682539682539683 on epoch=283
03/02/2022 02:03:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=286
03/02/2022 02:03:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=289
03/02/2022 02:03:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=293
03/02/2022 02:03:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=296
03/02/2022 02:03:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=299
03/02/2022 02:03:43 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.4575757575757576 on epoch=299
03/02/2022 02:03:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=303
03/02/2022 02:03:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
03/02/2022 02:03:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=309
03/02/2022 02:03:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=313
03/02/2022 02:03:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=316
03/02/2022 02:03:55 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.3636363636363636 on epoch=316
03/02/2022 02:03:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=319
03/02/2022 02:04:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=323
03/02/2022 02:04:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=326
03/02/2022 02:04:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
03/02/2022 02:04:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=333
03/02/2022 02:04:08 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.37566137566137564 on epoch=333
03/02/2022 02:04:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
03/02/2022 02:04:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=339
03/02/2022 02:04:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=343
03/02/2022 02:04:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=346
03/02/2022 02:04:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=349
03/02/2022 02:04:20 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.42953888552799624 on epoch=349
03/02/2022 02:04:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=353
03/02/2022 02:04:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
03/02/2022 02:04:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=359
03/02/2022 02:04:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=363
03/02/2022 02:04:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=366
03/02/2022 02:04:32 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.2744360902255639 on epoch=366
03/02/2022 02:04:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
03/02/2022 02:04:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=373
03/02/2022 02:04:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=376
03/02/2022 02:04:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
03/02/2022 02:04:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
03/02/2022 02:04:45 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.3226470588235294 on epoch=383
03/02/2022 02:04:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=386
03/02/2022 02:04:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=389
03/02/2022 02:04:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
03/02/2022 02:04:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=396
03/02/2022 02:04:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
03/02/2022 02:04:57 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.25833333333333336 on epoch=399
03/02/2022 02:04:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=403
03/02/2022 02:05:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
03/02/2022 02:05:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=409
03/02/2022 02:05:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
03/02/2022 02:05:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=416
03/02/2022 02:05:10 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.30259146341463417 on epoch=416
03/02/2022 02:05:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
03/02/2022 02:05:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
03/02/2022 02:05:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=426
03/02/2022 02:05:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=429
03/02/2022 02:05:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
03/02/2022 02:05:22 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.3381911323087794 on epoch=433
03/02/2022 02:05:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=436
03/02/2022 02:05:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
03/02/2022 02:05:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
03/02/2022 02:05:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 02:05:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
03/02/2022 02:05:35 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.32703081232493003 on epoch=449
03/02/2022 02:05:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=453
03/02/2022 02:05:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
03/02/2022 02:05:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
03/02/2022 02:05:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
03/02/2022 02:05:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=466
03/02/2022 02:05:47 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.4373504784688995 on epoch=466
03/02/2022 02:05:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=469
03/02/2022 02:05:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
03/02/2022 02:05:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=476
03/02/2022 02:05:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
03/02/2022 02:05:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
03/02/2022 02:06:00 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.3252871855813032 on epoch=483
03/02/2022 02:06:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
03/02/2022 02:06:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 02:06:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=493
03/02/2022 02:06:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
03/02/2022 02:06:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 02:06:12 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.3714881609618452 on epoch=499
03/02/2022 02:06:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
03/02/2022 02:06:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
03/02/2022 02:06:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
03/02/2022 02:06:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=513
03/02/2022 02:06:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
03/02/2022 02:06:24 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.3700980392156863 on epoch=516
03/02/2022 02:06:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=519
03/02/2022 02:06:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 02:06:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 02:06:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/02/2022 02:06:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=533
03/02/2022 02:06:37 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.2660368870895187 on epoch=533
03/02/2022 02:06:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
03/02/2022 02:06:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
03/02/2022 02:06:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
03/02/2022 02:06:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
03/02/2022 02:06:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
03/02/2022 02:06:49 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.26766076421248836 on epoch=549
03/02/2022 02:06:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
03/02/2022 02:06:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
03/02/2022 02:06:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
03/02/2022 02:06:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=563
03/02/2022 02:07:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
03/02/2022 02:07:01 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.265993265993266 on epoch=566
03/02/2022 02:07:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=569
03/02/2022 02:07:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=573
03/02/2022 02:07:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 02:07:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 02:07:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=583
03/02/2022 02:07:14 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.2888888888888889 on epoch=583
03/02/2022 02:07:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
03/02/2022 02:07:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
03/02/2022 02:07:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=593
03/02/2022 02:07:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 02:07:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
03/02/2022 02:07:26 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.20915032679738563 on epoch=599
03/02/2022 02:07:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/02/2022 02:07:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
03/02/2022 02:07:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 02:07:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 02:07:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=616
03/02/2022 02:07:38 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.21162790697674422 on epoch=616
03/02/2022 02:07:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
03/02/2022 02:07:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 02:07:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
03/02/2022 02:07:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
03/02/2022 02:07:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
03/02/2022 02:07:51 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.2596491228070176 on epoch=633
03/02/2022 02:07:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/02/2022 02:07:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
03/02/2022 02:07:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 02:08:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 02:08:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 02:08:03 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.24463320463320465 on epoch=649
03/02/2022 02:08:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/02/2022 02:08:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 02:08:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 02:08:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
03/02/2022 02:08:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
03/02/2022 02:08:15 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.2373411534701857 on epoch=666
03/02/2022 02:08:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 02:08:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
03/02/2022 02:08:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/02/2022 02:08:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
03/02/2022 02:08:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
03/02/2022 02:08:28 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.2360430283224401 on epoch=683
03/02/2022 02:08:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 02:08:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
03/02/2022 02:08:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
03/02/2022 02:08:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=696
03/02/2022 02:08:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/02/2022 02:08:40 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.21780604133545312 on epoch=699
03/02/2022 02:08:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 02:08:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
03/02/2022 02:08:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 02:08:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/02/2022 02:08:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 02:08:52 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.22539682539682543 on epoch=716
03/02/2022 02:08:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 02:08:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
03/02/2022 02:08:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 02:09:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 02:09:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 02:09:05 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1753211629479378 on epoch=733
03/02/2022 02:09:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
03/02/2022 02:09:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 02:09:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
03/02/2022 02:09:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
03/02/2022 02:09:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
03/02/2022 02:09:17 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.25925925925925924 on epoch=749
03/02/2022 02:09:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
03/02/2022 02:09:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:09:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:09:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
03/02/2022 02:09:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
03/02/2022 02:09:29 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.17088698758390228 on epoch=766
03/02/2022 02:09:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/02/2022 02:09:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=773
03/02/2022 02:09:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 02:09:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:09:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 02:09:41 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.27916467399968115 on epoch=783
03/02/2022 02:09:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 02:09:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:09:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 02:09:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:09:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 02:09:54 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.24052910052910054 on epoch=799
03/02/2022 02:09:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 02:09:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 02:10:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=809
03/02/2022 02:10:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 02:10:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 02:10:06 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.24091289903221144 on epoch=816
03/02/2022 02:10:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/02/2022 02:10:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 02:10:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=826
03/02/2022 02:10:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
03/02/2022 02:10:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
03/02/2022 02:10:19 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.3147619047619048 on epoch=833
03/02/2022 02:10:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:10:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 02:10:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/02/2022 02:10:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 02:10:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:10:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.40714285714285714 on epoch=849
03/02/2022 02:10:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 02:10:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 02:10:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
03/02/2022 02:10:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 02:10:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 02:10:44 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.28853698273053113 on epoch=866
03/02/2022 02:10:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:10:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:10:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 02:10:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 02:10:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 02:10:56 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.22221028762382147 on epoch=883
03/02/2022 02:10:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:11:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=889
03/02/2022 02:11:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:11:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:11:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:11:08 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.22037037037037036 on epoch=899
03/02/2022 02:11:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 02:11:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 02:11:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:11:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=913
03/02/2022 02:11:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=916
03/02/2022 02:11:21 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2563519813519814 on epoch=916
03/02/2022 02:11:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:11:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
03/02/2022 02:11:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 02:11:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 02:11:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
03/02/2022 02:11:33 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2897922312556459 on epoch=933
03/02/2022 02:11:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:11:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:11:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 02:11:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:11:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/02/2022 02:11:45 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.26374269005847956 on epoch=949
03/02/2022 02:11:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 02:11:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:11:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:11:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:11:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 02:11:57 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.25925445798404056 on epoch=966
03/02/2022 02:12:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:12:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:12:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 02:12:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:12:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:12:09 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.262155388471178 on epoch=983
03/02/2022 02:12:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:12:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:12:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:12:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=996
03/02/2022 02:12:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:12:21 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.21356421356421354 on epoch=999
03/02/2022 02:12:21 - INFO - __main__ - save last model!
03/02/2022 02:12:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:12:21 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:12:21 - INFO - __main__ - Printing 3 examples
03/02/2022 02:12:21 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:12:21 - INFO - __main__ - ['normal']
03/02/2022 02:12:21 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:12:21 - INFO - __main__ - ['normal']
03/02/2022 02:12:21 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:12:21 - INFO - __main__ - ['normal']
03/02/2022 02:12:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:12:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:12:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:12:23 - INFO - __main__ - Printing 3 examples
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:12:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:12:23 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:12:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:12:23 - INFO - __main__ - Printing 3 examples
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:12:23 - INFO - __main__ - ['offensive']
03/02/2022 02:12:23 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:12:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:12:23 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:12:24 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:12:35 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:12:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:12:36 - INFO - __main__ - Starting training!
03/02/2022 02:13:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_13_0.2_8_predictions.txt
03/02/2022 02:13:07 - INFO - __main__ - Classification-F1 on test data: 0.1058
03/02/2022 02:13:07 - INFO - __main__ - prefix=hatexplain_16_13, lr=0.2, bsz=8, dev_performance=0.5446428571428572, test_performance=0.1057637865970685
03/02/2022 02:13:07 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.5, bsz=8 ...
03/02/2022 02:13:08 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:13:08 - INFO - __main__ - Printing 3 examples
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:13:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:13:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:13:08 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:13:08 - INFO - __main__ - Printing 3 examples
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:13:08 - INFO - __main__ - ['offensive']
03/02/2022 02:13:08 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:13:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:13:08 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:13:22 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:13:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:13:23 - INFO - __main__ - Starting training!
03/02/2022 02:13:25 - INFO - __main__ - Step 10 Global step 10 Train loss 2.63 on epoch=3
03/02/2022 02:13:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.87 on epoch=6
03/02/2022 02:13:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.73 on epoch=9
03/02/2022 02:13:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=13
03/02/2022 02:13:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=16
03/02/2022 02:13:36 - INFO - __main__ - Global step 50 Train loss 1.06 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 02:13:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 02:13:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
03/02/2022 02:13:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=23
03/02/2022 02:13:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
03/02/2022 02:13:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=29
03/02/2022 02:13:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
03/02/2022 02:13:48 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.30686757461605335 on epoch=33
03/02/2022 02:13:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.30686757461605335 on epoch=33, global_step=100
03/02/2022 02:13:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 02:13:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
03/02/2022 02:13:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
03/02/2022 02:13:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/02/2022 02:13:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
03/02/2022 02:14:00 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 02:14:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
03/02/2022 02:14:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
03/02/2022 02:14:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=59
03/02/2022 02:14:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
03/02/2022 02:14:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
03/02/2022 02:14:13 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.16129032258064516 on epoch=66
03/02/2022 02:14:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
03/02/2022 02:14:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
03/02/2022 02:14:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
03/02/2022 02:14:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 02:14:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
03/02/2022 02:14:25 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
03/02/2022 02:14:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
03/02/2022 02:14:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
03/02/2022 02:14:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 02:14:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
03/02/2022 02:14:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
03/02/2022 02:14:37 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 02:14:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=103
03/02/2022 02:14:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
03/02/2022 02:14:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
03/02/2022 02:14:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
03/02/2022 02:14:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
03/02/2022 02:14:50 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.4216676569617746 on epoch=116
03/02/2022 02:14:50 - INFO - __main__ - Saving model with best Classification-F1: 0.30686757461605335 -> 0.4216676569617746 on epoch=116, global_step=350
03/02/2022 02:14:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
03/02/2022 02:14:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
03/02/2022 02:14:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=126
03/02/2022 02:14:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=129
03/02/2022 02:15:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
03/02/2022 02:15:02 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.41626899964651815 on epoch=133
03/02/2022 02:15:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
03/02/2022 02:15:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
03/02/2022 02:15:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
03/02/2022 02:15:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=146
03/02/2022 02:15:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=149
03/02/2022 02:15:15 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.3524130190796857 on epoch=149
03/02/2022 02:15:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=153
03/02/2022 02:15:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
03/02/2022 02:15:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=159
03/02/2022 02:15:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
03/02/2022 02:15:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=166
03/02/2022 02:15:27 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.4534072249589491 on epoch=166
03/02/2022 02:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4216676569617746 -> 0.4534072249589491 on epoch=166, global_step=500
03/02/2022 02:15:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=169
03/02/2022 02:15:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
03/02/2022 02:15:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
03/02/2022 02:15:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=179
03/02/2022 02:15:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=183
03/02/2022 02:15:39 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3886230728335991 on epoch=183
03/02/2022 02:15:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
03/02/2022 02:15:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=189
03/02/2022 02:15:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
03/02/2022 02:15:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=196
03/02/2022 02:15:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=199
03/02/2022 02:15:51 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.3258113045347088 on epoch=199
03/02/2022 02:15:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=203
03/02/2022 02:15:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=206
03/02/2022 02:15:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=209
03/02/2022 02:16:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
03/02/2022 02:16:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
03/02/2022 02:16:04 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.33423913043478265 on epoch=216
03/02/2022 02:16:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
03/02/2022 02:16:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
03/02/2022 02:16:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=226
03/02/2022 02:16:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
03/02/2022 02:16:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
03/02/2022 02:16:16 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.15518744551002614 on epoch=233
03/02/2022 02:16:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
03/02/2022 02:16:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
03/02/2022 02:16:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
03/02/2022 02:16:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
03/02/2022 02:16:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=249
03/02/2022 02:16:29 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.41224561173410273 on epoch=249
03/02/2022 02:16:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
03/02/2022 02:16:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
03/02/2022 02:16:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=259
03/02/2022 02:16:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=263
03/02/2022 02:16:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
03/02/2022 02:16:41 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.360679970436068 on epoch=266
03/02/2022 02:16:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
03/02/2022 02:16:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
03/02/2022 02:16:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
03/02/2022 02:16:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=279
03/02/2022 02:16:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
03/02/2022 02:16:54 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.17237386269644334 on epoch=283
03/02/2022 02:16:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
03/02/2022 02:16:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=289
03/02/2022 02:17:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
03/02/2022 02:17:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 02:17:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
03/02/2022 02:17:06 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.41958142792403086 on epoch=299
03/02/2022 02:17:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
03/02/2022 02:17:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
03/02/2022 02:17:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
03/02/2022 02:17:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=313
03/02/2022 02:17:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
03/02/2022 02:17:18 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.2828544061302682 on epoch=316
03/02/2022 02:17:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
03/02/2022 02:17:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
03/02/2022 02:17:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
03/02/2022 02:17:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
03/02/2022 02:17:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
03/02/2022 02:17:31 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.29642857142857143 on epoch=333
03/02/2022 02:17:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=336
03/02/2022 02:17:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
03/02/2022 02:17:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
03/02/2022 02:17:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
03/02/2022 02:17:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
03/02/2022 02:17:43 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.3155241935483871 on epoch=349
03/02/2022 02:17:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
03/02/2022 02:17:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
03/02/2022 02:17:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
03/02/2022 02:17:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
03/02/2022 02:17:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
03/02/2022 02:17:56 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.4372549019607843 on epoch=366
03/02/2022 02:17:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 02:18:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
03/02/2022 02:18:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
03/02/2022 02:18:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
03/02/2022 02:18:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
03/02/2022 02:18:08 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.3243978243978244 on epoch=383
03/02/2022 02:18:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
03/02/2022 02:18:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 02:18:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
03/02/2022 02:18:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 02:18:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
03/02/2022 02:18:21 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.2929457527333894 on epoch=399
03/02/2022 02:18:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 02:18:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
03/02/2022 02:18:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
03/02/2022 02:18:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
03/02/2022 02:18:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=416
03/02/2022 02:18:34 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.3458725182863114 on epoch=416
03/02/2022 02:18:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
03/02/2022 02:18:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 02:18:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 02:18:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 02:18:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 02:18:46 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.47882046269143047 on epoch=433
03/02/2022 02:18:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4534072249589491 -> 0.47882046269143047 on epoch=433, global_step=1300
03/02/2022 02:18:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
03/02/2022 02:18:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
03/02/2022 02:18:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 02:18:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 02:18:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
03/02/2022 02:18:58 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.41627973281199093 on epoch=449
03/02/2022 02:19:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
03/02/2022 02:19:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
03/02/2022 02:19:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
03/02/2022 02:19:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 02:19:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 02:19:10 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.36311389759665624 on epoch=466
03/02/2022 02:19:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
03/02/2022 02:19:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 02:19:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 02:19:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=479
03/02/2022 02:19:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
03/02/2022 02:19:22 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.2588522588522589 on epoch=483
03/02/2022 02:19:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
03/02/2022 02:19:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 02:19:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 02:19:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 02:19:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
03/02/2022 02:19:35 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.25098039215686274 on epoch=499
03/02/2022 02:19:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
03/02/2022 02:19:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
03/02/2022 02:19:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 02:19:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 02:19:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
03/02/2022 02:19:47 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.3127629733520337 on epoch=516
03/02/2022 02:19:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 02:19:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 02:19:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 02:19:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 02:19:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 02:19:59 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2736111111111111 on epoch=533
03/02/2022 02:20:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
03/02/2022 02:20:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 02:20:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 02:20:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:20:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
03/02/2022 02:20:11 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.302752849866602 on epoch=549
03/02/2022 02:20:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 02:20:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 02:20:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=559
03/02/2022 02:20:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 02:20:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 02:20:23 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2976708074534161 on epoch=566
03/02/2022 02:20:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
03/02/2022 02:20:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 02:20:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 02:20:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 02:20:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
03/02/2022 02:20:36 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.256309819213045 on epoch=583
03/02/2022 02:20:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 02:20:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 02:20:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 02:20:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 02:20:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 02:20:47 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.2708333333333333 on epoch=599
03/02/2022 02:20:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
03/02/2022 02:20:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 02:20:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 02:20:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
03/02/2022 02:20:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 02:21:00 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2683982683982684 on epoch=616
03/02/2022 02:21:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 02:21:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
03/02/2022 02:21:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 02:21:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 02:21:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
03/02/2022 02:21:12 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.2146334920528469 on epoch=633
03/02/2022 02:21:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 02:21:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 02:21:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 02:21:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 02:21:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 02:21:24 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.35160940029094184 on epoch=649
03/02/2022 02:21:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 02:21:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=656
03/02/2022 02:21:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 02:21:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
03/02/2022 02:21:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 02:21:36 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.3803418803418804 on epoch=666
03/02/2022 02:21:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 02:21:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 02:21:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 02:21:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:21:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 02:21:48 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.4366168170395089 on epoch=683
03/02/2022 02:21:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
03/02/2022 02:21:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 02:21:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 02:21:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 02:21:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 02:22:00 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.4137293976003653 on epoch=699
03/02/2022 02:22:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 02:22:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 02:22:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=709
03/02/2022 02:22:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 02:22:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
03/02/2022 02:22:13 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.27253526043848625 on epoch=716
03/02/2022 02:22:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 02:22:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 02:22:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 02:22:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 02:22:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 02:22:25 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.2533769063180828 on epoch=733
03/02/2022 02:22:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 02:22:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 02:22:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=743
03/02/2022 02:22:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 02:22:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 02:22:37 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.26708937198067634 on epoch=749
03/02/2022 02:22:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
03/02/2022 02:22:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:22:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:22:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 02:22:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:22:50 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.31573802541544477 on epoch=766
03/02/2022 02:22:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 02:22:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:22:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:22:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/02/2022 02:23:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 02:23:02 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.28909407665505227 on epoch=783
03/02/2022 02:23:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 02:23:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:23:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 02:23:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:23:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 02:23:14 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.3590038314176245 on epoch=799
03/02/2022 02:23:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 02:23:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 02:23:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 02:23:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
03/02/2022 02:23:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 02:23:27 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.31786952823398545 on epoch=816
03/02/2022 02:23:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 02:23:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:23:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 02:23:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 02:23:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:23:39 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.19913419913419914 on epoch=833
03/02/2022 02:23:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:23:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:23:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 02:23:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 02:23:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=849
03/02/2022 02:23:52 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.3184261974584556 on epoch=849
03/02/2022 02:23:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
03/02/2022 02:23:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:23:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 02:24:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:24:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 02:24:04 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2897435897435897 on epoch=866
03/02/2022 02:24:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:24:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:24:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 02:24:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 02:24:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:24:17 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.32238225399495374 on epoch=883
03/02/2022 02:24:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:24:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:24:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:24:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:24:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:24:29 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.2791107078039927 on epoch=899
03/02/2022 02:24:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 02:24:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:24:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:24:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 02:24:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 02:24:42 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.28108672936259144 on epoch=916
03/02/2022 02:24:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 02:24:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:24:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
03/02/2022 02:24:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:24:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 02:24:54 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.3424908424908425 on epoch=933
03/02/2022 02:24:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=936
03/02/2022 02:24:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:25:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 02:25:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:25:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:25:07 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.33302147375924446 on epoch=949
03/02/2022 02:25:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 02:25:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:25:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
03/02/2022 02:25:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:25:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 02:25:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.22865497076023394 on epoch=966
03/02/2022 02:25:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:25:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:25:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 02:25:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:25:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:25:32 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.22962962962962963 on epoch=983
03/02/2022 02:25:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:25:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:25:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:25:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:25:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:25:44 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.21954578754578752 on epoch=999
03/02/2022 02:25:44 - INFO - __main__ - save last model!
03/02/2022 02:25:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:25:44 - INFO - __main__ - Printing 3 examples
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:25:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:25:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:25:44 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:25:44 - INFO - __main__ - Printing 3 examples
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:25:44 - INFO - __main__ - ['normal']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:25:44 - INFO - __main__ - ['normal']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:25:44 - INFO - __main__ - ['normal']
03/02/2022 02:25:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:25:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:25:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:25:44 - INFO - __main__ - Printing 3 examples
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:25:44 - INFO - __main__ - ['offensive']
03/02/2022 02:25:44 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:25:44 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:25:45 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:25:45 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:25:47 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:25:57 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:25:58 - INFO - __main__ - Starting training!
03/02/2022 02:26:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_21_0.5_8_predictions.txt
03/02/2022 02:26:35 - INFO - __main__ - Classification-F1 on test data: 0.1518
03/02/2022 02:26:36 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.5, bsz=8, dev_performance=0.47882046269143047, test_performance=0.1518228588816221
03/02/2022 02:26:36 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.4, bsz=8 ...
03/02/2022 02:26:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:26:36 - INFO - __main__ - Printing 3 examples
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:26:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:26:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:26:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:26:36 - INFO - __main__ - Printing 3 examples
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:26:36 - INFO - __main__ - ['offensive']
03/02/2022 02:26:36 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:26:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:26:37 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:26:50 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:26:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:26:51 - INFO - __main__ - Starting training!
03/02/2022 02:26:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.91 on epoch=3
03/02/2022 02:26:57 - INFO - __main__ - Step 20 Global step 20 Train loss 1.17 on epoch=6
03/02/2022 02:26:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=9
03/02/2022 02:27:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.64 on epoch=13
03/02/2022 02:27:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=16
03/02/2022 02:27:05 - INFO - __main__ - Global step 50 Train loss 1.17 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 02:27:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 02:27:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=19
03/02/2022 02:27:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
03/02/2022 02:27:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
03/02/2022 02:27:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
03/02/2022 02:27:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=33
03/02/2022 02:27:17 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 02:27:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
03/02/2022 02:27:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=39
03/02/2022 02:27:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
03/02/2022 02:27:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
03/02/2022 02:27:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
03/02/2022 02:27:29 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=49
03/02/2022 02:27:29 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2085278555866791 on epoch=49, global_step=150
03/02/2022 02:27:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
03/02/2022 02:27:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
03/02/2022 02:27:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
03/02/2022 02:27:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
03/02/2022 02:27:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
03/02/2022 02:27:41 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.28815314832672595 on epoch=66
03/02/2022 02:27:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.28815314832672595 on epoch=66, global_step=200
03/02/2022 02:27:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/02/2022 02:27:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
03/02/2022 02:27:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 02:27:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
03/02/2022 02:27:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
03/02/2022 02:27:54 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.2706349206349206 on epoch=83
03/02/2022 02:27:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
03/02/2022 02:27:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
03/02/2022 02:28:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 02:28:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
03/02/2022 02:28:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
03/02/2022 02:28:06 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3851649564161977 on epoch=99
03/02/2022 02:28:06 - INFO - __main__ - Saving model with best Classification-F1: 0.28815314832672595 -> 0.3851649564161977 on epoch=99, global_step=300
03/02/2022 02:28:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
03/02/2022 02:28:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
03/02/2022 02:28:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
03/02/2022 02:28:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=113
03/02/2022 02:28:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=116
03/02/2022 02:28:18 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.29156010230179036 on epoch=116
03/02/2022 02:28:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
03/02/2022 02:28:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
03/02/2022 02:28:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
03/02/2022 02:28:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/02/2022 02:28:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=133
03/02/2022 02:28:31 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.23074111000332334 on epoch=133
03/02/2022 02:28:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
03/02/2022 02:28:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
03/02/2022 02:28:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
03/02/2022 02:28:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/02/2022 02:28:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
03/02/2022 02:28:43 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.27970951343500366 on epoch=149
03/02/2022 02:28:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=153
03/02/2022 02:28:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
03/02/2022 02:28:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
03/02/2022 02:28:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=163
03/02/2022 02:28:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=166
03/02/2022 02:28:55 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.27629460123739297 on epoch=166
03/02/2022 02:28:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 02:29:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=173
03/02/2022 02:29:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
03/02/2022 02:29:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
03/02/2022 02:29:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
03/02/2022 02:29:08 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.36370992841581073 on epoch=183
03/02/2022 02:29:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=186
03/02/2022 02:29:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=189
03/02/2022 02:29:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
03/02/2022 02:29:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
03/02/2022 02:29:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=199
03/02/2022 02:29:20 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.2839506172839506 on epoch=199
03/02/2022 02:29:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
03/02/2022 02:29:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=206
03/02/2022 02:29:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=209
03/02/2022 02:29:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=213
03/02/2022 02:29:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
03/02/2022 02:29:32 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.3798584298584298 on epoch=216
03/02/2022 02:29:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
03/02/2022 02:29:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
03/02/2022 02:29:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
03/02/2022 02:29:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=229
03/02/2022 02:29:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=233
03/02/2022 02:29:45 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.30549450549450546 on epoch=233
03/02/2022 02:29:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
03/02/2022 02:29:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=239
03/02/2022 02:29:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
03/02/2022 02:29:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
03/02/2022 02:29:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
03/02/2022 02:29:57 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.3697478991596639 on epoch=249
03/02/2022 02:29:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
03/02/2022 02:30:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=256
03/02/2022 02:30:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=259
03/02/2022 02:30:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
03/02/2022 02:30:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
03/02/2022 02:30:09 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.30924630924630925 on epoch=266
03/02/2022 02:30:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=269
03/02/2022 02:30:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
03/02/2022 02:30:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=276
03/02/2022 02:30:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=279
03/02/2022 02:30:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=283
03/02/2022 02:30:22 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.25827505827505826 on epoch=283
03/02/2022 02:30:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=286
03/02/2022 02:30:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=289
03/02/2022 02:30:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
03/02/2022 02:30:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 02:30:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=299
03/02/2022 02:30:34 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.2106060606060606 on epoch=299
03/02/2022 02:30:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
03/02/2022 02:30:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
03/02/2022 02:30:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=309
03/02/2022 02:30:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=313
03/02/2022 02:30:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
03/02/2022 02:30:46 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.3986111111111111 on epoch=316
03/02/2022 02:30:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3851649564161977 -> 0.3986111111111111 on epoch=316, global_step=950
03/02/2022 02:30:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
03/02/2022 02:30:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
03/02/2022 02:30:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
03/02/2022 02:30:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
03/02/2022 02:30:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
03/02/2022 02:30:58 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.1936111111111111 on epoch=333
03/02/2022 02:31:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 02:31:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
03/02/2022 02:31:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
03/02/2022 02:31:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
03/02/2022 02:31:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=349
03/02/2022 02:31:11 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.16637013188737326 on epoch=349
03/02/2022 02:31:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 02:31:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
03/02/2022 02:31:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
03/02/2022 02:31:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
03/02/2022 02:31:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 02:31:23 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.293040293040293 on epoch=366
03/02/2022 02:31:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 02:31:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 02:31:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
03/02/2022 02:31:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
03/02/2022 02:31:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
03/02/2022 02:31:35 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.19179487179487179 on epoch=383
03/02/2022 02:31:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
03/02/2022 02:31:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
03/02/2022 02:31:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
03/02/2022 02:31:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
03/02/2022 02:31:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
03/02/2022 02:31:47 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.14765173804075404 on epoch=399
03/02/2022 02:31:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
03/02/2022 02:31:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
03/02/2022 02:31:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
03/02/2022 02:31:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
03/02/2022 02:31:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
03/02/2022 02:31:59 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.24135338345864663 on epoch=416
03/02/2022 02:32:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
03/02/2022 02:32:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
03/02/2022 02:32:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
03/02/2022 02:32:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
03/02/2022 02:32:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 02:32:11 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.1717171717171717 on epoch=433
03/02/2022 02:32:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
03/02/2022 02:32:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 02:32:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
03/02/2022 02:32:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 02:32:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
03/02/2022 02:32:23 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.14968729427254776 on epoch=449
03/02/2022 02:32:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 02:32:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
03/02/2022 02:32:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
03/02/2022 02:32:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 02:32:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
03/02/2022 02:32:34 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.21993707093821507 on epoch=466
03/02/2022 02:32:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
03/02/2022 02:32:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 02:32:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 02:32:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 02:32:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
03/02/2022 02:32:46 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.2522824302134647 on epoch=483
03/02/2022 02:32:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
03/02/2022 02:32:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
03/02/2022 02:32:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 02:32:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
03/02/2022 02:32:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 02:32:58 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.2947089208432221 on epoch=499
03/02/2022 02:33:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
03/02/2022 02:33:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/02/2022 02:33:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
03/02/2022 02:33:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
03/02/2022 02:33:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
03/02/2022 02:33:09 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.2902173913043478 on epoch=516
03/02/2022 02:33:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
03/02/2022 02:33:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 02:33:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 02:33:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 02:33:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 02:33:21 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.142646340922203 on epoch=533
03/02/2022 02:33:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 02:33:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 02:33:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 02:33:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:33:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 02:33:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.14654631411597355 on epoch=549
03/02/2022 02:33:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 02:33:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/02/2022 02:33:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/02/2022 02:33:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 02:33:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 02:33:45 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.237138300296195 on epoch=566
03/02/2022 02:33:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
03/02/2022 02:33:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 02:33:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 02:33:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 02:33:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 02:33:57 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.16498316498316498 on epoch=583
03/02/2022 02:33:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=586
03/02/2022 02:34:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
03/02/2022 02:34:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
03/02/2022 02:34:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 02:34:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 02:34:09 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.21476702508960574 on epoch=599
03/02/2022 02:34:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 02:34:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 02:34:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 02:34:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 02:34:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 02:34:20 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.19576719576719578 on epoch=616
03/02/2022 02:34:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 02:34:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 02:34:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 02:34:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 02:34:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/02/2022 02:34:32 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.17573483427141964 on epoch=633
03/02/2022 02:34:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
03/02/2022 02:34:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 02:34:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
03/02/2022 02:34:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 02:34:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 02:34:44 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.2215038314176245 on epoch=649
03/02/2022 02:34:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 02:34:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 02:34:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 02:34:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/02/2022 02:34:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
03/02/2022 02:34:56 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.3096342647534793 on epoch=666
03/02/2022 02:34:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 02:35:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 02:35:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 02:35:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:35:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 02:35:08 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.20987012987012985 on epoch=683
03/02/2022 02:35:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 02:35:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 02:35:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 02:35:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 02:35:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 02:35:19 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.10882936507936507 on epoch=699
03/02/2022 02:35:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 02:35:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 02:35:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 02:35:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 02:35:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 02:35:31 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.10205278592375366 on epoch=716
03/02/2022 02:35:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 02:35:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 02:35:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 02:35:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 02:35:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 02:35:43 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.1956772809767365 on epoch=733
03/02/2022 02:35:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 02:35:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 02:35:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 02:35:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 02:35:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 02:35:56 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.12141723256019049 on epoch=749
03/02/2022 02:35:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 02:36:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:36:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:36:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 02:36:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 02:36:08 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.14057790509403412 on epoch=766
03/02/2022 02:36:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 02:36:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:36:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:36:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:36:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
03/02/2022 02:36:20 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.09893790849673204 on epoch=783
03/02/2022 02:36:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 02:36:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:36:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 02:36:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:36:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 02:36:33 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.126302765853958 on epoch=799
03/02/2022 02:36:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 02:36:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 02:36:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 02:36:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 02:36:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 02:36:45 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.20303232998885176 on epoch=816
03/02/2022 02:36:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/02/2022 02:36:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:36:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 02:36:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
03/02/2022 02:36:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:36:57 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.16075673053601744 on epoch=833
03/02/2022 02:36:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
03/02/2022 02:37:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 02:37:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
03/02/2022 02:37:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 02:37:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:37:09 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.15165376495425756 on epoch=849
03/02/2022 02:37:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
03/02/2022 02:37:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:37:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 02:37:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:37:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 02:37:21 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.15811090593699287 on epoch=866
03/02/2022 02:37:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 02:37:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:37:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 02:37:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 02:37:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:37:33 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.141005291005291 on epoch=883
03/02/2022 02:37:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
03/02/2022 02:37:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:37:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:37:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 02:37:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 02:37:44 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.21934030804998547 on epoch=899
03/02/2022 02:37:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 02:37:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:37:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:37:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=913
03/02/2022 02:37:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 02:37:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.11886886886886885 on epoch=916
03/02/2022 02:37:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
03/02/2022 02:38:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:38:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 02:38:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:38:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
03/02/2022 02:38:08 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.16298116298116297 on epoch=933
03/02/2022 02:38:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:38:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:38:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 02:38:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:38:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
03/02/2022 02:38:20 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.18821693657219973 on epoch=949
03/02/2022 02:38:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 02:38:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:38:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
03/02/2022 02:38:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:38:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 02:38:32 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.19894736842105265 on epoch=966
03/02/2022 02:38:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:38:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:38:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=976
03/02/2022 02:38:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 02:38:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:38:44 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.1405895691609977 on epoch=983
03/02/2022 02:38:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 02:38:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:38:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/02/2022 02:38:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:38:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 02:38:56 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2960391425908667 on epoch=999
03/02/2022 02:38:56 - INFO - __main__ - save last model!
03/02/2022 02:38:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:38:56 - INFO - __main__ - Printing 3 examples
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:38:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:38:56 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:38:56 - INFO - __main__ - Printing 3 examples
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:38:56 - INFO - __main__ - ['normal']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:38:56 - INFO - __main__ - ['normal']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:38:56 - INFO - __main__ - ['normal']
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:38:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:38:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:38:56 - INFO - __main__ - Printing 3 examples
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:38:56 - INFO - __main__ - ['offensive']
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:38:56 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:38:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:38:58 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:39:08 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:39:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:39:09 - INFO - __main__ - Starting training!
03/02/2022 02:39:42 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_21_0.4_8_predictions.txt
03/02/2022 02:39:42 - INFO - __main__ - Classification-F1 on test data: 0.0618
03/02/2022 02:39:42 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.4, bsz=8, dev_performance=0.3986111111111111, test_performance=0.06180744104886566
03/02/2022 02:39:43 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.3, bsz=8 ...
03/02/2022 02:39:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:39:43 - INFO - __main__ - Printing 3 examples
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:39:43 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:39:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:39:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:39:43 - INFO - __main__ - Printing 3 examples
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:39:43 - INFO - __main__ - ['offensive']
03/02/2022 02:39:43 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:39:43 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:39:43 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:39:56 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:39:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:39:57 - INFO - __main__ - Starting training!
03/02/2022 02:39:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.18 on epoch=3
03/02/2022 02:40:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.66 on epoch=6
03/02/2022 02:40:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.81 on epoch=9
03/02/2022 02:40:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.66 on epoch=13
03/02/2022 02:40:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
03/02/2022 02:40:09 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.254889318719106 on epoch=16
03/02/2022 02:40:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.254889318719106 on epoch=16, global_step=50
03/02/2022 02:40:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=19
03/02/2022 02:40:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
03/02/2022 02:40:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
03/02/2022 02:40:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
03/02/2022 02:40:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=33
03/02/2022 02:40:21 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 02:40:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
03/02/2022 02:40:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 02:40:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
03/02/2022 02:40:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
03/02/2022 02:40:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/02/2022 02:40:33 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 02:40:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
03/02/2022 02:40:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=56
03/02/2022 02:40:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/02/2022 02:40:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
03/02/2022 02:40:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
03/02/2022 02:40:44 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 02:40:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
03/02/2022 02:40:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
03/02/2022 02:40:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/02/2022 02:40:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
03/02/2022 02:40:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
03/02/2022 02:40:56 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=83
03/02/2022 02:40:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
03/02/2022 02:41:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
03/02/2022 02:41:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
03/02/2022 02:41:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
03/02/2022 02:41:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=99
03/02/2022 02:41:08 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 02:41:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
03/02/2022 02:41:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
03/02/2022 02:41:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
03/02/2022 02:41:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=113
03/02/2022 02:41:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
03/02/2022 02:41:20 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.26740497675690456 on epoch=116
03/02/2022 02:41:20 - INFO - __main__ - Saving model with best Classification-F1: 0.254889318719106 -> 0.26740497675690456 on epoch=116, global_step=350
03/02/2022 02:41:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=119
03/02/2022 02:41:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
03/02/2022 02:41:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
03/02/2022 02:41:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
03/02/2022 02:41:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
03/02/2022 02:41:32 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.4916682386971012 on epoch=133
03/02/2022 02:41:32 - INFO - __main__ - Saving model with best Classification-F1: 0.26740497675690456 -> 0.4916682386971012 on epoch=133, global_step=400
03/02/2022 02:41:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=136
03/02/2022 02:41:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
03/02/2022 02:41:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
03/02/2022 02:41:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=146
03/02/2022 02:41:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
03/02/2022 02:41:44 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.42347578347578346 on epoch=149
03/02/2022 02:41:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
03/02/2022 02:41:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
03/02/2022 02:41:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
03/02/2022 02:41:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=163
03/02/2022 02:41:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
03/02/2022 02:41:55 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.31859131859131856 on epoch=166
03/02/2022 02:41:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
03/02/2022 02:42:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=173
03/02/2022 02:42:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=176
03/02/2022 02:42:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=179
03/02/2022 02:42:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=183
03/02/2022 02:42:07 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.38095238095238093 on epoch=183
03/02/2022 02:42:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=186
03/02/2022 02:42:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=189
03/02/2022 02:42:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=193
03/02/2022 02:42:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=196
03/02/2022 02:42:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
03/02/2022 02:42:20 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.30614035087719293 on epoch=199
03/02/2022 02:42:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
03/02/2022 02:42:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=206
03/02/2022 02:42:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
03/02/2022 02:42:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
03/02/2022 02:42:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=216
03/02/2022 02:42:32 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.3277731635289775 on epoch=216
03/02/2022 02:42:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
03/02/2022 02:42:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=223
03/02/2022 02:42:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=226
03/02/2022 02:42:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=229
03/02/2022 02:42:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=233
03/02/2022 02:42:44 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3245726208103379 on epoch=233
03/02/2022 02:42:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
03/02/2022 02:42:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
03/02/2022 02:42:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=243
03/02/2022 02:42:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=246
03/02/2022 02:42:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=249
03/02/2022 02:42:57 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.38054695562435503 on epoch=249
03/02/2022 02:42:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
03/02/2022 02:43:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=256
03/02/2022 02:43:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
03/02/2022 02:43:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=263
03/02/2022 02:43:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
03/02/2022 02:43:09 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.2714285714285714 on epoch=266
03/02/2022 02:43:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
03/02/2022 02:43:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=273
03/02/2022 02:43:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=276
03/02/2022 02:43:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=279
03/02/2022 02:43:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
03/02/2022 02:43:21 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.3238948626045401 on epoch=283
03/02/2022 02:43:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
03/02/2022 02:43:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=289
03/02/2022 02:43:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
03/02/2022 02:43:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
03/02/2022 02:43:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=299
03/02/2022 02:43:34 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.2106081081081081 on epoch=299
03/02/2022 02:43:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
03/02/2022 02:43:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
03/02/2022 02:43:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=309
03/02/2022 02:43:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=313
03/02/2022 02:43:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
03/02/2022 02:43:46 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.2232596232596232 on epoch=316
03/02/2022 02:43:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
03/02/2022 02:43:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
03/02/2022 02:43:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=326
03/02/2022 02:43:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=329
03/02/2022 02:43:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=333
03/02/2022 02:43:59 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.25453878598936364 on epoch=333
03/02/2022 02:44:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
03/02/2022 02:44:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
03/02/2022 02:44:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=343
03/02/2022 02:44:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
03/02/2022 02:44:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
03/02/2022 02:44:11 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.20122850122850125 on epoch=349
03/02/2022 02:44:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
03/02/2022 02:44:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
03/02/2022 02:44:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
03/02/2022 02:44:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=363
03/02/2022 02:44:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
03/02/2022 02:44:24 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.19151515151515153 on epoch=366
03/02/2022 02:44:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
03/02/2022 02:44:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
03/02/2022 02:44:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
03/02/2022 02:44:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
03/02/2022 02:44:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
03/02/2022 02:44:37 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.24184466724789305 on epoch=383
03/02/2022 02:44:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=386
03/02/2022 02:44:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=389
03/02/2022 02:44:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
03/02/2022 02:44:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
03/02/2022 02:44:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
03/02/2022 02:44:49 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.19515151515151516 on epoch=399
03/02/2022 02:44:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 02:44:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
03/02/2022 02:44:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
03/02/2022 02:44:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
03/02/2022 02:45:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
03/02/2022 02:45:02 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.1734234234234234 on epoch=416
03/02/2022 02:45:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 02:45:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 02:45:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
03/02/2022 02:45:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
03/02/2022 02:45:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
03/02/2022 02:45:14 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.18629776021080366 on epoch=433
03/02/2022 02:45:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
03/02/2022 02:45:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 02:45:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=443
03/02/2022 02:45:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
03/02/2022 02:45:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 02:45:27 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.13323330832708177 on epoch=449
03/02/2022 02:45:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/02/2022 02:45:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
03/02/2022 02:45:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
03/02/2022 02:45:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 02:45:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
03/02/2022 02:45:39 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.16638001638001637 on epoch=466
03/02/2022 02:45:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/02/2022 02:45:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
03/02/2022 02:45:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 02:45:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
03/02/2022 02:45:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
03/02/2022 02:45:52 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.17820012995451592 on epoch=483
03/02/2022 02:45:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
03/02/2022 02:45:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 02:45:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
03/02/2022 02:46:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=496
03/02/2022 02:46:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/02/2022 02:46:04 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.172669220945083 on epoch=499
03/02/2022 02:46:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
03/02/2022 02:46:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
03/02/2022 02:46:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
03/02/2022 02:46:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 02:46:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
03/02/2022 02:46:17 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.16461988304093567 on epoch=516
03/02/2022 02:46:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
03/02/2022 02:46:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/02/2022 02:46:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
03/02/2022 02:46:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/02/2022 02:46:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 02:46:29 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.18989234449760767 on epoch=533
03/02/2022 02:46:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 02:46:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 02:46:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 02:46:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
03/02/2022 02:46:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/02/2022 02:46:42 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2505411255411255 on epoch=549
03/02/2022 02:46:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
03/02/2022 02:46:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 02:46:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 02:46:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 02:46:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 02:46:54 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.24865392381208776 on epoch=566
03/02/2022 02:46:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 02:46:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 02:47:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 02:47:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=579
03/02/2022 02:47:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 02:47:07 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.2850094876660342 on epoch=583
03/02/2022 02:47:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=586
03/02/2022 02:47:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 02:47:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
03/02/2022 02:47:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/02/2022 02:47:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 02:47:19 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3060171535781292 on epoch=599
03/02/2022 02:47:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 02:47:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 02:47:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
03/02/2022 02:47:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 02:47:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=616
03/02/2022 02:47:31 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.28599722717369774 on epoch=616
03/02/2022 02:47:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 02:47:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
03/02/2022 02:47:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 02:47:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
03/02/2022 02:47:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 02:47:43 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.3316993464052288 on epoch=633
03/02/2022 02:47:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/02/2022 02:47:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 02:47:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 02:47:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 02:47:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 02:47:56 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.22958944281524926 on epoch=649
03/02/2022 02:47:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
03/02/2022 02:48:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 02:48:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/02/2022 02:48:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 02:48:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 02:48:08 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.28383152173913045 on epoch=666
03/02/2022 02:48:10 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
03/02/2022 02:48:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 02:48:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 02:48:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 02:48:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=683
03/02/2022 02:48:20 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.35508589791125944 on epoch=683
03/02/2022 02:48:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
03/02/2022 02:48:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
03/02/2022 02:48:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 02:48:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 02:48:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 02:48:32 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.20034561415635582 on epoch=699
03/02/2022 02:48:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 02:48:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
03/02/2022 02:48:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 02:48:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
03/02/2022 02:48:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 02:48:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.25156325156325154 on epoch=716
03/02/2022 02:48:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 02:48:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
03/02/2022 02:48:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
03/02/2022 02:48:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 02:48:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 02:48:57 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.39829059829059826 on epoch=733
03/02/2022 02:48:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 02:49:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/02/2022 02:49:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
03/02/2022 02:49:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/02/2022 02:49:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 02:49:09 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.18445344129554653 on epoch=749
03/02/2022 02:49:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
03/02/2022 02:49:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 02:49:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 02:49:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/02/2022 02:49:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
03/02/2022 02:49:21 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.17346542346542346 on epoch=766
03/02/2022 02:49:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/02/2022 02:49:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 02:49:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 02:49:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 02:49:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
03/02/2022 02:49:34 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.14364488558036945 on epoch=783
03/02/2022 02:49:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 02:49:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 02:49:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=793
03/02/2022 02:49:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 02:49:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 02:49:46 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.20036486486486488 on epoch=799
03/02/2022 02:49:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 02:49:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 02:49:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 02:49:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 02:49:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
03/02/2022 02:49:58 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.37659127425534544 on epoch=816
03/02/2022 02:50:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
03/02/2022 02:50:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 02:50:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 02:50:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
03/02/2022 02:50:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 02:50:11 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.193015873015873 on epoch=833
03/02/2022 02:50:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 02:50:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 02:50:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 02:50:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 02:50:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 02:50:23 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.17428421970357455 on epoch=849
03/02/2022 02:50:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 02:50:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 02:50:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 02:50:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 02:50:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 02:50:35 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.1603174603174603 on epoch=866
03/02/2022 02:50:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
03/02/2022 02:50:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 02:50:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 02:50:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 02:50:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 02:50:48 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.1973401245048104 on epoch=883
03/02/2022 02:50:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 02:50:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 02:50:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 02:50:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
03/02/2022 02:50:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 02:51:00 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1774193548387097 on epoch=899
03/02/2022 02:51:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 02:51:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 02:51:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 02:51:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/02/2022 02:51:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=916
03/02/2022 02:51:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.23938461538461536 on epoch=916
03/02/2022 02:51:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=919
03/02/2022 02:51:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 02:51:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=926
03/02/2022 02:51:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 02:51:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 02:51:25 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.20276598437517976 on epoch=933
03/02/2022 02:51:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 02:51:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 02:51:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 02:51:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 02:51:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 02:51:37 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.251255980861244 on epoch=949
03/02/2022 02:51:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 02:51:42 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 02:51:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 02:51:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 02:51:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 02:51:50 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.22338461538461543 on epoch=966
03/02/2022 02:51:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 02:51:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 02:51:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 02:51:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 02:52:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 02:52:02 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.31089474567735437 on epoch=983
03/02/2022 02:52:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
03/02/2022 02:52:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 02:52:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 02:52:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 02:52:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=999
03/02/2022 02:52:14 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:52:14 - INFO - __main__ - Printing 3 examples
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 02:52:14 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.21075862068965517 on epoch=999
03/02/2022 02:52:14 - INFO - __main__ - save last model!
03/02/2022 02:52:14 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:52:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 02:52:14 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 02:52:14 - INFO - __main__ - Printing 3 examples
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 02:52:14 - INFO - __main__ - ['normal']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 02:52:14 - INFO - __main__ - ['normal']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 02:52:14 - INFO - __main__ - ['normal']
03/02/2022 02:52:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:52:14 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:52:14 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:52:14 - INFO - __main__ - Printing 3 examples
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:52:14 - INFO - __main__ - ['offensive']
03/02/2022 02:52:14 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:52:14 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:52:14 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:52:15 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:52:17 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 02:52:28 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:52:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:52:29 - INFO - __main__ - Starting training!
03/02/2022 02:53:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_21_0.3_8_predictions.txt
03/02/2022 02:53:05 - INFO - __main__ - Classification-F1 on test data: 0.1345
03/02/2022 02:53:05 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.3, bsz=8, dev_performance=0.4916682386971012, test_performance=0.13453513322443272
03/02/2022 02:53:05 - INFO - __main__ - Running ... prefix=hatexplain_16_21, lr=0.2, bsz=8 ...
03/02/2022 02:53:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:53:06 - INFO - __main__ - Printing 3 examples
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] <user> kill bitch i had to google his ass 😭 😭
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] please re read what you just posted how how can that not seem totally retarded to you brown skinned true british people ffs if they had that capability why are not their own countries just like britain
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] boi i thought we was fenna die like a hoe
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 02:53:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:53:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 02:53:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 02:53:06 - INFO - __main__ - Printing 3 examples
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] that yeboah chick does the most to prove shes street and ghetto like relax sisterin dear
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] <user> it ’ genuinely an annoying boring game now every game is the same and the amount of retards make me wanna kill myself
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ -  [hatexplain] inaki williams you fucking nigger
03/02/2022 02:53:06 - INFO - __main__ - ['offensive']
03/02/2022 02:53:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 02:53:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 02:53:06 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 02:53:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 02:53:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 02:53:19 - INFO - __main__ - Starting training!
03/02/2022 02:53:23 - INFO - __main__ - Step 10 Global step 10 Train loss 3.54 on epoch=3
03/02/2022 02:53:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=6
03/02/2022 02:53:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=9
03/02/2022 02:53:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=13
03/02/2022 02:53:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.76 on epoch=16
03/02/2022 02:53:34 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.26508667983322365 on epoch=16
03/02/2022 02:53:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.26508667983322365 on epoch=16, global_step=50
03/02/2022 02:53:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=19
03/02/2022 02:53:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
03/02/2022 02:53:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=26
03/02/2022 02:53:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.63 on epoch=29
03/02/2022 02:53:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=33
03/02/2022 02:53:46 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.2085278555866791 on epoch=33
03/02/2022 02:53:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
03/02/2022 02:53:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
03/02/2022 02:53:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=43
03/02/2022 02:53:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/02/2022 02:53:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
03/02/2022 02:53:58 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 02:54:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
03/02/2022 02:54:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
03/02/2022 02:54:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
03/02/2022 02:54:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=63
03/02/2022 02:54:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=66
03/02/2022 02:54:10 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=66
03/02/2022 02:54:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=69
03/02/2022 02:54:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
03/02/2022 02:54:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=76
03/02/2022 02:54:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 02:54:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
03/02/2022 02:54:22 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.1693121693121693 on epoch=83
03/02/2022 02:54:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=86
03/02/2022 02:54:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
03/02/2022 02:54:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=93
03/02/2022 02:54:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
03/02/2022 02:54:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
03/02/2022 02:54:34 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=99
03/02/2022 02:54:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
03/02/2022 02:54:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/02/2022 02:54:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 02:54:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 02:54:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
03/02/2022 02:54:46 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.1983273596176822 on epoch=116
03/02/2022 02:54:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
03/02/2022 02:54:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=123
03/02/2022 02:54:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
03/02/2022 02:54:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
03/02/2022 02:54:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
03/02/2022 02:54:58 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.24046259698433614 on epoch=133
03/02/2022 02:55:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
03/02/2022 02:55:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
03/02/2022 02:55:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/02/2022 02:55:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=146
03/02/2022 02:55:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=149
03/02/2022 02:55:10 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.15873015873015875 on epoch=149
03/02/2022 02:55:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=153
03/02/2022 02:55:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=156
03/02/2022 02:55:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=159
03/02/2022 02:55:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
03/02/2022 02:55:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=166
03/02/2022 02:55:23 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.2767676767676768 on epoch=166
03/02/2022 02:55:23 - INFO - __main__ - Saving model with best Classification-F1: 0.26508667983322365 -> 0.2767676767676768 on epoch=166, global_step=500
03/02/2022 02:55:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=169
03/02/2022 02:55:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=173
03/02/2022 02:55:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
03/02/2022 02:55:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=179
03/02/2022 02:55:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=183
03/02/2022 02:55:35 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.21944167497507475 on epoch=183
03/02/2022 02:55:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=186
03/02/2022 02:55:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
03/02/2022 02:55:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=193
03/02/2022 02:55:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=196
03/02/2022 02:55:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=199
03/02/2022 02:55:47 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.36294816294816296 on epoch=199
03/02/2022 02:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2767676767676768 -> 0.36294816294816296 on epoch=199, global_step=600
03/02/2022 02:55:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=203
03/02/2022 02:55:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=206
03/02/2022 02:55:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
03/02/2022 02:55:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=213
03/02/2022 02:55:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=216
03/02/2022 02:55:59 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.4021454536210885 on epoch=216
03/02/2022 02:55:59 - INFO - __main__ - Saving model with best Classification-F1: 0.36294816294816296 -> 0.4021454536210885 on epoch=216, global_step=650
03/02/2022 02:56:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=219
03/02/2022 02:56:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
03/02/2022 02:56:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
03/02/2022 02:56:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=229
03/02/2022 02:56:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=233
03/02/2022 02:56:12 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.42916475397293813 on epoch=233
03/02/2022 02:56:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4021454536210885 -> 0.42916475397293813 on epoch=233, global_step=700
03/02/2022 02:56:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
03/02/2022 02:56:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=239
03/02/2022 02:56:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=243
03/02/2022 02:56:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=246
03/02/2022 02:56:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
03/02/2022 02:56:24 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.4014396456256921 on epoch=249
03/02/2022 02:56:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=253
03/02/2022 02:56:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=256
03/02/2022 02:56:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=259
03/02/2022 02:56:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=263
03/02/2022 02:56:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=266
03/02/2022 02:56:36 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.4515328725855041 on epoch=266
03/02/2022 02:56:36 - INFO - __main__ - Saving model with best Classification-F1: 0.42916475397293813 -> 0.4515328725855041 on epoch=266, global_step=800
03/02/2022 02:56:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
03/02/2022 02:56:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=273
03/02/2022 02:56:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=276
03/02/2022 02:56:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=279
03/02/2022 02:56:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
03/02/2022 02:56:48 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.3611111111111111 on epoch=283
03/02/2022 02:56:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=286
03/02/2022 02:56:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=289
03/02/2022 02:56:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=293
03/02/2022 02:56:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=296
03/02/2022 02:56:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=299
03/02/2022 02:57:01 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.34515757994018864 on epoch=299
03/02/2022 02:57:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=303
03/02/2022 02:57:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=306
03/02/2022 02:57:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=309
03/02/2022 02:57:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=313
03/02/2022 02:57:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=316
03/02/2022 02:57:13 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.39494949494949494 on epoch=316
03/02/2022 02:57:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=319
03/02/2022 02:57:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=323
03/02/2022 02:57:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=326
03/02/2022 02:57:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=329
03/02/2022 02:57:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=333
03/02/2022 02:57:25 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3083725476823751 on epoch=333
03/02/2022 02:57:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=336
03/02/2022 02:57:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
03/02/2022 02:57:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=343
03/02/2022 02:57:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=346
03/02/2022 02:57:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=349
03/02/2022 02:57:37 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.30808080808080807 on epoch=349
03/02/2022 02:57:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=353
03/02/2022 02:57:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=356
03/02/2022 02:57:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
03/02/2022 02:57:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=363
03/02/2022 02:57:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=366
03/02/2022 02:57:50 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.4050659855257556 on epoch=366
03/02/2022 02:57:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
03/02/2022 02:57:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=373
03/02/2022 02:57:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=376
03/02/2022 02:57:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=379
03/02/2022 02:58:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
03/02/2022 02:58:02 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.3498677248677248 on epoch=383
03/02/2022 02:58:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=386
03/02/2022 02:58:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
03/02/2022 02:58:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=393
03/02/2022 02:58:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=396
03/02/2022 02:58:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
03/02/2022 02:58:14 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.45735617039964865 on epoch=399
03/02/2022 02:58:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4515328725855041 -> 0.45735617039964865 on epoch=399, global_step=1200
03/02/2022 02:58:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=403
03/02/2022 02:58:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
03/02/2022 02:58:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=409
03/02/2022 02:58:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
03/02/2022 02:58:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=416
03/02/2022 02:58:26 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.2937888198757764 on epoch=416
03/02/2022 02:58:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
03/02/2022 02:58:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
03/02/2022 02:58:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
03/02/2022 02:58:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
03/02/2022 02:58:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=433
03/02/2022 02:58:39 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.2672537975763782 on epoch=433
03/02/2022 02:58:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=436
03/02/2022 02:58:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=439
03/02/2022 02:58:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=443
03/02/2022 02:58:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
03/02/2022 02:58:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
03/02/2022 02:58:51 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.31107954545454547 on epoch=449
03/02/2022 02:58:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=453
03/02/2022 02:58:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
03/02/2022 02:58:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
03/02/2022 02:59:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
03/02/2022 02:59:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=466
03/02/2022 02:59:03 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.24977477477477478 on epoch=466
03/02/2022 02:59:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/02/2022 02:59:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
03/02/2022 02:59:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
03/02/2022 02:59:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
03/02/2022 02:59:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
03/02/2022 02:59:16 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.3686836741865205 on epoch=483
03/02/2022 02:59:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
03/02/2022 02:59:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 02:59:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=493
03/02/2022 02:59:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=496
03/02/2022 02:59:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/02/2022 02:59:28 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.22564935064935066 on epoch=499
03/02/2022 02:59:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 02:59:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 02:59:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
03/02/2022 02:59:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
03/02/2022 02:59:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
03/02/2022 02:59:40 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.15778122843340237 on epoch=516
03/02/2022 02:59:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=519
03/02/2022 02:59:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
03/02/2022 02:59:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
03/02/2022 02:59:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/02/2022 02:59:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
03/02/2022 02:59:53 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.13053460514640639 on epoch=533
03/02/2022 02:59:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=536
03/02/2022 02:59:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=539
03/02/2022 03:00:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
03/02/2022 03:00:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
03/02/2022 03:00:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
03/02/2022 03:00:05 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.28434343434343434 on epoch=549
03/02/2022 03:00:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
03/02/2022 03:00:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
03/02/2022 03:00:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=559
03/02/2022 03:00:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
03/02/2022 03:00:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
03/02/2022 03:00:18 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.26807563959955505 on epoch=566
03/02/2022 03:00:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
03/02/2022 03:00:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/02/2022 03:00:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
03/02/2022 03:00:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
03/02/2022 03:00:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
03/02/2022 03:00:30 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.15880198233139411 on epoch=583
03/02/2022 03:00:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
03/02/2022 03:00:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
03/02/2022 03:00:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
03/02/2022 03:00:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/02/2022 03:00:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
03/02/2022 03:00:42 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.31258615936035294 on epoch=599
03/02/2022 03:00:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/02/2022 03:00:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 03:00:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
03/02/2022 03:00:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 03:00:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
03/02/2022 03:00:55 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.19756819992035046 on epoch=616
03/02/2022 03:00:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
03/02/2022 03:00:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
03/02/2022 03:01:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 03:01:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 03:01:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
03/02/2022 03:01:07 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.23693233082706772 on epoch=633
03/02/2022 03:01:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
03/02/2022 03:01:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
03/02/2022 03:01:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=643
03/02/2022 03:01:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
03/02/2022 03:01:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/02/2022 03:01:20 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.23688888888888887 on epoch=649
03/02/2022 03:01:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
03/02/2022 03:01:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=656
03/02/2022 03:01:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
03/02/2022 03:01:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
03/02/2022 03:01:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 03:01:32 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.21060962566844918 on epoch=666
03/02/2022 03:01:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/02/2022 03:01:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
03/02/2022 03:01:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/02/2022 03:01:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
03/02/2022 03:01:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
03/02/2022 03:01:45 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.18551587301587302 on epoch=683
03/02/2022 03:01:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/02/2022 03:01:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 03:01:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 03:01:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 03:01:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 03:01:57 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.24948483741322613 on epoch=699
03/02/2022 03:01:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 03:02:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
03/02/2022 03:02:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 03:02:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
03/02/2022 03:02:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
03/02/2022 03:02:09 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.2517460317460317 on epoch=716
03/02/2022 03:02:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 03:02:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 03:02:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 03:02:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
03/02/2022 03:02:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=733
03/02/2022 03:02:22 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.2683982683982684 on epoch=733
03/02/2022 03:02:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=736
03/02/2022 03:02:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 03:02:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
03/02/2022 03:02:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=746
03/02/2022 03:02:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
03/02/2022 03:02:34 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.19706368899917284 on epoch=749
03/02/2022 03:02:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
03/02/2022 03:02:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 03:02:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
03/02/2022 03:02:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 03:02:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 03:02:47 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.24808120133481645 on epoch=766
03/02/2022 03:02:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=769
03/02/2022 03:02:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
03/02/2022 03:02:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 03:02:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 03:02:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=783
03/02/2022 03:02:59 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.17105508870214753 on epoch=783
03/02/2022 03:03:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
03/02/2022 03:03:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
03/02/2022 03:03:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
03/02/2022 03:03:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 03:03:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 03:03:11 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.19285714285714284 on epoch=799
03/02/2022 03:03:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=803
03/02/2022 03:03:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 03:03:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 03:03:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 03:03:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 03:03:24 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.20447957839262187 on epoch=816
03/02/2022 03:03:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 03:03:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
03/02/2022 03:03:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
03/02/2022 03:03:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:03:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/02/2022 03:03:36 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.173890608875129 on epoch=833
03/02/2022 03:03:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
03/02/2022 03:03:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 03:03:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 03:03:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=846
03/02/2022 03:03:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
03/02/2022 03:03:48 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3035369328052255 on epoch=849
03/02/2022 03:03:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 03:03:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 03:03:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=859
03/02/2022 03:03:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=863
03/02/2022 03:04:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 03:04:01 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.2679802955665025 on epoch=866
03/02/2022 03:04:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/02/2022 03:04:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
03/02/2022 03:04:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 03:04:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 03:04:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:04:14 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2648109243697479 on epoch=883
03/02/2022 03:04:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 03:04:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 03:04:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 03:04:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 03:04:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:04:26 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.27605727605727604 on epoch=899
03/02/2022 03:04:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 03:04:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 03:04:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 03:04:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 03:04:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 03:04:39 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2626045400238949 on epoch=916
03/02/2022 03:04:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 03:04:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/02/2022 03:04:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:04:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
03/02/2022 03:04:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
03/02/2022 03:04:51 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2846320346320346 on epoch=933
03/02/2022 03:04:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
03/02/2022 03:04:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=939
03/02/2022 03:04:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/02/2022 03:05:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 03:05:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:05:04 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.24391891891891895 on epoch=949
03/02/2022 03:05:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 03:05:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/02/2022 03:05:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 03:05:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
03/02/2022 03:05:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 03:05:16 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.2708370774866939 on epoch=966
03/02/2022 03:05:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=969
03/02/2022 03:05:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
03/02/2022 03:05:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 03:05:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:05:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
03/02/2022 03:05:28 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.2178461771729654 on epoch=983
03/02/2022 03:05:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
03/02/2022 03:05:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:05:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/02/2022 03:05:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:05:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
03/02/2022 03:05:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.3385463468583674 on epoch=999
03/02/2022 03:05:40 - INFO - __main__ - save last model!
03/02/2022 03:05:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:05:40 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:05:40 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:05:40 - INFO - __main__ - Printing 3 examples
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:05:40 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:05:40 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:05:40 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:05:40 - INFO - __main__ - Printing 3 examples
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:05:40 - INFO - __main__ - ['normal']
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:05:40 - INFO - __main__ - ['normal']
03/02/2022 03:05:40 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:05:40 - INFO - __main__ - ['normal']
03/02/2022 03:05:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:05:40 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:05:41 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:05:41 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:05:41 - INFO - __main__ - Printing 3 examples
03/02/2022 03:05:41 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:05:41 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:41 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:05:41 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:41 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:05:41 - INFO - __main__ - ['hatespeech']
03/02/2022 03:05:41 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:05:41 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:05:41 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:05:41 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:05:43 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:05:53 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:05:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:05:54 - INFO - __main__ - Starting training!
03/02/2022 03:06:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_21_0.2_8_predictions.txt
03/02/2022 03:06:29 - INFO - __main__ - Classification-F1 on test data: 0.1512
03/02/2022 03:06:29 - INFO - __main__ - prefix=hatexplain_16_21, lr=0.2, bsz=8, dev_performance=0.45735617039964865, test_performance=0.15115457199359636
03/02/2022 03:06:29 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.5, bsz=8 ...
03/02/2022 03:06:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:06:30 - INFO - __main__ - Printing 3 examples
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:06:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:06:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:06:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:06:30 - INFO - __main__ - Printing 3 examples
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:06:30 - INFO - __main__ - ['hatespeech']
03/02/2022 03:06:30 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:06:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:06:30 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:06:42 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:06:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:06:43 - INFO - __main__ - Starting training!
03/02/2022 03:06:46 - INFO - __main__ - Step 10 Global step 10 Train loss 2.92 on epoch=3
03/02/2022 03:06:48 - INFO - __main__ - Step 20 Global step 20 Train loss 1.01 on epoch=6
03/02/2022 03:06:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.70 on epoch=9
03/02/2022 03:06:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=13
03/02/2022 03:06:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=16
03/02/2022 03:06:56 - INFO - __main__ - Global step 50 Train loss 1.18 Classification-F1 0.16129032258064516 on epoch=16
03/02/2022 03:06:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16129032258064516 on epoch=16, global_step=50
03/02/2022 03:06:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=19
03/02/2022 03:07:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
03/02/2022 03:07:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
03/02/2022 03:07:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
03/02/2022 03:07:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
03/02/2022 03:07:08 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.1873873873873874 on epoch=33
03/02/2022 03:07:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16129032258064516 -> 0.1873873873873874 on epoch=33, global_step=100
03/02/2022 03:07:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
03/02/2022 03:07:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=39
03/02/2022 03:07:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
03/02/2022 03:07:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
03/02/2022 03:07:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/02/2022 03:07:21 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.19841269841269837 on epoch=49
03/02/2022 03:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1873873873873874 -> 0.19841269841269837 on epoch=49, global_step=150
03/02/2022 03:07:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
03/02/2022 03:07:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=56
03/02/2022 03:07:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
03/02/2022 03:07:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
03/02/2022 03:07:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=66
03/02/2022 03:07:33 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.28484848484848485 on epoch=66
03/02/2022 03:07:33 - INFO - __main__ - Saving model with best Classification-F1: 0.19841269841269837 -> 0.28484848484848485 on epoch=66, global_step=200
03/02/2022 03:07:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
03/02/2022 03:07:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=73
03/02/2022 03:07:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
03/02/2022 03:07:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
03/02/2022 03:07:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=83
03/02/2022 03:07:45 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.3778467908902692 on epoch=83
03/02/2022 03:07:45 - INFO - __main__ - Saving model with best Classification-F1: 0.28484848484848485 -> 0.3778467908902692 on epoch=83, global_step=250
03/02/2022 03:07:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=86
03/02/2022 03:07:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=89
03/02/2022 03:07:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=93
03/02/2022 03:07:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=96
03/02/2022 03:07:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=99
03/02/2022 03:07:57 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.2796847190439868 on epoch=99
03/02/2022 03:08:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=103
03/02/2022 03:08:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=106
03/02/2022 03:08:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=109
03/02/2022 03:08:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=113
03/02/2022 03:08:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=116
03/02/2022 03:08:10 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.38383838383838387 on epoch=116
03/02/2022 03:08:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3778467908902692 -> 0.38383838383838387 on epoch=116, global_step=350
03/02/2022 03:08:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=119
03/02/2022 03:08:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=123
03/02/2022 03:08:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.11 on epoch=126
03/02/2022 03:08:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=129
03/02/2022 03:08:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=133
03/02/2022 03:08:22 - INFO - __main__ - Global step 400 Train loss 0.17 Classification-F1 0.3534142019899141 on epoch=133
03/02/2022 03:08:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=136
03/02/2022 03:08:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=139
03/02/2022 03:08:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=143
03/02/2022 03:08:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=146
03/02/2022 03:08:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=149
03/02/2022 03:08:35 - INFO - __main__ - Global step 450 Train loss 0.17 Classification-F1 0.3510015680558316 on epoch=149
03/02/2022 03:08:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=153
03/02/2022 03:08:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=156
03/02/2022 03:08:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=159
03/02/2022 03:08:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=163
03/02/2022 03:08:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.13 on epoch=166
03/02/2022 03:08:47 - INFO - __main__ - Global step 500 Train loss 0.16 Classification-F1 0.4516594516594517 on epoch=166
03/02/2022 03:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.38383838383838387 -> 0.4516594516594517 on epoch=166, global_step=500
03/02/2022 03:08:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=169
03/02/2022 03:08:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.13 on epoch=173
03/02/2022 03:08:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=176
03/02/2022 03:08:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=179
03/02/2022 03:08:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=183
03/02/2022 03:09:00 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.4788614307572994 on epoch=183
03/02/2022 03:09:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4516594516594517 -> 0.4788614307572994 on epoch=183, global_step=550
03/02/2022 03:09:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=186
03/02/2022 03:09:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=189
03/02/2022 03:09:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=193
03/02/2022 03:09:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=196
03/02/2022 03:09:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=199
03/02/2022 03:09:12 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.2547283176593521 on epoch=199
03/02/2022 03:09:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=203
03/02/2022 03:09:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=206
03/02/2022 03:09:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=209
03/02/2022 03:09:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=213
03/02/2022 03:09:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=216
03/02/2022 03:09:25 - INFO - __main__ - Global step 650 Train loss 0.08 Classification-F1 0.41621294615849974 on epoch=216
03/02/2022 03:09:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=219
03/02/2022 03:09:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=223
03/02/2022 03:09:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=226
03/02/2022 03:09:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=229
03/02/2022 03:09:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=233
03/02/2022 03:09:37 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.26292735042735044 on epoch=233
03/02/2022 03:09:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=236
03/02/2022 03:09:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=239
03/02/2022 03:09:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=243
03/02/2022 03:09:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=246
03/02/2022 03:09:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=249
03/02/2022 03:09:50 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.2527195945945946 on epoch=249
03/02/2022 03:09:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=253
03/02/2022 03:09:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=256
03/02/2022 03:09:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
03/02/2022 03:09:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=263
03/02/2022 03:10:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
03/02/2022 03:10:02 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.27410101010101007 on epoch=266
03/02/2022 03:10:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=269
03/02/2022 03:10:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=273
03/02/2022 03:10:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
03/02/2022 03:10:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
03/02/2022 03:10:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
03/02/2022 03:10:15 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.28293545534924847 on epoch=283
03/02/2022 03:10:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
03/02/2022 03:10:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=289
03/02/2022 03:10:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
03/02/2022 03:10:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
03/02/2022 03:10:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
03/02/2022 03:10:27 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.23513957307060754 on epoch=299
03/02/2022 03:10:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
03/02/2022 03:10:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
03/02/2022 03:10:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=309
03/02/2022 03:10:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=313
03/02/2022 03:10:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
03/02/2022 03:10:40 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.2136514719848053 on epoch=316
03/02/2022 03:10:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
03/02/2022 03:10:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
03/02/2022 03:10:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=326
03/02/2022 03:10:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=329
03/02/2022 03:10:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=333
03/02/2022 03:10:52 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.24325274725274726 on epoch=333
03/02/2022 03:10:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
03/02/2022 03:10:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
03/02/2022 03:10:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
03/02/2022 03:11:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
03/02/2022 03:11:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=349
03/02/2022 03:11:05 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.1698485845951284 on epoch=349
03/02/2022 03:11:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 03:11:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
03/02/2022 03:11:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
03/02/2022 03:11:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
03/02/2022 03:11:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 03:11:17 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.19231964059550266 on epoch=366
03/02/2022 03:11:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
03/02/2022 03:11:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
03/02/2022 03:11:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
03/02/2022 03:11:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
03/02/2022 03:11:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
03/02/2022 03:11:30 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.09420289855072464 on epoch=383
03/02/2022 03:11:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
03/02/2022 03:11:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
03/02/2022 03:11:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=393
03/02/2022 03:11:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 03:11:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
03/02/2022 03:11:42 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.27856598016781087 on epoch=399
03/02/2022 03:11:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 03:11:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
03/02/2022 03:11:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
03/02/2022 03:11:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
03/02/2022 03:11:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
03/02/2022 03:11:54 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.2241077441077441 on epoch=416
03/02/2022 03:11:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
03/02/2022 03:11:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 03:12:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
03/02/2022 03:12:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
03/02/2022 03:12:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 03:12:06 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.11830651898563616 on epoch=433
03/02/2022 03:12:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
03/02/2022 03:12:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 03:12:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
03/02/2022 03:12:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 03:12:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
03/02/2022 03:12:19 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.21904761904761902 on epoch=449
03/02/2022 03:12:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
03/02/2022 03:12:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
03/02/2022 03:12:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 03:12:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
03/02/2022 03:12:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
03/02/2022 03:12:31 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3132034632034632 on epoch=466
03/02/2022 03:12:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
03/02/2022 03:12:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 03:12:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
03/02/2022 03:12:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
03/02/2022 03:12:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 03:12:43 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.2145454545454545 on epoch=483
03/02/2022 03:12:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
03/02/2022 03:12:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
03/02/2022 03:12:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 03:12:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
03/02/2022 03:12:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
03/02/2022 03:12:56 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.32925472747497214 on epoch=499
03/02/2022 03:12:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 03:13:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
03/02/2022 03:13:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
03/02/2022 03:13:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 03:13:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
03/02/2022 03:13:08 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.2313545216771023 on epoch=516
03/02/2022 03:13:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=519
03/02/2022 03:13:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
03/02/2022 03:13:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
03/02/2022 03:13:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 03:13:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/02/2022 03:13:20 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.16613503455608716 on epoch=533
03/02/2022 03:13:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 03:13:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
03/02/2022 03:13:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 03:13:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 03:13:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
03/02/2022 03:13:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.25276848755109627 on epoch=549
03/02/2022 03:13:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
03/02/2022 03:13:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 03:13:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 03:13:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
03/02/2022 03:13:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
03/02/2022 03:13:45 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.11267353336318853 on epoch=566
03/02/2022 03:13:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 03:13:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 03:13:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 03:13:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/02/2022 03:13:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
03/02/2022 03:13:57 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.2963054187192118 on epoch=583
03/02/2022 03:14:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 03:14:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 03:14:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
03/02/2022 03:14:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 03:14:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 03:14:10 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.15689994216310008 on epoch=599
03/02/2022 03:14:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 03:14:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
03/02/2022 03:14:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 03:14:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 03:14:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 03:14:22 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.20643939393939392 on epoch=616
03/02/2022 03:14:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 03:14:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 03:14:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
03/02/2022 03:14:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 03:14:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 03:14:34 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.18000397535281254 on epoch=633
03/02/2022 03:14:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 03:14:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 03:14:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 03:14:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
03/02/2022 03:14:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 03:14:47 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.11544011544011544 on epoch=649
03/02/2022 03:14:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 03:14:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 03:14:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 03:14:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 03:14:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 03:14:59 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.13551736484067312 on epoch=666
03/02/2022 03:15:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 03:15:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 03:15:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 03:15:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 03:15:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 03:15:11 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.1763350501490919 on epoch=683
03/02/2022 03:15:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 03:15:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 03:15:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 03:15:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 03:15:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 03:15:23 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.13533834586466165 on epoch=699
03/02/2022 03:15:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 03:15:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 03:15:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 03:15:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/02/2022 03:15:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 03:15:36 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.1537063379168642 on epoch=716
03/02/2022 03:15:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 03:15:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 03:15:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
03/02/2022 03:15:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 03:15:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 03:15:48 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1326593137254902 on epoch=733
03/02/2022 03:15:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 03:15:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 03:15:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 03:15:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 03:15:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 03:16:01 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.09761904761904762 on epoch=749
03/02/2022 03:16:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 03:16:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 03:16:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 03:16:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 03:16:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 03:16:13 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.11999608303956129 on epoch=766
03/02/2022 03:16:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 03:16:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 03:16:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 03:16:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 03:16:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 03:16:25 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.12941176470588237 on epoch=783
03/02/2022 03:16:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 03:16:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 03:16:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 03:16:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 03:16:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 03:16:37 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.13658170914542728 on epoch=799
03/02/2022 03:16:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 03:16:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 03:16:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 03:16:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 03:16:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 03:16:50 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.15043443529200648 on epoch=816
03/02/2022 03:16:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 03:16:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 03:16:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 03:16:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:17:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 03:17:02 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.2098375054896794 on epoch=833
03/02/2022 03:17:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 03:17:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 03:17:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 03:17:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 03:17:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=849
03/02/2022 03:17:14 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.1902097902097902 on epoch=849
03/02/2022 03:17:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 03:17:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 03:17:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 03:17:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 03:17:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 03:17:27 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.16098379629629628 on epoch=866
03/02/2022 03:17:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 03:17:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 03:17:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 03:17:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 03:17:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:17:39 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.2279941110525818 on epoch=883
03/02/2022 03:17:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 03:17:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 03:17:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=893
03/02/2022 03:17:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 03:17:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:17:51 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.17642220556388472 on epoch=899
03/02/2022 03:17:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=903
03/02/2022 03:17:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 03:17:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
03/02/2022 03:18:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 03:18:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=916
03/02/2022 03:18:04 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.1584295334295334 on epoch=916
03/02/2022 03:18:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:18:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 03:18:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:18:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 03:18:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 03:18:16 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.22354196301564722 on epoch=933
03/02/2022 03:18:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
03/02/2022 03:18:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 03:18:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 03:18:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 03:18:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:18:29 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.19247419247419245 on epoch=949
03/02/2022 03:18:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
03/02/2022 03:18:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 03:18:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 03:18:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 03:18:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=966
03/02/2022 03:18:41 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.17415701415701415 on epoch=966
03/02/2022 03:18:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:18:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:18:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 03:18:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:18:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 03:18:53 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.1746031746031746 on epoch=983
03/02/2022 03:18:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 03:18:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:19:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 03:19:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:19:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 03:19:06 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.19749103942652327 on epoch=999
03/02/2022 03:19:06 - INFO - __main__ - save last model!
03/02/2022 03:19:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:19:06 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:19:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:19:06 - INFO - __main__ - ['normal']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:19:06 - INFO - __main__ - ['normal']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:19:06 - INFO - __main__ - ['normal']
03/02/2022 03:19:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:19:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:19:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:19:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:19:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:19:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:19:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:19:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:19:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:19:06 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:19:07 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:19:08 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:19:20 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:19:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:19:21 - INFO - __main__ - Starting training!
03/02/2022 03:19:54 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_42_0.5_8_predictions.txt
03/02/2022 03:19:54 - INFO - __main__ - Classification-F1 on test data: 0.0524
03/02/2022 03:19:55 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.5, bsz=8, dev_performance=0.4788614307572994, test_performance=0.05240503590073506
03/02/2022 03:19:55 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.4, bsz=8 ...
03/02/2022 03:19:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:19:56 - INFO - __main__ - Printing 3 examples
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:19:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:19:56 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:19:56 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:19:56 - INFO - __main__ - Printing 3 examples
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:19:56 - INFO - __main__ - ['hatespeech']
03/02/2022 03:19:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:19:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:19:56 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:20:08 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:20:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:20:09 - INFO - __main__ - Starting training!
03/02/2022 03:20:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.13 on epoch=3
03/02/2022 03:20:14 - INFO - __main__ - Step 20 Global step 20 Train loss 1.32 on epoch=6
03/02/2022 03:20:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.74 on epoch=9
03/02/2022 03:20:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.72 on epoch=13
03/02/2022 03:20:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=16
03/02/2022 03:20:21 - INFO - __main__ - Global step 50 Train loss 1.32 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 03:20:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 03:20:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=19
03/02/2022 03:20:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=23
03/02/2022 03:20:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
03/02/2022 03:20:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
03/02/2022 03:20:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
03/02/2022 03:20:33 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 03:20:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 03:20:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
03/02/2022 03:20:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
03/02/2022 03:20:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/02/2022 03:20:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
03/02/2022 03:20:45 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 03:20:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
03/02/2022 03:20:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
03/02/2022 03:20:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
03/02/2022 03:20:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=63
03/02/2022 03:20:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/02/2022 03:20:57 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.32249025341130605 on epoch=66
03/02/2022 03:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.32249025341130605 on epoch=66, global_step=200
03/02/2022 03:20:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=69
03/02/2022 03:21:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=73
03/02/2022 03:21:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
03/02/2022 03:21:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
03/02/2022 03:21:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
03/02/2022 03:21:09 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.26565255731922394 on epoch=83
03/02/2022 03:21:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
03/02/2022 03:21:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
03/02/2022 03:21:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
03/02/2022 03:21:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=96
03/02/2022 03:21:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
03/02/2022 03:21:21 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.20097146326654525 on epoch=99
03/02/2022 03:21:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
03/02/2022 03:21:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
03/02/2022 03:21:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=109
03/02/2022 03:21:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
03/02/2022 03:21:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
03/02/2022 03:21:33 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.35752688172043007 on epoch=116
03/02/2022 03:21:33 - INFO - __main__ - Saving model with best Classification-F1: 0.32249025341130605 -> 0.35752688172043007 on epoch=116, global_step=350
03/02/2022 03:21:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
03/02/2022 03:21:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
03/02/2022 03:21:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
03/02/2022 03:21:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=129
03/02/2022 03:21:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=133
03/02/2022 03:21:45 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.2456709956709957 on epoch=133
03/02/2022 03:21:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
03/02/2022 03:21:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=139
03/02/2022 03:21:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=143
03/02/2022 03:21:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=146
03/02/2022 03:21:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=149
03/02/2022 03:21:57 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.30456658463221215 on epoch=149
03/02/2022 03:21:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=153
03/02/2022 03:22:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
03/02/2022 03:22:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=159
03/02/2022 03:22:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
03/02/2022 03:22:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=166
03/02/2022 03:22:09 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.3347283577462605 on epoch=166
03/02/2022 03:22:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=169
03/02/2022 03:22:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=173
03/02/2022 03:22:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=176
03/02/2022 03:22:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=179
03/02/2022 03:22:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=183
03/02/2022 03:22:21 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3532135621897192 on epoch=183
03/02/2022 03:22:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=186
03/02/2022 03:22:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
03/02/2022 03:22:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=193
03/02/2022 03:22:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=196
03/02/2022 03:22:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=199
03/02/2022 03:22:33 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.3828282828282828 on epoch=199
03/02/2022 03:22:34 - INFO - __main__ - Saving model with best Classification-F1: 0.35752688172043007 -> 0.3828282828282828 on epoch=199, global_step=600
03/02/2022 03:22:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=203
03/02/2022 03:22:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
03/02/2022 03:22:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=209
03/02/2022 03:22:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
03/02/2022 03:22:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
03/02/2022 03:22:46 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.32119241192411924 on epoch=216
03/02/2022 03:22:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
03/02/2022 03:22:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=223
03/02/2022 03:22:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=226
03/02/2022 03:22:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
03/02/2022 03:22:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
03/02/2022 03:22:58 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.460519740129935 on epoch=233
03/02/2022 03:22:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3828282828282828 -> 0.460519740129935 on epoch=233, global_step=700
03/02/2022 03:23:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=236
03/02/2022 03:23:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
03/02/2022 03:23:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
03/02/2022 03:23:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
03/02/2022 03:23:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=249
03/02/2022 03:23:10 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.3597756410256411 on epoch=249
03/02/2022 03:23:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
03/02/2022 03:23:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=256
03/02/2022 03:23:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=259
03/02/2022 03:23:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=263
03/02/2022 03:23:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
03/02/2022 03:23:22 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.3045212765957447 on epoch=266
03/02/2022 03:23:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=269
03/02/2022 03:23:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=273
03/02/2022 03:23:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
03/02/2022 03:23:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=279
03/02/2022 03:23:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=283
03/02/2022 03:23:35 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.2512169752444352 on epoch=283
03/02/2022 03:23:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=286
03/02/2022 03:23:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=289
03/02/2022 03:23:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=293
03/02/2022 03:23:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
03/02/2022 03:23:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
03/02/2022 03:23:47 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.44199528142905287 on epoch=299
03/02/2022 03:23:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
03/02/2022 03:23:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
03/02/2022 03:23:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=309
03/02/2022 03:23:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
03/02/2022 03:23:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=316
03/02/2022 03:23:59 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.3581432081432081 on epoch=316
03/02/2022 03:24:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
03/02/2022 03:24:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
03/02/2022 03:24:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
03/02/2022 03:24:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
03/02/2022 03:24:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
03/02/2022 03:24:11 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.4222408026755853 on epoch=333
03/02/2022 03:24:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=336
03/02/2022 03:24:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
03/02/2022 03:24:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
03/02/2022 03:24:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
03/02/2022 03:24:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
03/02/2022 03:24:24 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.36307345904869126 on epoch=349
03/02/2022 03:24:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 03:24:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
03/02/2022 03:24:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
03/02/2022 03:24:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
03/02/2022 03:24:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
03/02/2022 03:24:36 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.2528495887191539 on epoch=366
03/02/2022 03:24:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
03/02/2022 03:24:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
03/02/2022 03:24:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
03/02/2022 03:24:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
03/02/2022 03:24:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
03/02/2022 03:24:48 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.29782581135902636 on epoch=383
03/02/2022 03:24:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=386
03/02/2022 03:24:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
03/02/2022 03:24:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
03/02/2022 03:24:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
03/02/2022 03:24:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
03/02/2022 03:25:00 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.29967948717948717 on epoch=399
03/02/2022 03:25:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
03/02/2022 03:25:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
03/02/2022 03:25:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 03:25:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
03/02/2022 03:25:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
03/02/2022 03:25:12 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.2502010184937014 on epoch=416
03/02/2022 03:25:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
03/02/2022 03:25:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
03/02/2022 03:25:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 03:25:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=429
03/02/2022 03:25:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
03/02/2022 03:25:24 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.2839583333333333 on epoch=433
03/02/2022 03:25:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
03/02/2022 03:25:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
03/02/2022 03:25:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 03:25:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 03:25:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
03/02/2022 03:25:35 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.33717948717948715 on epoch=449
03/02/2022 03:25:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 03:25:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
03/02/2022 03:25:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 03:25:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 03:25:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 03:25:47 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.2992307692307692 on epoch=466
03/02/2022 03:25:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
03/02/2022 03:25:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 03:25:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
03/02/2022 03:25:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
03/02/2022 03:25:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/02/2022 03:25:59 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.30572084020359885 on epoch=483
03/02/2022 03:26:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
03/02/2022 03:26:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
03/02/2022 03:26:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 03:26:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 03:26:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
03/02/2022 03:26:11 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.25260390332692895 on epoch=499
03/02/2022 03:26:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
03/02/2022 03:26:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
03/02/2022 03:31:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:31:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/02/2022 03:31:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:31:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 03:31:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 03:31:27 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.22077956989247313 on epoch=933
03/02/2022 03:31:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 03:31:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 03:31:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 03:31:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 03:31:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
03/02/2022 03:31:40 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.26328052190121154 on epoch=949
03/02/2022 03:31:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 03:31:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 03:31:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
03/02/2022 03:31:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 03:31:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 03:31:52 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.24847670250896056 on epoch=966
03/02/2022 03:31:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:31:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:31:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 03:32:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/02/2022 03:32:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 03:32:03 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2561484409310496 on epoch=983
03/02/2022 03:32:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
03/02/2022 03:32:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:32:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 03:32:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:32:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 03:32:15 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2584733893557423 on epoch=999
03/02/2022 03:32:15 - INFO - __main__ - save last model!
03/02/2022 03:32:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:32:15 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:32:15 - INFO - __main__ - Printing 3 examples
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:32:15 - INFO - __main__ - ['normal']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:32:15 - INFO - __main__ - ['normal']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:32:15 - INFO - __main__ - ['normal']
03/02/2022 03:32:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:32:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:32:15 - INFO - __main__ - Printing 3 examples
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:32:15 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:32:15 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:32:15 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:32:15 - INFO - __main__ - Printing 3 examples
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:15 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:32:15 - INFO - __main__ - ['hatespeech']
03/02/2022 03:32:16 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:32:16 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:32:16 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:32:16 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:32:18 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:32:28 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:32:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:32:29 - INFO - __main__ - Starting training!
03/02/2022 03:33:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_42_0.4_8_predictions.txt
03/02/2022 03:33:04 - INFO - __main__ - Classification-F1 on test data: 0.0784
03/02/2022 03:33:04 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.4, bsz=8, dev_performance=0.460519740129935, test_performance=0.07837394351559368
03/02/2022 03:33:04 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.3, bsz=8 ...
03/02/2022 03:33:05 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:33:05 - INFO - __main__ - Printing 3 examples
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:33:05 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:33:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:33:05 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:33:05 - INFO - __main__ - Printing 3 examples
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:33:05 - INFO - __main__ - ['hatespeech']
03/02/2022 03:33:05 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:33:05 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:33:05 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:33:17 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:33:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:33:18 - INFO - __main__ - Starting training!
03/02/2022 03:33:24 - INFO - __main__ - Step 10 Global step 10 Train loss 3.38 on epoch=3
03/02/2022 03:33:26 - INFO - __main__ - Step 20 Global step 20 Train loss 1.72 on epoch=6
03/02/2022 03:33:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=9
03/02/2022 03:33:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=13
03/02/2022 03:33:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.70 on epoch=16
03/02/2022 03:33:33 - INFO - __main__ - Global step 50 Train loss 1.51 Classification-F1 0.2642424242424242 on epoch=16
03/02/2022 03:33:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2642424242424242 on epoch=16, global_step=50
03/02/2022 03:33:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=19
03/02/2022 03:33:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.61 on epoch=23
03/02/2022 03:33:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=26
03/02/2022 03:33:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=29
03/02/2022 03:33:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=33
03/02/2022 03:33:44 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.2085278555866791 on epoch=33
03/02/2022 03:33:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 03:33:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 03:33:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=43
03/02/2022 03:33:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=46
03/02/2022 03:33:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
03/02/2022 03:33:56 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.21001779811848462 on epoch=49
03/02/2022 03:33:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
03/02/2022 03:34:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=56
03/02/2022 03:34:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=59
03/02/2022 03:34:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=63
03/02/2022 03:34:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
03/02/2022 03:34:08 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.29794871794871797 on epoch=66
03/02/2022 03:34:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2642424242424242 -> 0.29794871794871797 on epoch=66, global_step=200
03/02/2022 03:34:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
03/02/2022 03:34:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
03/02/2022 03:34:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
03/02/2022 03:34:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
03/02/2022 03:34:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
03/02/2022 03:34:20 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.2528260270195754 on epoch=83
03/02/2022 03:34:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
03/02/2022 03:34:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
03/02/2022 03:34:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
03/02/2022 03:34:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
03/02/2022 03:34:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
03/02/2022 03:34:31 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.3724026378001742 on epoch=99
03/02/2022 03:34:31 - INFO - __main__ - Saving model with best Classification-F1: 0.29794871794871797 -> 0.3724026378001742 on epoch=99, global_step=300
03/02/2022 03:34:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
03/02/2022 03:34:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/02/2022 03:34:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=109
03/02/2022 03:34:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 03:34:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
03/02/2022 03:34:43 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.32903225806451614 on epoch=116
03/02/2022 03:34:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
03/02/2022 03:34:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=123
03/02/2022 03:34:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
03/02/2022 03:34:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=129
03/02/2022 03:34:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
03/02/2022 03:34:55 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.43292370711725553 on epoch=133
03/02/2022 03:34:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3724026378001742 -> 0.43292370711725553 on epoch=133, global_step=400
03/02/2022 03:34:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
03/02/2022 03:34:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
03/02/2022 03:35:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=143
03/02/2022 03:35:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=146
03/02/2022 03:35:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=149
03/02/2022 03:35:06 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.2488716956802063 on epoch=149
03/02/2022 03:35:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=153
03/02/2022 03:35:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=156
03/02/2022 03:35:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=159
03/02/2022 03:35:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=163
03/02/2022 03:35:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=166
03/02/2022 03:35:18 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.31503527336860665 on epoch=166
03/02/2022 03:35:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=169
03/02/2022 03:35:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=173
03/02/2022 03:35:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
03/02/2022 03:35:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=179
03/02/2022 03:35:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
03/02/2022 03:35:30 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3570873570873571 on epoch=183
03/02/2022 03:35:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=186
03/02/2022 03:35:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
03/02/2022 03:35:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=193
03/02/2022 03:35:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=196
03/02/2022 03:35:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=199
03/02/2022 03:35:41 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.27918313570487485 on epoch=199
03/02/2022 03:35:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
03/02/2022 03:35:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=206
03/02/2022 03:35:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
03/02/2022 03:35:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=213
03/02/2022 03:35:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=216
03/02/2022 03:35:53 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.3704377104377105 on epoch=216
03/02/2022 03:35:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=219
03/02/2022 03:35:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
03/02/2022 03:36:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=226
03/02/2022 03:36:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=229
03/02/2022 03:36:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=233
03/02/2022 03:36:05 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.44292497625830957 on epoch=233
03/02/2022 03:36:05 - INFO - __main__ - Saving model with best Classification-F1: 0.43292370711725553 -> 0.44292497625830957 on epoch=233, global_step=700
03/02/2022 03:36:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
03/02/2022 03:36:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
03/02/2022 03:36:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=243
03/02/2022 03:36:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
03/02/2022 03:36:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=249
03/02/2022 03:36:17 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.45926487305797653 on epoch=249
03/02/2022 03:36:17 - INFO - __main__ - Saving model with best Classification-F1: 0.44292497625830957 -> 0.45926487305797653 on epoch=249, global_step=750
03/02/2022 03:36:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
03/02/2022 03:36:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=256
03/02/2022 03:36:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=259
03/02/2022 03:36:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=263
03/02/2022 03:36:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
03/02/2022 03:36:28 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.49202226345083483 on epoch=266
03/02/2022 03:36:28 - INFO - __main__ - Saving model with best Classification-F1: 0.45926487305797653 -> 0.49202226345083483 on epoch=266, global_step=800
03/02/2022 03:36:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=269
03/02/2022 03:36:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=273
03/02/2022 03:36:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=276
03/02/2022 03:36:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
03/02/2022 03:36:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=283
03/02/2022 03:36:40 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.38230165649520487 on epoch=283
03/02/2022 03:36:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
03/02/2022 03:36:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
03/02/2022 03:36:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
03/02/2022 03:36:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
03/02/2022 03:36:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
03/02/2022 03:36:52 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.38382478632478634 on epoch=299
03/02/2022 03:36:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=303
03/02/2022 03:36:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
03/02/2022 03:36:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
03/02/2022 03:37:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=313
03/02/2022 03:37:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=316
03/02/2022 03:37:04 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.47625692137320047 on epoch=316
03/02/2022 03:37:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=319
03/02/2022 03:37:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
03/02/2022 03:37:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
03/02/2022 03:37:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
03/02/2022 03:37:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
03/02/2022 03:37:15 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.4741647241647242 on epoch=333
03/02/2022 03:37:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
03/02/2022 03:37:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
03/02/2022 03:37:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=343
03/02/2022 03:37:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
03/02/2022 03:37:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
03/02/2022 03:37:27 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.4332996292551399 on epoch=349
03/02/2022 03:37:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=353
03/02/2022 03:37:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
03/02/2022 03:37:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
03/02/2022 03:37:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
03/02/2022 03:37:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
03/02/2022 03:37:39 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.30731663685152055 on epoch=366
03/02/2022 03:37:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=369
03/02/2022 03:37:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
03/02/2022 03:37:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
03/02/2022 03:37:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=379
03/02/2022 03:37:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
03/02/2022 03:37:51 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.34626138074413937 on epoch=383
03/02/2022 03:37:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
03/02/2022 03:37:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
03/02/2022 03:37:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=393
03/02/2022 03:37:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
03/02/2022 03:38:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
03/02/2022 03:38:02 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.33287696577243286 on epoch=399
03/02/2022 03:38:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
03/02/2022 03:38:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
03/02/2022 03:38:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 03:38:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
03/02/2022 03:38:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
03/02/2022 03:38:14 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.340507610668901 on epoch=416
03/02/2022 03:38:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
03/02/2022 03:38:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 03:38:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
03/02/2022 03:38:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 03:38:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 03:38:26 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.31612232995552964 on epoch=433
03/02/2022 03:38:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
03/02/2022 03:38:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 03:38:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 03:38:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 03:38:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
03/02/2022 03:38:38 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.28935792560580337 on epoch=449
03/02/2022 03:38:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
03/02/2022 03:38:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/02/2022 03:38:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
03/02/2022 03:38:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 03:38:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
03/02/2022 03:38:49 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.3250547345374931 on epoch=466
03/02/2022 03:38:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
03/02/2022 03:38:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 03:38:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
03/02/2022 03:38:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
03/02/2022 03:39:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
03/02/2022 03:39:01 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.354914139568871 on epoch=483
03/02/2022 03:39:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
03/02/2022 03:39:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
03/02/2022 03:39:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
03/02/2022 03:39:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 03:39:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 03:39:13 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.17344877344877344 on epoch=499
03/02/2022 03:39:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
03/02/2022 03:39:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
03/02/2022 03:39:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
03/02/2022 03:39:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
03/02/2022 03:39:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
03/02/2022 03:39:24 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2385332701122175 on epoch=516
03/02/2022 03:39:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/02/2022 03:39:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=523
03/02/2022 03:39:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
03/02/2022 03:39:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/02/2022 03:39:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
03/02/2022 03:39:36 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.16122004357298478 on epoch=533
03/02/2022 03:39:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
03/02/2022 03:39:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
03/02/2022 03:39:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 03:39:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
03/02/2022 03:39:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=549
03/02/2022 03:39:48 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.1508784425451092 on epoch=549
03/02/2022 03:39:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
03/02/2022 03:39:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
03/02/2022 03:39:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 03:39:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/02/2022 03:39:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 03:39:59 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2758703661929468 on epoch=566
03/02/2022 03:40:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/02/2022 03:40:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 03:40:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 03:40:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
03/02/2022 03:40:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/02/2022 03:40:11 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.3262987012987013 on epoch=583
03/02/2022 03:40:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
03/02/2022 03:40:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/02/2022 03:40:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 03:40:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 03:40:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 03:40:23 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.24560117302052786 on epoch=599
03/02/2022 03:40:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 03:40:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
03/02/2022 03:40:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 03:40:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 03:40:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/02/2022 03:40:35 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.185084202085004 on epoch=616
03/02/2022 03:40:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/02/2022 03:40:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
03/02/2022 03:40:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/02/2022 03:40:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 03:40:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 03:40:46 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.21857585139318889 on epoch=633
03/02/2022 03:40:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 03:40:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 03:40:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
03/02/2022 03:40:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 03:40:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
03/02/2022 03:40:58 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.2869318181818182 on epoch=649
03/02/2022 03:41:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 03:41:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 03:41:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/02/2022 03:41:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 03:41:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
03/02/2022 03:41:10 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.28374474789915966 on epoch=666
03/02/2022 03:41:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
03/02/2022 03:41:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
03/02/2022 03:41:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/02/2022 03:41:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 03:41:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/02/2022 03:41:22 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2808012093726379 on epoch=683
03/02/2022 03:41:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
03/02/2022 03:41:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/02/2022 03:41:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 03:41:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 03:41:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
03/02/2022 03:41:33 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.342512077294686 on epoch=699
03/02/2022 03:41:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/02/2022 03:41:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
03/02/2022 03:41:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 03:41:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
03/02/2022 03:41:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
03/02/2022 03:41:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.23285553470919326 on epoch=716
03/02/2022 03:41:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/02/2022 03:41:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 03:41:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 03:41:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/02/2022 03:41:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
03/02/2022 03:41:57 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.27060226595110315 on epoch=733
03/02/2022 03:41:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 03:42:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 03:42:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/02/2022 03:42:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 03:42:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 03:42:09 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2771672771672772 on epoch=749
03/02/2022 03:42:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 03:42:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/02/2022 03:42:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 03:42:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 03:42:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 03:42:20 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.3611751152073733 on epoch=766
03/02/2022 03:42:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 03:42:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 03:42:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
03/02/2022 03:42:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 03:42:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 03:42:32 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3462535649468499 on epoch=783
03/02/2022 03:42:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 03:42:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/02/2022 03:42:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
03/02/2022 03:42:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 03:42:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=799
03/02/2022 03:42:44 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.34102564102564104 on epoch=799
03/02/2022 03:42:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 03:42:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/02/2022 03:42:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 03:42:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 03:42:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 03:42:56 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.2968834272060078 on epoch=816
03/02/2022 03:42:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/02/2022 03:43:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 03:43:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 03:43:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:43:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
03/02/2022 03:43:08 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.33965348703947146 on epoch=833
03/02/2022 03:43:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
03/02/2022 03:43:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
03/02/2022 03:43:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 03:43:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 03:43:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 03:43:20 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.365945822155336 on epoch=849
03/02/2022 03:43:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 03:43:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/02/2022 03:43:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=859
03/02/2022 03:43:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 03:43:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=866
03/02/2022 03:43:31 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.44683393070489846 on epoch=866
03/02/2022 03:43:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/02/2022 03:43:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 03:43:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
03/02/2022 03:43:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 03:43:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:43:43 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.28108791903858316 on epoch=883
03/02/2022 03:43:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 03:43:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 03:43:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 03:43:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 03:43:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:43:55 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.3355119825708061 on epoch=899
03/02/2022 03:43:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 03:43:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 03:44:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 03:44:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/02/2022 03:44:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 03:44:07 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.27090512972865916 on epoch=916
03/02/2022 03:44:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:44:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 03:44:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:44:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 03:44:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 03:44:18 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.37787524366471736 on epoch=933
03/02/2022 03:44:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 03:44:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 03:44:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 03:44:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
03/02/2022 03:44:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:44:30 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.23275862068965517 on epoch=949
03/02/2022 03:44:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/02/2022 03:44:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
03/02/2022 03:44:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 03:44:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 03:44:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 03:44:42 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3407658340917518 on epoch=966
03/02/2022 03:44:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:44:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:44:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 03:44:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:44:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 03:44:54 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.28282563025210083 on epoch=983
03/02/2022 03:44:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 03:44:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:45:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/02/2022 03:45:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:45:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 03:45:06 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.254377245966817 on epoch=999
03/02/2022 03:45:06 - INFO - __main__ - save last model!
03/02/2022 03:45:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:45:06 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:45:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:45:06 - INFO - __main__ - ['normal']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:45:06 - INFO - __main__ - ['normal']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:45:06 - INFO - __main__ - ['normal']
03/02/2022 03:45:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:45:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:45:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:45:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:45:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:45:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:45:06 - INFO - __main__ - Printing 3 examples
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:45:06 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:45:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:45:06 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:45:07 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:45:09 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:45:19 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:45:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:45:20 - INFO - __main__ - Starting training!
03/02/2022 03:45:54 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_42_0.3_8_predictions.txt
03/02/2022 03:45:54 - INFO - __main__ - Classification-F1 on test data: 0.1667
03/02/2022 03:45:54 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.3, bsz=8, dev_performance=0.49202226345083483, test_performance=0.16673944305453015
03/02/2022 03:45:54 - INFO - __main__ - Running ... prefix=hatexplain_16_42, lr=0.2, bsz=8 ...
03/02/2022 03:45:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:45:55 - INFO - __main__ - Printing 3 examples
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] currynigger and known race traitor <user> has now hit the force unfollow button on me <number> times in the last <number> hours this little freeway shitstain can not pull off even pretending to be pro speech it just wants to cut things into swiss cheese safespace hugboxes for it weakminded little self gab fam
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] why change the game for the nigger assholes if the anthem has been a traditional start it should stay screw the thug niggers
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] <user> the nigger has been prosperous too long so they gotta big up the below average play from the glorious white savior
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:45:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:45:55 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:45:55 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:45:55 - INFO - __main__ - Printing 3 examples
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] dont post sand nigger women you are putting me off my lunch bro
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] shut your filthy mouth kike
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ -  [hatexplain] conversely without it a stupid field nigger sans chapeau
03/02/2022 03:45:55 - INFO - __main__ - ['hatespeech']
03/02/2022 03:45:55 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:45:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:45:55 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:46:07 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:46:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:46:08 - INFO - __main__ - Starting training!
03/02/2022 03:46:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.71 on epoch=3
03/02/2022 03:46:13 - INFO - __main__ - Step 20 Global step 20 Train loss 2.38 on epoch=6
03/02/2022 03:46:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.45 on epoch=9
03/02/2022 03:46:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.96 on epoch=13
03/02/2022 03:46:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=16
03/02/2022 03:46:21 - INFO - __main__ - Global step 50 Train loss 1.85 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 03:46:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 03:46:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=19
03/02/2022 03:46:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=23
03/02/2022 03:46:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=26
03/02/2022 03:46:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=29
03/02/2022 03:46:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=33
03/02/2022 03:46:33 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 03:46:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=36
03/02/2022 03:46:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
03/02/2022 03:46:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
03/02/2022 03:46:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/02/2022 03:46:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=49
03/02/2022 03:46:44 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 03:46:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=53
03/02/2022 03:46:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
03/02/2022 03:46:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=59
03/02/2022 03:46:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=63
03/02/2022 03:46:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=66
03/02/2022 03:46:56 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.27777777777777773 on epoch=66
03/02/2022 03:46:56 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27777777777777773 on epoch=66, global_step=200
03/02/2022 03:46:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=69
03/02/2022 03:47:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=73
03/02/2022 03:47:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/02/2022 03:47:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=79
03/02/2022 03:47:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=83
03/02/2022 03:47:08 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.1639344262295082 on epoch=83
03/02/2022 03:47:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
03/02/2022 03:47:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
03/02/2022 03:47:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
03/02/2022 03:47:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
03/02/2022 03:47:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=99
03/02/2022 03:47:19 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.27777777777777773 on epoch=99
03/02/2022 03:47:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
03/02/2022 03:47:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=106
03/02/2022 03:47:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
03/02/2022 03:47:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/02/2022 03:47:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
03/02/2022 03:47:31 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.3186602870813397 on epoch=116
03/02/2022 03:47:31 - INFO - __main__ - Saving model with best Classification-F1: 0.27777777777777773 -> 0.3186602870813397 on epoch=116, global_step=350
03/02/2022 03:47:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
03/02/2022 03:47:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=123
03/02/2022 03:47:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=126
03/02/2022 03:47:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
03/02/2022 03:47:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=133
03/02/2022 03:47:43 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.30647527910685807 on epoch=133
03/02/2022 03:47:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
03/02/2022 03:47:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
03/02/2022 03:47:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
03/02/2022 03:47:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=146
03/02/2022 03:47:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=149
03/02/2022 03:47:54 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.30682713661437067 on epoch=149
03/02/2022 03:47:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
03/02/2022 03:47:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=156
03/02/2022 03:48:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=159
03/02/2022 03:48:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=163
03/02/2022 03:48:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=166
03/02/2022 03:48:06 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.2862802641232575 on epoch=166
03/02/2022 03:48:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
03/02/2022 03:48:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=173
03/02/2022 03:48:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=176
03/02/2022 03:48:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
03/02/2022 03:48:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=183
03/02/2022 03:48:18 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.28035254505842744 on epoch=183
03/02/2022 03:48:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=186
03/02/2022 03:48:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=189
03/02/2022 03:48:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=193
03/02/2022 03:48:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=196
03/02/2022 03:48:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
03/02/2022 03:48:30 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.28089173593634814 on epoch=199
03/02/2022 03:48:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=203
03/02/2022 03:48:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=206
03/02/2022 03:48:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=209
03/02/2022 03:48:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=213
03/02/2022 03:48:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=216
03/02/2022 03:48:41 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3857142857142857 on epoch=216
03/02/2022 03:48:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3186602870813397 -> 0.3857142857142857 on epoch=216, global_step=650
03/02/2022 03:48:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
03/02/2022 03:48:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
03/02/2022 03:48:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=226
03/02/2022 03:48:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
03/02/2022 03:48:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=233
03/02/2022 03:48:53 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3062678062678063 on epoch=233
03/02/2022 03:48:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=236
03/02/2022 03:48:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=239
03/02/2022 03:48:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
03/02/2022 03:49:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=246
03/02/2022 03:49:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=249
03/02/2022 03:49:05 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.32645502645502644 on epoch=249
03/02/2022 03:49:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=253
03/02/2022 03:49:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=256
03/02/2022 03:49:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=259
03/02/2022 03:49:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=263
03/02/2022 03:49:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=266
03/02/2022 03:49:16 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.23847517730496456 on epoch=266
03/02/2022 03:49:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=269
03/02/2022 03:49:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=273
03/02/2022 03:49:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=276
03/02/2022 03:49:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=279
03/02/2022 03:49:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=283
03/02/2022 03:49:28 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.27314814814814814 on epoch=283
03/02/2022 03:49:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
03/02/2022 03:49:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=289
03/02/2022 03:49:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=293
03/02/2022 03:49:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=296
03/02/2022 03:49:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=299
03/02/2022 03:49:40 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.23994726433750824 on epoch=299
03/02/2022 03:49:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
03/02/2022 03:49:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
03/02/2022 03:49:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=309
03/02/2022 03:49:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=313
03/02/2022 03:49:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=316
03/02/2022 03:49:51 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.25417201540436457 on epoch=316
03/02/2022 03:49:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=319
03/02/2022 03:49:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
03/02/2022 03:49:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=326
03/02/2022 03:50:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
03/02/2022 03:50:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=333
03/02/2022 03:50:03 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.23939393939393938 on epoch=333
03/02/2022 03:50:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=336
03/02/2022 03:50:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=339
03/02/2022 03:50:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=343
03/02/2022 03:50:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=346
03/02/2022 03:50:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=349
03/02/2022 03:50:15 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.20747520976353928 on epoch=349
03/02/2022 03:50:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
03/02/2022 03:50:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
03/02/2022 03:50:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
03/02/2022 03:50:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
03/02/2022 03:50:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=366
03/02/2022 03:50:27 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.22288572647252702 on epoch=366
03/02/2022 03:50:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
03/02/2022 03:50:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
03/02/2022 03:50:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
03/02/2022 03:50:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
03/02/2022 03:50:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
03/02/2022 03:50:38 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.22032085561497325 on epoch=383
03/02/2022 03:50:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
03/02/2022 03:50:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
03/02/2022 03:50:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=393
03/02/2022 03:50:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=396
03/02/2022 03:50:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=399
03/02/2022 03:50:50 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.29187526074259496 on epoch=399
03/02/2022 03:50:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=403
03/02/2022 03:50:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
03/02/2022 03:50:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
03/02/2022 03:50:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
03/02/2022 03:51:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
03/02/2022 03:51:02 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.25967400771432125 on epoch=416
03/02/2022 03:51:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=419
03/02/2022 03:51:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
03/02/2022 03:51:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
03/02/2022 03:51:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
03/02/2022 03:51:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
03/02/2022 03:51:14 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.3159814721482228 on epoch=433
03/02/2022 03:51:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
03/02/2022 03:51:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=439
03/02/2022 03:51:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
03/02/2022 03:51:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=446
03/02/2022 03:51:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
03/02/2022 03:51:25 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.15883040935672516 on epoch=449
03/02/2022 03:51:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
03/02/2022 03:51:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
03/02/2022 03:51:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=459
03/02/2022 03:51:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
03/02/2022 03:51:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
03/02/2022 03:51:37 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.20205415499533147 on epoch=466
03/02/2022 03:51:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/02/2022 03:51:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=473
03/02/2022 03:51:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
03/02/2022 03:51:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
03/02/2022 03:51:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=483
03/02/2022 03:51:49 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.17920963616863295 on epoch=483
03/02/2022 03:51:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
03/02/2022 03:51:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=489
03/02/2022 03:51:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
03/02/2022 03:51:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=496
03/02/2022 03:52:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 03:52:01 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.17119925914492978 on epoch=499
03/02/2022 03:52:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
03/02/2022 03:52:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 03:52:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=509
03/02/2022 03:52:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/02/2022 03:52:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
03/02/2022 03:52:13 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.2763157894736842 on epoch=516
03/02/2022 03:52:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 03:52:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
03/02/2022 03:52:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
03/02/2022 03:52:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=529
03/02/2022 03:52:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 03:52:24 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.23547237076648841 on epoch=533
03/02/2022 03:52:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
03/02/2022 03:52:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/02/2022 03:52:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
03/02/2022 03:52:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
03/02/2022 03:52:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
03/02/2022 03:52:36 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.13269794721407627 on epoch=549
03/02/2022 03:52:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
03/02/2022 03:52:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
03/02/2022 03:52:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=559
03/02/2022 03:52:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
03/02/2022 03:52:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=566
03/02/2022 03:52:47 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.2041343669250646 on epoch=566
03/02/2022 03:52:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/02/2022 03:52:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
03/02/2022 03:52:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
03/02/2022 03:52:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
03/02/2022 03:52:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 03:52:59 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.19052419354838712 on epoch=583
03/02/2022 03:53:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=586
03/02/2022 03:53:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
03/02/2022 03:53:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
03/02/2022 03:53:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
03/02/2022 03:53:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 03:53:10 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.15401509951956077 on epoch=599
03/02/2022 03:53:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
03/02/2022 03:53:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/02/2022 03:53:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
03/02/2022 03:53:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 03:53:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
03/02/2022 03:53:22 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.18194347657762294 on epoch=616
03/02/2022 03:53:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/02/2022 03:53:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
03/02/2022 03:53:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
03/02/2022 03:53:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
03/02/2022 03:53:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
03/02/2022 03:53:34 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.21243686868686867 on epoch=633
03/02/2022 03:53:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/02/2022 03:53:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 03:53:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
03/02/2022 03:53:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
03/02/2022 03:53:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
03/02/2022 03:53:45 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.18365079365079368 on epoch=649
03/02/2022 03:53:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=653
03/02/2022 03:53:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 03:53:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
03/02/2022 03:53:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/02/2022 03:53:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
03/02/2022 03:53:57 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.2106703146374829 on epoch=666
03/02/2022 03:53:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
03/02/2022 03:54:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
03/02/2022 03:54:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
03/02/2022 03:54:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
03/02/2022 03:54:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/02/2022 03:54:08 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.2038866930171278 on epoch=683
03/02/2022 03:54:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
03/02/2022 03:54:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 03:54:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
03/02/2022 03:54:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 03:54:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/02/2022 03:54:20 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.1556802244039271 on epoch=699
03/02/2022 03:54:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
03/02/2022 03:54:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
03/02/2022 03:54:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 03:54:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
03/02/2022 03:54:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 03:54:32 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.1643758765778401 on epoch=716
03/02/2022 03:54:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 03:54:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
03/02/2022 03:54:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/02/2022 03:54:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
03/02/2022 03:54:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 03:54:43 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.16122448979591836 on epoch=733
03/02/2022 03:54:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
03/02/2022 03:54:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=739
03/02/2022 03:54:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/02/2022 03:54:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 03:54:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
03/02/2022 03:54:55 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.15477916941331576 on epoch=749
03/02/2022 03:54:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 03:54:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
03/02/2022 03:55:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
03/02/2022 03:55:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/02/2022 03:55:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
03/02/2022 03:55:06 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.13361111111111112 on epoch=766
03/02/2022 03:55:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
03/02/2022 03:55:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=773
03/02/2022 03:55:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 03:55:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 03:55:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 03:55:18 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.1723529411764706 on epoch=783
03/02/2022 03:55:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 03:55:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 03:55:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 03:55:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 03:55:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 03:55:30 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.15229885057471265 on epoch=799
03/02/2022 03:55:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
03/02/2022 03:55:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 03:55:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
03/02/2022 03:55:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 03:55:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
03/02/2022 03:55:41 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.232046783625731 on epoch=816
03/02/2022 03:55:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
03/02/2022 03:55:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 03:55:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=826
03/02/2022 03:55:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 03:55:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
03/02/2022 03:55:53 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.16825396825396824 on epoch=833
03/02/2022 03:55:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/02/2022 03:55:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
03/02/2022 03:55:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 03:56:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
03/02/2022 03:56:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
03/02/2022 03:56:05 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.2015515074469468 on epoch=849
03/02/2022 03:56:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
03/02/2022 03:56:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/02/2022 03:56:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 03:56:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
03/02/2022 03:56:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 03:56:16 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2117948717948718 on epoch=866
03/02/2022 03:56:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/02/2022 03:56:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 03:56:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 03:56:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 03:56:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 03:56:28 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.194664942900237 on epoch=883
03/02/2022 03:56:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 03:56:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 03:56:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 03:56:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 03:56:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/02/2022 03:56:40 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.22757936507936508 on epoch=899
03/02/2022 03:56:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 03:56:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 03:56:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 03:56:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 03:56:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 03:56:52 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.19408882566777302 on epoch=916
03/02/2022 03:56:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 03:56:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
03/02/2022 03:56:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 03:57:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 03:57:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 03:57:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.13596491228070173 on epoch=933
03/02/2022 03:57:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 03:57:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 03:57:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 03:57:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 03:57:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 03:57:15 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.20193850267379682 on epoch=949
03/02/2022 03:57:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 03:57:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/02/2022 03:57:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
03/02/2022 03:57:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
03/02/2022 03:57:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/02/2022 03:57:26 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.17345502794419204 on epoch=966
03/02/2022 03:57:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 03:57:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 03:57:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/02/2022 03:57:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 03:57:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 03:57:38 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.16163101604278074 on epoch=983
03/02/2022 03:57:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
03/02/2022 03:57:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 03:57:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 03:57:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 03:57:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=999
03/02/2022 03:57:50 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.20995475113122172 on epoch=999
03/02/2022 03:57:50 - INFO - __main__ - save last model!
03/02/2022 03:57:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 03:57:50 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 03:57:50 - INFO - __main__ - Printing 3 examples
03/02/2022 03:57:50 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 03:57:50 - INFO - __main__ - ['normal']
03/02/2022 03:57:50 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 03:57:50 - INFO - __main__ - ['normal']
03/02/2022 03:57:50 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 03:57:50 - INFO - __main__ - ['normal']
03/02/2022 03:57:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:57:51 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:57:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:57:52 - INFO - __main__ - Printing 3 examples
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 03:57:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:57:52 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:57:52 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:57:52 - INFO - __main__ - Printing 3 examples
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 03:57:52 - INFO - __main__ - ['offensive']
03/02/2022 03:57:52 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:57:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:57:52 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:57:52 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 03:58:04 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:58:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:58:05 - INFO - __main__ - Starting training!
03/02/2022 03:58:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_42_0.2_8_predictions.txt
03/02/2022 03:58:35 - INFO - __main__ - Classification-F1 on test data: 0.0575
03/02/2022 03:58:35 - INFO - __main__ - prefix=hatexplain_16_42, lr=0.2, bsz=8, dev_performance=0.3857142857142857, test_performance=0.057511032888817225
03/02/2022 03:58:35 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.5, bsz=8 ...
03/02/2022 03:58:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:58:36 - INFO - __main__ - Printing 3 examples
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 03:58:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:58:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 03:58:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 03:58:36 - INFO - __main__ - Printing 3 examples
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 03:58:36 - INFO - __main__ - ['offensive']
03/02/2022 03:58:36 - INFO - __main__ - Tokenizing Input ...
03/02/2022 03:58:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 03:58:37 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 03:58:49 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 03:58:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 03:58:50 - INFO - __main__ - Starting training!
03/02/2022 03:58:54 - INFO - __main__ - Step 10 Global step 10 Train loss 2.75 on epoch=3
03/02/2022 03:58:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.92 on epoch=6
03/02/2022 03:58:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.68 on epoch=9
03/02/2022 03:59:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=13
03/02/2022 03:59:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=16
03/02/2022 03:59:03 - INFO - __main__ - Global step 50 Train loss 1.10 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 03:59:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 03:59:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
03/02/2022 03:59:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
03/02/2022 03:59:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
03/02/2022 03:59:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
03/02/2022 03:59:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
03/02/2022 03:59:15 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 03:59:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
03/02/2022 03:59:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
03/02/2022 03:59:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
03/02/2022 03:59:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
03/02/2022 03:59:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
03/02/2022 03:59:26 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 03:59:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
03/02/2022 03:59:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=56
03/02/2022 03:59:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
03/02/2022 03:59:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=63
03/02/2022 03:59:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/02/2022 03:59:38 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.30346013324736726 on epoch=66
03/02/2022 03:59:38 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.30346013324736726 on epoch=66, global_step=200
03/02/2022 03:59:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=69
03/02/2022 03:59:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=73
03/02/2022 03:59:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
03/02/2022 03:59:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
03/02/2022 03:59:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=83
03/02/2022 03:59:49 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.21666666666666667 on epoch=83
03/02/2022 03:59:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=86
03/02/2022 03:59:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=89
03/02/2022 03:59:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=93
03/02/2022 03:59:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
03/02/2022 04:00:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
03/02/2022 04:00:01 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.34341321713870737 on epoch=99
03/02/2022 04:00:01 - INFO - __main__ - Saving model with best Classification-F1: 0.30346013324736726 -> 0.34341321713870737 on epoch=99, global_step=300
03/02/2022 04:00:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=103
03/02/2022 04:00:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=106
03/02/2022 04:00:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=109
03/02/2022 04:00:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=113
03/02/2022 04:00:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=116
03/02/2022 04:00:13 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.4116264687693259 on epoch=116
03/02/2022 04:00:13 - INFO - __main__ - Saving model with best Classification-F1: 0.34341321713870737 -> 0.4116264687693259 on epoch=116, global_step=350
03/02/2022 04:00:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=119
03/02/2022 04:00:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
03/02/2022 04:00:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=126
03/02/2022 04:00:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=129
03/02/2022 04:00:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=133
03/02/2022 04:00:24 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.35593542260208927 on epoch=133
03/02/2022 04:00:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=136
03/02/2022 04:00:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=139
03/02/2022 04:00:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
03/02/2022 04:00:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=146
03/02/2022 04:00:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=149
03/02/2022 04:00:36 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.4663663663663664 on epoch=149
03/02/2022 04:00:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4116264687693259 -> 0.4663663663663664 on epoch=149, global_step=450
03/02/2022 04:00:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=153
03/02/2022 04:00:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=156
03/02/2022 04:00:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=159
03/02/2022 04:00:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=163
03/02/2022 04:00:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=166
03/02/2022 04:00:48 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.5072649572649572 on epoch=166
03/02/2022 04:00:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4663663663663664 -> 0.5072649572649572 on epoch=166, global_step=500
03/02/2022 04:00:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=169
03/02/2022 04:00:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
03/02/2022 04:00:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=176
03/02/2022 04:00:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
03/02/2022 04:00:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=183
03/02/2022 04:00:59 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.36605551311433665 on epoch=183
03/02/2022 04:01:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=186
03/02/2022 04:01:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=189
03/02/2022 04:01:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=193
03/02/2022 04:01:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=196
03/02/2022 04:01:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
03/02/2022 04:01:11 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.38319476625323695 on epoch=199
03/02/2022 04:01:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=203
03/02/2022 04:01:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=206
03/02/2022 04:01:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=209
03/02/2022 04:01:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=213
03/02/2022 04:01:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
03/02/2022 04:01:23 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.35669482846902206 on epoch=216
03/02/2022 04:01:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=219
03/02/2022 04:01:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
03/02/2022 04:01:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
03/02/2022 04:01:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=229
03/02/2022 04:01:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=233
03/02/2022 04:01:34 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.3152173913043478 on epoch=233
03/02/2022 04:01:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=236
03/02/2022 04:01:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.11 on epoch=239
03/02/2022 04:01:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=243
03/02/2022 04:01:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=246
03/02/2022 04:01:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=249
03/02/2022 04:01:46 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.18695652173913044 on epoch=249
03/02/2022 04:01:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
03/02/2022 04:01:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=256
03/02/2022 04:01:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
03/02/2022 04:01:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
03/02/2022 04:01:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
03/02/2022 04:01:57 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.22252747252747251 on epoch=266
03/02/2022 04:01:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
03/02/2022 04:02:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=273
03/02/2022 04:02:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=276
03/02/2022 04:02:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=279
03/02/2022 04:02:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
03/02/2022 04:02:09 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.35076923076923083 on epoch=283
03/02/2022 04:02:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
03/02/2022 04:02:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
03/02/2022 04:02:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
03/02/2022 04:02:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
03/02/2022 04:02:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=299
03/02/2022 04:02:20 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.24129593810444872 on epoch=299
03/02/2022 04:02:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=303
03/02/2022 04:02:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
03/02/2022 04:02:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=309
03/02/2022 04:02:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
03/02/2022 04:02:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
03/02/2022 04:02:32 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.2625948592411261 on epoch=316
03/02/2022 04:02:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
03/02/2022 04:02:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
03/02/2022 04:02:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
03/02/2022 04:02:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
03/02/2022 04:02:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
03/02/2022 04:02:43 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.13635610766045547 on epoch=333
03/02/2022 04:02:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=336
03/02/2022 04:02:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
03/02/2022 04:02:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
03/02/2022 04:02:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
03/02/2022 04:02:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
03/02/2022 04:02:55 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.22714285714285717 on epoch=349
03/02/2022 04:02:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
03/02/2022 04:02:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
03/02/2022 04:03:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
03/02/2022 04:03:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
03/02/2022 04:03:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
03/02/2022 04:03:06 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.21538461538461537 on epoch=366
03/02/2022 04:03:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
03/02/2022 04:03:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
03/02/2022 04:03:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
03/02/2022 04:03:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=379
03/02/2022 04:03:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
03/02/2022 04:03:18 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.2678030303030303 on epoch=383
03/02/2022 04:03:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
03/02/2022 04:03:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=389
03/02/2022 04:03:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
03/02/2022 04:03:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
03/02/2022 04:03:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=399
03/02/2022 04:03:30 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.1880952380952381 on epoch=399
03/02/2022 04:03:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
03/02/2022 04:03:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
03/02/2022 04:03:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
03/02/2022 04:03:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
03/02/2022 04:03:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=416
03/02/2022 04:03:41 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.26985645933014357 on epoch=416
03/02/2022 04:03:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
03/02/2022 04:03:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
03/02/2022 04:03:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
03/02/2022 04:03:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
03/02/2022 04:03:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
03/02/2022 04:03:53 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.20840787119856885 on epoch=433
03/02/2022 04:03:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
03/02/2022 04:03:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
03/02/2022 04:03:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
03/02/2022 04:04:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
03/02/2022 04:04:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
03/02/2022 04:04:04 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.1976127320954907 on epoch=449
03/02/2022 04:04:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=453
03/02/2022 04:04:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
03/02/2022 04:04:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
03/02/2022 04:04:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
03/02/2022 04:04:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
03/02/2022 04:04:16 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.21455671699574136 on epoch=466
03/02/2022 04:04:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 04:04:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=473
03/02/2022 04:04:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
03/02/2022 04:04:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
03/02/2022 04:04:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
03/02/2022 04:04:28 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.25824029771398194 on epoch=483
03/02/2022 04:04:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
03/02/2022 04:04:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
03/02/2022 04:04:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
03/02/2022 04:04:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
03/02/2022 04:04:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/02/2022 04:04:39 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.34074074074074073 on epoch=499
03/02/2022 04:04:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
03/02/2022 04:04:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
03/02/2022 04:04:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
03/02/2022 04:04:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 04:04:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
03/02/2022 04:04:51 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.23343904157857648 on epoch=516
03/02/2022 04:04:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/02/2022 04:04:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 04:04:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
03/02/2022 04:05:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
03/02/2022 04:05:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
03/02/2022 04:05:03 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.34656084656084657 on epoch=533
03/02/2022 04:05:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 04:05:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 04:05:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
03/02/2022 04:05:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
03/02/2022 04:05:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
03/02/2022 04:05:15 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2985714285714286 on epoch=549
03/02/2022 04:05:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
03/02/2022 04:05:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
03/02/2022 04:05:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
03/02/2022 04:05:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
03/02/2022 04:05:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 04:05:26 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.3171122576610381 on epoch=566
03/02/2022 04:05:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
03/02/2022 04:05:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 04:05:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
03/02/2022 04:05:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 04:05:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 04:05:38 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3156752667622233 on epoch=583
03/02/2022 04:05:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
03/02/2022 04:05:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 04:05:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 04:05:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
03/02/2022 04:05:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
03/02/2022 04:05:49 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.28952254641909814 on epoch=599
03/02/2022 04:05:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 04:05:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/02/2022 04:05:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 04:05:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/02/2022 04:06:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=616
03/02/2022 04:06:01 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.29583333333333334 on epoch=616
03/02/2022 04:06:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
03/02/2022 04:06:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 04:06:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
03/02/2022 04:06:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/02/2022 04:06:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 04:06:13 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.3242242242242242 on epoch=633
03/02/2022 04:06:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 04:06:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/02/2022 04:06:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/02/2022 04:06:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
03/02/2022 04:06:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/02/2022 04:06:24 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.3242242242242242 on epoch=649
03/02/2022 04:06:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 04:06:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
03/02/2022 04:06:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
03/02/2022 04:06:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
03/02/2022 04:06:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/02/2022 04:06:36 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.2788045288045288 on epoch=666
03/02/2022 04:06:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 04:06:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 04:06:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/02/2022 04:06:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/02/2022 04:06:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
03/02/2022 04:06:48 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.26686818928198236 on epoch=683
03/02/2022 04:06:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 04:06:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 04:06:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/02/2022 04:06:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/02/2022 04:06:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 04:06:59 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.30128205128205127 on epoch=699
03/02/2022 04:07:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/02/2022 04:07:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/02/2022 04:07:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/02/2022 04:07:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/02/2022 04:07:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 04:07:11 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.38571428571428573 on epoch=716
03/02/2022 04:07:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/02/2022 04:07:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 04:07:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/02/2022 04:07:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 04:07:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 04:07:23 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.2906227106227106 on epoch=733
03/02/2022 04:07:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/02/2022 04:07:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/02/2022 04:07:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
03/02/2022 04:07:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
03/02/2022 04:07:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/02/2022 04:07:34 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2717391304347826 on epoch=749
03/02/2022 04:07:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 04:07:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/02/2022 04:07:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 04:07:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 04:07:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 04:07:46 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.30100925147182506 on epoch=766
03/02/2022 04:07:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 04:07:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 04:07:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 04:07:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/02/2022 04:07:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/02/2022 04:07:58 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.30128205128205127 on epoch=783
03/02/2022 04:08:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/02/2022 04:08:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/02/2022 04:08:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 04:08:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
03/02/2022 04:08:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/02/2022 04:08:09 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2913919413919414 on epoch=799
03/02/2022 04:08:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 04:08:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/02/2022 04:08:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 04:08:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 04:08:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/02/2022 04:08:21 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.3091168091168091 on epoch=816
03/02/2022 04:08:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
03/02/2022 04:08:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 04:08:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 04:08:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 04:08:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/02/2022 04:08:33 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3182261208576998 on epoch=833
03/02/2022 04:08:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 04:08:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/02/2022 04:08:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 04:08:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 04:08:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 04:08:44 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.33333333333333337 on epoch=849
03/02/2022 04:08:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/02/2022 04:08:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 04:08:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 04:08:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 04:08:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 04:08:56 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.33975588491717523 on epoch=866
03/02/2022 04:08:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
03/02/2022 04:09:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 04:09:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/02/2022 04:09:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/02/2022 04:09:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 04:09:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.20111111111111113 on epoch=883
03/02/2022 04:09:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 04:09:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 04:09:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/02/2022 04:09:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 04:09:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 04:09:19 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.26683673469387753 on epoch=899
03/02/2022 04:09:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
03/02/2022 04:09:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 04:09:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/02/2022 04:09:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 04:09:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 04:09:31 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3474898785425101 on epoch=916
03/02/2022 04:09:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 04:09:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 04:09:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
03/02/2022 04:09:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 04:09:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 04:09:43 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2304273504273504 on epoch=933
03/02/2022 04:09:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 04:09:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 04:09:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 04:09:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 04:09:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 04:09:55 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.26266416510318946 on epoch=949
03/02/2022 04:09:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 04:09:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 04:10:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 04:10:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 04:10:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 04:10:06 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.22564102564102564 on epoch=966
03/02/2022 04:10:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 04:10:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=973
03/02/2022 04:10:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 04:10:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 04:10:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 04:10:18 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.21877289377289377 on epoch=983
03/02/2022 04:10:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 04:10:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 04:10:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
03/02/2022 04:10:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 04:10:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 04:10:30 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2706959706959707 on epoch=999
03/02/2022 04:10:30 - INFO - __main__ - save last model!
03/02/2022 04:10:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 04:10:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:10:30 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 04:10:30 - INFO - __main__ - Printing 3 examples
03/02/2022 04:10:30 - INFO - __main__ - Printing 3 examples
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:10:30 - INFO - __main__ - ['normal']
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:10:30 - INFO - __main__ - ['normal']
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 04:10:30 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:10:30 - INFO - __main__ - ['normal']
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 04:10:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:10:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:10:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:10:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:10:30 - INFO - __main__ - Printing 3 examples
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:10:30 - INFO - __main__ - ['offensive']
03/02/2022 04:10:30 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:10:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:10:30 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:10:31 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:10:33 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 04:10:43 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:10:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:10:44 - INFO - __main__ - Starting training!
03/02/2022 04:11:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_87_0.5_8_predictions.txt
03/02/2022 04:11:17 - INFO - __main__ - Classification-F1 on test data: 0.1756
03/02/2022 04:11:19 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.5, bsz=8, dev_performance=0.5072649572649572, test_performance=0.17555031678526578
03/02/2022 04:11:19 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.4, bsz=8 ...
03/02/2022 04:11:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:11:20 - INFO - __main__ - Printing 3 examples
03/02/2022 04:11:20 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:11:20 - INFO - __main__ - ['offensive']
03/02/2022 04:11:20 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:11:20 - INFO - __main__ - ['offensive']
03/02/2022 04:11:20 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:11:20 - INFO - __main__ - ['offensive']
03/02/2022 04:11:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:11:20 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:11:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:11:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:11:21 - INFO - __main__ - Printing 3 examples
03/02/2022 04:11:21 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:11:21 - INFO - __main__ - ['offensive']
03/02/2022 04:11:21 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:11:21 - INFO - __main__ - ['offensive']
03/02/2022 04:11:21 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:11:21 - INFO - __main__ - ['offensive']
03/02/2022 04:11:21 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:11:21 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:11:21 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:11:35 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:11:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:11:35 - INFO - __main__ - Starting training!
03/02/2022 04:11:38 - INFO - __main__ - Step 10 Global step 10 Train loss 3.30 on epoch=3
03/02/2022 04:11:40 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=6
03/02/2022 04:11:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.73 on epoch=9
03/02/2022 04:11:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=13
03/02/2022 04:11:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
03/02/2022 04:11:50 - INFO - __main__ - Global step 50 Train loss 1.31 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 04:11:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 04:11:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
03/02/2022 04:11:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
03/02/2022 04:11:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
03/02/2022 04:11:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=29
03/02/2022 04:12:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
03/02/2022 04:12:01 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.13333333333333333 on epoch=33
03/02/2022 04:12:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
03/02/2022 04:12:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
03/02/2022 04:12:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
03/02/2022 04:12:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=46
03/02/2022 04:12:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
03/02/2022 04:12:13 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 04:12:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
03/02/2022 04:12:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
03/02/2022 04:12:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=59
03/02/2022 04:12:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/02/2022 04:12:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
03/02/2022 04:12:24 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.17204301075268816 on epoch=66
03/02/2022 04:12:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17204301075268816 on epoch=66, global_step=200
03/02/2022 04:12:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
03/02/2022 04:12:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
03/02/2022 04:12:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
03/02/2022 04:12:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
03/02/2022 04:12:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
03/02/2022 04:12:36 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.4236874236874237 on epoch=83
03/02/2022 04:12:36 - INFO - __main__ - Saving model with best Classification-F1: 0.17204301075268816 -> 0.4236874236874237 on epoch=83, global_step=250
03/02/2022 04:12:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=86
03/02/2022 04:12:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=89
03/02/2022 04:12:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=93
03/02/2022 04:12:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=96
03/02/2022 04:12:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=99
03/02/2022 04:12:48 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.27289377289377287 on epoch=99
03/02/2022 04:12:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=103
03/02/2022 04:12:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=106
03/02/2022 04:12:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=109
03/02/2022 04:12:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
03/02/2022 04:12:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=116
03/02/2022 04:12:59 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.3849607182940516 on epoch=116
03/02/2022 04:13:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=119
03/02/2022 04:13:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=123
03/02/2022 04:13:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=126
03/02/2022 04:13:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
03/02/2022 04:13:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=133
03/02/2022 04:13:11 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.3 on epoch=133
03/02/2022 04:13:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=136
03/02/2022 04:13:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=139
03/02/2022 04:13:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=143
03/02/2022 04:13:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=146
03/02/2022 04:13:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=149
03/02/2022 04:13:23 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.45391815262690444 on epoch=149
03/02/2022 04:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4236874236874237 -> 0.45391815262690444 on epoch=149, global_step=450
03/02/2022 04:13:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=153
03/02/2022 04:13:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=156
03/02/2022 04:13:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=159
03/02/2022 04:13:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=163
03/02/2022 04:13:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=166
03/02/2022 04:13:34 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.43520074696545286 on epoch=166
03/02/2022 04:13:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
03/02/2022 04:13:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=173
03/02/2022 04:13:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
03/02/2022 04:13:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
03/02/2022 04:13:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
03/02/2022 04:13:46 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3809012106884447 on epoch=183
03/02/2022 04:13:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
03/02/2022 04:13:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=189
03/02/2022 04:13:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
03/02/2022 04:13:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
03/02/2022 04:13:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=199
03/02/2022 04:13:57 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3861634245265959 on epoch=199
03/02/2022 04:13:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=203
03/02/2022 04:14:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=206
03/02/2022 04:14:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=209
03/02/2022 04:14:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
03/02/2022 04:14:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=216
03/02/2022 04:14:09 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.45007999617928696 on epoch=216
03/02/2022 04:14:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=219
03/02/2022 04:14:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=223
03/02/2022 04:14:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=226
03/02/2022 04:14:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
03/02/2022 04:14:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=233
03/02/2022 04:14:21 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.38739952718676124 on epoch=233
03/02/2022 04:14:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=236
03/02/2022 04:14:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=239
03/02/2022 04:14:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
03/02/2022 04:14:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
03/02/2022 04:14:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=249
03/02/2022 04:14:32 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.2539488017429194 on epoch=249
03/02/2022 04:14:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=253
03/02/2022 04:14:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=256
03/02/2022 04:14:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
03/02/2022 04:14:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=263
03/02/2022 04:14:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
03/02/2022 04:14:44 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.4679622310057093 on epoch=266
03/02/2022 04:14:44 - INFO - __main__ - Saving model with best Classification-F1: 0.45391815262690444 -> 0.4679622310057093 on epoch=266, global_step=800
03/02/2022 04:14:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
03/02/2022 04:14:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=273
03/02/2022 04:14:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
03/02/2022 04:14:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
03/02/2022 04:14:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
03/02/2022 04:14:56 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.3176573426573427 on epoch=283
03/02/2022 04:14:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=286
03/02/2022 04:15:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=289
03/02/2022 04:15:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=293
03/02/2022 04:15:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=296
03/02/2022 04:15:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=299
03/02/2022 04:15:07 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.2080896686159844 on epoch=299
03/02/2022 04:15:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
03/02/2022 04:15:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
03/02/2022 04:15:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
03/02/2022 04:15:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=313
03/02/2022 04:15:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
03/02/2022 04:15:19 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.29839115133232785 on epoch=316
03/02/2022 04:15:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
03/02/2022 04:15:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
03/02/2022 04:15:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
03/02/2022 04:15:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
03/02/2022 04:15:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
03/02/2022 04:15:31 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.2285885167464115 on epoch=333
03/02/2022 04:15:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
03/02/2022 04:15:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
03/02/2022 04:15:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
03/02/2022 04:15:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
03/02/2022 04:15:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
03/02/2022 04:15:42 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.2309669522643819 on epoch=349
03/02/2022 04:15:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
03/02/2022 04:15:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
03/02/2022 04:15:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
03/02/2022 04:15:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
03/02/2022 04:15:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=366
03/02/2022 04:15:54 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.2877783669141039 on epoch=366
03/02/2022 04:15:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=369
03/02/2022 04:15:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=373
03/02/2022 04:16:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
03/02/2022 04:16:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=379
03/02/2022 04:16:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
03/02/2022 04:16:06 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.3486111111111111 on epoch=383
03/02/2022 04:16:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
03/02/2022 04:16:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
03/02/2022 04:16:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
03/02/2022 04:16:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
03/02/2022 04:16:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
03/02/2022 04:16:17 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.3902914614121511 on epoch=399
03/02/2022 04:16:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
03/02/2022 04:16:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
03/02/2022 04:16:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
03/02/2022 04:16:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
03/02/2022 04:16:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=416
03/02/2022 04:16:29 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.34093915343915343 on epoch=416
03/02/2022 04:16:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
03/02/2022 04:16:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
03/02/2022 04:16:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=426
03/02/2022 04:16:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
03/02/2022 04:16:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
03/02/2022 04:16:41 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.31214285714285717 on epoch=433
03/02/2022 04:16:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=436
03/02/2022 04:16:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
03/02/2022 04:16:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
03/02/2022 04:16:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
03/02/2022 04:16:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
03/02/2022 04:16:52 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.2920289855072464 on epoch=449
03/02/2022 04:16:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
03/02/2022 04:16:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
03/02/2022 04:16:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
03/02/2022 04:17:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
03/02/2022 04:17:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
03/02/2022 04:17:04 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.3268914075365688 on epoch=466
03/02/2022 04:17:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
03/02/2022 04:17:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/02/2022 04:17:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/02/2022 04:17:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
03/02/2022 04:17:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=483
03/02/2022 04:17:16 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.39365079365079364 on epoch=483
03/02/2022 04:17:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
03/02/2022 04:17:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
03/02/2022 04:17:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
03/02/2022 04:17:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=496
03/02/2022 04:17:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
03/02/2022 04:17:27 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.34618110933900414 on epoch=499
03/02/2022 04:17:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
03/02/2022 04:17:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
03/02/2022 04:17:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
03/02/2022 04:17:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
03/02/2022 04:17:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
03/02/2022 04:17:39 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.28454545454545455 on epoch=516
03/02/2022 04:17:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
03/02/2022 04:17:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 04:17:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/02/2022 04:17:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
03/02/2022 04:17:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/02/2022 04:17:51 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.3767696267696268 on epoch=533
03/02/2022 04:17:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/02/2022 04:17:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
03/02/2022 04:17:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
03/02/2022 04:17:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
03/02/2022 04:18:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
03/02/2022 04:18:03 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.20217560217560218 on epoch=549
03/02/2022 04:18:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
03/02/2022 04:18:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
03/02/2022 04:18:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
03/02/2022 04:18:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
03/02/2022 04:18:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
03/02/2022 04:18:14 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.48360774567671116 on epoch=566
03/02/2022 04:18:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4679622310057093 -> 0.48360774567671116 on epoch=566, global_step=1700
03/02/2022 04:18:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
03/02/2022 04:18:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
03/02/2022 04:18:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
03/02/2022 04:18:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
03/02/2022 04:18:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
03/02/2022 04:18:26 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2009337068160598 on epoch=583
03/02/2022 04:18:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
03/02/2022 04:18:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/02/2022 04:18:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/02/2022 04:18:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
03/02/2022 04:18:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/02/2022 04:18:38 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.1604938271604938 on epoch=599
03/02/2022 04:18:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/02/2022 04:18:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=606
03/02/2022 04:18:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
03/02/2022 04:18:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
03/02/2022 04:18:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
03/02/2022 04:18:49 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.11534043441938178 on epoch=616
03/02/2022 04:18:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
03/02/2022 04:18:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/02/2022 04:18:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=626
03/02/2022 04:18:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=629
03/02/2022 04:19:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
03/02/2022 04:19:01 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.20009337068160596 on epoch=633
03/02/2022 04:19:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
03/02/2022 04:19:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
03/02/2022 04:19:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=643
03/02/2022 04:19:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/02/2022 04:19:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/02/2022 04:19:13 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2669565217391304 on epoch=649
03/02/2022 04:19:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/02/2022 04:19:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/02/2022 04:19:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
03/02/2022 04:19:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/02/2022 04:19:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
03/02/2022 04:19:24 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.3319783197831978 on epoch=666
03/02/2022 04:19:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
03/02/2022 04:19:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 04:19:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=676
03/02/2022 04:19:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
03/02/2022 04:19:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/02/2022 04:19:36 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.24384615384615388 on epoch=683
03/02/2022 04:19:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
03/02/2022 04:19:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/02/2022 04:19:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/02/2022 04:19:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 04:19:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 04:19:48 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.31395038302059175 on epoch=699
03/02/2022 04:19:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
03/02/2022 04:19:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/02/2022 04:19:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/02/2022 04:19:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
03/02/2022 04:19:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/02/2022 04:19:59 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.24239766081871342 on epoch=716
03/02/2022 04:20:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 04:20:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/02/2022 04:20:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=726
03/02/2022 04:20:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/02/2022 04:20:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/02/2022 04:20:11 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.26666666666666666 on epoch=733
03/02/2022 04:20:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 04:20:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
03/02/2022 04:20:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/02/2022 04:20:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/02/2022 04:20:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 04:20:23 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2886720867208672 on epoch=749
03/02/2022 04:20:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/02/2022 04:20:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 04:20:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
03/02/2022 04:20:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 04:20:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/02/2022 04:20:34 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.24386074629977067 on epoch=766
03/02/2022 04:20:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/02/2022 04:20:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/02/2022 04:20:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/02/2022 04:20:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/02/2022 04:20:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
03/02/2022 04:20:46 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.31875 on epoch=783
03/02/2022 04:20:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 04:20:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
03/02/2022 04:20:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/02/2022 04:20:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
03/02/2022 04:20:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/02/2022 04:20:58 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.3627880184331797 on epoch=799
03/02/2022 04:21:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/02/2022 04:21:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 04:21:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/02/2022 04:21:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 04:21:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/02/2022 04:21:09 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2879885814668423 on epoch=816
03/02/2022 04:21:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
03/02/2022 04:21:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 04:21:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
03/02/2022 04:21:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
03/02/2022 04:21:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/02/2022 04:21:21 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3923035230352303 on epoch=833
03/02/2022 04:21:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/02/2022 04:21:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 04:21:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 04:21:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 04:21:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/02/2022 04:21:33 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.33169934640522875 on epoch=849
03/02/2022 04:21:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
03/02/2022 04:21:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/02/2022 04:21:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/02/2022 04:21:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/02/2022 04:21:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/02/2022 04:21:44 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2909688995215311 on epoch=866
03/02/2022 04:21:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
03/02/2022 04:21:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/02/2022 04:21:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
03/02/2022 04:21:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 04:21:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/02/2022 04:21:56 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.4533228881054967 on epoch=883
03/02/2022 04:21:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/02/2022 04:22:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/02/2022 04:22:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 04:22:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/02/2022 04:22:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
03/02/2022 04:22:08 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3294430794430794 on epoch=899
03/02/2022 04:22:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 04:22:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/02/2022 04:22:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 04:22:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/02/2022 04:22:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
03/02/2022 04:22:20 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2931818181818182 on epoch=916
03/02/2022 04:22:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
03/02/2022 04:22:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 04:22:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 04:22:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/02/2022 04:22:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/02/2022 04:22:31 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3237179487179487 on epoch=933
03/02/2022 04:22:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 04:22:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
03/02/2022 04:22:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/02/2022 04:22:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 04:22:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 04:22:43 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.47755991285403043 on epoch=949
03/02/2022 04:22:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 04:22:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/02/2022 04:22:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/02/2022 04:22:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/02/2022 04:22:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 04:22:55 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.31492969396195203 on epoch=966
03/02/2022 04:22:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 04:22:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 04:23:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 04:23:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 04:23:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/02/2022 04:23:06 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2574092574092574 on epoch=983
03/02/2022 04:23:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/02/2022 04:23:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 04:23:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 04:23:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 04:23:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 04:23:18 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2379150579150579 on epoch=999
03/02/2022 04:23:18 - INFO - __main__ - save last model!
03/02/2022 04:23:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 04:23:18 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 04:23:18 - INFO - __main__ - Printing 3 examples
03/02/2022 04:23:18 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 04:23:18 - INFO - __main__ - ['normal']
03/02/2022 04:23:18 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 04:23:18 - INFO - __main__ - ['normal']
03/02/2022 04:23:18 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 04:23:18 - INFO - __main__ - ['normal']
03/02/2022 04:23:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:23:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:23:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:23:20 - INFO - __main__ - Printing 3 examples
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 04:23:20 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:23:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:23:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:23:20 - INFO - __main__ - Printing 3 examples
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:23:20 - INFO - __main__ - ['offensive']
03/02/2022 04:23:20 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:23:20 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:23:20 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:23:21 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 04:23:33 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:23:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:23:33 - INFO - __main__ - Starting training!
03/02/2022 04:24:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_87_0.4_8_predictions.txt
03/02/2022 04:24:07 - INFO - __main__ - Classification-F1 on test data: 0.1803
03/02/2022 04:24:07 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.4, bsz=8, dev_performance=0.48360774567671116, test_performance=0.1802746307260343
03/02/2022 04:24:07 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.3, bsz=8 ...
03/02/2022 04:24:08 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:24:08 - INFO - __main__ - Printing 3 examples
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:24:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:24:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:24:08 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:24:08 - INFO - __main__ - Printing 3 examples
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:24:08 - INFO - __main__ - ['offensive']
03/02/2022 04:24:08 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:24:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:24:08 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:24:22 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:24:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:24:23 - INFO - __main__ - Starting training!
03/02/2022 04:24:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.20 on epoch=3
03/02/2022 04:24:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.52 on epoch=6
03/02/2022 04:24:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.81 on epoch=9
03/02/2022 04:24:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.77 on epoch=13
03/02/2022 04:24:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
03/02/2022 04:24:36 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.16666666666666666 on epoch=16
03/02/2022 04:24:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
03/02/2022 04:24:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=19
03/02/2022 04:24:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=23
03/02/2022 04:24:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
03/02/2022 04:24:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=29
03/02/2022 04:24:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
03/02/2022 04:24:48 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.15873015873015875 on epoch=33
03/02/2022 04:24:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/02/2022 04:24:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=39
03/02/2022 04:24:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
03/02/2022 04:24:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
03/02/2022 04:24:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=49
03/02/2022 04:25:00 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 04:25:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
03/02/2022 04:25:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
03/02/2022 04:25:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
03/02/2022 04:25:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
03/02/2022 04:25:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
03/02/2022 04:25:12 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.34820867379006915 on epoch=66
03/02/2022 04:25:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.34820867379006915 on epoch=66, global_step=200
03/02/2022 04:25:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=69
03/02/2022 04:25:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=73
03/02/2022 04:25:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/02/2022 04:25:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 04:25:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
03/02/2022 04:25:24 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.20032679738562095 on epoch=83
03/02/2022 04:25:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
03/02/2022 04:25:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
03/02/2022 04:25:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
03/02/2022 04:25:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
03/02/2022 04:25:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
03/02/2022 04:25:37 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3156139253700229 on epoch=99
03/02/2022 04:25:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
03/02/2022 04:25:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=106
03/02/2022 04:25:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=109
03/02/2022 04:25:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=113
03/02/2022 04:25:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
03/02/2022 04:25:49 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.37149014778325123 on epoch=116
03/02/2022 04:25:49 - INFO - __main__ - Saving model with best Classification-F1: 0.34820867379006915 -> 0.37149014778325123 on epoch=116, global_step=350
03/02/2022 04:25:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
03/02/2022 04:25:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
03/02/2022 04:25:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
03/02/2022 04:25:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=129
03/02/2022 04:26:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=133
03/02/2022 04:26:01 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.3717171717171717 on epoch=133
03/02/2022 04:26:01 - INFO - __main__ - Saving model with best Classification-F1: 0.37149014778325123 -> 0.3717171717171717 on epoch=133, global_step=400
03/02/2022 04:26:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=136
03/02/2022 04:26:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=139
03/02/2022 04:26:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
03/02/2022 04:26:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=146
03/02/2022 04:26:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
03/02/2022 04:26:14 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.3482676224611709 on epoch=149
03/02/2022 04:26:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=153
03/02/2022 04:26:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=156
03/02/2022 04:26:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=159
03/02/2022 04:26:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=163
03/02/2022 04:26:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
03/02/2022 04:26:26 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.4527863777089783 on epoch=166
03/02/2022 04:26:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3717171717171717 -> 0.4527863777089783 on epoch=166, global_step=500
03/02/2022 04:26:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=169
03/02/2022 04:26:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=173
03/02/2022 04:26:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=176
03/02/2022 04:26:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=179
03/02/2022 04:26:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=183
03/02/2022 04:26:38 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.32199217851391765 on epoch=183
03/02/2022 04:26:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
03/02/2022 04:26:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
03/02/2022 04:26:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
03/02/2022 04:26:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
03/02/2022 04:26:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=199
03/02/2022 04:26:50 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.39145299145299145 on epoch=199
03/02/2022 04:26:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=203
03/02/2022 04:26:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=206
03/02/2022 04:26:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=209
03/02/2022 04:26:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=213
03/02/2022 04:27:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=216
03/02/2022 04:27:02 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3304738562091503 on epoch=216
03/02/2022 04:27:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=219
03/02/2022 04:27:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
03/02/2022 04:27:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
03/02/2022 04:27:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=229
03/02/2022 04:27:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=233
03/02/2022 04:27:14 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3433886375062845 on epoch=233
03/02/2022 04:27:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=236
03/02/2022 04:27:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
03/02/2022 04:27:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=243
03/02/2022 04:27:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
03/02/2022 04:27:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=249
03/02/2022 04:27:25 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.47042042042042037 on epoch=249
03/02/2022 04:27:25 - INFO - __main__ - Saving model with best Classification-F1: 0.4527863777089783 -> 0.47042042042042037 on epoch=249, global_step=750
03/02/2022 04:27:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=253
03/02/2022 04:27:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
03/02/2022 04:27:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=259
03/02/2022 04:27:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=263
03/02/2022 04:27:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=266
03/02/2022 04:27:37 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.3650793650793651 on epoch=266
03/02/2022 04:27:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=269
03/02/2022 04:27:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=273
03/02/2022 04:27:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=276
03/02/2022 04:27:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=279
03/02/2022 04:27:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=283
03/02/2022 04:27:48 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.42791180629197595 on epoch=283
03/02/2022 04:27:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=286
03/02/2022 04:27:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=289
03/02/2022 04:27:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=293
03/02/2022 04:27:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=296
03/02/2022 04:27:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=299
03/02/2022 04:28:00 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.43738977072310403 on epoch=299
03/02/2022 04:28:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=303
03/02/2022 04:28:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=306
03/02/2022 04:28:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=309
03/02/2022 04:28:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=313
03/02/2022 04:28:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=316
03/02/2022 04:28:12 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.4611090512091624 on epoch=316
03/02/2022 04:28:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=319
03/02/2022 04:28:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=323
03/02/2022 04:28:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=326
03/02/2022 04:28:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
03/02/2022 04:28:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=333
03/02/2022 04:28:24 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.5095238095238095 on epoch=333
03/02/2022 04:28:24 - INFO - __main__ - Saving model with best Classification-F1: 0.47042042042042037 -> 0.5095238095238095 on epoch=333, global_step=1000
03/02/2022 04:28:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=336
03/02/2022 04:28:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=339
03/02/2022 04:28:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=343
03/02/2022 04:28:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
03/02/2022 04:28:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
03/02/2022 04:28:35 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.3493202293202293 on epoch=349
03/02/2022 04:28:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
03/02/2022 04:28:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
03/02/2022 04:28:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
03/02/2022 04:28:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=363
03/02/2022 04:28:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=366
03/02/2022 04:28:47 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.31392621703853957 on epoch=366
03/02/2022 04:28:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=369
03/02/2022 04:28:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
03/02/2022 04:28:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=376
03/02/2022 04:28:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=379
03/02/2022 04:28:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
03/02/2022 04:28:58 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.281184668989547 on epoch=383
03/02/2022 04:29:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
03/02/2022 04:29:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=389
03/02/2022 04:29:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
03/02/2022 04:29:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
03/02/2022 04:29:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=399
03/02/2022 04:29:10 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.44376985681333503 on epoch=399
03/02/2022 04:29:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=403
03/02/2022 04:29:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=406
03/02/2022 04:29:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=409
03/02/2022 04:29:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
03/02/2022 04:29:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
03/02/2022 04:29:22 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.4037487335359676 on epoch=416
03/02/2022 04:29:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
03/02/2022 04:29:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
03/02/2022 04:29:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=426
03/02/2022 04:29:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
03/02/2022 04:29:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
03/02/2022 04:29:33 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.33292633292633295 on epoch=433
03/02/2022 04:29:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=436
03/02/2022 04:29:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=439
03/02/2022 04:29:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
03/02/2022 04:29:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
03/02/2022 04:29:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
03/02/2022 04:29:45 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.31159636062861873 on epoch=449
03/02/2022 04:29:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=453
03/02/2022 04:29:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=456
03/02/2022 04:29:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
03/02/2022 04:29:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
03/02/2022 04:29:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
03/02/2022 04:29:56 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.34462771595124536 on epoch=466
03/02/2022 04:29:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/02/2022 04:30:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
03/02/2022 04:30:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=476
03/02/2022 04:30:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
03/02/2022 04:30:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
03/02/2022 04:30:08 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.33461538461538465 on epoch=483
03/02/2022 04:30:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
03/02/2022 04:30:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
03/02/2022 04:30:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=493
03/02/2022 04:30:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=496
03/02/2022 04:30:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/02/2022 04:30:20 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.45082574991573976 on epoch=499
03/02/2022 04:30:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
03/02/2022 04:30:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
03/02/2022 04:30:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
03/02/2022 04:30:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
03/02/2022 04:30:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=516
03/02/2022 04:30:31 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.43496367180577705 on epoch=516
03/02/2022 04:30:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=519
03/02/2022 04:30:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
03/02/2022 04:30:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
03/02/2022 04:30:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
03/02/2022 04:30:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=533
03/02/2022 04:30:43 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.3869565217391304 on epoch=533
03/02/2022 04:30:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
03/02/2022 04:30:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
03/02/2022 04:30:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=543
03/02/2022 04:30:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
03/02/2022 04:30:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
03/02/2022 04:30:54 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.5202175697865353 on epoch=549
03/02/2022 04:30:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5095238095238095 -> 0.5202175697865353 on epoch=549, global_step=1650
03/02/2022 04:30:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
03/02/2022 04:30:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/02/2022 04:31:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
03/02/2022 04:31:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
03/02/2022 04:31:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/02/2022 04:31:06 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.3066838673686412 on epoch=566
03/02/2022 04:31:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/02/2022 04:31:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/02/2022 04:31:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=576
03/02/2022 04:31:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
03/02/2022 04:31:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=583
03/02/2022 04:31:18 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.4074074074074074 on epoch=583
03/02/2022 04:31:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=586
03/02/2022 04:31:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=589
03/02/2022 04:31:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
03/02/2022 04:31:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/02/2022 04:31:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
03/02/2022 04:31:29 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.29296890016530197 on epoch=599
03/02/2022 04:31:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
03/02/2022 04:31:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
03/02/2022 04:31:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/02/2022 04:31:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
03/02/2022 04:31:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
03/02/2022 04:31:41 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.5177066467389049 on epoch=616
03/02/2022 04:31:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
03/02/2022 04:31:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
03/02/2022 04:31:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
03/02/2022 04:31:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 04:31:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
03/02/2022 04:31:53 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.3087114337568058 on epoch=633
03/02/2022 04:31:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/02/2022 04:31:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
03/02/2022 04:31:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
03/02/2022 04:32:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/02/2022 04:32:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
03/02/2022 04:32:04 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.449908555171713 on epoch=649
03/02/2022 04:32:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=653
03/02/2022 04:32:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/02/2022 04:32:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
03/02/2022 04:32:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/02/2022 04:32:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
03/02/2022 04:32:16 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.292601683029453 on epoch=666
03/02/2022 04:32:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/02/2022 04:32:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/02/2022 04:32:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=676
03/02/2022 04:32:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
03/02/2022 04:32:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/02/2022 04:32:28 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.4637898768333551 on epoch=683
03/02/2022 04:32:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
03/02/2022 04:32:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
03/02/2022 04:32:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 04:32:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/02/2022 04:32:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/02/2022 04:32:39 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.3674747474747475 on epoch=699
03/02/2022 04:32:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
03/02/2022 04:32:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
03/02/2022 04:32:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
03/02/2022 04:32:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=713
03/02/2022 04:32:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 04:32:51 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.46507177033492814 on epoch=716
03/02/2022 04:32:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 04:32:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=723
03/02/2022 04:32:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
03/02/2022 04:32:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
03/02/2022 04:33:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 04:33:02 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.4424010217113666 on epoch=733
03/02/2022 04:33:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/02/2022 04:33:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
03/02/2022 04:33:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/02/2022 04:33:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
03/02/2022 04:33:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/02/2022 04:33:14 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.41813741813741806 on epoch=749
03/02/2022 04:33:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/02/2022 04:33:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
03/02/2022 04:33:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/02/2022 04:33:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/02/2022 04:33:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=766
03/02/2022 04:33:26 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.4371850371850372 on epoch=766
03/02/2022 04:33:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
03/02/2022 04:33:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
03/02/2022 04:33:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=776
03/02/2022 04:33:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
03/02/2022 04:33:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/02/2022 04:33:38 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.4002397297591805 on epoch=783
03/02/2022 04:33:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/02/2022 04:33:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/02/2022 04:33:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
03/02/2022 04:33:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
03/02/2022 04:33:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/02/2022 04:33:49 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5237889983579639 on epoch=799
03/02/2022 04:33:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5202175697865353 -> 0.5237889983579639 on epoch=799, global_step=2400
03/02/2022 04:33:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=803
03/02/2022 04:33:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 04:33:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/02/2022 04:33:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/02/2022 04:34:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
03/02/2022 04:34:01 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.45980167810831424 on epoch=816
03/02/2022 04:34:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/02/2022 04:34:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/02/2022 04:34:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/02/2022 04:34:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=829
03/02/2022 04:34:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
03/02/2022 04:34:13 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.4948542203777066 on epoch=833
03/02/2022 04:34:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/02/2022 04:34:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/02/2022 04:34:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/02/2022 04:34:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/02/2022 04:34:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
03/02/2022 04:34:24 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3721417069243156 on epoch=849
03/02/2022 04:34:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
03/02/2022 04:34:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/02/2022 04:34:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/02/2022 04:34:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 04:34:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 04:34:36 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.48039215686274517 on epoch=866
03/02/2022 04:34:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/02/2022 04:34:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 04:34:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
03/02/2022 04:34:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 04:34:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/02/2022 04:34:48 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.39898989898989895 on epoch=883
03/02/2022 04:34:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 04:34:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 04:34:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=893
03/02/2022 04:34:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/02/2022 04:34:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/02/2022 04:34:59 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.47027540360873693 on epoch=899
03/02/2022 04:35:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/02/2022 04:35:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/02/2022 04:35:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=909
03/02/2022 04:35:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 04:35:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/02/2022 04:35:11 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2875 on epoch=916
03/02/2022 04:35:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 04:35:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/02/2022 04:35:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/02/2022 04:35:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/02/2022 04:35:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
03/02/2022 04:35:23 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.28967559563871315 on epoch=933
03/02/2022 04:35:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/02/2022 04:35:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/02/2022 04:35:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
03/02/2022 04:35:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/02/2022 04:35:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/02/2022 04:35:35 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.2894475357710652 on epoch=949
03/02/2022 04:35:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/02/2022 04:35:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
03/02/2022 04:35:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
03/02/2022 04:35:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
03/02/2022 04:35:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
03/02/2022 04:35:46 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.24786585365853658 on epoch=966
03/02/2022 04:35:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/02/2022 04:35:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/02/2022 04:35:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/02/2022 04:35:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 04:35:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 04:35:59 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.26521739130434785 on epoch=983
03/02/2022 04:36:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
03/02/2022 04:36:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/02/2022 04:36:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 04:36:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/02/2022 04:36:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 04:36:10 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.33571428571428574 on epoch=999
03/02/2022 04:36:10 - INFO - __main__ - save last model!
03/02/2022 04:36:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:36:10 - INFO - __main__ - Printing 3 examples
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:36:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:36:10 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:36:10 - INFO - __main__ - Printing 3 examples
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 04:36:10 - INFO - __main__ - ['normal']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 04:36:10 - INFO - __main__ - ['normal']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 04:36:10 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:36:10 - INFO - __main__ - ['normal']
03/02/2022 04:36:10 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:36:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:36:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:36:10 - INFO - __main__ - Printing 3 examples
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:36:10 - INFO - __main__ - ['offensive']
03/02/2022 04:36:10 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:36:11 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:36:11 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:36:11 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:36:13 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 04:36:23 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:36:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:36:24 - INFO - __main__ - Starting training!
03/02/2022 04:36:59 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_87_0.3_8_predictions.txt
03/02/2022 04:36:59 - INFO - __main__ - Classification-F1 on test data: 0.2219
03/02/2022 04:36:59 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.3, bsz=8, dev_performance=0.5237889983579639, test_performance=0.22194232300076636
03/02/2022 04:36:59 - INFO - __main__ - Running ... prefix=hatexplain_16_87, lr=0.2, bsz=8 ...
03/02/2022 04:37:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:37:00 - INFO - __main__ - Printing 3 examples
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] they replaced the jew mayor with a sheboon mayor they have not learned yet
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] all those poor police officers and social workers i mean they would have been raked over the coals and called some awful things if they d actually protected those vulnerable young girls who were raped and killed by muslim savages
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] <user> whatchu are <user> is a self hating homosexual who has to pay for your homosexual desires
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:37:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:37:00 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/02/2022 04:37:00 - INFO - __main__ - Start tokenizing ... 48 instances
03/02/2022 04:37:00 - INFO - __main__ - Printing 3 examples
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] well he is a retarded duck and he will get his ass kicked
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] <user> you must be retarded
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ -  [hatexplain] we know u were at chippendales u queer homo 🙄
03/02/2022 04:37:00 - INFO - __main__ - ['offensive']
03/02/2022 04:37:00 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:37:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:37:01 - INFO - __main__ - Loaded 48 examples from dev data
03/02/2022 04:37:13 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:37:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:37:14 - INFO - __main__ - Starting training!
03/02/2022 04:37:17 - INFO - __main__ - Step 10 Global step 10 Train loss 3.62 on epoch=3
03/02/2022 04:37:19 - INFO - __main__ - Step 20 Global step 20 Train loss 2.16 on epoch=6
03/02/2022 04:37:21 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=9
03/02/2022 04:37:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.88 on epoch=13
03/02/2022 04:37:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=16
03/02/2022 04:37:26 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.23979107312440648 on epoch=16
03/02/2022 04:37:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23979107312440648 on epoch=16, global_step=50
03/02/2022 04:37:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=19
03/02/2022 04:37:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=23
03/02/2022 04:37:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=26
03/02/2022 04:37:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
03/02/2022 04:37:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=33
03/02/2022 04:37:38 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=33
03/02/2022 04:37:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
03/02/2022 04:37:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=39
03/02/2022 04:37:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=43
03/02/2022 04:37:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=46
03/02/2022 04:37:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=49
03/02/2022 04:37:49 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=49
03/02/2022 04:37:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
03/02/2022 04:37:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
03/02/2022 04:37:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=59
03/02/2022 04:37:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
03/02/2022 04:37:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=66
03/02/2022 04:38:01 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.1983273596176822 on epoch=66
03/02/2022 04:38:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=69
03/02/2022 04:38:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
03/02/2022 04:38:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/02/2022 04:38:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
03/02/2022 04:38:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
03/02/2022 04:38:12 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.23410986482599946 on epoch=83
03/02/2022 04:38:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=86
03/02/2022 04:38:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=89
03/02/2022 04:38:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
03/02/2022 04:38:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
03/02/2022 04:38:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=99
03/02/2022 04:38:24 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2474747474747475 on epoch=99
03/02/2022 04:38:24 - INFO - __main__ - Saving model with best Classification-F1: 0.23979107312440648 -> 0.2474747474747475 on epoch=99, global_step=300
03/02/2022 04:38:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=103
03/02/2022 04:38:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/02/2022 04:38:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/02/2022 04:38:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
03/02/2022 04:38:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=116
03/02/2022 04:38:35 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.33495024025870507 on epoch=116
03/02/2022 04:38:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2474747474747475 -> 0.33495024025870507 on epoch=116, global_step=350
03/02/2022 04:38:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
03/02/2022 04:38:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
03/02/2022 04:38:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
03/02/2022 04:38:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
03/02/2022 04:38:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
03/02/2022 04:38:47 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.4095853858784893 on epoch=133
03/02/2022 04:38:47 - INFO - __main__ - Saving model with best Classification-F1: 0.33495024025870507 -> 0.4095853858784893 on epoch=133, global_step=400
03/02/2022 04:38:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=136
03/02/2022 04:38:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=139
03/02/2022 04:38:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
03/02/2022 04:38:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/02/2022 04:38:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
03/02/2022 04:38:59 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.2677118327734229 on epoch=149
03/02/2022 04:39:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=153
03/02/2022 04:39:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
03/02/2022 04:39:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=159
03/02/2022 04:39:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
03/02/2022 04:39:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=166
03/02/2022 04:39:10 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.289480167528948 on epoch=166
03/02/2022 04:39:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
03/02/2022 04:39:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=173
03/02/2022 04:39:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=176
03/02/2022 04:39:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
03/02/2022 04:39:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=183
03/02/2022 04:39:22 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.3002244668911335 on epoch=183
03/02/2022 04:39:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=186
03/02/2022 04:39:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=189
03/02/2022 04:39:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=193
03/02/2022 04:39:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=196
03/02/2022 04:39:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=199
03/02/2022 04:39:34 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.3657363657363657 on epoch=199
03/02/2022 04:39:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
03/02/2022 04:39:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=206
03/02/2022 04:39:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=209
03/02/2022 04:39:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=213
03/02/2022 04:39:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=216
03/02/2022 04:39:45 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.40997606262534775 on epoch=216
03/02/2022 04:39:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4095853858784893 -> 0.40997606262534775 on epoch=216, global_step=650
03/02/2022 04:39:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=219
03/02/2022 04:39:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=223
03/02/2022 04:39:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=226
03/02/2022 04:39:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
03/02/2022 04:39:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=233
03/02/2022 04:39:57 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.36807487438269604 on epoch=233
03/02/2022 04:39:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=236
03/02/2022 04:40:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=239
03/02/2022 04:40:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=243
03/02/2022 04:40:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=246
03/02/2022 04:40:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
03/02/2022 04:40:09 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.4275712802844585 on epoch=249
03/02/2022 04:40:09 - INFO - __main__ - Saving model with best Classification-F1: 0.40997606262534775 -> 0.4275712802844585 on epoch=249, global_step=750
03/02/2022 04:40:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=253
03/02/2022 04:40:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=256
03/02/2022 04:40:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=259
03/02/2022 04:40:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=263
03/02/2022 04:40:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
03/02/2022 04:40:21 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.4004273504273504 on epoch=266
03/02/2022 04:40:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
03/02/2022 04:40:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
03/02/2022 04:40:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=276
03/02/2022 04:40:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=279
03/02/2022 04:40:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
03/02/2022 04:40:32 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.432146829810901 on epoch=283
03/02/2022 04:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4275712802844585 -> 0.432146829810901 on epoch=283, global_step=850
03/02/2022 04:40:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
03/02/2022 04:40:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=289
03/02/2022 04:40:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=293
03/02/2022 04:40:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
03/02/2022 04:40:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=299
03/02/2022 04:40:44 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.41818181818181815 on epoch=299
03/02/2022 04:40:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=303
03/02/2022 04:40:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
03/02/2022 04:40:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=309
03/02/2022 04:40:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=313
03/02/2022 04:40:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=316
03/02/2022 04:40:56 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.4341928135031583 on epoch=316
03/02/2022 04:40:56 - INFO - __main__ - Saving model with best Classification-F1: 0.432146829810901 -> 0.4341928135031583 on epoch=316, global_step=950
03/02/2022 04:40:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=319
03/02/2022 04:41:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=323
03/02/2022 04:41:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=326
03/02/2022 04:41:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
03/02/2022 04:41:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=333
03/02/2022 04:41:07 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.4808080808080808 on epoch=333
03/02/2022 04:41:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4341928135031583 -> 0.4808080808080808 on epoch=333, global_step=1000
03/02/2022 04:41:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=336
03/02/2022 04:41:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=339
03/02/2022 04:41:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=343
03/02/2022 04:41:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=346
03/02/2022 04:41:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=349
03/02/2022 04:41:19 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.4791666666666667 on epoch=349
03/02/2022 04:41:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=353
03/02/2022 04:41:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=356
03/02/2022 04:41:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=359
03/02/2022 04:41:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
03/02/2022 04:41:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=366
03/02/2022 04:41:31 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.4781484196806778 on epoch=366
03/02/2022 04:41:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=369
03/02/2022 04:41:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
03/02/2022 04:41:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=376
03/02/2022 04:41:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=379
03/02/2022 04:41:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
03/02/2022 04:41:42 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.4996004671461061 on epoch=383
03/02/2022 04:41:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4808080808080808 -> 0.4996004671461061 on epoch=383, global_step=1150
03/02/2022 04:41:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
03/02/2022 04:41:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=389
03/02/2022 04:41:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=393
03/02/2022 04:41:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=396
03/02/2022 04:41:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=399
03/02/2022 04:41:54 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.4381598793363499 on epoch=399
03/02/2022 04:41:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=403
03/02/2022 04:41:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
03/02/2022 04:42:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=409
03/02/2022 04:42:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=413
03/02/2022 04:42:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=416
03/02/2022 04:42:06 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.39618406285072943 on epoch=416
03/02/2022 04:42:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=419
03/02/2022 04:42:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=423
03/02/2022 04:42:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=426
03/02/2022 04:42:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=429
03/02/2022 04:42:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=433
03/02/2022 04:42:18 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.45303776683087027 on epoch=433
03/02/2022 04:42:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
03/02/2022 04:42:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=439
03/02/2022 04:42:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=443
03/02/2022 04:42:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=446
03/02/2022 04:42:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=449
03/02/2022 04:42:29 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.19903149386845037 on epoch=449
03/02/2022 04:42:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=453
03/02/2022 04:42:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=456
03/02/2022 04:42:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=459
03/02/2022 04:42:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=463
03/02/2022 04:42:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
03/02/2022 04:42:41 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.478505291005291 on epoch=466
03/02/2022 04:42:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=469
03/02/2022 04:42:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=473
03/02/2022 04:42:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
03/02/2022 04:42:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=479
03/02/2022 04:42:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=483
03/02/2022 04:42:52 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.43091880341880345 on epoch=483
03/02/2022 04:42:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=486
03/02/2022 04:42:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=489
03/02/2022 04:42:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=493
03/02/2022 04:43:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
03/02/2022 04:43:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=499
03/02/2022 04:43:04 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.47559966914805624 on epoch=499
03/02/2022 04:43:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
03/02/2022 04:43:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=506
03/02/2022 04:43:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=509
03/02/2022 04:43:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=513
03/02/2022 04:43:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
03/02/2022 04:43:16 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.394422374066423 on epoch=516
03/02/2022 04:43:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=519
03/02/2022 04:43:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=523
03/02/2022 04:43:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=526
03/02/2022 04:43:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
03/02/2022 04:43:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=533
03/02/2022 04:43:27 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.5428366174055829 on epoch=533
03/02/2022 04:43:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4996004671461061 -> 0.5428366174055829 on epoch=533, global_step=1600
03/02/2022 04:43:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=536
03/02/2022 04:43:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=539
03/02/2022 04:43:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
03/02/2022 04:43:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=546
03/02/2022 04:43:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
03/02/2022 04:43:39 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.45201465201465196 on epoch=549
03/02/2022 04:43:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=553
03/02/2022 04:43:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
03/02/2022 04:43:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=559
03/02/2022 04:43:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
03/02/2022 04:43:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=566
03/02/2022 04:43:51 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.5008658008658008 on epoch=566
03/02/2022 04:43:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=569
03/02/2022 04:43:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=573
03/02/2022 04:43:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=576
03/02/2022 04:43:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=579
03/02/2022 04:44:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
03/02/2022 04:44:03 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.4296814296814297 on epoch=583
03/02/2022 04:44:05 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
03/02/2022 04:44:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=589
03/02/2022 04:44:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=593
03/02/2022 04:44:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/02/2022 04:44:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
03/02/2022 04:44:14 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.5238479859169515 on epoch=599
03/02/2022 04:44:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=603
03/02/2022 04:44:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=606
03/02/2022 04:44:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=609
03/02/2022 04:44:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
03/02/2022 04:44:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=616
03/02/2022 04:44:26 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.5008718395815169 on epoch=616
03/02/2022 04:44:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=619
03/02/2022 04:44:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
03/02/2022 04:44:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=626
03/02/2022 04:44:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
03/02/2022 04:44:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=633
03/02/2022 04:44:37 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.45895895895895894 on epoch=633
03/02/2022 04:44:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
03/02/2022 04:44:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
03/02/2022 04:44:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
03/02/2022 04:44:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
03/02/2022 04:44:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=649
03/02/2022 04:44:49 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.45895895895895894 on epoch=649
03/02/2022 04:44:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
03/02/2022 04:44:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=656
03/02/2022 04:44:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
03/02/2022 04:44:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=663
03/02/2022 04:44:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=666
03/02/2022 04:45:01 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.33076923076923076 on epoch=666
03/02/2022 04:45:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
03/02/2022 04:45:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
03/02/2022 04:45:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
03/02/2022 04:45:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=679
03/02/2022 04:45:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=683
03/02/2022 04:45:12 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.30420054200542 on epoch=683
03/02/2022 04:45:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
03/02/2022 04:45:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
03/02/2022 04:45:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/02/2022 04:45:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
03/02/2022 04:45:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/02/2022 04:45:24 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.2939227642276423 on epoch=699
03/02/2022 04:45:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=703
03/02/2022 04:45:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
03/02/2022 04:45:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=709
03/02/2022 04:45:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
03/02/2022 04:45:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/02/2022 04:45:36 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.2990956072351421 on epoch=716
03/02/2022 04:45:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
03/02/2022 04:45:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
03/02/2022 04:45:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/02/2022 04:45:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/02/2022 04:45:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/02/2022 04:45:47 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.3125 on epoch=733
03/02/2022 04:45:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
03/02/2022 04:45:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/02/2022 04:45:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
03/02/2022 04:45:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=746
03/02/2022 04:45:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
03/02/2022 04:45:59 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.3647052675585284 on epoch=749
03/02/2022 04:46:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
03/02/2022 04:46:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=756
03/02/2022 04:46:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
03/02/2022 04:46:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
03/02/2022 04:46:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=766
03/02/2022 04:46:11 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.3684210526315789 on epoch=766
03/02/2022 04:46:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/02/2022 04:46:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
03/02/2022 04:46:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/02/2022 04:46:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
03/02/2022 04:46:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
03/02/2022 04:46:22 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.352883387366146 on epoch=783
03/02/2022 04:46:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
03/02/2022 04:46:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
03/02/2022 04:46:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=793
03/02/2022 04:46:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/02/2022 04:46:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=799
03/02/2022 04:46:34 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.3799549549549549 on epoch=799
03/02/2022 04:46:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/02/2022 04:46:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
03/02/2022 04:46:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
03/02/2022 04:46:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
03/02/2022 04:46:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=816
03/02/2022 04:46:46 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.33630952380952384 on epoch=816
03/02/2022 04:46:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
03/02/2022 04:46:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/02/2022 04:46:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
03/02/2022 04:46:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/02/2022 04:46:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
03/02/2022 04:46:57 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.3589507918552036 on epoch=833
03/02/2022 04:46:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
03/02/2022 04:47:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=839
03/02/2022 04:47:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
03/02/2022 04:47:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
03/02/2022 04:47:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/02/2022 04:47:09 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.30420054200542 on epoch=849
03/02/2022 04:47:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
03/02/2022 04:47:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/02/2022 04:47:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
03/02/2022 04:47:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
03/02/2022 04:47:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
03/02/2022 04:47:21 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.47852965747702586 on epoch=866
03/02/2022 04:47:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
03/02/2022 04:47:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/02/2022 04:47:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/02/2022 04:47:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/02/2022 04:47:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
03/02/2022 04:47:32 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.29464285714285715 on epoch=883
03/02/2022 04:47:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/02/2022 04:47:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
03/02/2022 04:47:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/02/2022 04:47:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=896
03/02/2022 04:47:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
03/02/2022 04:47:44 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.36658986175115205 on epoch=899
03/02/2022 04:47:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/02/2022 04:47:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=906
03/02/2022 04:47:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
03/02/2022 04:47:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/02/2022 04:47:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/02/2022 04:47:55 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.27131077318483615 on epoch=916
03/02/2022 04:47:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/02/2022 04:48:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/02/2022 04:48:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=926
03/02/2022 04:48:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
03/02/2022 04:48:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=933
03/02/2022 04:48:07 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.3041489765627696 on epoch=933
03/02/2022 04:48:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=936
03/02/2022 04:48:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
03/02/2022 04:48:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
03/02/2022 04:48:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
03/02/2022 04:48:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
03/02/2022 04:48:19 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.2658119658119658 on epoch=949
03/02/2022 04:48:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
03/02/2022 04:48:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
03/02/2022 04:48:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=959
03/02/2022 04:48:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=963
03/02/2022 04:48:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/02/2022 04:48:30 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.4550807217473884 on epoch=966
03/02/2022 04:48:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
03/02/2022 04:48:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/02/2022 04:48:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
03/02/2022 04:48:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/02/2022 04:48:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/02/2022 04:48:42 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.4237071172555043 on epoch=983
03/02/2022 04:48:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
03/02/2022 04:48:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
03/02/2022 04:48:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/02/2022 04:48:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/02/2022 04:48:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/02/2022 04:48:54 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.49725829725829723 on epoch=999
03/02/2022 04:48:54 - INFO - __main__ - save last model!
03/02/2022 04:48:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 04:48:54 - INFO - __main__ - Start tokenizing ... 1922 instances
03/02/2022 04:48:54 - INFO - __main__ - Printing 3 examples
03/02/2022 04:48:54 - INFO - __main__ -  [hatexplain] me getting books from the library about queer cowboys and homoeroticism in cinema instead of anything useful for my thesis
03/02/2022 04:48:54 - INFO - __main__ - ['normal']
03/02/2022 04:48:54 - INFO - __main__ -  [hatexplain] y si fuera top <number> me faltarían how to get away with murder gossip girl the last ship orphan black downton abbey
03/02/2022 04:48:54 - INFO - __main__ - ['normal']
03/02/2022 04:48:54 - INFO - __main__ -  [hatexplain] <user> <user> <user> i am a lesbian not queer and quite frankly his disco tits video was repulsive if he thinks being an overweight gay bloke brings out the woman in him then i am offended it pathetic attention seeking
03/02/2022 04:48:54 - INFO - __main__ - ['normal']
03/02/2022 04:48:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:48:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:48:56 - INFO - __main__ - Loaded 1922 examples from test data
03/02/2022 04:49:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-hatexplain/hatexplain_16_87_0.2_8_predictions.txt
03/02/2022 04:49:44 - INFO - __main__ - Classification-F1 on test data: 0.2186
03/02/2022 04:49:45 - INFO - __main__ - prefix=hatexplain_16_87, lr=0.2, bsz=8, dev_performance=0.5428366174055829, test_performance=0.21855253883162706
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0027468204498291016 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2765", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15771, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2766", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15771, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 15771, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (3452): No such process
kill: write error: Disk quota exceeded
Task: circa, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_7mp8nk5a/none_m7q_f_od
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_7mp8nk5a/none_m7q_f_od/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_7mp8nk5a/none_m7q_f_od/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/02/2022 04:49:50 - INFO - __main__ - Namespace(task_dir='data/circa/', task_name='circa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 04:49:50 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa
03/02/2022 04:49:50 - INFO - __main__ - Namespace(task_dir='data/circa/', task_name='circa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 04:49:50 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa
03/02/2022 04:49:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 04:49:52 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 04:49:52 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/02/2022 04:49:52 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/02/2022 04:49:52 - INFO - __main__ - args.device: cuda:0
03/02/2022 04:49:52 - INFO - __main__ - args.device: cuda:1
03/02/2022 04:49:52 - INFO - __main__ - Using 2 gpus
03/02/2022 04:49:52 - INFO - __main__ - Using 2 gpus
03/02/2022 04:49:52 - INFO - __main__ - Fine-tuning the following samples: ['circa_16_100', 'circa_16_13', 'circa_16_21', 'circa_16_42', 'circa_16_87']
03/02/2022 04:49:52 - INFO - __main__ - Fine-tuning the following samples: ['circa_16_100', 'circa_16_13', 'circa_16_21', 'circa_16_42', 'circa_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/02/2022 04:49:57 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.5, bsz=8 ...
03/02/2022 04:49:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 04:49:58 - INFO - __main__ - Printing 3 examples
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:49:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 04:49:58 - INFO - __main__ - Printing 3 examples
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:49:58 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 04:49:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 04:49:58 - INFO - __main__ - Printing 3 examples
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:49:58 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 04:49:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 04:49:58 - INFO - __main__ - Printing 3 examples
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 04:49:58 - INFO - __main__ - ['No']
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Input ...
03/02/2022 04:49:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 04:49:58 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 04:49:58 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 04:50:13 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:50:13 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 04:50:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:50:14 - INFO - __main__ - Starting training!
03/02/2022 04:50:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 04:50:23 - INFO - __main__ - Starting training!
03/02/2022 04:50:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.41 on epoch=1
03/02/2022 04:50:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=3
03/02/2022 04:50:30 - INFO - __main__ - Step 30 Global step 30 Train loss 1.57 on epoch=5
03/02/2022 04:50:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=7
03/02/2022 04:50:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=9
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/02/2022 04:50:36 - INFO - __main__ - Global step 50 Train loss 1.86 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 04:50:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 04:50:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.72 on epoch=11
03/02/2022 04:50:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=13
03/02/2022 04:50:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=15
03/02/2022 04:50:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=17
03/02/2022 04:50:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=19
03/02/2022 04:50:49 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.10476190476190476 on epoch=19
03/02/2022 04:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.10476190476190476 on epoch=19, global_step=100
03/02/2022 04:50:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=21
03/02/2022 04:50:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=23
03/02/2022 04:50:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=25
03/02/2022 04:50:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=27
03/02/2022 04:51:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=29
03/02/2022 04:51:03 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.06363636363636363 on epoch=29
03/02/2022 04:51:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=31
03/02/2022 04:51:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=33
03/02/2022 04:51:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=35
03/02/2022 04:51:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=37
03/02/2022 04:51:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=39
03/02/2022 04:51:16 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.14751131221719457 on epoch=39
03/02/2022 04:51:16 - INFO - __main__ - Saving model with best Classification-F1: 0.10476190476190476 -> 0.14751131221719457 on epoch=39, global_step=200
03/02/2022 04:51:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=41
03/02/2022 04:51:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=43
03/02/2022 04:51:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=45
03/02/2022 04:51:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=47
03/02/2022 04:51:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=49
03/02/2022 04:51:30 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.17020626432391137 on epoch=49
03/02/2022 04:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.14751131221719457 -> 0.17020626432391137 on epoch=49, global_step=250
03/02/2022 04:51:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=51
03/02/2022 04:51:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=53
03/02/2022 04:51:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=55
03/02/2022 04:51:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=57
03/02/2022 04:51:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=59
03/02/2022 04:51:43 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.14423897581792317 on epoch=59
03/02/2022 04:51:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=61
03/02/2022 04:51:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=63
03/02/2022 04:51:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=65
03/02/2022 04:51:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=67
03/02/2022 04:51:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=69
03/02/2022 04:51:56 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.1306252489048188 on epoch=69
03/02/2022 04:51:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=71
03/02/2022 04:52:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=73
03/02/2022 04:52:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=75
03/02/2022 04:52:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=77
03/02/2022 04:52:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=79
03/02/2022 04:52:10 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.1479492600422833 on epoch=79
03/02/2022 04:52:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=81
03/02/2022 04:52:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=83
03/02/2022 04:52:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=85
03/02/2022 04:52:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=87
03/02/2022 04:52:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=89
03/02/2022 04:52:24 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.24836601307189543 on epoch=89
03/02/2022 04:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.17020626432391137 -> 0.24836601307189543 on epoch=89, global_step=450
03/02/2022 04:52:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=91
03/02/2022 04:52:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=93
03/02/2022 04:52:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=95
03/02/2022 04:52:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=97
03/02/2022 04:52:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=99
03/02/2022 04:52:37 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.16556776556776556 on epoch=99
03/02/2022 04:52:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=101
03/02/2022 04:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=103
03/02/2022 04:52:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=105
03/02/2022 04:52:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=107
03/02/2022 04:52:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=109
03/02/2022 04:52:51 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.32091081593927895 on epoch=109
03/02/2022 04:52:51 - INFO - __main__ - Saving model with best Classification-F1: 0.24836601307189543 -> 0.32091081593927895 on epoch=109, global_step=550
03/02/2022 04:52:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=111
03/02/2022 04:52:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=113
03/02/2022 04:52:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=115
03/02/2022 04:53:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=117
03/02/2022 04:53:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=119
03/02/2022 04:53:05 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.34292989923242023 on epoch=119
03/02/2022 04:53:05 - INFO - __main__ - Saving model with best Classification-F1: 0.32091081593927895 -> 0.34292989923242023 on epoch=119, global_step=600
03/02/2022 04:53:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=121
03/02/2022 04:53:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=123
03/02/2022 04:53:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=125
03/02/2022 04:53:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=127
03/02/2022 04:53:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=129
03/02/2022 04:53:19 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.3471362508459283 on epoch=129
03/02/2022 04:53:19 - INFO - __main__ - Saving model with best Classification-F1: 0.34292989923242023 -> 0.3471362508459283 on epoch=129, global_step=650
03/02/2022 04:53:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=131
03/02/2022 04:53:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=133
03/02/2022 04:53:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=135
03/02/2022 04:53:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=137
03/02/2022 04:53:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=139
03/02/2022 04:53:32 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.3749290060851927 on epoch=139
03/02/2022 04:53:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3471362508459283 -> 0.3749290060851927 on epoch=139, global_step=700
03/02/2022 04:53:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=141
03/02/2022 04:53:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=143
03/02/2022 04:53:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=145
03/02/2022 04:53:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=147
03/02/2022 04:53:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=149
03/02/2022 04:53:46 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.4036612903225807 on epoch=149
03/02/2022 04:53:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3749290060851927 -> 0.4036612903225807 on epoch=149, global_step=750
03/02/2022 04:53:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=151
03/02/2022 04:53:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=153
03/02/2022 04:53:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=155
03/02/2022 04:53:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=157
03/02/2022 04:53:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=159
03/02/2022 04:53:59 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.3775217613927292 on epoch=159
03/02/2022 04:54:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=161
03/02/2022 04:54:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=163
03/02/2022 04:54:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=165
03/02/2022 04:54:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=167
03/02/2022 04:54:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=169
03/02/2022 04:54:13 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.3728435344554114 on epoch=169
03/02/2022 04:54:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=171
03/02/2022 04:54:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=173
03/02/2022 04:54:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=175
03/02/2022 04:54:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=177
03/02/2022 04:54:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=179
03/02/2022 04:54:27 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.49300865800865806 on epoch=179
03/02/2022 04:54:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4036612903225807 -> 0.49300865800865806 on epoch=179, global_step=900
03/02/2022 04:54:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=181
03/02/2022 04:54:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=183
03/02/2022 04:54:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=185
03/02/2022 04:54:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=187
03/02/2022 04:54:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=189
03/02/2022 04:54:41 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.49207459207459203 on epoch=189
03/02/2022 04:54:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=191
03/02/2022 04:54:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=193
03/02/2022 04:54:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=195
03/02/2022 04:54:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=197
03/02/2022 04:54:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=199
03/02/2022 04:54:54 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.42258750324086075 on epoch=199
03/02/2022 04:54:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=201
03/02/2022 04:54:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=203
03/02/2022 04:55:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=205
03/02/2022 04:55:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=207
03/02/2022 04:55:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=209
03/02/2022 04:55:07 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.39902248289345066 on epoch=209
03/02/2022 04:55:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=211
03/02/2022 04:55:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=213
03/02/2022 04:55:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=215
03/02/2022 04:55:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=217
03/02/2022 04:55:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=219
03/02/2022 04:55:20 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.38657194060419864 on epoch=219
03/02/2022 04:55:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=221
03/02/2022 04:55:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=223
03/02/2022 04:55:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=225
03/02/2022 04:55:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=227
03/02/2022 04:55:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=229
03/02/2022 04:55:33 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.35868842364532016 on epoch=229
03/02/2022 04:55:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=231
03/02/2022 04:55:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=233
03/02/2022 04:55:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=235
03/02/2022 04:55:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=237
03/02/2022 04:55:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=239
03/02/2022 04:55:46 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.3627991452991453 on epoch=239
03/02/2022 04:55:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=241
03/02/2022 04:55:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=243
03/02/2022 04:55:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=245
03/02/2022 04:55:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=247
03/02/2022 04:55:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=249
03/02/2022 04:55:59 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.4421212121212121 on epoch=249
03/02/2022 04:56:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=251
03/02/2022 04:56:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=253
03/02/2022 04:56:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=255
03/02/2022 04:56:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=257
03/02/2022 04:56:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=259
03/02/2022 04:56:14 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.4057730524642289 on epoch=259
03/02/2022 04:56:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=261
03/02/2022 04:56:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=263
03/02/2022 04:56:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=265
03/02/2022 04:56:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=267
03/02/2022 04:56:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=269
03/02/2022 04:56:27 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.5293288576825163 on epoch=269
03/02/2022 04:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.49300865800865806 -> 0.5293288576825163 on epoch=269, global_step=1350
03/02/2022 04:56:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=271
03/02/2022 04:56:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=273
03/02/2022 04:56:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=275
03/02/2022 04:56:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=277
03/02/2022 04:56:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=279
03/02/2022 04:56:41 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.4066126053742153 on epoch=279
03/02/2022 04:56:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=281
03/02/2022 04:56:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=283
03/02/2022 04:56:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=285
03/02/2022 04:56:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=287
03/02/2022 04:56:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=289
03/02/2022 04:56:55 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.42489375714775207 on epoch=289
03/02/2022 04:56:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=291
03/02/2022 04:56:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=293
03/02/2022 04:57:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=295
03/02/2022 04:57:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=297
03/02/2022 04:57:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=299
03/02/2022 04:57:09 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.4383559524791684 on epoch=299
03/02/2022 04:57:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=301
03/02/2022 04:57:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=303
03/02/2022 04:57:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=305
03/02/2022 04:57:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=307
03/02/2022 04:57:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=309
03/02/2022 04:57:23 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.5692526540396723 on epoch=309
03/02/2022 04:57:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5293288576825163 -> 0.5692526540396723 on epoch=309, global_step=1550
03/02/2022 04:57:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=311
03/02/2022 04:57:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=313
03/02/2022 04:57:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=315
03/02/2022 04:57:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=317
03/02/2022 04:57:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=319
03/02/2022 04:57:36 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.3334534993358522 on epoch=319
03/02/2022 04:57:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=321
03/02/2022 04:57:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=323
03/02/2022 04:57:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=325
03/02/2022 04:57:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=327
03/02/2022 04:57:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=329
03/02/2022 04:57:50 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.3328694454188209 on epoch=329
03/02/2022 04:57:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=331
03/02/2022 04:57:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=333
03/02/2022 04:57:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=335
03/02/2022 04:57:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=337
03/02/2022 04:58:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=339
03/02/2022 04:58:04 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.2840667501438688 on epoch=339
03/02/2022 04:58:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=341
03/02/2022 04:58:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=343
03/02/2022 04:58:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=345
03/02/2022 04:58:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=347
03/02/2022 04:58:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=349
03/02/2022 04:58:18 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.37757044368966114 on epoch=349
03/02/2022 04:58:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=351
03/02/2022 04:58:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=353
03/02/2022 04:58:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=355
03/02/2022 04:58:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=357
03/02/2022 04:58:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=359
03/02/2022 04:58:31 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.33196111536904377 on epoch=359
03/02/2022 04:58:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=361
03/02/2022 04:58:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=363
03/02/2022 04:58:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=365
03/02/2022 04:58:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=367
03/02/2022 04:58:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=369
03/02/2022 04:58:45 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.3331304239183498 on epoch=369
03/02/2022 04:58:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=371
03/02/2022 04:58:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=373
03/02/2022 04:58:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=375
03/02/2022 04:58:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=377
03/02/2022 04:58:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=379
03/02/2022 04:58:59 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.2520436507936508 on epoch=379
03/02/2022 04:59:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=381
03/02/2022 04:59:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=383
03/02/2022 04:59:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=385
03/02/2022 04:59:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=387
03/02/2022 04:59:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=389
03/02/2022 04:59:13 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.29386336483110675 on epoch=389
03/02/2022 04:59:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=391
03/02/2022 04:59:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=393
03/02/2022 04:59:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=395
03/02/2022 04:59:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=397
03/02/2022 04:59:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=399
03/02/2022 04:59:27 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.33529693646025177 on epoch=399
03/02/2022 04:59:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=401
03/02/2022 04:59:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=403
03/02/2022 04:59:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=405
03/02/2022 04:59:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=407
03/02/2022 04:59:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=409
03/02/2022 04:59:41 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.36480236390867493 on epoch=409
03/02/2022 04:59:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=411
03/02/2022 04:59:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=413
03/02/2022 04:59:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=415
03/02/2022 04:59:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=417
03/02/2022 04:59:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=419
03/02/2022 04:59:55 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.32460550202485683 on epoch=419
03/02/2022 04:59:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=421
03/02/2022 05:00:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=423
03/02/2022 05:00:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=425
03/02/2022 05:00:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=427
03/02/2022 05:00:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=429
03/02/2022 05:00:09 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.3174889135254989 on epoch=429
03/02/2022 05:00:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=431
03/02/2022 05:00:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=433
03/02/2022 05:00:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=435
03/02/2022 05:00:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=437
03/02/2022 05:00:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=439
03/02/2022 05:00:23 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.35360153958944285 on epoch=439
03/02/2022 05:00:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=441
03/02/2022 05:00:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=443
03/02/2022 05:00:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=445
03/02/2022 05:00:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=447
03/02/2022 05:00:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=449
03/02/2022 05:00:37 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.30518500643500646 on epoch=449
03/02/2022 05:00:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=451
03/02/2022 05:00:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=453
03/02/2022 05:00:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=455
03/02/2022 05:00:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=457
03/02/2022 05:00:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=459
03/02/2022 05:00:51 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.32511539325309924 on epoch=459
03/02/2022 05:00:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=461
03/02/2022 05:00:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=463
03/02/2022 05:00:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=465
03/02/2022 05:01:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=467
03/02/2022 05:01:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=469
03/02/2022 05:01:05 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.38947054562711525 on epoch=469
03/02/2022 05:01:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=471
03/02/2022 05:01:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=473
03/02/2022 05:01:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=475
03/02/2022 05:01:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=477
03/02/2022 05:01:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=479
03/02/2022 05:01:19 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.30613593922417454 on epoch=479
03/02/2022 05:01:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=481
03/02/2022 05:01:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=483
03/02/2022 05:01:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=485
03/02/2022 05:01:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 05:01:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=489
03/02/2022 05:01:33 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.33591678040769224 on epoch=489
03/02/2022 05:01:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=491
03/02/2022 05:01:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 05:01:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=495
03/02/2022 05:01:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 05:01:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=499
03/02/2022 05:01:47 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.36170860554162265 on epoch=499
03/02/2022 05:01:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=501
03/02/2022 05:01:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=503
03/02/2022 05:01:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=505
03/02/2022 05:01:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=507
03/02/2022 05:01:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=509
03/02/2022 05:02:01 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3924929865134429 on epoch=509
03/02/2022 05:02:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=511
03/02/2022 05:02:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=513
03/02/2022 05:02:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 05:02:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=517
03/02/2022 05:02:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=519
03/02/2022 05:02:15 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.32657575294813557 on epoch=519
03/02/2022 05:02:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=521
03/02/2022 05:02:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=523
03/02/2022 05:02:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=525
03/02/2022 05:02:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=527
03/02/2022 05:02:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=529
03/02/2022 05:02:29 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.3536153881615342 on epoch=529
03/02/2022 05:02:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 05:02:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 05:02:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=535
03/02/2022 05:02:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=537
03/02/2022 05:02:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=539
03/02/2022 05:02:43 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.3101052542229013 on epoch=539
03/02/2022 05:02:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=541
03/02/2022 05:02:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=543
03/02/2022 05:02:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=545
03/02/2022 05:02:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=547
03/02/2022 05:02:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 05:02:57 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3824249165739711 on epoch=549
03/02/2022 05:03:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 05:03:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 05:03:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=555
03/02/2022 05:03:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=557
03/02/2022 05:03:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=559
03/02/2022 05:03:11 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.3201826052058251 on epoch=559
03/02/2022 05:03:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=561
03/02/2022 05:03:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=563
03/02/2022 05:03:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=565
03/02/2022 05:03:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 05:03:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=569
03/02/2022 05:03:25 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.36576667060538026 on epoch=569
03/02/2022 05:03:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=571
03/02/2022 05:03:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=573
03/02/2022 05:03:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 05:03:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=577
03/02/2022 05:03:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=579
03/02/2022 05:03:39 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.37836472836472834 on epoch=579
03/02/2022 05:03:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=581
03/02/2022 05:03:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=583
03/02/2022 05:03:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 05:03:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 05:03:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=589
03/02/2022 05:03:53 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.41526718611847924 on epoch=589
03/02/2022 05:03:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 05:03:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=593
03/02/2022 05:04:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=595
03/02/2022 05:04:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=597
03/02/2022 05:04:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=599
03/02/2022 05:04:06 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:04:06 - INFO - __main__ - Printing 3 examples
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 05:04:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:04:06 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:04:06 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:04:06 - INFO - __main__ - Printing 3 examples
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:04:06 - INFO - __main__ - ['No']
03/02/2022 05:04:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:04:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:04:06 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:04:08 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.4580654283355106 on epoch=599
03/02/2022 05:04:08 - INFO - __main__ - save last model!
03/02/2022 05:04:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 05:04:08 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 05:04:08 - INFO - __main__ - Printing 3 examples
03/02/2022 05:04:08 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 05:04:08 - INFO - __main__ - ['No']
03/02/2022 05:04:08 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 05:04:08 - INFO - __main__ - ['Yes']
03/02/2022 05:04:08 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 05:04:08 - INFO - __main__ - ['Yes']
03/02/2022 05:04:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:04:11 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:04:17 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 05:04:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:04:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:04:19 - INFO - __main__ - Starting training!
03/02/2022 05:08:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_100_0.5_8_predictions.txt
03/02/2022 05:08:41 - INFO - __main__ - Classification-F1 on test data: 0.0258
03/02/2022 05:08:41 - INFO - __main__ - prefix=circa_16_100, lr=0.5, bsz=8, dev_performance=0.5692526540396723, test_performance=0.025794584344563238
03/02/2022 05:08:41 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.4, bsz=8 ...
03/02/2022 05:08:42 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:08:42 - INFO - __main__ - Printing 3 examples
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:08:42 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:08:42 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:08:42 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:08:42 - INFO - __main__ - Printing 3 examples
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:08:42 - INFO - __main__ - ['No']
03/02/2022 05:08:42 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:08:42 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:08:42 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:08:54 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:08:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:08:55 - INFO - __main__ - Starting training!
03/02/2022 05:08:58 - INFO - __main__ - Step 10 Global step 10 Train loss 3.63 on epoch=1
03/02/2022 05:09:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.53 on epoch=3
03/02/2022 05:09:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.93 on epoch=5
03/02/2022 05:09:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.40 on epoch=7
03/02/2022 05:09:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=9
03/02/2022 05:09:08 - INFO - __main__ - Global step 50 Train loss 2.11 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 05:09:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 05:09:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=11
03/02/2022 05:09:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=13
03/02/2022 05:09:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=15
03/02/2022 05:09:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=17
03/02/2022 05:09:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=19
03/02/2022 05:09:21 - INFO - __main__ - Global step 100 Train loss 0.73 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 05:09:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=21
03/02/2022 05:09:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=23
03/02/2022 05:09:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=25
03/02/2022 05:09:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=27
03/02/2022 05:09:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=29
03/02/2022 05:09:35 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.04836415362731152 on epoch=29
03/02/2022 05:09:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=31
03/02/2022 05:09:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=33
03/02/2022 05:09:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=35
03/02/2022 05:09:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=37
03/02/2022 05:09:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=39
03/02/2022 05:09:49 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.07162442874948068 on epoch=39
03/02/2022 05:09:49 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.07162442874948068 on epoch=39, global_step=200
03/02/2022 05:09:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=41
03/02/2022 05:09:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=43
03/02/2022 05:09:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=45
03/02/2022 05:09:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=47
03/02/2022 05:10:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=49
03/02/2022 05:10:02 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.06315789473684211 on epoch=49
03/02/2022 05:10:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=51
03/02/2022 05:10:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=53
03/02/2022 05:10:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=55
03/02/2022 05:10:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=57
03/02/2022 05:10:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=59
03/02/2022 05:10:16 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 05:10:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=61
03/02/2022 05:10:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=63
03/02/2022 05:10:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=65
03/02/2022 05:10:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=67
03/02/2022 05:10:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=69
03/02/2022 05:10:29 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 05:10:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=71
03/02/2022 05:10:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=73
03/02/2022 05:10:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=75
03/02/2022 05:10:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=77
03/02/2022 05:10:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=79
03/02/2022 05:10:43 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.11973856209150328 on epoch=79
03/02/2022 05:10:43 - INFO - __main__ - Saving model with best Classification-F1: 0.07162442874948068 -> 0.11973856209150328 on epoch=79, global_step=400
03/02/2022 05:10:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=81
03/02/2022 05:10:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=83
03/02/2022 05:10:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=85
03/02/2022 05:10:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=87
03/02/2022 05:10:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=89
03/02/2022 05:10:56 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.1695860566448802 on epoch=89
03/02/2022 05:10:56 - INFO - __main__ - Saving model with best Classification-F1: 0.11973856209150328 -> 0.1695860566448802 on epoch=89, global_step=450
03/02/2022 05:10:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=91
03/02/2022 05:11:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=93
03/02/2022 05:11:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=95
03/02/2022 05:11:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=97
03/02/2022 05:11:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=99
03/02/2022 05:11:10 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.13077905491698597 on epoch=99
03/02/2022 05:11:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=101
03/02/2022 05:11:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=103
03/02/2022 05:11:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=105
03/02/2022 05:11:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=107
03/02/2022 05:11:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=109
03/02/2022 05:11:23 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.15971670273460553 on epoch=109
03/02/2022 05:11:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=111
03/02/2022 05:11:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=113
03/02/2022 05:11:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=115
03/02/2022 05:11:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=117
03/02/2022 05:11:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=119
03/02/2022 05:11:37 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.2196481033091203 on epoch=119
03/02/2022 05:11:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1695860566448802 -> 0.2196481033091203 on epoch=119, global_step=600
03/02/2022 05:11:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=121
03/02/2022 05:11:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=123
03/02/2022 05:11:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=125
03/02/2022 05:11:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=127
03/02/2022 05:11:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=129
03/02/2022 05:11:51 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.2479143931360262 on epoch=129
03/02/2022 05:11:51 - INFO - __main__ - Saving model with best Classification-F1: 0.2196481033091203 -> 0.2479143931360262 on epoch=129, global_step=650
03/02/2022 05:11:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=131
03/02/2022 05:11:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=133
03/02/2022 05:11:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=135
03/02/2022 05:12:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=137
03/02/2022 05:12:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=139
03/02/2022 05:12:05 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.30399690282617114 on epoch=139
03/02/2022 05:12:05 - INFO - __main__ - Saving model with best Classification-F1: 0.2479143931360262 -> 0.30399690282617114 on epoch=139, global_step=700
03/02/2022 05:12:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=141
03/02/2022 05:12:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=143
03/02/2022 05:12:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=145
03/02/2022 05:12:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=147
03/02/2022 05:12:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=149
03/02/2022 05:12:18 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.2366482213438735 on epoch=149
03/02/2022 05:12:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=151
03/02/2022 05:12:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=153
03/02/2022 05:12:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=155
03/02/2022 05:12:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=157
03/02/2022 05:12:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=159
03/02/2022 05:12:31 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.26920688007644533 on epoch=159
03/02/2022 05:12:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=161
03/02/2022 05:12:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=163
03/02/2022 05:12:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=165
03/02/2022 05:12:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=167
03/02/2022 05:12:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=169
03/02/2022 05:12:45 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.3765942028985507 on epoch=169
03/02/2022 05:12:45 - INFO - __main__ - Saving model with best Classification-F1: 0.30399690282617114 -> 0.3765942028985507 on epoch=169, global_step=850
03/02/2022 05:12:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=171
03/02/2022 05:12:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=173
03/02/2022 05:12:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=175
03/02/2022 05:12:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=177
03/02/2022 05:12:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=179
03/02/2022 05:12:59 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.35182795698924735 on epoch=179
03/02/2022 05:13:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=181
03/02/2022 05:13:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=183
03/02/2022 05:13:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=185
03/02/2022 05:13:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=187
03/02/2022 05:13:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=189
03/02/2022 05:13:12 - INFO - __main__ - Global step 950 Train loss 0.25 Classification-F1 0.28588311688311685 on epoch=189
03/02/2022 05:13:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=191
03/02/2022 05:13:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=193
03/02/2022 05:13:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=195
03/02/2022 05:13:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=197
03/02/2022 05:13:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=199
03/02/2022 05:13:26 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.34565434565434566 on epoch=199
03/02/2022 05:13:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=201
03/02/2022 05:13:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=203
03/02/2022 05:13:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=205
03/02/2022 05:13:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=207
03/02/2022 05:13:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=209
03/02/2022 05:13:40 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.3017836536277316 on epoch=209
03/02/2022 05:13:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=211
03/02/2022 05:13:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=213
03/02/2022 05:13:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=215
03/02/2022 05:13:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=217
03/02/2022 05:13:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=219
03/02/2022 05:13:54 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3656855791962175 on epoch=219
03/02/2022 05:13:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=221
03/02/2022 05:13:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=223
03/02/2022 05:14:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=225
03/02/2022 05:14:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=227
03/02/2022 05:14:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=229
03/02/2022 05:14:08 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.32165112889475567 on epoch=229
03/02/2022 05:14:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=231
03/02/2022 05:14:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=233
03/02/2022 05:14:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=235
03/02/2022 05:14:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=237
03/02/2022 05:14:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=239
03/02/2022 05:14:21 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.29801013471758747 on epoch=239
03/02/2022 05:14:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=241
03/02/2022 05:14:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=243
03/02/2022 05:14:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=245
03/02/2022 05:14:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=247
03/02/2022 05:14:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=249
03/02/2022 05:14:35 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3446895424836601 on epoch=249
03/02/2022 05:14:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=251
03/02/2022 05:14:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=253
03/02/2022 05:14:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=255
03/02/2022 05:14:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=257
03/02/2022 05:14:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=259
03/02/2022 05:14:48 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.38644688644688646 on epoch=259
03/02/2022 05:14:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3765942028985507 -> 0.38644688644688646 on epoch=259, global_step=1300
03/02/2022 05:14:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=261
03/02/2022 05:14:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=263
03/02/2022 05:14:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=265
03/02/2022 05:14:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=267
03/02/2022 05:14:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=269
03/02/2022 05:15:02 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.31236897274633124 on epoch=269
03/02/2022 05:15:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=271
03/02/2022 05:15:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=273
03/02/2022 05:15:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=275
03/02/2022 05:15:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=277
03/02/2022 05:15:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=279
03/02/2022 05:15:16 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.3905981703026038 on epoch=279
03/02/2022 05:15:16 - INFO - __main__ - Saving model with best Classification-F1: 0.38644688644688646 -> 0.3905981703026038 on epoch=279, global_step=1400
03/02/2022 05:15:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=281
03/02/2022 05:15:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=283
03/02/2022 05:15:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=285
03/02/2022 05:15:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=287
03/02/2022 05:15:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=289
03/02/2022 05:15:30 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.30680568523705776 on epoch=289
03/02/2022 05:15:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=291
03/02/2022 05:15:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=293
03/02/2022 05:15:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=295
03/02/2022 05:15:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=297
03/02/2022 05:15:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=299
03/02/2022 05:15:43 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.2947162627977568 on epoch=299
03/02/2022 05:15:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=301
03/02/2022 05:15:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=303
03/02/2022 05:15:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=305
03/02/2022 05:15:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=307
03/02/2022 05:15:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=309
03/02/2022 05:15:57 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.24676809037140604 on epoch=309
03/02/2022 05:15:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=311
03/02/2022 05:16:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=313
03/02/2022 05:16:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=315
03/02/2022 05:16:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=317
03/02/2022 05:16:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=319
03/02/2022 05:16:11 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.3461959955047513 on epoch=319
03/02/2022 05:16:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=321
03/02/2022 05:16:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=323
03/02/2022 05:16:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=325
03/02/2022 05:16:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=327
03/02/2022 05:16:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=329
03/02/2022 05:16:25 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.22814263535326157 on epoch=329
03/02/2022 05:16:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=331
03/02/2022 05:16:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=333
03/02/2022 05:16:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=335
03/02/2022 05:16:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=337
03/02/2022 05:16:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=339
03/02/2022 05:16:38 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.2343888946912954 on epoch=339
03/02/2022 05:16:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=341
03/02/2022 05:16:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=343
03/02/2022 05:16:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=345
03/02/2022 05:16:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=347
03/02/2022 05:16:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=349
03/02/2022 05:16:52 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.31251221896383186 on epoch=349
03/02/2022 05:16:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=351
03/02/2022 05:16:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=353
03/02/2022 05:16:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=355
03/02/2022 05:17:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=357
03/02/2022 05:17:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=359
03/02/2022 05:17:06 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.27748271380084505 on epoch=359
03/02/2022 05:17:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=361
03/02/2022 05:17:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=363
03/02/2022 05:17:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=365
03/02/2022 05:17:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=367
03/02/2022 05:17:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=369
03/02/2022 05:17:20 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.23597386814332144 on epoch=369
03/02/2022 05:17:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=371
03/02/2022 05:17:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=373
03/02/2022 05:17:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=375
03/02/2022 05:17:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=377
03/02/2022 05:17:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=379
03/02/2022 05:17:34 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.19190293014755996 on epoch=379
03/02/2022 05:17:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=381
03/02/2022 05:17:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=383
03/02/2022 05:17:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=385
03/02/2022 05:17:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=387
03/02/2022 05:17:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=389
03/02/2022 05:17:48 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.2591991341991342 on epoch=389
03/02/2022 05:17:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=391
03/02/2022 05:17:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=393
03/02/2022 05:17:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=395
03/02/2022 05:17:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=397
03/02/2022 05:17:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=399
03/02/2022 05:18:01 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.16390006516233724 on epoch=399
03/02/2022 05:18:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=401
03/02/2022 05:18:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=403
03/02/2022 05:18:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=405
03/02/2022 05:18:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=407
03/02/2022 05:18:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=409
03/02/2022 05:18:15 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.2590586718246293 on epoch=409
03/02/2022 05:18:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=411
03/02/2022 05:18:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=413
03/02/2022 05:18:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=415
03/02/2022 05:18:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=417
03/02/2022 05:18:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=419
03/02/2022 05:18:29 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.23122294372294372 on epoch=419
03/02/2022 05:18:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=421
03/02/2022 05:18:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=423
03/02/2022 05:18:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=425
03/02/2022 05:18:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=427
03/02/2022 05:18:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=429
03/02/2022 05:18:43 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.20932910981156597 on epoch=429
03/02/2022 05:18:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=431
03/02/2022 05:18:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=433
03/02/2022 05:18:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 05:18:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=437
03/02/2022 05:18:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=439
03/02/2022 05:18:57 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.1529934461823007 on epoch=439
03/02/2022 05:18:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=441
03/02/2022 05:19:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=443
03/02/2022 05:19:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=445
03/02/2022 05:19:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=447
03/02/2022 05:19:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=449
03/02/2022 05:19:10 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.16859384882985454 on epoch=449
03/02/2022 05:19:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=451
03/02/2022 05:19:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=453
03/02/2022 05:19:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=455
03/02/2022 05:19:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=457
03/02/2022 05:19:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=459
03/02/2022 05:19:24 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.13160903250188963 on epoch=459
03/02/2022 05:19:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=461
03/02/2022 05:19:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=463
03/02/2022 05:19:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=465
03/02/2022 05:19:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=467
03/02/2022 05:19:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=469
03/02/2022 05:19:38 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.19056847545219638 on epoch=469
03/02/2022 05:19:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=471
03/02/2022 05:19:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=473
03/02/2022 05:19:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=475
03/02/2022 05:19:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=477
03/02/2022 05:19:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=479
03/02/2022 05:19:52 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.16618739226394727 on epoch=479
03/02/2022 05:19:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=481
03/02/2022 05:19:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=483
03/02/2022 05:19:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=485
03/02/2022 05:20:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 05:20:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=489
03/02/2022 05:20:06 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.15643920595533498 on epoch=489
03/02/2022 05:20:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=491
03/02/2022 05:20:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=493
03/02/2022 05:20:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=495
03/02/2022 05:20:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=497
03/02/2022 05:20:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=499
03/02/2022 05:20:20 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.24938574938574937 on epoch=499
03/02/2022 05:20:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=501
03/02/2022 05:20:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=503
03/02/2022 05:20:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=505
03/02/2022 05:20:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=507
03/02/2022 05:20:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=509
03/02/2022 05:20:33 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.19509523809523813 on epoch=509
03/02/2022 05:20:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=511
03/02/2022 05:20:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=513
03/02/2022 05:20:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=515
03/02/2022 05:20:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=517
03/02/2022 05:20:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=519
03/02/2022 05:20:47 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.23860877684407095 on epoch=519
03/02/2022 05:20:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=521
03/02/2022 05:20:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=523
03/02/2022 05:20:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=525
03/02/2022 05:20:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=527
03/02/2022 05:20:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=529
03/02/2022 05:21:01 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.26564134141720347 on epoch=529
03/02/2022 05:21:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=531
03/02/2022 05:21:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 05:21:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=535
03/02/2022 05:21:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=537
03/02/2022 05:21:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=539
03/02/2022 05:21:15 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.25222763430648737 on epoch=539
03/02/2022 05:21:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=541
03/02/2022 05:21:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=543
03/02/2022 05:21:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 05:21:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=547
03/02/2022 05:21:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=549
03/02/2022 05:21:29 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.22989953516269304 on epoch=549
03/02/2022 05:21:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 05:21:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 05:21:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=555
03/02/2022 05:21:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=557
03/02/2022 05:21:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=559
03/02/2022 05:21:42 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.24598765432098768 on epoch=559
03/02/2022 05:21:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=561
03/02/2022 05:21:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=563
03/02/2022 05:21:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=565
03/02/2022 05:21:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=567
03/02/2022 05:21:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=569
03/02/2022 05:21:56 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.23172704637235533 on epoch=569
03/02/2022 05:21:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=571
03/02/2022 05:22:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=573
03/02/2022 05:22:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=575
03/02/2022 05:22:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=577
03/02/2022 05:22:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=579
03/02/2022 05:22:10 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2508900029061319 on epoch=579
03/02/2022 05:22:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=581
03/02/2022 05:22:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=583
03/02/2022 05:22:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 05:22:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=587
03/02/2022 05:22:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=589
03/02/2022 05:22:24 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2140348621966269 on epoch=589
03/02/2022 05:22:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 05:22:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 05:22:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=595
03/02/2022 05:22:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=597
03/02/2022 05:22:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=599
03/02/2022 05:22:36 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:22:36 - INFO - __main__ - Printing 3 examples
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 05:22:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:22:36 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:22:36 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:22:36 - INFO - __main__ - Printing 3 examples
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:22:36 - INFO - __main__ - ['No']
03/02/2022 05:22:36 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:22:37 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:22:37 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:22:38 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.2344731665520196 on epoch=599
03/02/2022 05:22:38 - INFO - __main__ - save last model!
03/02/2022 05:22:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 05:22:38 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 05:22:38 - INFO - __main__ - Printing 3 examples
03/02/2022 05:22:38 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 05:22:38 - INFO - __main__ - ['No']
03/02/2022 05:22:38 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 05:22:38 - INFO - __main__ - ['Yes']
03/02/2022 05:22:38 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 05:22:38 - INFO - __main__ - ['Yes']
03/02/2022 05:22:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:22:41 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:22:47 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 05:22:50 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:22:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:22:51 - INFO - __main__ - Starting training!
03/02/2022 05:26:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_100_0.4_8_predictions.txt
03/02/2022 05:26:20 - INFO - __main__ - Classification-F1 on test data: 0.0274
03/02/2022 05:26:21 - INFO - __main__ - prefix=circa_16_100, lr=0.4, bsz=8, dev_performance=0.3905981703026038, test_performance=0.027418455119247807
03/02/2022 05:26:21 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.3, bsz=8 ...
03/02/2022 05:26:22 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:26:22 - INFO - __main__ - Printing 3 examples
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:26:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:26:22 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:26:22 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:26:22 - INFO - __main__ - Printing 3 examples
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:26:22 - INFO - __main__ - ['No']
03/02/2022 05:26:22 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:26:22 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:26:22 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:26:36 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:26:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:26:37 - INFO - __main__ - Starting training!
03/02/2022 05:26:40 - INFO - __main__ - Step 10 Global step 10 Train loss 3.82 on epoch=1
03/02/2022 05:26:42 - INFO - __main__ - Step 20 Global step 20 Train loss 2.91 on epoch=3
03/02/2022 05:26:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.23 on epoch=5
03/02/2022 05:26:47 - INFO - __main__ - Step 40 Global step 40 Train loss 1.69 on epoch=7
03/02/2022 05:26:49 - INFO - __main__ - Step 50 Global step 50 Train loss 1.39 on epoch=9
03/02/2022 05:26:51 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 05:26:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 05:26:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.13 on epoch=11
03/02/2022 05:26:56 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=13
03/02/2022 05:26:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.81 on epoch=15
03/02/2022 05:27:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=17
03/02/2022 05:27:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=19
03/02/2022 05:27:04 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 05:27:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=21
03/02/2022 05:27:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=23
03/02/2022 05:27:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=25
03/02/2022 05:27:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=27
03/02/2022 05:27:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.63 on epoch=29
03/02/2022 05:27:17 - INFO - __main__ - Global step 150 Train loss 0.63 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 05:27:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=31
03/02/2022 05:27:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=33
03/02/2022 05:27:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=35
03/02/2022 05:27:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=37
03/02/2022 05:27:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=39
03/02/2022 05:27:30 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.05957446808510638 on epoch=39
03/02/2022 05:27:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=41
03/02/2022 05:27:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=43
03/02/2022 05:27:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=45
03/02/2022 05:27:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=47
03/02/2022 05:27:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=49
03/02/2022 05:27:44 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.15606012152222576 on epoch=49
03/02/2022 05:27:44 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.15606012152222576 on epoch=49, global_step=250
03/02/2022 05:27:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=51
03/02/2022 05:27:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=53
03/02/2022 05:27:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=55
03/02/2022 05:27:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=57
03/02/2022 05:27:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=59
03/02/2022 05:27:57 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 05:27:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=61
03/02/2022 05:28:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=63
03/02/2022 05:28:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=65
03/02/2022 05:28:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=67
03/02/2022 05:28:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 05:28:10 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 05:28:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=71
03/02/2022 05:28:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=73
03/02/2022 05:28:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=75
03/02/2022 05:28:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=77
03/02/2022 05:28:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=79
03/02/2022 05:28:23 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.06666666666666668 on epoch=79
03/02/2022 05:28:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=81
03/02/2022 05:28:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=83
03/02/2022 05:28:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=85
03/02/2022 05:28:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=87
03/02/2022 05:28:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=89
03/02/2022 05:28:37 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.14288557213930347 on epoch=89
03/02/2022 05:28:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=91
03/02/2022 05:28:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=93
03/02/2022 05:28:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=95
03/02/2022 05:28:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=97
03/02/2022 05:28:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=99
03/02/2022 05:28:50 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.2 on epoch=99
03/02/2022 05:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.15606012152222576 -> 0.2 on epoch=99, global_step=500
03/02/2022 05:28:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=101
03/02/2022 05:28:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=103
03/02/2022 05:28:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=105
03/02/2022 05:28:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=107
03/02/2022 05:29:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=109
03/02/2022 05:29:02 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.15056689342403626 on epoch=109
03/02/2022 05:29:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=111
03/02/2022 05:29:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=113
03/02/2022 05:29:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=115
03/02/2022 05:29:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=117
03/02/2022 05:29:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=119
03/02/2022 05:29:16 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.14514231499051233 on epoch=119
03/02/2022 05:29:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=121
03/02/2022 05:29:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=123
03/02/2022 05:29:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=125
03/02/2022 05:29:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=127
03/02/2022 05:29:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=129
03/02/2022 05:29:30 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.18 on epoch=129
03/02/2022 05:29:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=131
03/02/2022 05:29:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=133
03/02/2022 05:29:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=135
03/02/2022 05:29:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=137
03/02/2022 05:29:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=139
03/02/2022 05:29:44 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.15032967032967032 on epoch=139
03/02/2022 05:29:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=141
03/02/2022 05:29:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=143
03/02/2022 05:29:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=145
03/02/2022 05:29:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=147
03/02/2022 05:29:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=149
03/02/2022 05:29:58 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.15191011235955057 on epoch=149
03/02/2022 05:30:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=151
03/02/2022 05:30:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=153
03/02/2022 05:30:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=155
03/02/2022 05:30:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=157
03/02/2022 05:30:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=159
03/02/2022 05:30:12 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.35876860818037287 on epoch=159
03/02/2022 05:30:12 - INFO - __main__ - Saving model with best Classification-F1: 0.2 -> 0.35876860818037287 on epoch=159, global_step=800
03/02/2022 05:30:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=161
03/02/2022 05:30:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=163
03/02/2022 05:30:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=165
03/02/2022 05:30:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=167
03/02/2022 05:30:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=169
03/02/2022 05:30:26 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.23273679615888257 on epoch=169
03/02/2022 05:30:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=171
03/02/2022 05:30:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=173
03/02/2022 05:30:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=175
03/02/2022 05:30:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=177
03/02/2022 05:30:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=179
03/02/2022 05:30:40 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.37465067340067343 on epoch=179
03/02/2022 05:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.35876860818037287 -> 0.37465067340067343 on epoch=179, global_step=900
03/02/2022 05:30:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=181
03/02/2022 05:30:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=183
03/02/2022 05:30:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=185
03/02/2022 05:30:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=187
03/02/2022 05:30:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=189
03/02/2022 05:30:54 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.35551211035082003 on epoch=189
03/02/2022 05:30:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=191
03/02/2022 05:30:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=193
03/02/2022 05:31:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=195
03/02/2022 05:31:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=197
03/02/2022 05:31:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=199
03/02/2022 05:31:08 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.35751736431249614 on epoch=199
03/02/2022 05:31:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=201
03/02/2022 05:31:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=203
03/02/2022 05:31:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=205
03/02/2022 05:31:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=207
03/02/2022 05:31:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=209
03/02/2022 05:31:22 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.4122527340174399 on epoch=209
03/02/2022 05:31:22 - INFO - __main__ - Saving model with best Classification-F1: 0.37465067340067343 -> 0.4122527340174399 on epoch=209, global_step=1050
03/02/2022 05:31:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=211
03/02/2022 05:31:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=213
03/02/2022 05:31:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=215
03/02/2022 05:31:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=217
03/02/2022 05:31:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=219
03/02/2022 05:31:36 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.4140167057814116 on epoch=219
03/02/2022 05:31:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4122527340174399 -> 0.4140167057814116 on epoch=219, global_step=1100
03/02/2022 05:31:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=221
03/02/2022 05:31:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=223
03/02/2022 05:31:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=225
03/02/2022 05:31:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=227
03/02/2022 05:31:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=229
03/02/2022 05:31:50 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.353307266210492 on epoch=229
03/02/2022 05:31:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=231
03/02/2022 05:31:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=233
03/02/2022 05:31:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=235
03/02/2022 05:31:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=237
03/02/2022 05:32:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=239
03/02/2022 05:32:04 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.45284786447577136 on epoch=239
03/02/2022 05:32:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4140167057814116 -> 0.45284786447577136 on epoch=239, global_step=1200
03/02/2022 05:32:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=241
03/02/2022 05:32:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=243
03/02/2022 05:32:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=245
03/02/2022 05:32:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=247
03/02/2022 05:32:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=249
03/02/2022 05:32:17 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.36058108418074775 on epoch=249
03/02/2022 05:32:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=251
03/02/2022 05:32:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=253
03/02/2022 05:32:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=255
03/02/2022 05:32:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=257
03/02/2022 05:32:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=259
03/02/2022 05:32:31 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3570430217720448 on epoch=259
03/02/2022 05:32:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=261
03/02/2022 05:32:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=263
03/02/2022 05:32:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=265
03/02/2022 05:32:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=267
03/02/2022 05:32:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=269
03/02/2022 05:32:45 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.4624787775891342 on epoch=269
03/02/2022 05:32:45 - INFO - __main__ - Saving model with best Classification-F1: 0.45284786447577136 -> 0.4624787775891342 on epoch=269, global_step=1350
03/02/2022 05:32:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=271
03/02/2022 05:32:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=273
03/02/2022 05:32:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=275
03/02/2022 05:32:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=277
03/02/2022 05:32:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=279
03/02/2022 05:32:59 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.41831320837838926 on epoch=279
03/02/2022 05:33:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=281
03/02/2022 05:33:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=283
03/02/2022 05:33:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=285
03/02/2022 05:33:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=287
03/02/2022 05:33:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=289
03/02/2022 05:33:13 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.43947874087272465 on epoch=289
03/02/2022 05:33:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=291
03/02/2022 05:33:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=293
03/02/2022 05:33:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=295
03/02/2022 05:33:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=297
03/02/2022 05:33:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=299
03/02/2022 05:33:26 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.39303400353060186 on epoch=299
03/02/2022 05:33:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=301
03/02/2022 05:33:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=303
03/02/2022 05:33:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=305
03/02/2022 05:33:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=307
03/02/2022 05:33:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=309
03/02/2022 05:33:40 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.3787053140096618 on epoch=309
03/02/2022 05:33:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=311
03/02/2022 05:33:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=313
03/02/2022 05:33:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=315
03/02/2022 05:33:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=317
03/02/2022 05:33:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=319
03/02/2022 05:33:54 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.4305275567344533 on epoch=319
03/02/2022 05:33:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=321
03/02/2022 05:33:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=323
03/02/2022 05:34:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=325
03/02/2022 05:34:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=327
03/02/2022 05:34:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=329
03/02/2022 05:34:07 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.4357658709565818 on epoch=329
03/02/2022 05:34:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=331
03/02/2022 05:34:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=333
03/02/2022 05:34:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=335
03/02/2022 05:34:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=337
03/02/2022 05:34:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=339
03/02/2022 05:34:21 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.38149659863945573 on epoch=339
03/02/2022 05:34:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=341
03/02/2022 05:34:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=343
03/02/2022 05:34:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=345
03/02/2022 05:34:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=347
03/02/2022 05:34:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=349
03/02/2022 05:34:35 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.38503633813591737 on epoch=349
03/02/2022 05:34:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=351
03/02/2022 05:34:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=353
03/02/2022 05:34:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=355
03/02/2022 05:34:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=357
03/02/2022 05:34:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=359
03/02/2022 05:34:49 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.4095238095238095 on epoch=359
03/02/2022 05:34:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=361
03/02/2022 05:34:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=363
03/02/2022 05:34:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=365
03/02/2022 05:34:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=367
03/02/2022 05:35:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=369
03/02/2022 05:35:02 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.4302661877129962 on epoch=369
03/02/2022 05:35:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=371
03/02/2022 05:35:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=373
03/02/2022 05:35:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=375
03/02/2022 05:35:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=377
03/02/2022 05:35:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=379
03/02/2022 05:35:16 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.34588675213675213 on epoch=379
03/02/2022 05:35:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=381
03/02/2022 05:35:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=383
03/02/2022 05:35:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=385
03/02/2022 05:35:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=387
03/02/2022 05:35:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=389
03/02/2022 05:35:30 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.46760133091349065 on epoch=389
03/02/2022 05:35:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4624787775891342 -> 0.46760133091349065 on epoch=389, global_step=1950
03/02/2022 05:35:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=391
03/02/2022 05:35:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=393
03/02/2022 05:35:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=395
03/02/2022 05:35:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=397
03/02/2022 05:35:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=399
03/02/2022 05:35:44 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.31858013593156664 on epoch=399
03/02/2022 05:35:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=401
03/02/2022 05:35:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=403
03/02/2022 05:35:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=405
03/02/2022 05:35:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=407
03/02/2022 05:35:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=409
03/02/2022 05:35:58 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.35687406275641564 on epoch=409
03/02/2022 05:36:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=411
03/02/2022 05:36:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=413
03/02/2022 05:36:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=415
03/02/2022 05:36:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=417
03/02/2022 05:36:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=419
03/02/2022 05:36:12 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.39913433852606034 on epoch=419
03/02/2022 05:36:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=421
03/02/2022 05:36:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=423
03/02/2022 05:36:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=425
03/02/2022 05:36:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=427
03/02/2022 05:36:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=429
03/02/2022 05:36:26 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.3621472918624343 on epoch=429
03/02/2022 05:36:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=431
03/02/2022 05:36:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=433
03/02/2022 05:36:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 05:36:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=437
03/02/2022 05:36:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=439
03/02/2022 05:36:40 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.4262189821400348 on epoch=439
03/02/2022 05:36:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=441
03/02/2022 05:36:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=443
03/02/2022 05:36:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=445
03/02/2022 05:36:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=447
03/02/2022 05:36:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=449
03/02/2022 05:36:54 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.5275231630874225 on epoch=449
03/02/2022 05:36:54 - INFO - __main__ - Saving model with best Classification-F1: 0.46760133091349065 -> 0.5275231630874225 on epoch=449, global_step=2250
03/02/2022 05:36:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=451
03/02/2022 05:36:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=453
03/02/2022 05:37:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=455
03/02/2022 05:37:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=457
03/02/2022 05:37:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=459
03/02/2022 05:37:08 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.4358739911735304 on epoch=459
03/02/2022 05:37:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=461
03/02/2022 05:37:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=463
03/02/2022 05:37:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=465
03/02/2022 05:37:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=467
03/02/2022 05:37:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=469
03/02/2022 05:37:22 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.4901129239838917 on epoch=469
03/02/2022 05:37:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=471
03/02/2022 05:37:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=473
03/02/2022 05:37:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=475
03/02/2022 05:37:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=477
03/02/2022 05:37:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=479
03/02/2022 05:37:37 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.3953209957818253 on epoch=479
03/02/2022 05:37:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=481
03/02/2022 05:37:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=483
03/02/2022 05:37:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=485
03/02/2022 05:37:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=487
03/02/2022 05:37:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=489
03/02/2022 05:37:51 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.4194583194583194 on epoch=489
03/02/2022 05:37:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=491
03/02/2022 05:37:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=493
03/02/2022 05:37:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=495
03/02/2022 05:38:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 05:38:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=499
03/02/2022 05:38:05 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.3360583601457965 on epoch=499
03/02/2022 05:38:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=501
03/02/2022 05:38:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=503
03/02/2022 05:38:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=505
03/02/2022 05:38:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=507
03/02/2022 05:38:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=509
03/02/2022 05:38:19 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.4750318019969912 on epoch=509
03/02/2022 05:38:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=511
03/02/2022 05:38:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=513
03/02/2022 05:38:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 05:38:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=517
03/02/2022 05:38:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=519
03/02/2022 05:38:33 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3984580498866213 on epoch=519
03/02/2022 05:38:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=521
03/02/2022 05:38:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=523
03/02/2022 05:38:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=525
03/02/2022 05:38:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=527
03/02/2022 05:38:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=529
03/02/2022 05:38:47 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.29382538923428453 on epoch=529
03/02/2022 05:38:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 05:38:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 05:38:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=535
03/02/2022 05:38:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=537
03/02/2022 05:38:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=539
03/02/2022 05:39:01 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.31291617667058663 on epoch=539
03/02/2022 05:39:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=541
03/02/2022 05:39:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=543
03/02/2022 05:39:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=545
03/02/2022 05:39:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=547
03/02/2022 05:39:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 05:39:15 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2968275888601092 on epoch=549
03/02/2022 05:39:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 05:39:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=553
03/02/2022 05:39:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=555
03/02/2022 05:39:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=557
03/02/2022 05:39:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=559
03/02/2022 05:39:29 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.27863163656267104 on epoch=559
03/02/2022 05:39:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=561
03/02/2022 05:39:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 05:39:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=565
03/02/2022 05:39:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=567
03/02/2022 05:39:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=569
03/02/2022 05:39:43 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.304120148856991 on epoch=569
03/02/2022 05:39:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=571
03/02/2022 05:39:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=573
03/02/2022 05:39:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 05:39:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=577
03/02/2022 05:39:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=579
03/02/2022 05:39:57 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.4431240438759236 on epoch=579
03/02/2022 05:40:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=581
03/02/2022 05:40:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=583
03/02/2022 05:40:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=585
03/02/2022 05:40:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 05:40:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=589
03/02/2022 05:40:12 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5129060879060878 on epoch=589
03/02/2022 05:40:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=591
03/02/2022 05:40:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 05:40:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=595
03/02/2022 05:40:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=597
03/02/2022 05:40:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 05:40:24 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:40:24 - INFO - __main__ - Printing 3 examples
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 05:40:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:40:24 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:40:24 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:40:24 - INFO - __main__ - Printing 3 examples
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:40:24 - INFO - __main__ - ['No']
03/02/2022 05:40:24 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:40:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:40:24 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:40:25 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3931670029231004 on epoch=599
03/02/2022 05:40:25 - INFO - __main__ - save last model!
03/02/2022 05:40:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 05:40:26 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 05:40:26 - INFO - __main__ - Printing 3 examples
03/02/2022 05:40:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 05:40:26 - INFO - __main__ - ['No']
03/02/2022 05:40:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 05:40:26 - INFO - __main__ - ['Yes']
03/02/2022 05:40:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 05:40:26 - INFO - __main__ - ['Yes']
03/02/2022 05:40:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:40:29 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:40:35 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 05:40:38 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:40:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:40:39 - INFO - __main__ - Starting training!
03/02/2022 05:44:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_100_0.3_8_predictions.txt
03/02/2022 05:44:39 - INFO - __main__ - Classification-F1 on test data: 0.0518
03/02/2022 05:44:40 - INFO - __main__ - prefix=circa_16_100, lr=0.3, bsz=8, dev_performance=0.5275231630874225, test_performance=0.051821335977193704
03/02/2022 05:44:40 - INFO - __main__ - Running ... prefix=circa_16_100, lr=0.2, bsz=8 ...
03/02/2022 05:44:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:44:41 - INFO - __main__ - Printing 3 examples
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you not like your boss? [SEP] answer Y: He's hard to get along with.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Is country music your favorite music? [SEP] answer Y: It is my least favourite.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Have you talked to anyone else from our childhood? [SEP] answer Y: I'm not good at keeping tabs on people from the past.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:44:41 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:44:41 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:44:41 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:44:41 - INFO - __main__ - Printing 3 examples
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Do you still visit the area often? [SEP] answer Y: I haven't been back since I left.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you feel like sharing a pizza with me? [SEP] answer Y: I'd rather have one of my own.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you allergic to soy? [SEP] answer Y: I don't have any allergies.
03/02/2022 05:44:41 - INFO - __main__ - ['No']
03/02/2022 05:44:41 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:44:41 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:44:41 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:44:55 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:44:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:44:56 - INFO - __main__ - Starting training!
03/02/2022 05:44:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.99 on epoch=1
03/02/2022 05:45:01 - INFO - __main__ - Step 20 Global step 20 Train loss 3.20 on epoch=3
03/02/2022 05:45:03 - INFO - __main__ - Step 30 Global step 30 Train loss 2.71 on epoch=5
03/02/2022 05:45:05 - INFO - __main__ - Step 40 Global step 40 Train loss 2.28 on epoch=7
03/02/2022 05:45:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.95 on epoch=9
03/02/2022 05:45:09 - INFO - __main__ - Global step 50 Train loss 2.83 Classification-F1 0.06808510638297872 on epoch=9
03/02/2022 05:45:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06808510638297872 on epoch=9, global_step=50
03/02/2022 05:45:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.61 on epoch=11
03/02/2022 05:45:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.38 on epoch=13
03/02/2022 05:45:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=15
03/02/2022 05:45:18 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=17
03/02/2022 05:45:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=19
03/02/2022 05:45:22 - INFO - __main__ - Global step 100 Train loss 1.21 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 05:45:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=21
03/02/2022 05:45:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=23
03/02/2022 05:45:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=25
03/02/2022 05:45:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=27
03/02/2022 05:45:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.73 on epoch=29
03/02/2022 05:45:35 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.09806451612903226 on epoch=29
03/02/2022 05:45:35 - INFO - __main__ - Saving model with best Classification-F1: 0.06808510638297872 -> 0.09806451612903226 on epoch=29, global_step=150
03/02/2022 05:45:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=31
03/02/2022 05:45:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.63 on epoch=33
03/02/2022 05:45:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=35
03/02/2022 05:45:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=37
03/02/2022 05:45:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=39
03/02/2022 05:45:49 - INFO - __main__ - Global step 200 Train loss 0.61 Classification-F1 0.07495495495495495 on epoch=39
03/02/2022 05:45:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=41
03/02/2022 05:45:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=43
03/02/2022 05:45:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=45
03/02/2022 05:45:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=47
03/02/2022 05:46:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=49
03/02/2022 05:46:02 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.06451612903225808 on epoch=49
03/02/2022 05:46:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=51
03/02/2022 05:46:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=53
03/02/2022 05:46:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 05:46:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=57
03/02/2022 05:46:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=59
03/02/2022 05:46:16 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16079785193709245 on epoch=59
03/02/2022 05:46:16 - INFO - __main__ - Saving model with best Classification-F1: 0.09806451612903226 -> 0.16079785193709245 on epoch=59, global_step=300
03/02/2022 05:46:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=61
03/02/2022 05:46:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=63
03/02/2022 05:46:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=65
03/02/2022 05:46:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=67
03/02/2022 05:46:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 05:46:30 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 05:46:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=71
03/02/2022 05:46:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=73
03/02/2022 05:46:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=75
03/02/2022 05:46:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=77
03/02/2022 05:46:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=79
03/02/2022 05:46:42 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.06736842105263158 on epoch=79
03/02/2022 05:46:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=81
03/02/2022 05:46:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=83
03/02/2022 05:46:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=85
03/02/2022 05:46:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=87
03/02/2022 05:46:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=89
03/02/2022 05:46:56 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=89
03/02/2022 05:46:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=91
03/02/2022 05:47:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=93
03/02/2022 05:47:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=95
03/02/2022 05:47:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=97
03/02/2022 05:47:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=99
03/02/2022 05:47:09 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.13958333333333334 on epoch=99
03/02/2022 05:47:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=101
03/02/2022 05:47:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=103
03/02/2022 05:47:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=105
03/02/2022 05:47:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=107
03/02/2022 05:47:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=109
03/02/2022 05:47:22 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.14285714285714285 on epoch=109
03/02/2022 05:47:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=111
03/02/2022 05:47:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=113
03/02/2022 05:47:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=115
03/02/2022 05:47:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=117
03/02/2022 05:47:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=119
03/02/2022 05:47:35 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.06666666666666668 on epoch=119
03/02/2022 05:47:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=121
03/02/2022 05:47:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=123
03/02/2022 05:47:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=125
03/02/2022 05:47:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=127
03/02/2022 05:47:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=129
03/02/2022 05:47:49 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.17412229714019994 on epoch=129
03/02/2022 05:47:49 - INFO - __main__ - Saving model with best Classification-F1: 0.16079785193709245 -> 0.17412229714019994 on epoch=129, global_step=650
03/02/2022 05:47:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=131
03/02/2022 05:47:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=133
03/02/2022 05:47:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=135
03/02/2022 05:47:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=137
03/02/2022 05:48:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=139
03/02/2022 05:48:02 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.14875283446712015 on epoch=139
03/02/2022 05:48:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=141
03/02/2022 05:48:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=143
03/02/2022 05:48:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=145
03/02/2022 05:48:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=147
03/02/2022 05:48:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=149
03/02/2022 05:48:15 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.15407149950347568 on epoch=149
03/02/2022 05:48:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=151
03/02/2022 05:48:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=153
03/02/2022 05:48:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=155
03/02/2022 05:48:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=157
03/02/2022 05:48:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=159
03/02/2022 05:48:29 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.16941176470588232 on epoch=159
03/02/2022 05:48:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=161
03/02/2022 05:48:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=163
03/02/2022 05:48:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=165
03/02/2022 05:48:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=167
03/02/2022 05:48:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=169
03/02/2022 05:48:42 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.28596825396825387 on epoch=169
03/02/2022 05:48:42 - INFO - __main__ - Saving model with best Classification-F1: 0.17412229714019994 -> 0.28596825396825387 on epoch=169, global_step=850
03/02/2022 05:48:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=171
03/02/2022 05:48:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=173
03/02/2022 05:48:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=175
03/02/2022 05:48:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=177
03/02/2022 05:48:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=179
03/02/2022 05:48:56 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.3588268106162843 on epoch=179
03/02/2022 05:48:56 - INFO - __main__ - Saving model with best Classification-F1: 0.28596825396825387 -> 0.3588268106162843 on epoch=179, global_step=900
03/02/2022 05:48:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=181
03/02/2022 05:49:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=183
03/02/2022 05:49:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=185
03/02/2022 05:49:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=187
03/02/2022 05:49:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=189
03/02/2022 05:49:10 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.23244755244755244 on epoch=189
03/02/2022 05:49:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=191
03/02/2022 05:49:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=193
03/02/2022 05:49:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=195
03/02/2022 05:49:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=197
03/02/2022 05:49:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=199
03/02/2022 05:49:24 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.2926282051282051 on epoch=199
03/02/2022 05:49:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=201
03/02/2022 05:49:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=203
03/02/2022 05:49:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=205
03/02/2022 05:49:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=207
03/02/2022 05:49:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=209
03/02/2022 05:49:37 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.28058416945373466 on epoch=209
03/02/2022 05:49:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=211
03/02/2022 05:49:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=213
03/02/2022 05:49:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=215
03/02/2022 05:49:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=217
03/02/2022 05:49:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=219
03/02/2022 05:49:51 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3563314358001265 on epoch=219
03/02/2022 05:49:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=221
03/02/2022 05:49:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=223
03/02/2022 05:49:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=225
03/02/2022 05:50:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=227
03/02/2022 05:50:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=229
03/02/2022 05:50:05 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.3479430434036713 on epoch=229
03/02/2022 05:50:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=231
03/02/2022 05:50:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=233
03/02/2022 05:50:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=235
03/02/2022 05:50:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=237
03/02/2022 05:50:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=239
03/02/2022 05:50:19 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.33704081632653055 on epoch=239
03/02/2022 05:50:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=241
03/02/2022 05:50:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=243
03/02/2022 05:50:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=245
03/02/2022 05:50:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=247
03/02/2022 05:50:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=249
03/02/2022 05:50:32 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.3318695652173913 on epoch=249
03/02/2022 05:50:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=251
03/02/2022 05:50:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=253
03/02/2022 05:50:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=255
03/02/2022 05:50:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=257
03/02/2022 05:50:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=259
03/02/2022 05:50:45 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.2982445141065831 on epoch=259
03/02/2022 05:50:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=261
03/02/2022 05:50:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=263
03/02/2022 05:50:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=265
03/02/2022 05:50:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=267
03/02/2022 05:50:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=269
03/02/2022 05:50:59 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.37700534759358295 on epoch=269
03/02/2022 05:50:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3588268106162843 -> 0.37700534759358295 on epoch=269, global_step=1350
03/02/2022 05:51:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=271
03/02/2022 05:51:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=273
03/02/2022 05:51:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=275
03/02/2022 05:51:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=277
03/02/2022 05:51:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=279
03/02/2022 05:51:13 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.3126798588740604 on epoch=279
03/02/2022 05:51:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=281
03/02/2022 05:51:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=283
03/02/2022 05:51:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=285
03/02/2022 05:51:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=287
03/02/2022 05:51:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=289
03/02/2022 05:51:26 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.31132597472975665 on epoch=289
03/02/2022 05:51:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=291
03/02/2022 05:51:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=293
03/02/2022 05:51:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=295
03/02/2022 05:51:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=297
03/02/2022 05:51:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=299
03/02/2022 05:51:40 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.30968553459119497 on epoch=299
03/02/2022 05:51:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=301
03/02/2022 05:51:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=303
03/02/2022 05:51:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=305
03/02/2022 05:51:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=307
03/02/2022 05:51:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=309
03/02/2022 05:51:54 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.297008547008547 on epoch=309
03/02/2022 05:51:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=311
03/02/2022 05:51:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=313
03/02/2022 05:52:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=315
03/02/2022 05:52:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=317
03/02/2022 05:52:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=319
03/02/2022 05:52:07 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.30517837883265864 on epoch=319
03/02/2022 05:52:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=321
03/02/2022 05:52:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=323
03/02/2022 05:52:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=325
03/02/2022 05:52:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=327
03/02/2022 05:52:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=329
03/02/2022 05:52:21 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.31468583326202904 on epoch=329
03/02/2022 05:52:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=331
03/02/2022 05:52:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=333
03/02/2022 05:52:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=335
03/02/2022 05:52:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=337
03/02/2022 05:52:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=339
03/02/2022 05:52:34 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.32489603237046194 on epoch=339
03/02/2022 05:52:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=341
03/02/2022 05:52:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=343
03/02/2022 05:52:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=345
03/02/2022 05:52:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=347
03/02/2022 05:52:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=349
03/02/2022 05:52:48 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.316969696969697 on epoch=349
03/02/2022 05:52:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=351
03/02/2022 05:52:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=353
03/02/2022 05:52:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=355
03/02/2022 05:52:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=357
03/02/2022 05:52:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=359
03/02/2022 05:53:02 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3104788717143553 on epoch=359
03/02/2022 05:53:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=361
03/02/2022 05:53:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=363
03/02/2022 05:53:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=365
03/02/2022 05:53:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=367
03/02/2022 05:53:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=369
03/02/2022 05:53:15 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.2927899686520376 on epoch=369
03/02/2022 05:53:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=371
03/02/2022 05:53:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=373
03/02/2022 05:53:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=375
03/02/2022 05:53:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=377
03/02/2022 05:53:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=379
03/02/2022 05:53:29 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.326139301874596 on epoch=379
03/02/2022 05:53:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=381
03/02/2022 05:53:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=383
03/02/2022 05:53:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=385
03/02/2022 05:53:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=387
03/02/2022 05:53:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=389
03/02/2022 05:53:42 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.35777777777777775 on epoch=389
03/02/2022 05:53:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=391
03/02/2022 05:53:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=393
03/02/2022 05:53:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=395
03/02/2022 05:53:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=397
03/02/2022 05:53:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=399
03/02/2022 05:53:56 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.32159342177998895 on epoch=399
03/02/2022 05:53:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=401
03/02/2022 05:54:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=403
03/02/2022 05:54:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=405
03/02/2022 05:54:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=407
03/02/2022 05:54:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=409
03/02/2022 05:54:10 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.3319037960011591 on epoch=409
03/02/2022 05:54:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=411
03/02/2022 05:54:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=413
03/02/2022 05:54:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=415
03/02/2022 05:54:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=417
03/02/2022 05:54:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=419
03/02/2022 05:54:23 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.38797647739323243 on epoch=419
03/02/2022 05:54:23 - INFO - __main__ - Saving model with best Classification-F1: 0.37700534759358295 -> 0.38797647739323243 on epoch=419, global_step=2100
03/02/2022 05:54:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=421
03/02/2022 05:54:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=423
03/02/2022 05:54:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=425
03/02/2022 05:54:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=427
03/02/2022 05:54:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=429
03/02/2022 05:54:37 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.39360639360639366 on epoch=429
03/02/2022 05:54:37 - INFO - __main__ - Saving model with best Classification-F1: 0.38797647739323243 -> 0.39360639360639366 on epoch=429, global_step=2150
03/02/2022 05:54:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=431
03/02/2022 05:54:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=433
03/02/2022 05:54:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=435
03/02/2022 05:54:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=437
03/02/2022 05:54:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=439
03/02/2022 05:54:51 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.38902074798467023 on epoch=439
03/02/2022 05:54:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=441
03/02/2022 05:54:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=443
03/02/2022 05:54:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=445
03/02/2022 05:55:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=447
03/02/2022 05:55:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=449
03/02/2022 05:55:04 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.3796660482374768 on epoch=449
03/02/2022 05:55:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=451
03/02/2022 05:55:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=453
03/02/2022 05:55:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=455
03/02/2022 05:55:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=457
03/02/2022 05:55:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=459
03/02/2022 05:55:18 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.37604081284444374 on epoch=459
03/02/2022 05:55:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=461
03/02/2022 05:55:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=463
03/02/2022 05:55:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=465
03/02/2022 05:55:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=467
03/02/2022 05:55:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=469
03/02/2022 05:55:32 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.3892787320206675 on epoch=469
03/02/2022 05:55:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=471
03/02/2022 05:55:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=473
03/02/2022 05:55:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=475
03/02/2022 05:55:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=477
03/02/2022 05:55:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=479
03/02/2022 05:55:46 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.4053704482381633 on epoch=479
03/02/2022 05:55:46 - INFO - __main__ - Saving model with best Classification-F1: 0.39360639360639366 -> 0.4053704482381633 on epoch=479, global_step=2400
03/02/2022 05:55:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=481
03/02/2022 05:55:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=483
03/02/2022 05:55:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=485
03/02/2022 05:55:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=487
03/02/2022 05:55:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=489
03/02/2022 05:56:00 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.3980808560210553 on epoch=489
03/02/2022 05:56:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=491
03/02/2022 05:56:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=493
03/02/2022 05:56:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=495
03/02/2022 05:56:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=497
03/02/2022 05:56:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=499
03/02/2022 05:56:14 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.38098901098901095 on epoch=499
03/02/2022 05:56:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=501
03/02/2022 05:56:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=503
03/02/2022 05:56:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=505
03/02/2022 05:56:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=507
03/02/2022 05:56:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=509
03/02/2022 05:56:28 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.320963527529875 on epoch=509
03/02/2022 05:56:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=511
03/02/2022 05:56:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=513
03/02/2022 05:56:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=515
03/02/2022 05:56:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=517
03/02/2022 05:56:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=519
03/02/2022 05:56:42 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.356679046911605 on epoch=519
03/02/2022 05:56:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=521
03/02/2022 05:56:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=523
03/02/2022 05:56:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=525
03/02/2022 05:56:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=527
03/02/2022 05:56:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=529
03/02/2022 05:56:56 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.3706986135415264 on epoch=529
03/02/2022 05:56:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=531
03/02/2022 05:57:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=533
03/02/2022 05:57:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=535
03/02/2022 05:57:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=537
03/02/2022 05:57:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=539
03/02/2022 05:57:10 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.3197649371562415 on epoch=539
03/02/2022 05:57:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=541
03/02/2022 05:57:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=543
03/02/2022 05:57:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=545
03/02/2022 05:57:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=547
03/02/2022 05:57:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=549
03/02/2022 05:57:24 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.2642152098934812 on epoch=549
03/02/2022 05:57:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=551
03/02/2022 05:57:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=553
03/02/2022 05:57:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=555
03/02/2022 05:57:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=557
03/02/2022 05:57:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=559
03/02/2022 05:57:38 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.24463868707836015 on epoch=559
03/02/2022 05:57:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=561
03/02/2022 05:57:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=563
03/02/2022 05:57:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=565
03/02/2022 05:57:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=567
03/02/2022 05:57:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=569
03/02/2022 05:57:51 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.2603656080372348 on epoch=569
03/02/2022 05:57:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=571
03/02/2022 05:57:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=573
03/02/2022 05:57:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=575
03/02/2022 05:58:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=577
03/02/2022 05:58:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=579
03/02/2022 05:58:06 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.1912625068653437 on epoch=579
03/02/2022 05:58:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=581
03/02/2022 05:58:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=583
03/02/2022 05:58:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=585
03/02/2022 05:58:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=587
03/02/2022 05:58:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=589
03/02/2022 05:58:20 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.21109258818237148 on epoch=589
03/02/2022 05:58:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=591
03/02/2022 05:58:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=593
03/02/2022 05:58:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=595
03/02/2022 05:58:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=597
03/02/2022 05:58:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=599
03/02/2022 05:58:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:58:33 - INFO - __main__ - Printing 3 examples
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 05:58:33 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:58:33 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 05:58:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 05:58:33 - INFO - __main__ - Printing 3 examples
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 05:58:33 - INFO - __main__ - ['No']
03/02/2022 05:58:33 - INFO - __main__ - Tokenizing Input ...
03/02/2022 05:58:33 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:58:33 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 05:58:34 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.21194083694083693 on epoch=599
03/02/2022 05:58:34 - INFO - __main__ - save last model!
03/02/2022 05:58:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 05:58:34 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 05:58:34 - INFO - __main__ - Printing 3 examples
03/02/2022 05:58:34 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 05:58:34 - INFO - __main__ - ['No']
03/02/2022 05:58:34 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 05:58:34 - INFO - __main__ - ['Yes']
03/02/2022 05:58:34 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 05:58:34 - INFO - __main__ - ['Yes']
03/02/2022 05:58:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 05:58:37 - INFO - __main__ - Tokenizing Output ...
03/02/2022 05:58:44 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 05:58:47 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 05:58:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 05:58:48 - INFO - __main__ - Starting training!
03/02/2022 06:01:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_100_0.2_8_predictions.txt
03/02/2022 06:01:56 - INFO - __main__ - Classification-F1 on test data: 0.0412
03/02/2022 06:01:57 - INFO - __main__ - prefix=circa_16_100, lr=0.2, bsz=8, dev_performance=0.4053704482381633, test_performance=0.04123437894829556
03/02/2022 06:01:57 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.5, bsz=8 ...
03/02/2022 06:01:57 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:01:57 - INFO - __main__ - Printing 3 examples
03/02/2022 06:01:57 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:01:57 - INFO - __main__ - ['No']
03/02/2022 06:01:57 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:01:57 - INFO - __main__ - ['No']
03/02/2022 06:01:57 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:01:57 - INFO - __main__ - ['No']
03/02/2022 06:01:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:01:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:01:58 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:01:58 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:01:58 - INFO - __main__ - Printing 3 examples
03/02/2022 06:01:58 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:01:58 - INFO - __main__ - ['No']
03/02/2022 06:01:58 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:01:58 - INFO - __main__ - ['No']
03/02/2022 06:01:58 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:01:58 - INFO - __main__ - ['No']
03/02/2022 06:01:58 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:01:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:01:58 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:02:12 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:02:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:02:13 - INFO - __main__ - Starting training!
03/02/2022 06:02:16 - INFO - __main__ - Step 10 Global step 10 Train loss 3.46 on epoch=1
03/02/2022 06:02:18 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=3
03/02/2022 06:02:20 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=5
03/02/2022 06:02:22 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=7
03/02/2022 06:02:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.83 on epoch=9
03/02/2022 06:02:26 - INFO - __main__ - Global step 50 Train loss 1.82 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 06:02:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 06:02:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.76 on epoch=11
03/02/2022 06:02:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.74 on epoch=13
03/02/2022 06:02:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=15
03/02/2022 06:02:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=17
03/02/2022 06:02:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=19
03/02/2022 06:02:39 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 06:02:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=21
03/02/2022 06:02:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=23
03/02/2022 06:02:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=25
03/02/2022 06:02:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=27
03/02/2022 06:02:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=29
03/02/2022 06:02:53 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.07027027027027027 on epoch=29
03/02/2022 06:02:53 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.07027027027027027 on epoch=29, global_step=150
03/02/2022 06:02:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=31
03/02/2022 06:02:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=33
03/02/2022 06:02:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=35
03/02/2022 06:03:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=37
03/02/2022 06:03:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=39
03/02/2022 06:03:05 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 06:03:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=41
03/02/2022 06:03:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=43
03/02/2022 06:03:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=45
03/02/2022 06:03:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=47
03/02/2022 06:03:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=49
03/02/2022 06:03:19 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 06:03:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=51
03/02/2022 06:03:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=53
03/02/2022 06:03:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=55
03/02/2022 06:03:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=57
03/02/2022 06:03:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=59
03/02/2022 06:03:32 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 06:03:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=61
03/02/2022 06:03:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=63
03/02/2022 06:03:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=65
03/02/2022 06:03:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=67
03/02/2022 06:03:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=69
03/02/2022 06:03:45 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.10669272105520274 on epoch=69
03/02/2022 06:03:45 - INFO - __main__ - Saving model with best Classification-F1: 0.07027027027027027 -> 0.10669272105520274 on epoch=69, global_step=350
03/02/2022 06:03:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=71
03/02/2022 06:03:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=73
03/02/2022 06:03:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=75
03/02/2022 06:03:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=77
03/02/2022 06:03:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=79
03/02/2022 06:03:59 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16634920634920633 on epoch=79
03/02/2022 06:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.10669272105520274 -> 0.16634920634920633 on epoch=79, global_step=400
03/02/2022 06:04:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=81
03/02/2022 06:04:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=83
03/02/2022 06:04:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=85
03/02/2022 06:04:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=87
03/02/2022 06:04:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=89
03/02/2022 06:04:13 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.23752380952380953 on epoch=89
03/02/2022 06:04:13 - INFO - __main__ - Saving model with best Classification-F1: 0.16634920634920633 -> 0.23752380952380953 on epoch=89, global_step=450
03/02/2022 06:04:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=91
03/02/2022 06:04:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=93
03/02/2022 06:04:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=95
03/02/2022 06:04:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=97
03/02/2022 06:04:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=99
03/02/2022 06:04:26 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.27879159885298 on epoch=99
03/02/2022 06:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.23752380952380953 -> 0.27879159885298 on epoch=99, global_step=500
03/02/2022 06:04:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=101
03/02/2022 06:04:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=103
03/02/2022 06:04:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=105
03/02/2022 06:04:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=107
03/02/2022 06:04:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=109
03/02/2022 06:04:40 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.35964958299704064 on epoch=109
03/02/2022 06:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.27879159885298 -> 0.35964958299704064 on epoch=109, global_step=550
03/02/2022 06:04:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=111
03/02/2022 06:04:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=113
03/02/2022 06:04:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=115
03/02/2022 06:04:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=117
03/02/2022 06:04:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=119
03/02/2022 06:04:54 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.4032769222397562 on epoch=119
03/02/2022 06:04:54 - INFO - __main__ - Saving model with best Classification-F1: 0.35964958299704064 -> 0.4032769222397562 on epoch=119, global_step=600
03/02/2022 06:04:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=121
03/02/2022 06:04:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=123
03/02/2022 06:05:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=125
03/02/2022 06:05:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=127
03/02/2022 06:05:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=129
03/02/2022 06:05:08 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.32041935483870965 on epoch=129
03/02/2022 06:05:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=131
03/02/2022 06:05:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=133
03/02/2022 06:05:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=135
03/02/2022 06:05:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=137
03/02/2022 06:05:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=139
03/02/2022 06:05:22 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.37746438746438743 on epoch=139
03/02/2022 06:05:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=141
03/02/2022 06:05:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=143
03/02/2022 06:05:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=145
03/02/2022 06:05:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=147
03/02/2022 06:05:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=149
03/02/2022 06:05:37 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.3573596973596974 on epoch=149
03/02/2022 06:05:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=151
03/02/2022 06:05:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=153
03/02/2022 06:05:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=155
03/02/2022 06:05:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=157
03/02/2022 06:05:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=159
03/02/2022 06:05:51 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.3836810130288391 on epoch=159
03/02/2022 06:05:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=161
03/02/2022 06:05:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=163
03/02/2022 06:05:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=165
03/02/2022 06:06:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=167
03/02/2022 06:06:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=169
03/02/2022 06:06:05 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.28837976539589444 on epoch=169
03/02/2022 06:06:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=171
03/02/2022 06:06:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=173
03/02/2022 06:06:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=175
03/02/2022 06:06:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=177
03/02/2022 06:06:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=179
03/02/2022 06:06:19 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.37389569626857766 on epoch=179
03/02/2022 06:06:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=181
03/02/2022 06:06:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=183
03/02/2022 06:06:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=185
03/02/2022 06:06:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=187
03/02/2022 06:06:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=189
03/02/2022 06:06:33 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3638835904628331 on epoch=189
03/02/2022 06:06:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=191
03/02/2022 06:06:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=193
03/02/2022 06:06:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=195
03/02/2022 06:06:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=197
03/02/2022 06:06:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=199
03/02/2022 06:06:48 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.29965611449482416 on epoch=199
03/02/2022 06:06:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=201
03/02/2022 06:06:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=203
03/02/2022 06:06:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=205
03/02/2022 06:06:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=207
03/02/2022 06:06:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=209
03/02/2022 06:07:02 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.3080346475507766 on epoch=209
03/02/2022 06:07:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=211
03/02/2022 06:07:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=213
03/02/2022 06:07:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=215
03/02/2022 06:07:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=217
03/02/2022 06:07:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=219
03/02/2022 06:07:16 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.3414835164835165 on epoch=219
03/02/2022 06:07:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=221
03/02/2022 06:07:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=223
03/02/2022 06:07:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=225
03/02/2022 06:07:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=227
03/02/2022 06:07:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=229
03/02/2022 06:07:30 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.3486624110352924 on epoch=229
03/02/2022 06:07:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=231
03/02/2022 06:07:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=233
03/02/2022 06:07:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=235
03/02/2022 06:07:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=237
03/02/2022 06:07:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=239
03/02/2022 06:07:45 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.3993073593073593 on epoch=239
03/02/2022 06:07:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=241
03/02/2022 06:07:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=243
03/02/2022 06:07:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=245
03/02/2022 06:07:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=247
03/02/2022 06:07:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=249
03/02/2022 06:07:59 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.41948866475852986 on epoch=249
03/02/2022 06:07:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4032769222397562 -> 0.41948866475852986 on epoch=249, global_step=1250
03/02/2022 06:08:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=251
03/02/2022 06:08:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=253
03/02/2022 06:08:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=255
03/02/2022 06:08:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=257
03/02/2022 06:08:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=259
03/02/2022 06:08:13 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.28218347464724275 on epoch=259
03/02/2022 06:08:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=261
03/02/2022 06:08:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=263
03/02/2022 06:08:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=265
03/02/2022 06:08:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=267
03/02/2022 06:08:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=269
03/02/2022 06:08:27 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.27412146676852556 on epoch=269
03/02/2022 06:08:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=271
03/02/2022 06:08:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=273
03/02/2022 06:08:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=275
03/02/2022 06:08:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=277
03/02/2022 06:08:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=279
03/02/2022 06:08:42 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.3820374800637959 on epoch=279
03/02/2022 06:08:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=281
03/02/2022 06:08:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=283
03/02/2022 06:08:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=285
03/02/2022 06:08:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=287
03/02/2022 06:08:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=289
03/02/2022 06:08:56 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.3584065110301992 on epoch=289
03/02/2022 06:08:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=291
03/02/2022 06:09:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=293
03/02/2022 06:09:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=295
03/02/2022 06:09:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=297
03/02/2022 06:09:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=299
03/02/2022 06:09:10 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.341705069124424 on epoch=299
03/02/2022 06:09:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=301
03/02/2022 06:09:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=303
03/02/2022 06:09:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=305
03/02/2022 06:09:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=307
03/02/2022 06:09:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=309
03/02/2022 06:09:25 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.390175983436853 on epoch=309
03/02/2022 06:09:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=311
03/02/2022 06:09:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=313
03/02/2022 06:09:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=315
03/02/2022 06:09:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=317
03/02/2022 06:09:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=319
03/02/2022 06:09:39 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.4220832203849282 on epoch=319
03/02/2022 06:09:39 - INFO - __main__ - Saving model with best Classification-F1: 0.41948866475852986 -> 0.4220832203849282 on epoch=319, global_step=1600
03/02/2022 06:09:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=321
03/02/2022 06:09:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=323
03/02/2022 06:09:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=325
03/02/2022 06:09:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=327
03/02/2022 06:09:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=329
03/02/2022 06:09:53 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.4042011755613887 on epoch=329
03/02/2022 06:09:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=331
03/02/2022 06:09:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=333
03/02/2022 06:10:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=335
03/02/2022 06:10:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=337
03/02/2022 06:10:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=339
03/02/2022 06:10:08 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.37242510068597023 on epoch=339
03/02/2022 06:10:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=341
03/02/2022 06:10:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=343
03/02/2022 06:10:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=345
03/02/2022 06:10:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=347
03/02/2022 06:10:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=349
03/02/2022 06:10:22 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.3896769628653687 on epoch=349
03/02/2022 06:10:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=351
03/02/2022 06:10:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=353
03/02/2022 06:10:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=355
03/02/2022 06:10:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=357
03/02/2022 06:10:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=359
03/02/2022 06:10:37 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.34069279885867393 on epoch=359
03/02/2022 06:10:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=361
03/02/2022 06:10:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=363
03/02/2022 06:10:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=365
03/02/2022 06:10:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=367
03/02/2022 06:10:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=369
03/02/2022 06:10:51 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.3506012160961946 on epoch=369
03/02/2022 06:10:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=371
03/02/2022 06:10:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=373
03/02/2022 06:10:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=375
03/02/2022 06:11:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=377
03/02/2022 06:11:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=379
03/02/2022 06:11:05 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.39724289640893834 on epoch=379
03/02/2022 06:11:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=381
03/02/2022 06:11:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=383
03/02/2022 06:11:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=385
03/02/2022 06:11:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=387
03/02/2022 06:11:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=389
03/02/2022 06:11:20 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.38680043680043674 on epoch=389
03/02/2022 06:11:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=391
03/02/2022 06:11:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=393
03/02/2022 06:11:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=395
03/02/2022 06:11:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=397
03/02/2022 06:11:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=399
03/02/2022 06:11:35 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.3869569757727653 on epoch=399
03/02/2022 06:11:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=401
03/02/2022 06:11:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=403
03/02/2022 06:11:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=405
03/02/2022 06:11:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=407
03/02/2022 06:11:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=409
03/02/2022 06:11:49 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.33104150751209577 on epoch=409
03/02/2022 06:11:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=411
03/02/2022 06:11:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=413
03/02/2022 06:11:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=415
03/02/2022 06:11:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=417
03/02/2022 06:12:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=419
03/02/2022 06:12:04 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.33484324814969973 on epoch=419
03/02/2022 06:12:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=421
03/02/2022 06:12:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=423
03/02/2022 06:12:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=425
03/02/2022 06:12:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=427
03/02/2022 06:12:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=429
03/02/2022 06:12:18 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.40140936390936394 on epoch=429
03/02/2022 06:12:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=431
03/02/2022 06:12:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=433
03/02/2022 06:12:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 06:12:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=437
03/02/2022 06:12:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=439
03/02/2022 06:12:33 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.39968281169739805 on epoch=439
03/02/2022 06:12:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=441
03/02/2022 06:12:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=443
03/02/2022 06:12:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=445
03/02/2022 06:12:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=447
03/02/2022 06:12:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=449
03/02/2022 06:12:47 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.37449997119658096 on epoch=449
03/02/2022 06:12:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=451
03/02/2022 06:12:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=453
03/02/2022 06:12:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=455
03/02/2022 06:12:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=457
03/02/2022 06:12:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=459
03/02/2022 06:13:02 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.4125469828102382 on epoch=459
03/02/2022 06:13:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=461
03/02/2022 06:13:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=463
03/02/2022 06:13:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=465
03/02/2022 06:13:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=467
03/02/2022 06:13:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=469
03/02/2022 06:13:16 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.38858465608465603 on epoch=469
03/02/2022 06:13:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=471
03/02/2022 06:13:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=473
03/02/2022 06:13:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=475
03/02/2022 06:13:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=477
03/02/2022 06:13:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=479
03/02/2022 06:13:31 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.38917118917118915 on epoch=479
03/02/2022 06:13:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=481
03/02/2022 06:13:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=483
03/02/2022 06:13:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=485
03/02/2022 06:13:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 06:13:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=489
03/02/2022 06:13:46 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.42594491278701807 on epoch=489
03/02/2022 06:13:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4220832203849282 -> 0.42594491278701807 on epoch=489, global_step=2450
03/02/2022 06:13:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=491
03/02/2022 06:13:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 06:13:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=495
03/02/2022 06:13:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 06:13:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=499
03/02/2022 06:14:00 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.31764641608391614 on epoch=499
03/02/2022 06:14:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=501
03/02/2022 06:14:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=503
03/02/2022 06:14:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=505
03/02/2022 06:14:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=507
03/02/2022 06:14:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=509
03/02/2022 06:14:15 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3487274965535835 on epoch=509
03/02/2022 06:14:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=511
03/02/2022 06:14:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=513
03/02/2022 06:14:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=515
03/02/2022 06:14:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=517
03/02/2022 06:14:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=519
03/02/2022 06:14:29 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.3702184619961618 on epoch=519
03/02/2022 06:14:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=521
03/02/2022 06:14:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=523
03/02/2022 06:14:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=525
03/02/2022 06:14:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=527
03/02/2022 06:14:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=529
03/02/2022 06:14:44 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.3267333815246529 on epoch=529
03/02/2022 06:14:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=531
03/02/2022 06:14:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=533
03/02/2022 06:14:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=535
03/02/2022 06:14:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=537
03/02/2022 06:14:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=539
03/02/2022 06:14:59 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.44332369586453074 on epoch=539
03/02/2022 06:14:59 - INFO - __main__ - Saving model with best Classification-F1: 0.42594491278701807 -> 0.44332369586453074 on epoch=539, global_step=2700
03/02/2022 06:15:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=541
03/02/2022 06:15:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=543
03/02/2022 06:15:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 06:15:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=547
03/02/2022 06:15:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=549
03/02/2022 06:15:13 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.334521580750029 on epoch=549
03/02/2022 06:15:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 06:15:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 06:15:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=555
03/02/2022 06:15:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=557
03/02/2022 06:15:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=559
03/02/2022 06:15:28 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.32764338999633114 on epoch=559
03/02/2022 06:15:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=561
03/02/2022 06:15:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 06:15:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=565
03/02/2022 06:15:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=567
03/02/2022 06:15:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=569
03/02/2022 06:15:42 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.41302153786987544 on epoch=569
03/02/2022 06:15:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=571
03/02/2022 06:15:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=573
03/02/2022 06:15:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 06:15:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=577
03/02/2022 06:15:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=579
03/02/2022 06:15:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.42534540810402877 on epoch=579
03/02/2022 06:15:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 06:16:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=583
03/02/2022 06:16:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 06:16:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=587
03/02/2022 06:16:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=589
03/02/2022 06:16:12 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.47977793811127145 on epoch=589
03/02/2022 06:16:12 - INFO - __main__ - Saving model with best Classification-F1: 0.44332369586453074 -> 0.47977793811127145 on epoch=589, global_step=2950
03/02/2022 06:16:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=591
03/02/2022 06:16:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 06:16:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=595
03/02/2022 06:16:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 06:16:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=599
03/02/2022 06:16:24 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:16:24 - INFO - __main__ - Printing 3 examples
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 06:16:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:16:24 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:16:24 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:16:24 - INFO - __main__ - Printing 3 examples
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:16:24 - INFO - __main__ - ['No']
03/02/2022 06:16:24 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:16:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:16:24 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:16:26 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.520940170940171 on epoch=599
03/02/2022 06:16:26 - INFO - __main__ - Saving model with best Classification-F1: 0.47977793811127145 -> 0.520940170940171 on epoch=599, global_step=3000
03/02/2022 06:16:26 - INFO - __main__ - save last model!
03/02/2022 06:16:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 06:16:26 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 06:16:26 - INFO - __main__ - Printing 3 examples
03/02/2022 06:16:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 06:16:26 - INFO - __main__ - ['No']
03/02/2022 06:16:26 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 06:16:26 - INFO - __main__ - ['Yes']
03/02/2022 06:16:26 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 06:16:26 - INFO - __main__ - ['Yes']
03/02/2022 06:16:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:16:30 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:16:36 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 06:16:37 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:16:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:16:38 - INFO - __main__ - Starting training!
03/02/2022 06:21:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_13_0.5_8_predictions.txt
03/02/2022 06:21:29 - INFO - __main__ - Classification-F1 on test data: 0.0451
03/02/2022 06:21:30 - INFO - __main__ - prefix=circa_16_13, lr=0.5, bsz=8, dev_performance=0.520940170940171, test_performance=0.045055549515800514
03/02/2022 06:21:30 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.4, bsz=8 ...
03/02/2022 06:21:31 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:21:31 - INFO - __main__ - Printing 3 examples
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:21:31 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:21:31 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:21:31 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:21:31 - INFO - __main__ - Printing 3 examples
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:21:31 - INFO - __main__ - ['No']
03/02/2022 06:21:31 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:21:31 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:21:31 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:21:45 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:21:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:21:46 - INFO - __main__ - Starting training!
03/02/2022 06:21:49 - INFO - __main__ - Step 10 Global step 10 Train loss 3.65 on epoch=1
03/02/2022 06:21:51 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=3
03/02/2022 06:21:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.72 on epoch=5
03/02/2022 06:21:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.32 on epoch=7
03/02/2022 06:21:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=9
03/02/2022 06:21:59 - INFO - __main__ - Global step 50 Train loss 2.07 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 06:21:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 06:22:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=11
03/02/2022 06:22:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.74 on epoch=13
03/02/2022 06:22:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=15
03/02/2022 06:22:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=17
03/02/2022 06:22:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=19
03/02/2022 06:22:12 - INFO - __main__ - Global step 100 Train loss 0.73 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 06:22:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=21
03/02/2022 06:22:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=23
03/02/2022 06:22:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=25
03/02/2022 06:22:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=27
03/02/2022 06:22:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=29
03/02/2022 06:22:25 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.14872305140961856 on epoch=29
03/02/2022 06:22:25 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.14872305140961856 on epoch=29, global_step=150
03/02/2022 06:22:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=31
03/02/2022 06:22:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.57 on epoch=33
03/02/2022 06:22:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=35
03/02/2022 06:22:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=37
03/02/2022 06:22:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=39
03/02/2022 06:22:38 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.10784547988849065 on epoch=39
03/02/2022 06:22:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=41
03/02/2022 06:22:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=43
03/02/2022 06:22:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=45
03/02/2022 06:22:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=47
03/02/2022 06:22:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=49
03/02/2022 06:22:52 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.17666666666666667 on epoch=49
03/02/2022 06:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.14872305140961856 -> 0.17666666666666667 on epoch=49, global_step=250
03/02/2022 06:22:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=51
03/02/2022 06:22:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=53
03/02/2022 06:22:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=55
03/02/2022 06:23:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=57
03/02/2022 06:23:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=59
03/02/2022 06:23:05 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.14956521739130435 on epoch=59
03/02/2022 06:23:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=61
03/02/2022 06:23:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=63
03/02/2022 06:23:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=65
03/02/2022 06:23:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=67
03/02/2022 06:23:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=69
03/02/2022 06:23:19 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.254392666157372 on epoch=69
03/02/2022 06:23:19 - INFO - __main__ - Saving model with best Classification-F1: 0.17666666666666667 -> 0.254392666157372 on epoch=69, global_step=350
03/02/2022 06:23:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=71
03/02/2022 06:23:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=73
03/02/2022 06:23:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=75
03/02/2022 06:23:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=77
03/02/2022 06:23:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=79
03/02/2022 06:23:32 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.2771810771810772 on epoch=79
03/02/2022 06:23:32 - INFO - __main__ - Saving model with best Classification-F1: 0.254392666157372 -> 0.2771810771810772 on epoch=79, global_step=400
03/02/2022 06:23:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=81
03/02/2022 06:23:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=83
03/02/2022 06:23:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=85
03/02/2022 06:23:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=87
03/02/2022 06:23:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=89
03/02/2022 06:23:46 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.25696969696969696 on epoch=89
03/02/2022 06:23:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=91
03/02/2022 06:23:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=93
03/02/2022 06:23:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=95
03/02/2022 06:23:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=97
03/02/2022 06:23:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=99
03/02/2022 06:24:00 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3192797540623628 on epoch=99
03/02/2022 06:24:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2771810771810772 -> 0.3192797540623628 on epoch=99, global_step=500
03/02/2022 06:24:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=101
03/02/2022 06:24:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=103
03/02/2022 06:24:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=105
03/02/2022 06:24:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=107
03/02/2022 06:24:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=109
03/02/2022 06:24:14 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.3000276533377578 on epoch=109
03/02/2022 06:24:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=111
03/02/2022 06:24:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=113
03/02/2022 06:24:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=115
03/02/2022 06:24:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=117
03/02/2022 06:24:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=119
03/02/2022 06:24:28 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.28408963585434177 on epoch=119
03/02/2022 06:24:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=121
03/02/2022 06:24:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=123
03/02/2022 06:24:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=125
03/02/2022 06:24:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=127
03/02/2022 06:24:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=129
03/02/2022 06:24:42 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.25885885885885884 on epoch=129
03/02/2022 06:24:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=131
03/02/2022 06:24:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=133
03/02/2022 06:24:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=135
03/02/2022 06:24:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=137
03/02/2022 06:24:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=139
03/02/2022 06:24:57 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.2683469236471461 on epoch=139
03/02/2022 06:24:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=141
03/02/2022 06:25:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=143
03/02/2022 06:25:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=145
03/02/2022 06:25:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=147
03/02/2022 06:25:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=149
03/02/2022 06:25:10 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.33863908640379226 on epoch=149
03/02/2022 06:25:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3192797540623628 -> 0.33863908640379226 on epoch=149, global_step=750
03/02/2022 06:25:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=151
03/02/2022 06:25:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=153
03/02/2022 06:25:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=155
03/02/2022 06:25:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=157
03/02/2022 06:25:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=159
03/02/2022 06:25:24 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.32230402698713895 on epoch=159
03/02/2022 06:25:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=161
03/02/2022 06:25:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=163
03/02/2022 06:25:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=165
03/02/2022 06:25:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=167
03/02/2022 06:25:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=169
03/02/2022 06:25:38 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.3010774410774411 on epoch=169
03/02/2022 06:25:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=171
03/02/2022 06:25:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=173
03/02/2022 06:25:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=175
03/02/2022 06:25:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=177
03/02/2022 06:25:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=179
03/02/2022 06:25:52 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3128526645768025 on epoch=179
03/02/2022 06:25:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=181
03/02/2022 06:25:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=183
03/02/2022 06:25:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=185
03/02/2022 06:26:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=187
03/02/2022 06:26:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=189
03/02/2022 06:26:06 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.3098669623059867 on epoch=189
03/02/2022 06:26:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=191
03/02/2022 06:26:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=193
03/02/2022 06:26:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=195
03/02/2022 06:26:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=197
03/02/2022 06:26:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=199
03/02/2022 06:26:19 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.31486186379803405 on epoch=199
03/02/2022 06:26:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=201
03/02/2022 06:26:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=203
03/02/2022 06:26:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=205
03/02/2022 06:26:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=207
03/02/2022 06:26:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=209
03/02/2022 06:26:33 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.2994969942338363 on epoch=209
03/02/2022 06:26:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=211
03/02/2022 06:26:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=213
03/02/2022 06:26:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=215
03/02/2022 06:26:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=217
03/02/2022 06:26:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=219
03/02/2022 06:26:47 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.30308515014397364 on epoch=219
03/02/2022 06:26:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=221
03/02/2022 06:26:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=223
03/02/2022 06:26:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=225
03/02/2022 06:26:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=227
03/02/2022 06:26:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=229
03/02/2022 06:27:00 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.3188140906434302 on epoch=229
03/02/2022 06:27:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=231
03/02/2022 06:27:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=233
03/02/2022 06:27:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=235
03/02/2022 06:27:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=237
03/02/2022 06:27:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=239
03/02/2022 06:27:14 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.26753613618020394 on epoch=239
03/02/2022 06:27:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=241
03/02/2022 06:27:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=243
03/02/2022 06:27:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=245
03/02/2022 06:27:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=247
03/02/2022 06:27:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=249
03/02/2022 06:27:27 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.32130650217413287 on epoch=249
03/02/2022 06:27:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=251
03/02/2022 06:27:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=253
03/02/2022 06:27:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=255
03/02/2022 06:27:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=257
03/02/2022 06:27:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=259
03/02/2022 06:27:41 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.30733416218580617 on epoch=259
03/02/2022 06:27:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=261
03/02/2022 06:27:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=263
03/02/2022 06:27:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=265
03/02/2022 06:27:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=267
03/02/2022 06:27:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=269
03/02/2022 06:27:55 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.3388074291300097 on epoch=269
03/02/2022 06:27:55 - INFO - __main__ - Saving model with best Classification-F1: 0.33863908640379226 -> 0.3388074291300097 on epoch=269, global_step=1350
03/02/2022 06:27:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=271
03/02/2022 06:27:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=273
03/02/2022 06:28:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=275
03/02/2022 06:28:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=277
03/02/2022 06:28:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=279
03/02/2022 06:28:09 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.2861429350791053 on epoch=279
03/02/2022 06:28:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=281
03/02/2022 06:28:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=283
03/02/2022 06:28:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=285
03/02/2022 06:28:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=287
03/02/2022 06:28:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=289
03/02/2022 06:28:23 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.3200587172538392 on epoch=289
03/02/2022 06:28:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=291
03/02/2022 06:28:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=293
03/02/2022 06:28:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=295
03/02/2022 06:28:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=297
03/02/2022 06:28:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=299
03/02/2022 06:28:37 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.3154462558717878 on epoch=299
03/02/2022 06:28:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=301
03/02/2022 06:28:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=303
03/02/2022 06:28:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=305
03/02/2022 06:28:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=307
03/02/2022 06:28:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=309
03/02/2022 06:28:51 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.2730927698032961 on epoch=309
03/02/2022 06:28:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=311
03/02/2022 06:28:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=313
03/02/2022 06:28:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=315
03/02/2022 06:29:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=317
03/02/2022 06:29:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=319
03/02/2022 06:29:05 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.25956546325431645 on epoch=319
03/02/2022 06:29:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=321
03/02/2022 06:29:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=323
03/02/2022 06:29:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=325
03/02/2022 06:29:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=327
03/02/2022 06:29:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=329
03/02/2022 06:29:19 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.3287131971342498 on epoch=329
03/02/2022 06:29:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=331
03/02/2022 06:29:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=333
03/02/2022 06:29:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=335
03/02/2022 06:29:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=337
03/02/2022 06:29:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=339
03/02/2022 06:29:32 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.2906647746818025 on epoch=339
03/02/2022 06:29:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=341
03/02/2022 06:29:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=343
03/02/2022 06:29:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=345
03/02/2022 06:29:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=347
03/02/2022 06:29:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=349
03/02/2022 06:29:46 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.2778622343839735 on epoch=349
03/02/2022 06:29:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=351
03/02/2022 06:29:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=353
03/02/2022 06:29:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=355
03/02/2022 06:29:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=357
03/02/2022 06:29:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=359
03/02/2022 06:30:00 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.22556446821152704 on epoch=359
03/02/2022 06:30:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=361
03/02/2022 06:30:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=363
03/02/2022 06:30:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=365
03/02/2022 06:30:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=367
03/02/2022 06:30:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=369
03/02/2022 06:30:13 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.27780601575269753 on epoch=369
03/02/2022 06:30:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=371
03/02/2022 06:30:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=373
03/02/2022 06:30:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=375
03/02/2022 06:30:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=377
03/02/2022 06:30:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=379
03/02/2022 06:30:27 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.24728961441035746 on epoch=379
03/02/2022 06:30:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=381
03/02/2022 06:30:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=383
03/02/2022 06:30:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=385
03/02/2022 06:30:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=387
03/02/2022 06:30:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=389
03/02/2022 06:30:41 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.25917239623121974 on epoch=389
03/02/2022 06:30:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=391
03/02/2022 06:30:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=393
03/02/2022 06:30:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=395
03/02/2022 06:30:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=397
03/02/2022 06:30:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=399
03/02/2022 06:30:55 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.22974142974142975 on epoch=399
03/02/2022 06:30:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=401
03/02/2022 06:31:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=403
03/02/2022 06:31:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=405
03/02/2022 06:31:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=407
03/02/2022 06:31:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=409
03/02/2022 06:31:09 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.22902555301296718 on epoch=409
03/02/2022 06:31:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=411
03/02/2022 06:31:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=413
03/02/2022 06:31:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=415
03/02/2022 06:31:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=417
03/02/2022 06:31:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=419
03/02/2022 06:31:23 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.22628205128205126 on epoch=419
03/02/2022 06:31:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=421
03/02/2022 06:31:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=423
03/02/2022 06:31:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=425
03/02/2022 06:31:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=427
03/02/2022 06:31:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=429
03/02/2022 06:31:37 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.24336053261884466 on epoch=429
03/02/2022 06:31:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=431
03/02/2022 06:31:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=433
03/02/2022 06:31:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=435
03/02/2022 06:31:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=437
03/02/2022 06:31:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=439
03/02/2022 06:31:52 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.3614007421150278 on epoch=439
03/02/2022 06:31:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3388074291300097 -> 0.3614007421150278 on epoch=439, global_step=2200
03/02/2022 06:31:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=441
03/02/2022 06:31:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=443
03/02/2022 06:31:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=445
03/02/2022 06:32:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=447
03/02/2022 06:32:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=449
03/02/2022 06:32:06 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.3729540300968872 on epoch=449
03/02/2022 06:32:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3614007421150278 -> 0.3729540300968872 on epoch=449, global_step=2250
03/02/2022 06:32:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=451
03/02/2022 06:32:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=453
03/02/2022 06:32:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=455
03/02/2022 06:32:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=457
03/02/2022 06:32:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=459
03/02/2022 06:32:20 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.31414682539682537 on epoch=459
03/02/2022 06:32:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=461
03/02/2022 06:32:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=463
03/02/2022 06:32:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=465
03/02/2022 06:32:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=467
03/02/2022 06:32:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=469
03/02/2022 06:32:34 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.3478756478668022 on epoch=469
03/02/2022 06:32:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=471
03/02/2022 06:32:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=473
03/02/2022 06:32:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=475
03/02/2022 06:32:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=477
03/02/2022 06:32:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=479
03/02/2022 06:32:48 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.331167163470574 on epoch=479
03/02/2022 06:32:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=481
03/02/2022 06:32:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=483
03/02/2022 06:32:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=485
03/02/2022 06:32:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 06:33:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=489
03/02/2022 06:33:02 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.3280933648580707 on epoch=489
03/02/2022 06:33:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=491
03/02/2022 06:33:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 06:33:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=495
03/02/2022 06:33:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=497
03/02/2022 06:33:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=499
03/02/2022 06:33:17 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2906308772321844 on epoch=499
03/02/2022 06:33:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=501
03/02/2022 06:33:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=503
03/02/2022 06:33:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=505
03/02/2022 06:33:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=507
03/02/2022 06:33:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=509
03/02/2022 06:33:31 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.32107100415923945 on epoch=509
03/02/2022 06:33:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=511
03/02/2022 06:33:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=513
03/02/2022 06:33:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=515
03/02/2022 06:33:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=517
03/02/2022 06:33:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=519
03/02/2022 06:33:44 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.2504515853528344 on epoch=519
03/02/2022 06:33:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=521
03/02/2022 06:33:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=523
03/02/2022 06:33:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=525
03/02/2022 06:33:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 06:33:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=529
03/02/2022 06:33:59 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2520800912957776 on epoch=529
03/02/2022 06:34:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 06:34:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 06:34:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=535
03/02/2022 06:34:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=537
03/02/2022 06:34:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=539
03/02/2022 06:34:13 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.23628212062994672 on epoch=539
03/02/2022 06:34:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=541
03/02/2022 06:34:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=543
03/02/2022 06:34:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=545
03/02/2022 06:34:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=547
03/02/2022 06:34:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=549
03/02/2022 06:34:27 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.28139550597066937 on epoch=549
03/02/2022 06:34:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 06:34:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=553
03/02/2022 06:34:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=555
03/02/2022 06:34:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=557
03/02/2022 06:34:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=559
03/02/2022 06:34:41 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.32349705873454265 on epoch=559
03/02/2022 06:34:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=561
03/02/2022 06:34:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 06:34:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=565
03/02/2022 06:34:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 06:34:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=569
03/02/2022 06:34:55 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.30792207792207793 on epoch=569
03/02/2022 06:34:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=571
03/02/2022 06:34:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=573
03/02/2022 06:35:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=575
03/02/2022 06:35:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=577
03/02/2022 06:35:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=579
03/02/2022 06:35:09 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.29617372256424246 on epoch=579
03/02/2022 06:35:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 06:35:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=583
03/02/2022 06:35:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=585
03/02/2022 06:35:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=587
03/02/2022 06:35:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=589
03/02/2022 06:35:24 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.3448453346855984 on epoch=589
03/02/2022 06:35:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 06:35:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=593
03/02/2022 06:35:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=595
03/02/2022 06:35:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=597
03/02/2022 06:35:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 06:35:36 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:35:36 - INFO - __main__ - Printing 3 examples
03/02/2022 06:35:36 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:35:36 - INFO - __main__ - ['No']
03/02/2022 06:35:36 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:35:36 - INFO - __main__ - ['No']
03/02/2022 06:35:36 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:35:36 - INFO - __main__ - ['No']
03/02/2022 06:35:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 06:35:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:35:36 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:35:36 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:35:36 - INFO - __main__ - Printing 3 examples
03/02/2022 06:35:36 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:35:36 - INFO - __main__ - ['No']
03/02/2022 06:35:36 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:35:36 - INFO - __main__ - ['No']
03/02/2022 06:35:37 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:35:37 - INFO - __main__ - ['No']
03/02/2022 06:35:37 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:35:37 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:35:37 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:35:37 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.39498826557650085 on epoch=599
03/02/2022 06:35:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3729540300968872 -> 0.39498826557650085 on epoch=599, global_step=3000
03/02/2022 06:35:37 - INFO - __main__ - save last model!
03/02/2022 06:35:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 06:35:37 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 06:35:37 - INFO - __main__ - Printing 3 examples
03/02/2022 06:35:37 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 06:35:37 - INFO - __main__ - ['No']
03/02/2022 06:35:37 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 06:35:37 - INFO - __main__ - ['Yes']
03/02/2022 06:35:37 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 06:35:37 - INFO - __main__ - ['Yes']
03/02/2022 06:35:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:35:40 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:35:47 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 06:35:49 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:35:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:35:50 - INFO - __main__ - Starting training!
03/02/2022 06:39:45 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_13_0.4_8_predictions.txt
03/02/2022 06:39:45 - INFO - __main__ - Classification-F1 on test data: 0.1177
03/02/2022 06:39:46 - INFO - __main__ - prefix=circa_16_13, lr=0.4, bsz=8, dev_performance=0.39498826557650085, test_performance=0.11772717254515404
03/02/2022 06:39:46 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.3, bsz=8 ...
03/02/2022 06:39:47 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:39:47 - INFO - __main__ - Printing 3 examples
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:39:47 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:39:47 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:39:47 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:39:47 - INFO - __main__ - Printing 3 examples
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:39:47 - INFO - __main__ - ['No']
03/02/2022 06:39:47 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:39:47 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:39:47 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:39:59 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:40:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:40:00 - INFO - __main__ - Starting training!
03/02/2022 06:40:03 - INFO - __main__ - Step 10 Global step 10 Train loss 3.76 on epoch=1
03/02/2022 06:40:05 - INFO - __main__ - Step 20 Global step 20 Train loss 2.85 on epoch=3
03/02/2022 06:40:07 - INFO - __main__ - Step 30 Global step 30 Train loss 2.33 on epoch=5
03/02/2022 06:40:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.72 on epoch=7
03/02/2022 06:40:12 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=9
03/02/2022 06:40:14 - INFO - __main__ - Global step 50 Train loss 2.40 Classification-F1 0.056140350877192984 on epoch=9
03/02/2022 06:40:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.056140350877192984 on epoch=9, global_step=50
03/02/2022 06:40:16 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=11
03/02/2022 06:40:18 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=13
03/02/2022 06:40:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=15
03/02/2022 06:40:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.75 on epoch=17
03/02/2022 06:40:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=19
03/02/2022 06:40:28 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.15149342891278375 on epoch=19
03/02/2022 06:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.056140350877192984 -> 0.15149342891278375 on epoch=19, global_step=100
03/02/2022 06:40:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.71 on epoch=21
03/02/2022 06:40:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=23
03/02/2022 06:40:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=25
03/02/2022 06:40:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=27
03/02/2022 06:40:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=29
03/02/2022 06:40:41 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 06:40:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=31
03/02/2022 06:40:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.63 on epoch=33
03/02/2022 06:40:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=35
03/02/2022 06:40:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=37
03/02/2022 06:40:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=39
03/02/2022 06:40:54 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 06:40:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=41
03/02/2022 06:40:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=43
03/02/2022 06:41:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=45
03/02/2022 06:41:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=47
03/02/2022 06:41:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=49
03/02/2022 06:41:08 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.055913978494623665 on epoch=49
03/02/2022 06:41:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=51
03/02/2022 06:41:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=53
03/02/2022 06:41:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 06:41:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=57
03/02/2022 06:41:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=59
03/02/2022 06:41:21 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 06:41:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=61
03/02/2022 06:41:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=63
03/02/2022 06:41:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=65
03/02/2022 06:41:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=67
03/02/2022 06:41:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 06:41:35 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.05957446808510638 on epoch=69
03/02/2022 06:41:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=71
03/02/2022 06:41:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=73
03/02/2022 06:41:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=75
03/02/2022 06:41:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=77
03/02/2022 06:41:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=79
03/02/2022 06:41:49 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.10606060606060605 on epoch=79
03/02/2022 06:41:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=81
03/02/2022 06:41:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=83
03/02/2022 06:41:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=85
03/02/2022 06:41:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=87
03/02/2022 06:42:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=89
03/02/2022 06:42:03 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.18233618233618235 on epoch=89
03/02/2022 06:42:03 - INFO - __main__ - Saving model with best Classification-F1: 0.15149342891278375 -> 0.18233618233618235 on epoch=89, global_step=450
03/02/2022 06:42:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=91
03/02/2022 06:42:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=93
03/02/2022 06:42:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=95
03/02/2022 06:42:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=97
03/02/2022 06:42:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=99
03/02/2022 06:42:17 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.3283412114935693 on epoch=99
03/02/2022 06:42:17 - INFO - __main__ - Saving model with best Classification-F1: 0.18233618233618235 -> 0.3283412114935693 on epoch=99, global_step=500
03/02/2022 06:42:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=101
03/02/2022 06:42:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=103
03/02/2022 06:42:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=105
03/02/2022 06:42:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=107
03/02/2022 06:42:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=109
03/02/2022 06:42:31 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.33286580189571924 on epoch=109
03/02/2022 06:42:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3283412114935693 -> 0.33286580189571924 on epoch=109, global_step=550
03/02/2022 06:42:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=111
03/02/2022 06:42:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=113
03/02/2022 06:42:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=115
03/02/2022 06:42:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=117
03/02/2022 06:42:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=119
03/02/2022 06:42:44 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.3183882339562564 on epoch=119
03/02/2022 06:42:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=121
03/02/2022 06:42:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=123
03/02/2022 06:42:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=125
03/02/2022 06:42:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=127
03/02/2022 06:42:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=129
03/02/2022 06:42:57 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.27005931137186434 on epoch=129
03/02/2022 06:42:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=131
03/02/2022 06:43:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=133
03/02/2022 06:43:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=135
03/02/2022 06:43:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=137
03/02/2022 06:43:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=139
03/02/2022 06:43:10 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.2758750907770516 on epoch=139
03/02/2022 06:43:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=141
03/02/2022 06:43:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=143
03/02/2022 06:43:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=145
03/02/2022 06:43:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=147
03/02/2022 06:43:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=149
03/02/2022 06:43:24 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.3124152886961236 on epoch=149
03/02/2022 06:43:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=151
03/02/2022 06:43:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=153
03/02/2022 06:43:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=155
03/02/2022 06:43:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=157
03/02/2022 06:43:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=159
03/02/2022 06:43:37 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.32359319595046 on epoch=159
03/02/2022 06:43:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=161
03/02/2022 06:43:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=163
03/02/2022 06:43:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=165
03/02/2022 06:43:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=167
03/02/2022 06:43:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=169
03/02/2022 06:43:51 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.35893812070282655 on epoch=169
03/02/2022 06:43:51 - INFO - __main__ - Saving model with best Classification-F1: 0.33286580189571924 -> 0.35893812070282655 on epoch=169, global_step=850
03/02/2022 06:43:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=171
03/02/2022 06:43:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=173
03/02/2022 06:43:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=175
03/02/2022 06:44:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=177
03/02/2022 06:44:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=179
03/02/2022 06:44:05 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.34137862137862135 on epoch=179
03/02/2022 06:44:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=181
03/02/2022 06:44:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=183
03/02/2022 06:44:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=185
03/02/2022 06:44:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=187
03/02/2022 06:44:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=189
03/02/2022 06:44:18 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.3183922558922559 on epoch=189
03/02/2022 06:44:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=191
03/02/2022 06:44:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=193
03/02/2022 06:44:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=195
03/02/2022 06:44:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=197
03/02/2022 06:44:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=199
03/02/2022 06:44:32 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.32121212121212117 on epoch=199
03/02/2022 06:44:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=201
03/02/2022 06:44:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=203
03/02/2022 06:44:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=205
03/02/2022 06:44:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=207
03/02/2022 06:44:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=209
03/02/2022 06:44:46 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.33165945165945165 on epoch=209
03/02/2022 06:44:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=211
03/02/2022 06:44:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=213
03/02/2022 06:44:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=215
03/02/2022 06:44:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=217
03/02/2022 06:44:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=219
03/02/2022 06:44:59 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.30462519936204147 on epoch=219
03/02/2022 06:45:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=221
03/02/2022 06:45:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=223
03/02/2022 06:45:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=225
03/02/2022 06:45:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=227
03/02/2022 06:45:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=229
03/02/2022 06:45:13 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.30595611285266455 on epoch=229
03/02/2022 06:45:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=231
03/02/2022 06:45:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=233
03/02/2022 06:45:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=235
03/02/2022 06:45:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=237
03/02/2022 06:45:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=239
03/02/2022 06:45:27 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.26415882967607107 on epoch=239
03/02/2022 06:45:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=241
03/02/2022 06:45:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=243
03/02/2022 06:45:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=245
03/02/2022 06:45:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=247
03/02/2022 06:45:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=249
03/02/2022 06:45:40 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.331637519872814 on epoch=249
03/02/2022 06:45:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=251
03/02/2022 06:45:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=253
03/02/2022 06:45:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=255
03/02/2022 06:45:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=257
03/02/2022 06:45:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=259
03/02/2022 06:45:54 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3549760765550239 on epoch=259
03/02/2022 06:45:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=261
03/02/2022 06:45:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=263
03/02/2022 06:46:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=265
03/02/2022 06:46:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=267
03/02/2022 06:46:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=269
03/02/2022 06:46:08 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.3718311100664041 on epoch=269
03/02/2022 06:46:08 - INFO - __main__ - Saving model with best Classification-F1: 0.35893812070282655 -> 0.3718311100664041 on epoch=269, global_step=1350
03/02/2022 06:46:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=271
03/02/2022 06:46:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=273
03/02/2022 06:46:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=275
03/02/2022 06:46:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=277
03/02/2022 06:46:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=279
03/02/2022 06:46:22 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.3507716389072321 on epoch=279
03/02/2022 06:46:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=281
03/02/2022 06:46:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=283
03/02/2022 06:46:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=285
03/02/2022 06:46:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=287
03/02/2022 06:46:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=289
03/02/2022 06:46:36 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.26086259419592756 on epoch=289
03/02/2022 06:46:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=291
03/02/2022 06:46:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=293
03/02/2022 06:46:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=295
03/02/2022 06:46:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=297
03/02/2022 06:46:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=299
03/02/2022 06:46:49 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.23194444444444445 on epoch=299
03/02/2022 06:46:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=301
03/02/2022 06:46:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=303
03/02/2022 06:46:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=305
03/02/2022 06:46:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=307
03/02/2022 06:47:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=309
03/02/2022 06:47:03 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.2877331002331002 on epoch=309
03/02/2022 06:47:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=311
03/02/2022 06:47:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=313
03/02/2022 06:47:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=315
03/02/2022 06:47:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=317
03/02/2022 06:47:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=319
03/02/2022 06:47:17 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.35699512128083555 on epoch=319
03/02/2022 06:47:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=321
03/02/2022 06:47:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=323
03/02/2022 06:47:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=325
03/02/2022 06:47:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=327
03/02/2022 06:47:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=329
03/02/2022 06:47:32 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.266962381769684 on epoch=329
03/02/2022 06:47:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=331
03/02/2022 06:47:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=333
03/02/2022 06:47:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=335
03/02/2022 06:47:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=337
03/02/2022 06:47:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=339
03/02/2022 06:47:46 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.3666490641319017 on epoch=339
03/02/2022 06:47:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=341
03/02/2022 06:47:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=343
03/02/2022 06:47:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=345
03/02/2022 06:47:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=347
03/02/2022 06:47:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=349
03/02/2022 06:48:00 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.3755829516699081 on epoch=349
03/02/2022 06:48:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3718311100664041 -> 0.3755829516699081 on epoch=349, global_step=1750
03/02/2022 06:48:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=351
03/02/2022 06:48:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=353
03/02/2022 06:48:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=355
03/02/2022 06:48:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=357
03/02/2022 06:48:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=359
03/02/2022 06:48:15 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.5007122482984553 on epoch=359
03/02/2022 06:48:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3755829516699081 -> 0.5007122482984553 on epoch=359, global_step=1800
03/02/2022 06:48:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=361
03/02/2022 06:48:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=363
03/02/2022 06:48:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=365
03/02/2022 06:48:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=367
03/02/2022 06:48:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=369
03/02/2022 06:48:29 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.41604582317240096 on epoch=369
03/02/2022 06:48:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=371
03/02/2022 06:48:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=373
03/02/2022 06:48:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=375
03/02/2022 06:48:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=377
03/02/2022 06:48:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=379
03/02/2022 06:48:43 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.24005873445601797 on epoch=379
03/02/2022 06:48:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=381
03/02/2022 06:48:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=383
03/02/2022 06:48:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=385
03/02/2022 06:48:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=387
03/02/2022 06:48:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=389
03/02/2022 06:48:58 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.4642055816523902 on epoch=389
03/02/2022 06:49:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=391
03/02/2022 06:49:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=393
03/02/2022 06:49:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=395
03/02/2022 06:49:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=397
03/02/2022 06:49:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=399
03/02/2022 06:49:12 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.3306990668428852 on epoch=399
03/02/2022 06:49:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=401
03/02/2022 06:49:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=403
03/02/2022 06:49:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=405
03/02/2022 06:49:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=407
03/02/2022 06:49:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=409
03/02/2022 06:49:26 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.36634908605351957 on epoch=409
03/02/2022 06:49:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=411
03/02/2022 06:49:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=413
03/02/2022 06:49:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=415
03/02/2022 06:49:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=417
03/02/2022 06:49:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=419
03/02/2022 06:49:40 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.34525622161090136 on epoch=419
03/02/2022 06:49:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=421
03/02/2022 06:49:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=423
03/02/2022 06:49:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=425
03/02/2022 06:49:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=427
03/02/2022 06:49:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=429
03/02/2022 06:49:54 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.2236159562362771 on epoch=429
03/02/2022 06:49:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=431
03/02/2022 06:49:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=433
03/02/2022 06:50:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 06:50:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=437
03/02/2022 06:50:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=439
03/02/2022 06:50:08 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.2626984126984127 on epoch=439
03/02/2022 06:50:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=441
03/02/2022 06:50:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=443
03/02/2022 06:50:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=445
03/02/2022 06:50:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=447
03/02/2022 06:50:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=449
03/02/2022 06:50:22 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.378481240981241 on epoch=449
03/02/2022 06:50:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=451
03/02/2022 06:50:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=453
03/02/2022 06:50:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=455
03/02/2022 06:50:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=457
03/02/2022 06:50:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=459
03/02/2022 06:50:36 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.3530710329097426 on epoch=459
03/02/2022 06:50:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=461
03/02/2022 06:50:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=463
03/02/2022 06:50:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=465
03/02/2022 06:50:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=467
03/02/2022 06:50:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=469
03/02/2022 06:50:51 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.36992721330956624 on epoch=469
03/02/2022 06:50:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=471
03/02/2022 06:50:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=473
03/02/2022 06:50:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=475
03/02/2022 06:51:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=477
03/02/2022 06:51:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=479
03/02/2022 06:51:05 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.3603761175900154 on epoch=479
03/02/2022 06:51:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=481
03/02/2022 06:51:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=483
03/02/2022 06:51:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=485
03/02/2022 06:51:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 06:51:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=489
03/02/2022 06:51:19 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.4998011868390721 on epoch=489
03/02/2022 06:51:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=491
03/02/2022 06:51:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=493
03/02/2022 06:51:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=495
03/02/2022 06:51:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 06:51:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=499
03/02/2022 06:51:34 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.4789751546217063 on epoch=499
03/02/2022 06:51:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=501
03/02/2022 06:51:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=503
03/02/2022 06:51:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=505
03/02/2022 06:51:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=507
03/02/2022 06:51:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=509
03/02/2022 06:51:48 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5029048656499636 on epoch=509
03/02/2022 06:51:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5007122482984553 -> 0.5029048656499636 on epoch=509, global_step=2550
03/02/2022 06:51:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=511
03/02/2022 06:51:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=513
03/02/2022 06:51:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 06:51:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=517
03/02/2022 06:52:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=519
03/02/2022 06:52:03 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.30509739048638473 on epoch=519
03/02/2022 06:52:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=521
03/02/2022 06:52:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=523
03/02/2022 06:52:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=525
03/02/2022 06:52:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 06:52:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=529
03/02/2022 06:52:17 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.41779332422624227 on epoch=529
03/02/2022 06:52:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=531
03/02/2022 06:52:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 06:52:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=535
03/02/2022 06:52:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=537
03/02/2022 06:52:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=539
03/02/2022 06:52:32 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.4809464420142951 on epoch=539
03/02/2022 06:52:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=541
03/02/2022 06:52:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=543
03/02/2022 06:52:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 06:52:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=547
03/02/2022 06:52:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 06:52:46 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.4921620541314939 on epoch=549
03/02/2022 06:52:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 06:52:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 06:52:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=555
03/02/2022 06:52:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=557
03/02/2022 06:52:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=559
03/02/2022 06:53:00 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.5271768726129781 on epoch=559
03/02/2022 06:53:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5029048656499636 -> 0.5271768726129781 on epoch=559, global_step=2800
03/02/2022 06:53:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=561
03/02/2022 06:53:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 06:53:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=565
03/02/2022 06:53:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=567
03/02/2022 06:53:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=569
03/02/2022 06:53:14 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5222808268785281 on epoch=569
03/02/2022 06:53:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=571
03/02/2022 06:53:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=573
03/02/2022 06:53:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=575
03/02/2022 06:53:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=577
03/02/2022 06:53:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=579
03/02/2022 06:53:29 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.4837560055950861 on epoch=579
03/02/2022 06:53:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 06:53:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=583
03/02/2022 06:53:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=585
03/02/2022 06:53:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=587
03/02/2022 06:53:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=589
03/02/2022 06:53:43 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.3765192653350548 on epoch=589
03/02/2022 06:53:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=591
03/02/2022 06:53:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=593
03/02/2022 06:53:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=595
03/02/2022 06:53:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=597
03/02/2022 06:53:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=599
03/02/2022 06:53:55 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:53:55 - INFO - __main__ - Printing 3 examples
03/02/2022 06:53:55 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:53:55 - INFO - __main__ - ['No']
03/02/2022 06:53:55 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:53:55 - INFO - __main__ - ['No']
03/02/2022 06:53:55 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:53:55 - INFO - __main__ - ['No']
03/02/2022 06:53:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 06:53:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:53:56 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:53:56 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:53:56 - INFO - __main__ - Printing 3 examples
03/02/2022 06:53:56 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:53:56 - INFO - __main__ - ['No']
03/02/2022 06:53:56 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:53:56 - INFO - __main__ - ['No']
03/02/2022 06:53:56 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:53:56 - INFO - __main__ - ['No']
03/02/2022 06:53:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:53:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:53:56 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:53:57 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3144770987908243 on epoch=599
03/02/2022 06:53:57 - INFO - __main__ - save last model!
03/02/2022 06:53:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 06:53:57 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 06:53:57 - INFO - __main__ - Printing 3 examples
03/02/2022 06:53:57 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 06:53:57 - INFO - __main__ - ['No']
03/02/2022 06:53:57 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 06:53:57 - INFO - __main__ - ['Yes']
03/02/2022 06:53:57 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 06:53:57 - INFO - __main__ - ['Yes']
03/02/2022 06:53:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:54:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:54:07 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 06:54:10 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:54:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:54:10 - INFO - __main__ - Starting training!
03/02/2022 06:58:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_13_0.3_8_predictions.txt
03/02/2022 06:58:28 - INFO - __main__ - Classification-F1 on test data: 0.0557
03/02/2022 06:58:28 - INFO - __main__ - prefix=circa_16_13, lr=0.3, bsz=8, dev_performance=0.5271768726129781, test_performance=0.05570846399987052
03/02/2022 06:58:28 - INFO - __main__ - Running ... prefix=circa_16_13, lr=0.2, bsz=8 ...
03/02/2022 06:58:29 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:58:29 - INFO - __main__ - Printing 3 examples
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: Y has just moved into a neighbourhood and meets his/her new neighbour X. [SEP] question X: You living alone? [SEP] answer Y: My roommate is out of town.
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Do you smoke? [SEP] answer Y: I tried a few cigarettes in college.
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Have you ever ready Stephen King books? [SEP] answer Y: Who is he?
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 06:58:29 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:58:29 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 06:58:29 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 06:58:29 - INFO - __main__ - Printing 3 examples
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Do you enjoy cooking or baking? [SEP] answer Y: It isn't something I enjoy.
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are your kids still living with you? [SEP] answer Y: I'm an empty nester.
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you like long books better than short ones? [SEP] answer Y: I prefer shorter books because I don't have much time for reading.
03/02/2022 06:58:29 - INFO - __main__ - ['No']
03/02/2022 06:58:29 - INFO - __main__ - Tokenizing Input ...
03/02/2022 06:58:29 - INFO - __main__ - Tokenizing Output ...
03/02/2022 06:58:30 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 06:58:44 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 06:58:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 06:58:44 - INFO - __main__ - Starting training!
03/02/2022 06:58:47 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=1
03/02/2022 06:58:50 - INFO - __main__ - Step 20 Global step 20 Train loss 3.15 on epoch=3
03/02/2022 06:58:52 - INFO - __main__ - Step 30 Global step 30 Train loss 2.75 on epoch=5
03/02/2022 06:58:54 - INFO - __main__ - Step 40 Global step 40 Train loss 2.25 on epoch=7
03/02/2022 06:58:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.89 on epoch=9
03/02/2022 06:58:58 - INFO - __main__ - Global step 50 Train loss 2.80 Classification-F1 0.0819047619047619 on epoch=9
03/02/2022 06:58:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0819047619047619 on epoch=9, global_step=50
03/02/2022 06:59:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=11
03/02/2022 06:59:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.34 on epoch=13
03/02/2022 06:59:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.18 on epoch=15
03/02/2022 06:59:07 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=17
03/02/2022 06:59:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=19
03/02/2022 06:59:11 - INFO - __main__ - Global step 100 Train loss 1.22 Classification-F1 0.07121212121212121 on epoch=19
03/02/2022 06:59:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=21
03/02/2022 06:59:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=23
03/02/2022 06:59:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=25
03/02/2022 06:59:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.66 on epoch=27
03/02/2022 06:59:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=29
03/02/2022 06:59:25 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.09103840682788052 on epoch=29
03/02/2022 06:59:25 - INFO - __main__ - Saving model with best Classification-F1: 0.0819047619047619 -> 0.09103840682788052 on epoch=29, global_step=150
03/02/2022 06:59:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=31
03/02/2022 06:59:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=33
03/02/2022 06:59:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=35
03/02/2022 06:59:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=37
03/02/2022 06:59:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=39
03/02/2022 06:59:39 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.1295238095238095 on epoch=39
03/02/2022 06:59:39 - INFO - __main__ - Saving model with best Classification-F1: 0.09103840682788052 -> 0.1295238095238095 on epoch=39, global_step=200
03/02/2022 06:59:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=41
03/02/2022 06:59:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=43
03/02/2022 06:59:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=45
03/02/2022 06:59:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=47
03/02/2022 06:59:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=49
03/02/2022 06:59:52 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 06:59:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=51
03/02/2022 06:59:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=53
03/02/2022 06:59:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 07:00:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=57
03/02/2022 07:00:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=59
03/02/2022 07:00:05 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 07:00:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=61
03/02/2022 07:00:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=63
03/02/2022 07:00:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=65
03/02/2022 07:00:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=67
03/02/2022 07:00:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=69
03/02/2022 07:00:19 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.08938321536905966 on epoch=69
03/02/2022 07:00:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=71
03/02/2022 07:00:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=73
03/02/2022 07:00:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=75
03/02/2022 07:00:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=77
03/02/2022 07:00:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=79
03/02/2022 07:00:33 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.06153846153846153 on epoch=79
03/02/2022 07:00:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=81
03/02/2022 07:00:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=83
03/02/2022 07:00:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=85
03/02/2022 07:00:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=87
03/02/2022 07:00:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=89
03/02/2022 07:00:46 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.08480703468490473 on epoch=89
03/02/2022 07:00:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=91
03/02/2022 07:00:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=93
03/02/2022 07:00:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=95
03/02/2022 07:00:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=97
03/02/2022 07:00:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=99
03/02/2022 07:01:00 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.21655172413793103 on epoch=99
03/02/2022 07:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1295238095238095 -> 0.21655172413793103 on epoch=99, global_step=500
03/02/2022 07:01:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=101
03/02/2022 07:01:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=103
03/02/2022 07:01:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=105
03/02/2022 07:01:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=107
03/02/2022 07:01:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=109
03/02/2022 07:01:14 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.203988603988604 on epoch=109
03/02/2022 07:01:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=111
03/02/2022 07:01:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=113
03/02/2022 07:01:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=115
03/02/2022 07:01:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=117
03/02/2022 07:01:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=119
03/02/2022 07:01:27 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.2263157894736842 on epoch=119
03/02/2022 07:01:27 - INFO - __main__ - Saving model with best Classification-F1: 0.21655172413793103 -> 0.2263157894736842 on epoch=119, global_step=600
03/02/2022 07:01:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=121
03/02/2022 07:01:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=123
03/02/2022 07:01:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=125
03/02/2022 07:01:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=127
03/02/2022 07:01:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=129
03/02/2022 07:01:41 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.21271794871794875 on epoch=129
03/02/2022 07:01:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=131
03/02/2022 07:01:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=133
03/02/2022 07:01:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=135
03/02/2022 07:01:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=137
03/02/2022 07:01:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=139
03/02/2022 07:01:55 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.2566036331149494 on epoch=139
03/02/2022 07:01:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2263157894736842 -> 0.2566036331149494 on epoch=139, global_step=700
03/02/2022 07:01:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=141
03/02/2022 07:01:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=143
03/02/2022 07:02:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=145
03/02/2022 07:02:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=147
03/02/2022 07:02:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=149
03/02/2022 07:02:09 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.2507843352347521 on epoch=149
03/02/2022 07:02:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=151
03/02/2022 07:02:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=153
03/02/2022 07:02:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=155
03/02/2022 07:02:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=157
03/02/2022 07:02:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=159
03/02/2022 07:02:23 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.30528100908432565 on epoch=159
03/02/2022 07:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2566036331149494 -> 0.30528100908432565 on epoch=159, global_step=800
03/02/2022 07:02:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=161
03/02/2022 07:02:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=163
03/02/2022 07:02:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=165
03/02/2022 07:02:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=167
03/02/2022 07:02:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=169
03/02/2022 07:02:37 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.2755211013896037 on epoch=169
03/02/2022 07:02:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=171
03/02/2022 07:02:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=173
03/02/2022 07:02:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=175
03/02/2022 07:02:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=177
03/02/2022 07:02:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=179
03/02/2022 07:02:50 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.24551724137931036 on epoch=179
03/02/2022 07:02:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=181
03/02/2022 07:02:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=183
03/02/2022 07:02:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=185
03/02/2022 07:02:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=187
03/02/2022 07:03:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=189
03/02/2022 07:03:04 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.2723923780038331 on epoch=189
03/02/2022 07:03:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.31 on epoch=191
03/02/2022 07:03:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=193
03/02/2022 07:03:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=195
03/02/2022 07:03:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=197
03/02/2022 07:03:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=199
03/02/2022 07:03:17 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.3072315314813784 on epoch=199
03/02/2022 07:03:17 - INFO - __main__ - Saving model with best Classification-F1: 0.30528100908432565 -> 0.3072315314813784 on epoch=199, global_step=1000
03/02/2022 07:03:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=201
03/02/2022 07:03:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=203
03/02/2022 07:03:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=205
03/02/2022 07:03:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=207
03/02/2022 07:03:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=209
03/02/2022 07:03:31 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.3476790385295157 on epoch=209
03/02/2022 07:03:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3072315314813784 -> 0.3476790385295157 on epoch=209, global_step=1050
03/02/2022 07:03:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=211
03/02/2022 07:03:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=213
03/02/2022 07:03:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=215
03/02/2022 07:03:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=217
03/02/2022 07:03:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=219
03/02/2022 07:03:45 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.29292017188161956 on epoch=219
03/02/2022 07:03:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=221
03/02/2022 07:03:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=223
03/02/2022 07:03:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=225
03/02/2022 07:03:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=227
03/02/2022 07:03:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=229
03/02/2022 07:03:59 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.2584192361491212 on epoch=229
03/02/2022 07:04:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=231
03/02/2022 07:04:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=233
03/02/2022 07:04:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=235
03/02/2022 07:04:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=237
03/02/2022 07:04:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=239
03/02/2022 07:04:13 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.3441557496993039 on epoch=239
03/02/2022 07:04:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=241
03/02/2022 07:04:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=243
03/02/2022 07:04:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=245
03/02/2022 07:04:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=247
03/02/2022 07:04:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=249
03/02/2022 07:04:26 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.2995116174148432 on epoch=249
03/02/2022 07:04:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=251
03/02/2022 07:04:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=253
03/02/2022 07:04:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=255
03/02/2022 07:04:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=257
03/02/2022 07:04:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=259
03/02/2022 07:04:40 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.22826770554965284 on epoch=259
03/02/2022 07:04:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=261
03/02/2022 07:04:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=263
03/02/2022 07:04:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=265
03/02/2022 07:04:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=267
03/02/2022 07:04:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=269
03/02/2022 07:04:54 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.24191445514572907 on epoch=269
03/02/2022 07:04:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=271
03/02/2022 07:04:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=273
03/02/2022 07:05:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=275
03/02/2022 07:05:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=277
03/02/2022 07:05:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=279
03/02/2022 07:05:07 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3272224549643904 on epoch=279
03/02/2022 07:05:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=281
03/02/2022 07:05:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=283
03/02/2022 07:05:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=285
03/02/2022 07:05:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=287
03/02/2022 07:05:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=289
03/02/2022 07:05:21 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.24904601571268237 on epoch=289
03/02/2022 07:05:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=291
03/02/2022 07:05:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=293
03/02/2022 07:05:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=295
03/02/2022 07:05:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=297
03/02/2022 07:05:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=299
03/02/2022 07:05:34 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.2842577030812325 on epoch=299
03/02/2022 07:05:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=301
03/02/2022 07:05:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=303
03/02/2022 07:05:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=305
03/02/2022 07:05:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=307
03/02/2022 07:05:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=309
03/02/2022 07:05:48 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.29595539595539594 on epoch=309
03/02/2022 07:05:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=311
03/02/2022 07:05:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=313
03/02/2022 07:05:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=315
03/02/2022 07:05:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=317
03/02/2022 07:05:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=319
03/02/2022 07:06:01 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.2922943722943723 on epoch=319
03/02/2022 07:06:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=321
03/02/2022 07:06:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=323
03/02/2022 07:06:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=325
03/02/2022 07:06:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=327
03/02/2022 07:06:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=329
03/02/2022 07:06:15 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.3243389740877843 on epoch=329
03/02/2022 07:06:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=331
03/02/2022 07:06:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=333
03/02/2022 07:06:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=335
03/02/2022 07:06:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=337
03/02/2022 07:06:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=339
03/02/2022 07:06:29 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.3199733180865256 on epoch=339
03/02/2022 07:06:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=341
03/02/2022 07:06:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=343
03/02/2022 07:06:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=345
03/02/2022 07:06:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=347
03/02/2022 07:06:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=349
03/02/2022 07:06:42 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.2587878787878788 on epoch=349
03/02/2022 07:06:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=351
03/02/2022 07:06:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=353
03/02/2022 07:06:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=355
03/02/2022 07:06:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=357
03/02/2022 07:06:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=359
03/02/2022 07:06:56 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.2786059878496853 on epoch=359
03/02/2022 07:06:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=361
03/02/2022 07:07:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=363
03/02/2022 07:07:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=365
03/02/2022 07:07:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=367
03/02/2022 07:07:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=369
03/02/2022 07:07:09 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.28833333333333333 on epoch=369
03/02/2022 07:07:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=371
03/02/2022 07:07:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=373
03/02/2022 07:07:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=375
03/02/2022 07:07:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=377
03/02/2022 07:07:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=379
03/02/2022 07:07:23 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.2505952380952381 on epoch=379
03/02/2022 07:07:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=381
03/02/2022 07:07:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=383
03/02/2022 07:07:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=385
03/02/2022 07:07:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=387
03/02/2022 07:07:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=389
03/02/2022 07:07:36 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.26415882967607107 on epoch=389
03/02/2022 07:07:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=391
03/02/2022 07:07:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=393
03/02/2022 07:07:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=395
03/02/2022 07:07:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=397
03/02/2022 07:07:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=399
03/02/2022 07:07:50 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.24872716382150342 on epoch=399
03/02/2022 07:07:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=401
03/02/2022 07:07:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=403
03/02/2022 07:07:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=405
03/02/2022 07:07:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=407
03/02/2022 07:08:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=409
03/02/2022 07:08:04 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.3921929824561403 on epoch=409
03/02/2022 07:08:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3476790385295157 -> 0.3921929824561403 on epoch=409, global_step=2050
03/02/2022 07:08:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=411
03/02/2022 07:08:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=413
03/02/2022 07:08:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=415
03/02/2022 07:08:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=417
03/02/2022 07:08:15 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=419
03/02/2022 07:08:17 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.2625 on epoch=419
03/02/2022 07:08:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=421
03/02/2022 07:08:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=423
03/02/2022 07:08:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=425
03/02/2022 07:08:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=427
03/02/2022 07:08:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=429
03/02/2022 07:08:31 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.2921145667198299 on epoch=429
03/02/2022 07:08:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=431
03/02/2022 07:08:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=433
03/02/2022 07:08:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=435
03/02/2022 07:08:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=437
03/02/2022 07:08:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=439
03/02/2022 07:08:45 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.37611392116655273 on epoch=439
03/02/2022 07:08:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=441
03/02/2022 07:08:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=443
03/02/2022 07:08:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=445
03/02/2022 07:08:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=447
03/02/2022 07:08:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=449
03/02/2022 07:08:59 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.3421597350541354 on epoch=449
03/02/2022 07:09:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=451
03/02/2022 07:09:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=453
03/02/2022 07:09:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=455
03/02/2022 07:09:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=457
03/02/2022 07:09:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=459
03/02/2022 07:09:13 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.38755503755503756 on epoch=459
03/02/2022 07:09:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=461
03/02/2022 07:09:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=463
03/02/2022 07:09:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=465
03/02/2022 07:09:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=467
03/02/2022 07:09:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=469
03/02/2022 07:09:27 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.31493506493506496 on epoch=469
03/02/2022 07:09:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=471
03/02/2022 07:09:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=473
03/02/2022 07:09:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=475
03/02/2022 07:09:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=477
03/02/2022 07:09:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=479
03/02/2022 07:09:41 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.40177303369523043 on epoch=479
03/02/2022 07:09:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3921929824561403 -> 0.40177303369523043 on epoch=479, global_step=2400
03/02/2022 07:09:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=481
03/02/2022 07:09:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=483
03/02/2022 07:09:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=485
03/02/2022 07:09:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=487
03/02/2022 07:09:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=489
03/02/2022 07:09:55 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.3611328976034858 on epoch=489
03/02/2022 07:09:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=491
03/02/2022 07:10:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=493
03/02/2022 07:10:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=495
03/02/2022 07:10:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=497
03/02/2022 07:10:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=499
03/02/2022 07:10:10 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.30661729574773056 on epoch=499
03/02/2022 07:10:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=501
03/02/2022 07:10:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=503
03/02/2022 07:10:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=505
03/02/2022 07:10:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=507
03/02/2022 07:10:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=509
03/02/2022 07:10:24 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.4174301151045337 on epoch=509
03/02/2022 07:10:24 - INFO - __main__ - Saving model with best Classification-F1: 0.40177303369523043 -> 0.4174301151045337 on epoch=509, global_step=2550
03/02/2022 07:10:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=511
03/02/2022 07:10:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=513
03/02/2022 07:10:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=515
03/02/2022 07:10:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=517
03/02/2022 07:10:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=519
03/02/2022 07:10:38 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.38203054255685837 on epoch=519
03/02/2022 07:10:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=521
03/02/2022 07:10:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=523
03/02/2022 07:10:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=525
03/02/2022 07:10:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=527
03/02/2022 07:10:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=529
03/02/2022 07:10:52 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.3015028937949831 on epoch=529
03/02/2022 07:10:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=531
03/02/2022 07:10:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=533
03/02/2022 07:10:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=535
03/02/2022 07:11:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=537
03/02/2022 07:11:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=539
03/02/2022 07:11:06 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.30144300144300146 on epoch=539
03/02/2022 07:11:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=541
03/02/2022 07:11:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=543
03/02/2022 07:11:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=545
03/02/2022 07:11:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=547
03/02/2022 07:11:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=549
03/02/2022 07:11:20 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.34384319384319384 on epoch=549
03/02/2022 07:11:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=551
03/02/2022 07:11:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=553
03/02/2022 07:11:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=555
03/02/2022 07:11:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=557
03/02/2022 07:11:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=559
03/02/2022 07:11:34 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.36087365631686014 on epoch=559
03/02/2022 07:11:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=561
03/02/2022 07:11:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=563
03/02/2022 07:11:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=565
03/02/2022 07:11:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=567
03/02/2022 07:11:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=569
03/02/2022 07:11:48 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.209652076318743 on epoch=569
03/02/2022 07:11:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=571
03/02/2022 07:11:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=573
03/02/2022 07:11:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=575
03/02/2022 07:11:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=577
03/02/2022 07:12:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=579
03/02/2022 07:12:03 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.24776334776334774 on epoch=579
03/02/2022 07:12:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=581
03/02/2022 07:12:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=583
03/02/2022 07:12:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=585
03/02/2022 07:12:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=587
03/02/2022 07:12:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=589
03/02/2022 07:12:17 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.29318818436465494 on epoch=589
03/02/2022 07:12:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=591
03/02/2022 07:12:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=593
03/02/2022 07:12:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=595
03/02/2022 07:12:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=597
03/02/2022 07:12:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=599
03/02/2022 07:12:29 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:12:29 - INFO - __main__ - Printing 3 examples
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 07:12:29 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:12:29 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:12:29 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:12:29 - INFO - __main__ - Printing 3 examples
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:12:29 - INFO - __main__ - ['Yes']
03/02/2022 07:12:29 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:12:29 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:12:30 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:12:31 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.22193444415666638 on epoch=599
03/02/2022 07:12:31 - INFO - __main__ - save last model!
03/02/2022 07:12:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 07:12:31 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 07:12:31 - INFO - __main__ - Printing 3 examples
03/02/2022 07:12:31 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 07:12:31 - INFO - __main__ - ['No']
03/02/2022 07:12:31 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 07:12:31 - INFO - __main__ - ['Yes']
03/02/2022 07:12:31 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 07:12:31 - INFO - __main__ - ['Yes']
03/02/2022 07:12:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:12:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:12:40 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 07:12:44 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:12:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:12:44 - INFO - __main__ - Starting training!
03/02/2022 07:16:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_13_0.2_8_predictions.txt
03/02/2022 07:16:50 - INFO - __main__ - Classification-F1 on test data: 0.0758
03/02/2022 07:16:51 - INFO - __main__ - prefix=circa_16_13, lr=0.2, bsz=8, dev_performance=0.4174301151045337, test_performance=0.07580786109466027
03/02/2022 07:16:51 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.5, bsz=8 ...
03/02/2022 07:16:52 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:16:52 - INFO - __main__ - Printing 3 examples
03/02/2022 07:16:52 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:16:52 - INFO - __main__ - ['Yes']
03/02/2022 07:16:52 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:16:52 - INFO - __main__ - ['Yes']
03/02/2022 07:16:52 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:16:52 - INFO - __main__ - ['Yes']
03/02/2022 07:16:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:16:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:16:53 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:16:53 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:16:53 - INFO - __main__ - Printing 3 examples
03/02/2022 07:16:53 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:16:53 - INFO - __main__ - ['Yes']
03/02/2022 07:16:53 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:16:53 - INFO - __main__ - ['Yes']
03/02/2022 07:16:53 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:16:53 - INFO - __main__ - ['Yes']
03/02/2022 07:16:53 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:16:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:16:53 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:17:07 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:17:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:17:08 - INFO - __main__ - Starting training!
03/02/2022 07:17:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.67 on epoch=1
03/02/2022 07:17:13 - INFO - __main__ - Step 20 Global step 20 Train loss 2.33 on epoch=3
03/02/2022 07:17:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.54 on epoch=5
03/02/2022 07:17:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=7
03/02/2022 07:17:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=9
03/02/2022 07:17:22 - INFO - __main__ - Global step 50 Train loss 1.93 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 07:17:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 07:17:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=11
03/02/2022 07:17:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=13
03/02/2022 07:17:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=15
03/02/2022 07:17:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=17
03/02/2022 07:17:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=19
03/02/2022 07:17:35 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 07:17:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=21
03/02/2022 07:17:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=23
03/02/2022 07:17:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=25
03/02/2022 07:17:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=27
03/02/2022 07:17:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=29
03/02/2022 07:17:49 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.13130590339892664 on epoch=29
03/02/2022 07:17:49 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.13130590339892664 on epoch=29, global_step=150
03/02/2022 07:17:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=31
03/02/2022 07:17:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=33
03/02/2022 07:17:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=35
03/02/2022 07:17:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=37
03/02/2022 07:18:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=39
03/02/2022 07:18:02 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.06736842105263158 on epoch=39
03/02/2022 07:18:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=41
03/02/2022 07:18:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=43
03/02/2022 07:18:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=45
03/02/2022 07:18:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=47
03/02/2022 07:18:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=49
03/02/2022 07:18:15 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.1535632183908046 on epoch=49
03/02/2022 07:18:15 - INFO - __main__ - Saving model with best Classification-F1: 0.13130590339892664 -> 0.1535632183908046 on epoch=49, global_step=250
03/02/2022 07:18:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=51
03/02/2022 07:18:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=53
03/02/2022 07:18:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=55
03/02/2022 07:18:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=57
03/02/2022 07:18:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=59
03/02/2022 07:18:28 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.1322222222222222 on epoch=59
03/02/2022 07:18:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=61
03/02/2022 07:18:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=63
03/02/2022 07:18:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=65
03/02/2022 07:18:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=67
03/02/2022 07:18:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=69
03/02/2022 07:18:42 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.09791921664626682 on epoch=69
03/02/2022 07:18:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=71
03/02/2022 07:18:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=73
03/02/2022 07:18:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=75
03/02/2022 07:18:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=77
03/02/2022 07:18:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=79
03/02/2022 07:18:54 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.13197509903791738 on epoch=79
03/02/2022 07:18:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=81
03/02/2022 07:18:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=83
03/02/2022 07:19:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=85
03/02/2022 07:19:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=87
03/02/2022 07:19:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=89
03/02/2022 07:19:08 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.18789473684210528 on epoch=89
03/02/2022 07:19:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1535632183908046 -> 0.18789473684210528 on epoch=89, global_step=450
03/02/2022 07:19:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=91
03/02/2022 07:19:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=93
03/02/2022 07:19:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=95
03/02/2022 07:19:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=97
03/02/2022 07:19:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=99
03/02/2022 07:19:22 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.3120906679527369 on epoch=99
03/02/2022 07:19:22 - INFO - __main__ - Saving model with best Classification-F1: 0.18789473684210528 -> 0.3120906679527369 on epoch=99, global_step=500
03/02/2022 07:19:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=101
03/02/2022 07:19:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=103
03/02/2022 07:19:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=105
03/02/2022 07:19:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=107
03/02/2022 07:19:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=109
03/02/2022 07:19:35 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.42112586970271976 on epoch=109
03/02/2022 07:19:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3120906679527369 -> 0.42112586970271976 on epoch=109, global_step=550
03/02/2022 07:19:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=111
03/02/2022 07:19:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=113
03/02/2022 07:19:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=115
03/02/2022 07:19:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=117
03/02/2022 07:19:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=119
03/02/2022 07:19:49 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.39616673648931716 on epoch=119
03/02/2022 07:19:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=121
03/02/2022 07:19:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=123
03/02/2022 07:19:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=125
03/02/2022 07:19:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=127
03/02/2022 07:20:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=129
03/02/2022 07:20:03 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.3075098814229249 on epoch=129
03/02/2022 07:20:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=131
03/02/2022 07:20:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=133
03/02/2022 07:20:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=135
03/02/2022 07:20:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=137
03/02/2022 07:20:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=139
03/02/2022 07:20:16 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.3148540489642184 on epoch=139
03/02/2022 07:20:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=141
03/02/2022 07:20:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=143
03/02/2022 07:20:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=145
03/02/2022 07:20:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=147
03/02/2022 07:20:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=149
03/02/2022 07:20:28 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.26393925991449213 on epoch=149
03/02/2022 07:20:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=151
03/02/2022 07:20:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=153
03/02/2022 07:20:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=155
03/02/2022 07:20:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=157
03/02/2022 07:20:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=159
03/02/2022 07:20:42 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.29826932923707117 on epoch=159
03/02/2022 07:20:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=161
03/02/2022 07:20:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=163
03/02/2022 07:20:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=165
03/02/2022 07:20:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=167
03/02/2022 07:20:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=169
03/02/2022 07:20:55 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.3296741854636592 on epoch=169
03/02/2022 07:20:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=171
03/02/2022 07:21:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=173
03/02/2022 07:21:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=175
03/02/2022 07:21:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=177
03/02/2022 07:21:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=179
03/02/2022 07:21:09 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.3269980652589348 on epoch=179
03/02/2022 07:21:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=181
03/02/2022 07:21:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=183
03/02/2022 07:21:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=185
03/02/2022 07:21:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=187
03/02/2022 07:21:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=189
03/02/2022 07:21:23 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.29777777777777775 on epoch=189
03/02/2022 07:21:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=191
03/02/2022 07:21:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=193
03/02/2022 07:21:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=195
03/02/2022 07:21:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=197
03/02/2022 07:21:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=199
03/02/2022 07:21:36 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.33356120826709057 on epoch=199
03/02/2022 07:21:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=201
03/02/2022 07:21:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=203
03/02/2022 07:21:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=205
03/02/2022 07:21:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=207
03/02/2022 07:21:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=209
03/02/2022 07:21:49 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.28916723691648566 on epoch=209
03/02/2022 07:21:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=211
03/02/2022 07:21:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=213
03/02/2022 07:21:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=215
03/02/2022 07:21:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=217
03/02/2022 07:22:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=219
03/02/2022 07:22:02 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.36806945663837676 on epoch=219
03/02/2022 07:22:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=221
03/02/2022 07:22:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=223
03/02/2022 07:22:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=225
03/02/2022 07:22:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=227
03/02/2022 07:22:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=229
03/02/2022 07:22:16 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.31834101382488483 on epoch=229
03/02/2022 07:22:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=231
03/02/2022 07:22:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=233
03/02/2022 07:22:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=235
03/02/2022 07:22:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=237
03/02/2022 07:22:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=239
03/02/2022 07:22:29 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.33536644165863067 on epoch=239
03/02/2022 07:22:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=241
03/02/2022 07:22:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=243
03/02/2022 07:22:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=245
03/02/2022 07:22:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=247
03/02/2022 07:22:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=249
03/02/2022 07:22:42 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.25980281901334534 on epoch=249
03/02/2022 07:22:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=251
03/02/2022 07:22:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=253
03/02/2022 07:22:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=255
03/02/2022 07:22:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=257
03/02/2022 07:22:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=259
03/02/2022 07:22:55 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.31268170426065167 on epoch=259
03/02/2022 07:22:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=261
03/02/2022 07:23:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=263
03/02/2022 07:23:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=265
03/02/2022 07:23:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=267
03/02/2022 07:23:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=269
03/02/2022 07:23:09 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.3157695831424645 on epoch=269
03/02/2022 07:23:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=271
03/02/2022 07:23:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=273
03/02/2022 07:23:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=275
03/02/2022 07:23:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=277
03/02/2022 07:23:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=279
03/02/2022 07:23:22 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.3314763988973255 on epoch=279
03/02/2022 07:23:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=281
03/02/2022 07:23:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=283
03/02/2022 07:23:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=285
03/02/2022 07:23:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=287
03/02/2022 07:23:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=289
03/02/2022 07:23:36 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.32351097178683386 on epoch=289
03/02/2022 07:23:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=291
03/02/2022 07:23:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=293
03/02/2022 07:23:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=295
03/02/2022 07:23:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=297
03/02/2022 07:23:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=299
03/02/2022 07:23:50 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.3886404272611169 on epoch=299
03/02/2022 07:23:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=301
03/02/2022 07:23:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=303
03/02/2022 07:23:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=305
03/02/2022 07:23:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=307
03/02/2022 07:24:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=309
03/02/2022 07:24:03 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.24524963908105443 on epoch=309
03/02/2022 07:24:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=311
03/02/2022 07:24:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=313
03/02/2022 07:24:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=315
03/02/2022 07:24:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=317
03/02/2022 07:24:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=319
03/02/2022 07:24:17 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.31717171717171716 on epoch=319
03/02/2022 07:24:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=321
03/02/2022 07:24:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=323
03/02/2022 07:24:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=325
03/02/2022 07:24:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=327
03/02/2022 07:24:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=329
03/02/2022 07:24:31 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.22481546755131662 on epoch=329
03/02/2022 07:24:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=331
03/02/2022 07:24:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=333
03/02/2022 07:24:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=335
03/02/2022 07:24:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=337
03/02/2022 07:24:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=339
03/02/2022 07:24:45 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.30285228282000165 on epoch=339
03/02/2022 07:24:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=341
03/02/2022 07:24:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=343
03/02/2022 07:24:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=345
03/02/2022 07:24:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=347
03/02/2022 07:24:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=349
03/02/2022 07:24:58 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.30954852194258975 on epoch=349
03/02/2022 07:25:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=351
03/02/2022 07:25:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=353
03/02/2022 07:25:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=355
03/02/2022 07:25:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=357
03/02/2022 07:25:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=359
03/02/2022 07:25:12 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.24225382781971083 on epoch=359
03/02/2022 07:25:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=361
03/02/2022 07:25:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=363
03/02/2022 07:25:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=365
03/02/2022 07:25:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=367
03/02/2022 07:25:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=369
03/02/2022 07:25:25 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.26252587991718423 on epoch=369
03/02/2022 07:25:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=371
03/02/2022 07:25:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=373
03/02/2022 07:25:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=375
03/02/2022 07:25:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=377
03/02/2022 07:25:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=379
03/02/2022 07:25:39 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.21060206133735548 on epoch=379
03/02/2022 07:25:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=381
03/02/2022 07:25:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=383
03/02/2022 07:25:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=385
03/02/2022 07:25:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=387
03/02/2022 07:25:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=389
03/02/2022 07:25:53 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.24078642445989384 on epoch=389
03/02/2022 07:25:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=391
03/02/2022 07:25:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=393
03/02/2022 07:26:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=395
03/02/2022 07:26:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=397
03/02/2022 07:26:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=399
03/02/2022 07:26:07 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.2015266106442577 on epoch=399
03/02/2022 07:26:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=401
03/02/2022 07:26:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=403
03/02/2022 07:26:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=405
03/02/2022 07:26:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=407
03/02/2022 07:26:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=409
03/02/2022 07:26:20 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.15933952528379774 on epoch=409
03/02/2022 07:26:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=411
03/02/2022 07:26:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=413
03/02/2022 07:26:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=415
03/02/2022 07:26:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=417
03/02/2022 07:26:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=419
03/02/2022 07:26:35 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.251469383822325 on epoch=419
03/02/2022 07:26:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=421
03/02/2022 07:26:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=423
03/02/2022 07:26:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=425
03/02/2022 07:26:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=427
03/02/2022 07:26:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=429
03/02/2022 07:26:48 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.22569675586916968 on epoch=429
03/02/2022 07:26:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=431
03/02/2022 07:26:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=433
03/02/2022 07:26:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=435
03/02/2022 07:26:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=437
03/02/2022 07:27:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=439
03/02/2022 07:27:02 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.16583344970441743 on epoch=439
03/02/2022 07:27:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=441
03/02/2022 07:27:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=443
03/02/2022 07:27:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=445
03/02/2022 07:27:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=447
03/02/2022 07:27:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=449
03/02/2022 07:27:16 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.19934640522875818 on epoch=449
03/02/2022 07:27:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=451
03/02/2022 07:27:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=453
03/02/2022 07:27:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=455
03/02/2022 07:27:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=457
03/02/2022 07:27:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=459
03/02/2022 07:27:30 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.25368573088937835 on epoch=459
03/02/2022 07:27:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=461
03/02/2022 07:27:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=463
03/02/2022 07:27:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=465
03/02/2022 07:27:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=467
03/02/2022 07:27:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=469
03/02/2022 07:27:44 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.2340220038632737 on epoch=469
03/02/2022 07:27:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=471
03/02/2022 07:27:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=473
03/02/2022 07:27:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=475
03/02/2022 07:27:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=477
03/02/2022 07:27:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=479
03/02/2022 07:27:59 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.2106209719461917 on epoch=479
03/02/2022 07:28:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=481
03/02/2022 07:28:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=483
03/02/2022 07:28:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=485
03/02/2022 07:28:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=487
03/02/2022 07:28:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=489
03/02/2022 07:28:12 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.23859306716449577 on epoch=489
03/02/2022 07:28:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=491
03/02/2022 07:28:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 07:28:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=495
03/02/2022 07:28:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=497
03/02/2022 07:28:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=499
03/02/2022 07:28:27 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.24333999333999334 on epoch=499
03/02/2022 07:28:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=501
03/02/2022 07:28:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=503
03/02/2022 07:28:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=505
03/02/2022 07:28:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=507
03/02/2022 07:28:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=509
03/02/2022 07:28:41 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.20862882928586404 on epoch=509
03/02/2022 07:28:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=511
03/02/2022 07:28:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=513
03/02/2022 07:28:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=515
03/02/2022 07:28:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=517
03/02/2022 07:28:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=519
03/02/2022 07:28:55 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.31807320516997933 on epoch=519
03/02/2022 07:28:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=521
03/02/2022 07:29:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=523
03/02/2022 07:29:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=525
03/02/2022 07:29:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 07:29:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=529
03/02/2022 07:29:09 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.21843825249398005 on epoch=529
03/02/2022 07:29:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=531
03/02/2022 07:29:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=533
03/02/2022 07:29:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=535
03/02/2022 07:29:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=537
03/02/2022 07:29:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=539
03/02/2022 07:29:23 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.2431499309902214 on epoch=539
03/02/2022 07:29:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=541
03/02/2022 07:29:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=543
03/02/2022 07:29:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=545
03/02/2022 07:29:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=547
03/02/2022 07:29:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=549
03/02/2022 07:29:38 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.19898820479465643 on epoch=549
03/02/2022 07:29:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 07:29:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 07:29:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=555
03/02/2022 07:29:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=557
03/02/2022 07:29:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=559
03/02/2022 07:29:52 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.20846243769843092 on epoch=559
03/02/2022 07:29:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=561
03/02/2022 07:29:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=563
03/02/2022 07:29:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=565
03/02/2022 07:30:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=567
03/02/2022 07:30:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=569
03/02/2022 07:30:06 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.17296378200402968 on epoch=569
03/02/2022 07:30:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=571
03/02/2022 07:30:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=573
03/02/2022 07:30:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 07:30:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=577
03/02/2022 07:30:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=579
03/02/2022 07:30:21 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.21261371925623132 on epoch=579
03/02/2022 07:30:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=581
03/02/2022 07:30:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=583
03/02/2022 07:30:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=585
03/02/2022 07:30:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=587
03/02/2022 07:30:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=589
03/02/2022 07:30:35 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.31242567108081687 on epoch=589
03/02/2022 07:30:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 07:30:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=593
03/02/2022 07:30:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=595
03/02/2022 07:30:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 07:30:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=599
03/02/2022 07:30:50 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:30:50 - INFO - __main__ - Printing 3 examples
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 07:30:50 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:30:50 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.23581206804891014 on epoch=599
03/02/2022 07:30:50 - INFO - __main__ - save last model!
03/02/2022 07:30:50 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:30:50 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:30:50 - INFO - __main__ - Printing 3 examples
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:30:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 07:30:50 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:30:50 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 07:30:50 - INFO - __main__ - Printing 3 examples
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 07:30:50 - INFO - __main__ - ['No']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 07:30:50 - INFO - __main__ - ['Yes']
03/02/2022 07:30:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:30:50 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:30:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:31:00 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 07:31:04 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:31:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:31:05 - INFO - __main__ - Starting training!
03/02/2022 07:35:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_21_0.5_8_predictions.txt
03/02/2022 07:35:14 - INFO - __main__ - Classification-F1 on test data: 0.0324
03/02/2022 07:35:14 - INFO - __main__ - prefix=circa_16_21, lr=0.5, bsz=8, dev_performance=0.42112586970271976, test_performance=0.032389972150283906
03/02/2022 07:35:14 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.4, bsz=8 ...
03/02/2022 07:35:15 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:35:15 - INFO - __main__ - Printing 3 examples
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:35:15 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:35:15 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:35:15 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:35:15 - INFO - __main__ - Printing 3 examples
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:35:15 - INFO - __main__ - ['Yes']
03/02/2022 07:35:15 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:35:15 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:35:16 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:35:29 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:35:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:35:30 - INFO - __main__ - Starting training!
03/02/2022 07:35:34 - INFO - __main__ - Step 10 Global step 10 Train loss 3.67 on epoch=1
03/02/2022 07:35:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.66 on epoch=3
03/02/2022 07:35:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.79 on epoch=5
03/02/2022 07:35:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.36 on epoch=7
03/02/2022 07:35:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=9
03/02/2022 07:35:44 - INFO - __main__ - Global step 50 Train loss 2.10 Classification-F1 0.09928498467824312 on epoch=9
03/02/2022 07:35:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09928498467824312 on epoch=9, global_step=50
03/02/2022 07:35:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.85 on epoch=11
03/02/2022 07:35:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=13
03/02/2022 07:35:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.76 on epoch=15
03/02/2022 07:35:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=17
03/02/2022 07:35:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=19
03/02/2022 07:35:56 - INFO - __main__ - Global step 100 Train loss 0.71 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 07:35:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.61 on epoch=21
03/02/2022 07:36:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=23
03/02/2022 07:36:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=25
03/02/2022 07:36:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=27
03/02/2022 07:36:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=29
03/02/2022 07:36:09 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.06736842105263158 on epoch=29
03/02/2022 07:36:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=31
03/02/2022 07:36:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=33
03/02/2022 07:36:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=35
03/02/2022 07:36:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.53 on epoch=37
03/02/2022 07:36:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=39
03/02/2022 07:36:22 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 07:36:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=41
03/02/2022 07:36:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=43
03/02/2022 07:36:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=45
03/02/2022 07:36:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=47
03/02/2022 07:36:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=49
03/02/2022 07:36:35 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.1358983167296462 on epoch=49
03/02/2022 07:36:35 - INFO - __main__ - Saving model with best Classification-F1: 0.09928498467824312 -> 0.1358983167296462 on epoch=49, global_step=250
03/02/2022 07:36:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=51
03/02/2022 07:36:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=53
03/02/2022 07:36:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 07:36:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=57
03/02/2022 07:36:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=59
03/02/2022 07:36:48 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 07:36:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=61
03/02/2022 07:36:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=63
03/02/2022 07:36:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=65
03/02/2022 07:36:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=67
03/02/2022 07:37:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=69
03/02/2022 07:37:02 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 07:37:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=71
03/02/2022 07:37:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=73
03/02/2022 07:37:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=75
03/02/2022 07:37:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=77
03/02/2022 07:37:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=79
03/02/2022 07:37:15 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.06666666666666668 on epoch=79
03/02/2022 07:37:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=81
03/02/2022 07:37:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=83
03/02/2022 07:37:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=85
03/02/2022 07:37:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=87
03/02/2022 07:37:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=89
03/02/2022 07:37:29 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.06666666666666668 on epoch=89
03/02/2022 07:37:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=91
03/02/2022 07:37:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=93
03/02/2022 07:37:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=95
03/02/2022 07:37:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=97
03/02/2022 07:37:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=99
03/02/2022 07:37:41 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.06666666666666668 on epoch=99
03/02/2022 07:37:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=101
03/02/2022 07:37:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=103
03/02/2022 07:37:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=105
03/02/2022 07:37:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=107
03/02/2022 07:37:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=109
03/02/2022 07:37:55 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.06666666666666668 on epoch=109
03/02/2022 07:37:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=111
03/02/2022 07:37:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=113
03/02/2022 07:38:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=115
03/02/2022 07:38:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=117
03/02/2022 07:38:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=119
03/02/2022 07:38:09 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.06666666666666668 on epoch=119
03/02/2022 07:38:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=121
03/02/2022 07:38:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=123
03/02/2022 07:38:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=125
03/02/2022 07:38:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=127
03/02/2022 07:38:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=129
03/02/2022 07:38:23 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.23648120300751874 on epoch=129
03/02/2022 07:38:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1358983167296462 -> 0.23648120300751874 on epoch=129, global_step=650
03/02/2022 07:38:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=131
03/02/2022 07:38:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=133
03/02/2022 07:38:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=135
03/02/2022 07:38:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=137
03/02/2022 07:38:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=139
03/02/2022 07:38:37 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.2779448621553885 on epoch=139
03/02/2022 07:38:37 - INFO - __main__ - Saving model with best Classification-F1: 0.23648120300751874 -> 0.2779448621553885 on epoch=139, global_step=700
03/02/2022 07:38:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=141
03/02/2022 07:38:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=143
03/02/2022 07:38:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=145
03/02/2022 07:38:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=147
03/02/2022 07:38:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=149
03/02/2022 07:38:51 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.24333333333333335 on epoch=149
03/02/2022 07:38:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=151
03/02/2022 07:38:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=153
03/02/2022 07:38:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=155
03/02/2022 07:39:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=157
03/02/2022 07:39:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=159
03/02/2022 07:39:05 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.27323232323232327 on epoch=159
03/02/2022 07:39:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=161
03/02/2022 07:39:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=163
03/02/2022 07:39:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=165
03/02/2022 07:39:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=167
03/02/2022 07:39:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=169
03/02/2022 07:39:19 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.308107420943906 on epoch=169
03/02/2022 07:39:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2779448621553885 -> 0.308107420943906 on epoch=169, global_step=850
03/02/2022 07:39:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=171
03/02/2022 07:39:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=173
03/02/2022 07:39:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=175
03/02/2022 07:39:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=177
03/02/2022 07:39:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=179
03/02/2022 07:39:33 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.3014085753216188 on epoch=179
03/02/2022 07:39:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=181
03/02/2022 07:39:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=183
03/02/2022 07:39:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=185
03/02/2022 07:39:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=187
03/02/2022 07:39:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=189
03/02/2022 07:39:47 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.30454206999255395 on epoch=189
03/02/2022 07:39:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.28 on epoch=191
03/02/2022 07:39:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=193
03/02/2022 07:39:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=195
03/02/2022 07:39:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=197
03/02/2022 07:39:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=199
03/02/2022 07:40:01 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.22222415795586525 on epoch=199
03/02/2022 07:40:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=201
03/02/2022 07:40:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=203
03/02/2022 07:40:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=205
03/02/2022 07:40:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=207
03/02/2022 07:40:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=209
03/02/2022 07:40:14 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.32331082086221774 on epoch=209
03/02/2022 07:40:14 - INFO - __main__ - Saving model with best Classification-F1: 0.308107420943906 -> 0.32331082086221774 on epoch=209, global_step=1050
03/02/2022 07:40:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=211
03/02/2022 07:40:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=213
03/02/2022 07:40:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=215
03/02/2022 07:40:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=217
03/02/2022 07:40:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=219
03/02/2022 07:40:28 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.33066884806015234 on epoch=219
03/02/2022 07:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.32331082086221774 -> 0.33066884806015234 on epoch=219, global_step=1100
03/02/2022 07:40:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=221
03/02/2022 07:40:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=223
03/02/2022 07:40:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=225
03/02/2022 07:40:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=227
03/02/2022 07:40:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=229
03/02/2022 07:40:41 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3497556207233627 on epoch=229
03/02/2022 07:40:41 - INFO - __main__ - Saving model with best Classification-F1: 0.33066884806015234 -> 0.3497556207233627 on epoch=229, global_step=1150
03/02/2022 07:40:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=231
03/02/2022 07:40:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=233
03/02/2022 07:40:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=235
03/02/2022 07:40:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=237
03/02/2022 07:40:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=239
03/02/2022 07:40:55 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.2624484505714622 on epoch=239
03/02/2022 07:40:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=241
03/02/2022 07:41:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=243
03/02/2022 07:41:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=245
03/02/2022 07:41:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=247
03/02/2022 07:41:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=249
03/02/2022 07:41:09 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.36238157322903086 on epoch=249
03/02/2022 07:41:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3497556207233627 -> 0.36238157322903086 on epoch=249, global_step=1250
03/02/2022 07:41:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=251
03/02/2022 07:41:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=253
03/02/2022 07:41:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=255
03/02/2022 07:41:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=257
03/02/2022 07:41:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=259
03/02/2022 07:41:23 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.35474835095328455 on epoch=259
03/02/2022 07:41:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=261
03/02/2022 07:41:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=263
03/02/2022 07:41:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=265
03/02/2022 07:41:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=267
03/02/2022 07:41:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=269
03/02/2022 07:41:36 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.2995238095238096 on epoch=269
03/02/2022 07:41:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=271
03/02/2022 07:41:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=273
03/02/2022 07:41:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=275
03/02/2022 07:41:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=277
03/02/2022 07:41:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=279
03/02/2022 07:41:50 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.36801263252876154 on epoch=279
03/02/2022 07:41:50 - INFO - __main__ - Saving model with best Classification-F1: 0.36238157322903086 -> 0.36801263252876154 on epoch=279, global_step=1400
03/02/2022 07:41:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=281
03/02/2022 07:41:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=283
03/02/2022 07:41:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=285
03/02/2022 07:41:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=287
03/02/2022 07:42:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=289
03/02/2022 07:42:04 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.29384615384615387 on epoch=289
03/02/2022 07:42:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=291
03/02/2022 07:42:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=293
03/02/2022 07:42:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=295
03/02/2022 07:42:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=297
03/02/2022 07:42:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=299
03/02/2022 07:42:18 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.39303846153846156 on epoch=299
03/02/2022 07:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.36801263252876154 -> 0.39303846153846156 on epoch=299, global_step=1500
03/02/2022 07:42:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=301
03/02/2022 07:42:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=303
03/02/2022 07:42:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=305
03/02/2022 07:42:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=307
03/02/2022 07:42:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=309
03/02/2022 07:42:32 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.350263128949902 on epoch=309
03/02/2022 07:42:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=311
03/02/2022 07:42:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=313
03/02/2022 07:42:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=315
03/02/2022 07:42:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=317
03/02/2022 07:42:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=319
03/02/2022 07:42:45 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.2423160173160173 on epoch=319
03/02/2022 07:42:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=321
03/02/2022 07:42:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=323
03/02/2022 07:42:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=325
03/02/2022 07:42:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=327
03/02/2022 07:42:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=329
03/02/2022 07:43:00 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.23408157051774076 on epoch=329
03/02/2022 07:43:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=331
03/02/2022 07:43:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=333
03/02/2022 07:43:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=335
03/02/2022 07:43:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=337
03/02/2022 07:43:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=339
03/02/2022 07:43:14 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.29583333333333334 on epoch=339
03/02/2022 07:43:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=341
03/02/2022 07:43:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=343
03/02/2022 07:43:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=345
03/02/2022 07:43:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=347
03/02/2022 07:43:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=349
03/02/2022 07:43:28 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.2307347670250896 on epoch=349
03/02/2022 07:43:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=351
03/02/2022 07:43:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=353
03/02/2022 07:43:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=355
03/02/2022 07:43:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=357
03/02/2022 07:43:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=359
03/02/2022 07:43:42 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.273543964333438 on epoch=359
03/02/2022 07:43:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=361
03/02/2022 07:43:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=363
03/02/2022 07:43:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=365
03/02/2022 07:43:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=367
03/02/2022 07:43:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=369
03/02/2022 07:43:56 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.35401138716356106 on epoch=369
03/02/2022 07:43:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=371
03/02/2022 07:44:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=373
03/02/2022 07:44:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=375
03/02/2022 07:44:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=377
03/02/2022 07:44:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=379
03/02/2022 07:44:10 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.30087606837606834 on epoch=379
03/02/2022 07:44:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=381
03/02/2022 07:44:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=383
03/02/2022 07:44:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=385
03/02/2022 07:44:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=387
03/02/2022 07:44:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=389
03/02/2022 07:44:24 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.32896900695762177 on epoch=389
03/02/2022 07:44:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=391
03/02/2022 07:44:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=393
03/02/2022 07:44:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=395
03/02/2022 07:44:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=397
03/02/2022 07:44:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=399
03/02/2022 07:44:38 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.21161936182097474 on epoch=399
03/02/2022 07:44:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=401
03/02/2022 07:44:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=403
03/02/2022 07:44:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=405
03/02/2022 07:44:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=407
03/02/2022 07:44:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=409
03/02/2022 07:44:52 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.21265280058383507 on epoch=409
03/02/2022 07:44:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=411
03/02/2022 07:44:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=413
03/02/2022 07:44:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=415
03/02/2022 07:45:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=417
03/02/2022 07:45:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=419
03/02/2022 07:45:06 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.32543154013742254 on epoch=419
03/02/2022 07:45:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=421
03/02/2022 07:45:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=423
03/02/2022 07:45:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=425
03/02/2022 07:45:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=427
03/02/2022 07:45:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=429
03/02/2022 07:45:21 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.3354220739092819 on epoch=429
03/02/2022 07:45:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=431
03/02/2022 07:45:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=433
03/02/2022 07:45:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=435
03/02/2022 07:45:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=437
03/02/2022 07:45:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=439
03/02/2022 07:45:35 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.3962098941046309 on epoch=439
03/02/2022 07:45:35 - INFO - __main__ - Saving model with best Classification-F1: 0.39303846153846156 -> 0.3962098941046309 on epoch=439, global_step=2200
03/02/2022 07:45:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=441
03/02/2022 07:45:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=443
03/02/2022 07:45:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=445
03/02/2022 07:45:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=447
03/02/2022 07:45:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=449
03/02/2022 07:45:49 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.2146582622667161 on epoch=449
03/02/2022 07:45:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=451
03/02/2022 07:45:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=453
03/02/2022 07:45:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=455
03/02/2022 07:45:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=457
03/02/2022 07:46:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=459
03/02/2022 07:46:03 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.3414706273258905 on epoch=459
03/02/2022 07:46:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=461
03/02/2022 07:46:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=463
03/02/2022 07:46:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=465
03/02/2022 07:46:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=467
03/02/2022 07:46:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=469
03/02/2022 07:46:17 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.23492664742664743 on epoch=469
03/02/2022 07:46:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=471
03/02/2022 07:46:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=473
03/02/2022 07:46:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=475
03/02/2022 07:46:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=477
03/02/2022 07:46:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=479
03/02/2022 07:46:32 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.3324125950854061 on epoch=479
03/02/2022 07:46:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=481
03/02/2022 07:46:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=483
03/02/2022 07:46:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=485
03/02/2022 07:46:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=487
03/02/2022 07:46:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=489
03/02/2022 07:46:46 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.2792402462093932 on epoch=489
03/02/2022 07:46:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=491
03/02/2022 07:46:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=493
03/02/2022 07:46:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=495
03/02/2022 07:46:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=497
03/02/2022 07:46:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=499
03/02/2022 07:47:00 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.26141215106732346 on epoch=499
03/02/2022 07:47:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=501
03/02/2022 07:47:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=503
03/02/2022 07:47:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=505
03/02/2022 07:47:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=507
03/02/2022 07:47:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=509
03/02/2022 07:47:15 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.29510939510939505 on epoch=509
03/02/2022 07:47:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=511
03/02/2022 07:47:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=513
03/02/2022 07:47:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 07:47:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=517
03/02/2022 07:47:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=519
03/02/2022 07:47:29 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.3023835015648956 on epoch=519
03/02/2022 07:47:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=521
03/02/2022 07:47:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=523
03/02/2022 07:47:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=525
03/02/2022 07:47:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 07:47:40 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=529
03/02/2022 07:47:43 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.4311292654713707 on epoch=529
03/02/2022 07:47:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3962098941046309 -> 0.4311292654713707 on epoch=529, global_step=2650
03/02/2022 07:47:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=531
03/02/2022 07:47:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 07:47:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=535
03/02/2022 07:47:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=537
03/02/2022 07:47:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=539
03/02/2022 07:47:58 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.2490724320361417 on epoch=539
03/02/2022 07:48:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=541
03/02/2022 07:48:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=543
03/02/2022 07:48:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 07:48:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=547
03/02/2022 07:48:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=549
03/02/2022 07:48:12 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.29856709382571456 on epoch=549
03/02/2022 07:48:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=551
03/02/2022 07:48:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 07:48:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=555
03/02/2022 07:48:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=557
03/02/2022 07:48:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=559
03/02/2022 07:48:27 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.31028820130202617 on epoch=559
03/02/2022 07:48:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=561
03/02/2022 07:48:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=563
03/02/2022 07:48:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=565
03/02/2022 07:48:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 07:48:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=569
03/02/2022 07:48:41 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.2838827838827839 on epoch=569
03/02/2022 07:48:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=571
03/02/2022 07:48:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=573
03/02/2022 07:48:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=575
03/02/2022 07:48:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=577
03/02/2022 07:48:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=579
03/02/2022 07:48:55 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.33862011932187364 on epoch=579
03/02/2022 07:48:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=581
03/02/2022 07:49:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=583
03/02/2022 07:49:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=585
03/02/2022 07:49:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 07:49:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=589
03/02/2022 07:49:09 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2895575968746701 on epoch=589
03/02/2022 07:49:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=591
03/02/2022 07:49:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=593
03/02/2022 07:49:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=595
03/02/2022 07:49:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 07:49:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 07:49:23 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:49:23 - INFO - __main__ - Printing 3 examples
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 07:49:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:49:23 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:49:23 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:49:23 - INFO - __main__ - Printing 3 examples
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:49:23 - INFO - __main__ - ['Yes']
03/02/2022 07:49:23 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:49:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:49:23 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:49:24 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3627518315018315 on epoch=599
03/02/2022 07:49:24 - INFO - __main__ - save last model!
03/02/2022 07:49:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 07:49:24 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 07:49:24 - INFO - __main__ - Printing 3 examples
03/02/2022 07:49:24 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 07:49:24 - INFO - __main__ - ['No']
03/02/2022 07:49:24 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 07:49:24 - INFO - __main__ - ['Yes']
03/02/2022 07:49:24 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 07:49:24 - INFO - __main__ - ['Yes']
03/02/2022 07:49:24 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:49:27 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:49:34 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 07:49:37 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:49:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:49:38 - INFO - __main__ - Starting training!
03/02/2022 07:53:54 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_21_0.4_8_predictions.txt
03/02/2022 07:53:54 - INFO - __main__ - Classification-F1 on test data: 0.0952
03/02/2022 07:53:55 - INFO - __main__ - prefix=circa_16_21, lr=0.4, bsz=8, dev_performance=0.4311292654713707, test_performance=0.09519929541095735
03/02/2022 07:53:55 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.3, bsz=8 ...
03/02/2022 07:53:56 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:53:56 - INFO - __main__ - Printing 3 examples
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 07:53:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:53:56 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 07:53:56 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 07:53:56 - INFO - __main__ - Printing 3 examples
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 07:53:56 - INFO - __main__ - ['Yes']
03/02/2022 07:53:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 07:53:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 07:53:56 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 07:54:10 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 07:54:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 07:54:11 - INFO - __main__ - Starting training!
03/02/2022 07:54:14 - INFO - __main__ - Step 10 Global step 10 Train loss 3.80 on epoch=1
03/02/2022 07:54:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.81 on epoch=3
03/02/2022 07:54:19 - INFO - __main__ - Step 30 Global step 30 Train loss 2.19 on epoch=5
03/02/2022 07:54:21 - INFO - __main__ - Step 40 Global step 40 Train loss 1.68 on epoch=7
03/02/2022 07:54:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.43 on epoch=9
03/02/2022 07:54:25 - INFO - __main__ - Global step 50 Train loss 2.38 Classification-F1 0.13314366998577526 on epoch=9
03/02/2022 07:54:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13314366998577526 on epoch=9, global_step=50
03/02/2022 07:54:28 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=11
03/02/2022 07:54:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=13
03/02/2022 07:54:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=15
03/02/2022 07:54:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=17
03/02/2022 07:54:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=19
03/02/2022 07:54:39 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.11286486486486487 on epoch=19
03/02/2022 07:54:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=21
03/02/2022 07:54:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=23
03/02/2022 07:54:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.64 on epoch=25
03/02/2022 07:54:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=27
03/02/2022 07:54:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=29
03/02/2022 07:54:53 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.1167479674796748 on epoch=29
03/02/2022 07:54:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=31
03/02/2022 07:54:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=33
03/02/2022 07:54:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=35
03/02/2022 07:55:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=37
03/02/2022 07:55:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=39
03/02/2022 07:55:05 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.06736842105263158 on epoch=39
03/02/2022 07:55:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=41
03/02/2022 07:55:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=43
03/02/2022 07:55:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=45
03/02/2022 07:55:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=47
03/02/2022 07:55:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=49
03/02/2022 07:55:18 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 07:55:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=51
03/02/2022 07:55:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=53
03/02/2022 07:55:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=55
03/02/2022 07:55:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=57
03/02/2022 07:55:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=59
03/02/2022 07:55:31 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 07:55:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=61
03/02/2022 07:55:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=63
03/02/2022 07:55:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=65
03/02/2022 07:55:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=67
03/02/2022 07:55:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 07:55:45 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.13563218390804596 on epoch=69
03/02/2022 07:55:45 - INFO - __main__ - Saving model with best Classification-F1: 0.13314366998577526 -> 0.13563218390804596 on epoch=69, global_step=350
03/02/2022 07:55:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=71
03/02/2022 07:55:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=73
03/02/2022 07:55:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=75
03/02/2022 07:55:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=77
03/02/2022 07:55:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=79
03/02/2022 07:55:59 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.06881720430107527 on epoch=79
03/02/2022 07:56:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=81
03/02/2022 07:56:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=83
03/02/2022 07:56:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=85
03/02/2022 07:56:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=87
03/02/2022 07:56:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=89
03/02/2022 07:56:13 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.07032967032967033 on epoch=89
03/02/2022 07:56:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=91
03/02/2022 07:56:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=93
03/02/2022 07:56:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=95
03/02/2022 07:56:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 07:56:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=99
03/02/2022 07:56:27 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.1 on epoch=99
03/02/2022 07:56:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=101
03/02/2022 07:56:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=103
03/02/2022 07:56:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=105
03/02/2022 07:56:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=107
03/02/2022 07:56:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=109
03/02/2022 07:56:41 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.06666666666666668 on epoch=109
03/02/2022 07:56:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=111
03/02/2022 07:56:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=113
03/02/2022 07:56:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=115
03/02/2022 07:56:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=117
03/02/2022 07:56:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=119
03/02/2022 07:56:54 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.06666666666666668 on epoch=119
03/02/2022 07:56:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=121
03/02/2022 07:56:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=123
03/02/2022 07:57:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=125
03/02/2022 07:57:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=127
03/02/2022 07:57:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=129
03/02/2022 07:57:08 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.06956521739130435 on epoch=129
03/02/2022 07:57:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=131
03/02/2022 07:57:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=133
03/02/2022 07:57:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=135
03/02/2022 07:57:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=137
03/02/2022 07:57:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=139
03/02/2022 07:57:22 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.09309462915601023 on epoch=139
03/02/2022 07:57:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=141
03/02/2022 07:57:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=143
03/02/2022 07:57:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=145
03/02/2022 07:57:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=147
03/02/2022 07:57:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=149
03/02/2022 07:57:36 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.25483135466624734 on epoch=149
03/02/2022 07:57:36 - INFO - __main__ - Saving model with best Classification-F1: 0.13563218390804596 -> 0.25483135466624734 on epoch=149, global_step=750
03/02/2022 07:57:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=151
03/02/2022 07:57:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=153
03/02/2022 07:57:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=155
03/02/2022 07:57:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=157
03/02/2022 07:57:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=159
03/02/2022 07:57:50 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.19232736572890025 on epoch=159
03/02/2022 07:57:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=161
03/02/2022 07:57:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=163
03/02/2022 07:57:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=165
03/02/2022 07:57:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=167
03/02/2022 07:58:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=169
03/02/2022 07:58:04 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.2137674418604651 on epoch=169
03/02/2022 07:58:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=171
03/02/2022 07:58:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=173
03/02/2022 07:58:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=175
03/02/2022 07:58:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=177
03/02/2022 07:58:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=179
03/02/2022 07:58:18 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.2244343891402715 on epoch=179
03/02/2022 07:58:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=181
03/02/2022 07:58:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=183
03/02/2022 07:58:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=185
03/02/2022 07:58:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=187
03/02/2022 07:58:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=189
03/02/2022 07:58:32 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.26544566544566545 on epoch=189
03/02/2022 07:58:32 - INFO - __main__ - Saving model with best Classification-F1: 0.25483135466624734 -> 0.26544566544566545 on epoch=189, global_step=950
03/02/2022 07:58:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=191
03/02/2022 07:58:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=193
03/02/2022 07:58:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=195
03/02/2022 07:58:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=197
03/02/2022 07:58:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=199
03/02/2022 07:58:46 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.2633816425120773 on epoch=199
03/02/2022 07:58:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=201
03/02/2022 07:58:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=203
03/02/2022 07:58:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=205
03/02/2022 07:58:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=207
03/02/2022 07:58:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=209
03/02/2022 07:59:00 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.219 on epoch=209
03/02/2022 07:59:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=211
03/02/2022 07:59:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=213
03/02/2022 07:59:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=215
03/02/2022 07:59:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=217
03/02/2022 07:59:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=219
03/02/2022 07:59:13 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.3317647058823529 on epoch=219
03/02/2022 07:59:13 - INFO - __main__ - Saving model with best Classification-F1: 0.26544566544566545 -> 0.3317647058823529 on epoch=219, global_step=1100
03/02/2022 07:59:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=221
03/02/2022 07:59:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=223
03/02/2022 07:59:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=225
03/02/2022 07:59:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=227
03/02/2022 07:59:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=229
03/02/2022 07:59:27 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.20476190476190476 on epoch=229
03/02/2022 07:59:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=231
03/02/2022 07:59:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=233
03/02/2022 07:59:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=235
03/02/2022 07:59:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=237
03/02/2022 07:59:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=239
03/02/2022 07:59:41 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.2665552770815928 on epoch=239
03/02/2022 07:59:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=241
03/02/2022 07:59:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=243
03/02/2022 07:59:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=245
03/02/2022 07:59:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=247
03/02/2022 07:59:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=249
03/02/2022 07:59:55 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.3096682024358013 on epoch=249
03/02/2022 07:59:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=251
03/02/2022 08:00:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=253
03/02/2022 08:00:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=255
03/02/2022 08:00:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=257
03/02/2022 08:00:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=259
03/02/2022 08:00:09 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.3102094679514035 on epoch=259
03/02/2022 08:00:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=261
03/02/2022 08:00:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=263
03/02/2022 08:00:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=265
03/02/2022 08:00:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=267
03/02/2022 08:00:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=269
03/02/2022 08:00:23 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.348807056698018 on epoch=269
03/02/2022 08:00:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3317647058823529 -> 0.348807056698018 on epoch=269, global_step=1350
03/02/2022 08:00:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=271
03/02/2022 08:00:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=273
03/02/2022 08:00:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=275
03/02/2022 08:00:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=277
03/02/2022 08:00:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=279
03/02/2022 08:00:36 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.29145394217696774 on epoch=279
03/02/2022 08:00:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=281
03/02/2022 08:00:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=283
03/02/2022 08:00:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=285
03/02/2022 08:00:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=287
03/02/2022 08:00:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=289
03/02/2022 08:00:50 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.347008547008547 on epoch=289
03/02/2022 08:00:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=291
03/02/2022 08:00:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=293
03/02/2022 08:00:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=295
03/02/2022 08:00:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=297
03/02/2022 08:01:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=299
03/02/2022 08:01:04 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.2653175716289321 on epoch=299
03/02/2022 08:01:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=301
03/02/2022 08:01:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=303
03/02/2022 08:01:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=305
03/02/2022 08:01:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=307
03/02/2022 08:01:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=309
03/02/2022 08:01:17 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.3048621553884711 on epoch=309
03/02/2022 08:01:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=311
03/02/2022 08:01:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=313
03/02/2022 08:01:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=315
03/02/2022 08:01:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=317
03/02/2022 08:01:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=319
03/02/2022 08:01:31 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.3165775401069519 on epoch=319
03/02/2022 08:01:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=321
03/02/2022 08:01:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=323
03/02/2022 08:01:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=325
03/02/2022 08:01:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=327
03/02/2022 08:01:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=329
03/02/2022 08:01:45 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.3205453519324176 on epoch=329
03/02/2022 08:01:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=331
03/02/2022 08:01:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=333
03/02/2022 08:01:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=335
03/02/2022 08:01:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=337
03/02/2022 08:01:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=339
03/02/2022 08:01:58 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.2769991645781119 on epoch=339
03/02/2022 08:02:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=341
03/02/2022 08:02:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=343
03/02/2022 08:02:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=345
03/02/2022 08:02:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=347
03/02/2022 08:02:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=349
03/02/2022 08:02:12 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.3294545454545454 on epoch=349
03/02/2022 08:02:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=351
03/02/2022 08:02:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=353
03/02/2022 08:02:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=355
03/02/2022 08:02:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=357
03/02/2022 08:02:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=359
03/02/2022 08:02:26 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.31579763520062026 on epoch=359
03/02/2022 08:02:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=361
03/02/2022 08:02:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=363
03/02/2022 08:02:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=365
03/02/2022 08:02:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=367
03/02/2022 08:02:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=369
03/02/2022 08:02:39 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.3904106443363899 on epoch=369
03/02/2022 08:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.348807056698018 -> 0.3904106443363899 on epoch=369, global_step=1850
03/02/2022 08:02:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=371
03/02/2022 08:02:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=373
03/02/2022 08:02:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=375
03/02/2022 08:02:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=377
03/02/2022 08:02:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=379
03/02/2022 08:02:53 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.24849705964257046 on epoch=379
03/02/2022 08:02:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=381
03/02/2022 08:02:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=383
03/02/2022 08:03:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=385
03/02/2022 08:03:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=387
03/02/2022 08:03:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=389
03/02/2022 08:03:07 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.35060903149138445 on epoch=389
03/02/2022 08:03:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=391
03/02/2022 08:03:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=393
03/02/2022 08:03:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=395
03/02/2022 08:03:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=397
03/02/2022 08:03:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=399
03/02/2022 08:03:21 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.2584385660422527 on epoch=399
03/02/2022 08:03:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=401
03/02/2022 08:03:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=403
03/02/2022 08:03:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=405
03/02/2022 08:03:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=407
03/02/2022 08:03:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=409
03/02/2022 08:03:35 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.285014985014985 on epoch=409
03/02/2022 08:03:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=411
03/02/2022 08:03:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=413
03/02/2022 08:03:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=415
03/02/2022 08:03:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=417
03/02/2022 08:03:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=419
03/02/2022 08:03:48 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.3256995980210266 on epoch=419
03/02/2022 08:03:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=421
03/02/2022 08:03:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=423
03/02/2022 08:03:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=425
03/02/2022 08:03:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=427
03/02/2022 08:03:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=429
03/02/2022 08:04:02 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.3761802232854864 on epoch=429
03/02/2022 08:04:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=431
03/02/2022 08:04:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=433
03/02/2022 08:04:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=435
03/02/2022 08:04:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=437
03/02/2022 08:04:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=439
03/02/2022 08:04:16 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.3327651515151515 on epoch=439
03/02/2022 08:04:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=441
03/02/2022 08:04:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=443
03/02/2022 08:04:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=445
03/02/2022 08:04:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=447
03/02/2022 08:04:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=449
03/02/2022 08:04:30 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.33767879012595364 on epoch=449
03/02/2022 08:04:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=451
03/02/2022 08:04:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=453
03/02/2022 08:04:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=455
03/02/2022 08:04:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=457
03/02/2022 08:04:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=459
03/02/2022 08:04:44 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.31900871856766 on epoch=459
03/02/2022 08:04:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=461
03/02/2022 08:04:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=463
03/02/2022 08:04:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=465
03/02/2022 08:04:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=467
03/02/2022 08:04:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=469
03/02/2022 08:04:58 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.18824012416198177 on epoch=469
03/02/2022 08:05:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=471
03/02/2022 08:05:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=473
03/02/2022 08:05:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=475
03/02/2022 08:05:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=477
03/02/2022 08:05:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=479
03/02/2022 08:05:12 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.2506595661520422 on epoch=479
03/02/2022 08:05:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=481
03/02/2022 08:05:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=483
03/02/2022 08:05:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=485
03/02/2022 08:05:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=487
03/02/2022 08:05:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=489
03/02/2022 08:05:26 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.2575719932241671 on epoch=489
03/02/2022 08:05:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=491
03/02/2022 08:05:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=493
03/02/2022 08:05:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=495
03/02/2022 08:05:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=497
03/02/2022 08:05:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=499
03/02/2022 08:05:40 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.20593602245862883 on epoch=499
03/02/2022 08:05:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=501
03/02/2022 08:05:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=503
03/02/2022 08:05:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=505
03/02/2022 08:05:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=507
03/02/2022 08:05:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=509
03/02/2022 08:05:54 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.15813453386552803 on epoch=509
03/02/2022 08:05:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=511
03/02/2022 08:05:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=513
03/02/2022 08:06:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=515
03/02/2022 08:06:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=517
03/02/2022 08:06:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=519
03/02/2022 08:06:08 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.19492781019138217 on epoch=519
03/02/2022 08:06:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=521
03/02/2022 08:06:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=523
03/02/2022 08:06:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=525
03/02/2022 08:06:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=527
03/02/2022 08:06:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=529
03/02/2022 08:06:23 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.17723502304147468 on epoch=529
03/02/2022 08:06:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=531
03/02/2022 08:06:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=533
03/02/2022 08:06:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=535
03/02/2022 08:06:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=537
03/02/2022 08:06:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=539
03/02/2022 08:06:37 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.22804232804232805 on epoch=539
03/02/2022 08:06:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=541
03/02/2022 08:06:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=543
03/02/2022 08:06:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=545
03/02/2022 08:06:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=547
03/02/2022 08:06:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=549
03/02/2022 08:06:51 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.18896331738437 on epoch=549
03/02/2022 08:06:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=551
03/02/2022 08:06:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=553
03/02/2022 08:06:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=555
03/02/2022 08:07:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=557
03/02/2022 08:07:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=559
03/02/2022 08:07:05 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.19223471026127573 on epoch=559
03/02/2022 08:07:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=561
03/02/2022 08:07:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=563
03/02/2022 08:07:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=565
03/02/2022 08:07:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=567
03/02/2022 08:07:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=569
03/02/2022 08:07:19 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.1469229530892817 on epoch=569
03/02/2022 08:07:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=571
03/02/2022 08:07:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=573
03/02/2022 08:07:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=575
03/02/2022 08:07:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=577
03/02/2022 08:07:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=579
03/02/2022 08:07:33 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.19607010857010856 on epoch=579
03/02/2022 08:07:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=581
03/02/2022 08:07:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=583
03/02/2022 08:07:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=585
03/02/2022 08:07:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=587
03/02/2022 08:07:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=589
03/02/2022 08:07:47 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.24713876687560898 on epoch=589
03/02/2022 08:07:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=591
03/02/2022 08:07:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=593
03/02/2022 08:07:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=595
03/02/2022 08:07:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=597
03/02/2022 08:07:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=599
03/02/2022 08:08:00 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:08:00 - INFO - __main__ - Printing 3 examples
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 08:08:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:08:00 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:08:00 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:08:00 - INFO - __main__ - Printing 3 examples
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 08:08:00 - INFO - __main__ - ['Yes']
03/02/2022 08:08:00 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:08:00 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:08:01 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:08:02 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.22989440446995626 on epoch=599
03/02/2022 08:08:02 - INFO - __main__ - save last model!
03/02/2022 08:08:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 08:08:02 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 08:08:02 - INFO - __main__ - Printing 3 examples
03/02/2022 08:08:02 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 08:08:02 - INFO - __main__ - ['No']
03/02/2022 08:08:02 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 08:08:02 - INFO - __main__ - ['Yes']
03/02/2022 08:08:02 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 08:08:02 - INFO - __main__ - ['Yes']
03/02/2022 08:08:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:08:05 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:08:11 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 08:08:15 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:08:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:08:15 - INFO - __main__ - Starting training!
03/02/2022 08:12:22 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_21_0.3_8_predictions.txt
03/02/2022 08:12:22 - INFO - __main__ - Classification-F1 on test data: 0.0807
03/02/2022 08:12:22 - INFO - __main__ - prefix=circa_16_21, lr=0.3, bsz=8, dev_performance=0.3904106443363899, test_performance=0.08074606369202716
03/02/2022 08:12:22 - INFO - __main__ - Running ... prefix=circa_16_21, lr=0.2, bsz=8 ...
03/02/2022 08:12:23 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:12:23 - INFO - __main__ - Printing 3 examples
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: X and Y are childhood neighbours who unexpectedly run into each other at a cafe. [SEP] question X: Are your parents okay? [SEP] answer Y: They are very well.
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you play any instruments? [SEP] answer Y: I can play the piano
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: X wants to know what sorts of books Y likes to read. [SEP] question X: Do you own an electronic reader? [SEP] answer Y: I've had a Kindle for a few years.
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:12:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:12:23 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:12:23 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:12:23 - INFO - __main__ - Printing 3 examples
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you get on with your colleagues? [SEP] answer Y: We are tight.
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Have you ever try Mexican food? [SEP] answer Y: I grew up eating tacos.
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Want to watch a game this weekend? [SEP] answer Y: I wouldn't mind going to a game and drinking some beers.
03/02/2022 08:12:23 - INFO - __main__ - ['Yes']
03/02/2022 08:12:23 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:12:23 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:12:23 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:12:37 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:12:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:12:38 - INFO - __main__ - Starting training!
03/02/2022 08:12:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.97 on epoch=1
03/02/2022 08:12:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.21 on epoch=3
03/02/2022 08:12:46 - INFO - __main__ - Step 30 Global step 30 Train loss 2.68 on epoch=5
03/02/2022 08:12:48 - INFO - __main__ - Step 40 Global step 40 Train loss 2.24 on epoch=7
03/02/2022 08:12:50 - INFO - __main__ - Step 50 Global step 50 Train loss 1.82 on epoch=9
03/02/2022 08:12:52 - INFO - __main__ - Global step 50 Train loss 2.78 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 08:12:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 08:12:54 - INFO - __main__ - Step 60 Global step 60 Train loss 1.62 on epoch=11
03/02/2022 08:12:57 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=13
03/02/2022 08:12:59 - INFO - __main__ - Step 80 Global step 80 Train loss 1.22 on epoch=15
03/02/2022 08:13:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=17
03/02/2022 08:13:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=19
03/02/2022 08:13:06 - INFO - __main__ - Global step 100 Train loss 1.24 Classification-F1 0.19541773231031545 on epoch=19
03/02/2022 08:13:06 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.19541773231031545 on epoch=19, global_step=100
03/02/2022 08:13:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=21
03/02/2022 08:13:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=23
03/02/2022 08:13:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=25
03/02/2022 08:13:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.74 on epoch=27
03/02/2022 08:13:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.64 on epoch=29
03/02/2022 08:13:19 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 08:13:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=31
03/02/2022 08:13:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=33
03/02/2022 08:13:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.66 on epoch=35
03/02/2022 08:13:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=37
03/02/2022 08:13:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=39
03/02/2022 08:13:33 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 08:13:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=41
03/02/2022 08:13:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=43
03/02/2022 08:13:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=45
03/02/2022 08:13:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=47
03/02/2022 08:13:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=49
03/02/2022 08:13:46 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.06808510638297872 on epoch=49
03/02/2022 08:13:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=51
03/02/2022 08:13:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=53
03/02/2022 08:13:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=55
03/02/2022 08:13:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=57
03/02/2022 08:13:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=59
03/02/2022 08:13:59 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.10476190476190476 on epoch=59
03/02/2022 08:14:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=61
03/02/2022 08:14:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=63
03/02/2022 08:14:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=65
03/02/2022 08:14:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=67
03/02/2022 08:14:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=69
03/02/2022 08:14:13 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 08:14:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=71
03/02/2022 08:14:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=73
03/02/2022 08:14:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=75
03/02/2022 08:14:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=77
03/02/2022 08:14:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=79
03/02/2022 08:14:26 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=79
03/02/2022 08:14:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=81
03/02/2022 08:14:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=83
03/02/2022 08:14:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=85
03/02/2022 08:14:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=87
03/02/2022 08:14:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=89
03/02/2022 08:14:39 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=89
03/02/2022 08:14:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=91
03/02/2022 08:14:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=93
03/02/2022 08:14:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=95
03/02/2022 08:14:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 08:14:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=99
03/02/2022 08:14:52 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.06666666666666668 on epoch=99
03/02/2022 08:14:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=101
03/02/2022 08:14:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=103
03/02/2022 08:14:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=105
03/02/2022 08:15:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=107
03/02/2022 08:15:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=109
03/02/2022 08:15:06 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.13067414745331812 on epoch=109
03/02/2022 08:15:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=111
03/02/2022 08:15:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=113
03/02/2022 08:15:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=115
03/02/2022 08:15:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=117
03/02/2022 08:15:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=119
03/02/2022 08:15:20 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.12565656565656566 on epoch=119
03/02/2022 08:15:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=121
03/02/2022 08:15:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=123
03/02/2022 08:15:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=125
03/02/2022 08:15:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=127
03/02/2022 08:15:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=129
03/02/2022 08:15:33 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.06666666666666668 on epoch=129
03/02/2022 08:15:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=131
03/02/2022 08:15:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=133
03/02/2022 08:15:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=135
03/02/2022 08:15:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=137
03/02/2022 08:15:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=139
03/02/2022 08:15:46 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.09030732860520094 on epoch=139
03/02/2022 08:15:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=141
03/02/2022 08:15:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=143
03/02/2022 08:15:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=145
03/02/2022 08:15:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=147
03/02/2022 08:15:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=149
03/02/2022 08:15:59 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.20124636476942254 on epoch=149
03/02/2022 08:15:59 - INFO - __main__ - Saving model with best Classification-F1: 0.19541773231031545 -> 0.20124636476942254 on epoch=149, global_step=750
03/02/2022 08:16:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=151
03/02/2022 08:16:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=153
03/02/2022 08:16:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=155
03/02/2022 08:16:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=157
03/02/2022 08:16:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=159
03/02/2022 08:16:13 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.11282051282051282 on epoch=159
03/02/2022 08:16:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=161
03/02/2022 08:16:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=163
03/02/2022 08:16:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=165
03/02/2022 08:16:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=167
03/02/2022 08:16:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=169
03/02/2022 08:16:27 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.10416666666666667 on epoch=169
03/02/2022 08:16:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=171
03/02/2022 08:16:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=173
03/02/2022 08:16:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=175
03/02/2022 08:16:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=177
03/02/2022 08:16:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=179
03/02/2022 08:16:40 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.08735919899874844 on epoch=179
03/02/2022 08:16:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=181
03/02/2022 08:16:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=183
03/02/2022 08:16:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=185
03/02/2022 08:16:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=187
03/02/2022 08:16:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=189
03/02/2022 08:16:54 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.125296853625171 on epoch=189
03/02/2022 08:16:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=191
03/02/2022 08:16:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=193
03/02/2022 08:17:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=195
03/02/2022 08:17:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=197
03/02/2022 08:17:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=199
03/02/2022 08:17:08 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.18596491228070175 on epoch=199
03/02/2022 08:17:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=201
03/02/2022 08:17:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=203
03/02/2022 08:17:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=205
03/02/2022 08:17:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=207
03/02/2022 08:17:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=209
03/02/2022 08:17:22 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.18221153846153845 on epoch=209
03/02/2022 08:17:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=211
03/02/2022 08:17:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=213
03/02/2022 08:17:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=215
03/02/2022 08:17:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=217
03/02/2022 08:17:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=219
03/02/2022 08:17:35 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.27878787878787875 on epoch=219
03/02/2022 08:17:35 - INFO - __main__ - Saving model with best Classification-F1: 0.20124636476942254 -> 0.27878787878787875 on epoch=219, global_step=1100
03/02/2022 08:17:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=221
03/02/2022 08:17:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=223
03/02/2022 08:17:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=225
03/02/2022 08:17:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=227
03/02/2022 08:17:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=229
03/02/2022 08:17:48 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.26566154152361043 on epoch=229
03/02/2022 08:17:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=231
03/02/2022 08:17:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=233
03/02/2022 08:17:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=235
03/02/2022 08:17:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=237
03/02/2022 08:17:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=239
03/02/2022 08:18:02 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3442850661135598 on epoch=239
03/02/2022 08:18:02 - INFO - __main__ - Saving model with best Classification-F1: 0.27878787878787875 -> 0.3442850661135598 on epoch=239, global_step=1200
03/02/2022 08:18:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=241
03/02/2022 08:18:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=243
03/02/2022 08:18:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=245
03/02/2022 08:18:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=247
03/02/2022 08:18:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=249
03/02/2022 08:18:15 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.2862111801242236 on epoch=249
03/02/2022 08:18:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=251
03/02/2022 08:18:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=253
03/02/2022 08:18:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=255
03/02/2022 08:18:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=257
03/02/2022 08:18:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=259
03/02/2022 08:18:28 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.2765208647561589 on epoch=259
03/02/2022 08:18:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=261
03/02/2022 08:18:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=263
03/02/2022 08:18:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=265
03/02/2022 08:18:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=267
03/02/2022 08:18:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=269
03/02/2022 08:18:41 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.308040404040404 on epoch=269
03/02/2022 08:18:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=271
03/02/2022 08:18:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=273
03/02/2022 08:18:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=275
03/02/2022 08:18:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=277
03/02/2022 08:18:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=279
03/02/2022 08:18:53 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.3281318681318681 on epoch=279
03/02/2022 08:18:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=281
03/02/2022 08:18:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=283
03/02/2022 08:19:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=285
03/02/2022 08:19:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=287
03/02/2022 08:19:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=289
03/02/2022 08:19:06 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.2950694769711163 on epoch=289
03/02/2022 08:19:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=291
03/02/2022 08:19:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=293
03/02/2022 08:19:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=295
03/02/2022 08:19:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=297
03/02/2022 08:19:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=299
03/02/2022 08:19:19 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.31887444256014075 on epoch=299
03/02/2022 08:19:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=301
03/02/2022 08:19:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=303
03/02/2022 08:19:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=305
03/02/2022 08:19:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=307
03/02/2022 08:19:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=309
03/02/2022 08:19:33 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.30493506493506495 on epoch=309
03/02/2022 08:19:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=311
03/02/2022 08:19:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=313
03/02/2022 08:19:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=315
03/02/2022 08:19:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=317
03/02/2022 08:19:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=319
03/02/2022 08:19:46 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.30428571428571427 on epoch=319
03/02/2022 08:19:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=321
03/02/2022 08:19:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=323
03/02/2022 08:19:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=325
03/02/2022 08:19:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=327
03/02/2022 08:19:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=329
03/02/2022 08:19:59 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.3023767082590612 on epoch=329
03/02/2022 08:20:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=331
03/02/2022 08:20:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=333
03/02/2022 08:20:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=335
03/02/2022 08:20:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=337
03/02/2022 08:20:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=339
03/02/2022 08:20:12 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3011477411477411 on epoch=339
03/02/2022 08:20:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=341
03/02/2022 08:20:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=343
03/02/2022 08:20:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=345
03/02/2022 08:20:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=347
03/02/2022 08:20:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=349
03/02/2022 08:20:25 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.28097840038138544 on epoch=349
03/02/2022 08:20:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=351
03/02/2022 08:20:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=353
03/02/2022 08:20:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=355
03/02/2022 08:20:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=357
03/02/2022 08:20:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=359
03/02/2022 08:20:38 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.2917317976141506 on epoch=359
03/02/2022 08:20:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=361
03/02/2022 08:20:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=363
03/02/2022 08:20:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=365
03/02/2022 08:20:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=367
03/02/2022 08:20:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=369
03/02/2022 08:20:52 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.26621049201694363 on epoch=369
03/02/2022 08:20:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=371
03/02/2022 08:20:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=373
03/02/2022 08:20:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=375
03/02/2022 08:21:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=377
03/02/2022 08:21:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=379
03/02/2022 08:21:04 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.2753729242893329 on epoch=379
03/02/2022 08:21:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=381
03/02/2022 08:21:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=383
03/02/2022 08:21:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=385
03/02/2022 08:21:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=387
03/02/2022 08:21:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=389
03/02/2022 08:21:17 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.2663063063063063 on epoch=389
03/02/2022 08:21:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=391
03/02/2022 08:21:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=393
03/02/2022 08:21:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=395
03/02/2022 08:21:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=397
03/02/2022 08:21:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=399
03/02/2022 08:21:30 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.2946147640177491 on epoch=399
03/02/2022 08:21:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=401
03/02/2022 08:21:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=403
03/02/2022 08:21:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.27 on epoch=405
03/02/2022 08:21:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=407
03/02/2022 08:21:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=409
03/02/2022 08:21:44 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.29561904761904767 on epoch=409
03/02/2022 08:21:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=411
03/02/2022 08:21:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=413
03/02/2022 08:21:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=415
03/02/2022 08:21:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=417
03/02/2022 08:21:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=419
03/02/2022 08:21:58 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.2582739840804357 on epoch=419
03/02/2022 08:22:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=421
03/02/2022 08:22:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=423
03/02/2022 08:22:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=425
03/02/2022 08:22:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=427
03/02/2022 08:22:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=429
03/02/2022 08:22:11 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.31596255596255596 on epoch=429
03/02/2022 08:22:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=431
03/02/2022 08:22:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=433
03/02/2022 08:22:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=435
03/02/2022 08:22:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=437
03/02/2022 08:22:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=439
03/02/2022 08:22:24 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.28345424584831364 on epoch=439
03/02/2022 08:22:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=441
03/02/2022 08:22:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=443
03/02/2022 08:22:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=445
03/02/2022 08:22:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=447
03/02/2022 08:22:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=449
03/02/2022 08:22:38 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.28894230769230766 on epoch=449
03/02/2022 08:22:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=451
03/02/2022 08:22:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=453
03/02/2022 08:22:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=455
03/02/2022 08:22:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=457
03/02/2022 08:22:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=459
03/02/2022 08:22:51 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.27358066067743486 on epoch=459
03/02/2022 08:22:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=461
03/02/2022 08:22:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=463
03/02/2022 08:22:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=465
03/02/2022 08:23:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=467
03/02/2022 08:23:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=469
03/02/2022 08:23:05 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.25909740982043544 on epoch=469
03/02/2022 08:23:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=471
03/02/2022 08:23:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=473
03/02/2022 08:23:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=475
03/02/2022 08:23:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=477
03/02/2022 08:23:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=479
03/02/2022 08:23:19 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.2638453500522466 on epoch=479
03/02/2022 08:23:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=481
03/02/2022 08:23:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=483
03/02/2022 08:23:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=485
03/02/2022 08:23:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=487
03/02/2022 08:23:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=489
03/02/2022 08:23:32 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.278584455667789 on epoch=489
03/02/2022 08:23:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=491
03/02/2022 08:23:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=493
03/02/2022 08:23:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=495
03/02/2022 08:23:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=497
03/02/2022 08:23:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=499
03/02/2022 08:23:46 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.2636053822198719 on epoch=499
03/02/2022 08:23:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=501
03/02/2022 08:23:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=503
03/02/2022 08:23:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=505
03/02/2022 08:23:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=507
03/02/2022 08:23:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=509
03/02/2022 08:24:00 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.2681381118881119 on epoch=509
03/02/2022 08:24:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=511
03/02/2022 08:24:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=513
03/02/2022 08:24:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=515
03/02/2022 08:24:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=517
03/02/2022 08:24:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=519
03/02/2022 08:24:14 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.2735775572811811 on epoch=519
03/02/2022 08:24:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=521
03/02/2022 08:24:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=523
03/02/2022 08:24:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=525
03/02/2022 08:24:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=527
03/02/2022 08:24:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=529
03/02/2022 08:24:28 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.3294138418079096 on epoch=529
03/02/2022 08:24:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=531
03/02/2022 08:24:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=533
03/02/2022 08:24:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=535
03/02/2022 08:24:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=537
03/02/2022 08:24:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=539
03/02/2022 08:24:42 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.2957818226920195 on epoch=539
03/02/2022 08:24:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=541
03/02/2022 08:24:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=543
03/02/2022 08:24:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=545
03/02/2022 08:24:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=547
03/02/2022 08:24:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=549
03/02/2022 08:24:56 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.3317430197885605 on epoch=549
03/02/2022 08:24:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=551
03/02/2022 08:25:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=553
03/02/2022 08:25:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=555
03/02/2022 08:25:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=557
03/02/2022 08:25:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=559
03/02/2022 08:25:09 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.3155138960223706 on epoch=559
03/02/2022 08:25:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=561
03/02/2022 08:25:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=563
03/02/2022 08:25:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=565
03/02/2022 08:25:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=567
03/02/2022 08:25:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=569
03/02/2022 08:25:23 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.2609848484848485 on epoch=569
03/02/2022 08:25:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=571
03/02/2022 08:25:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=573
03/02/2022 08:25:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=575
03/02/2022 08:25:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=577
03/02/2022 08:25:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=579
03/02/2022 08:25:37 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.3270952375847368 on epoch=579
03/02/2022 08:25:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=581
03/02/2022 08:25:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=583
03/02/2022 08:25:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=585
03/02/2022 08:25:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=587
03/02/2022 08:25:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=589
03/02/2022 08:25:51 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.316215034965035 on epoch=589
03/02/2022 08:25:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=591
03/02/2022 08:25:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=593
03/02/2022 08:25:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=595
03/02/2022 08:26:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=597
03/02/2022 08:26:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=599
03/02/2022 08:26:04 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:26:04 - INFO - __main__ - Printing 3 examples
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 08:26:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:26:04 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:26:04 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:26:04 - INFO - __main__ - Printing 3 examples
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 08:26:04 - INFO - __main__ - ['Yes']
03/02/2022 08:26:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:26:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:26:04 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:26:05 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.3160251584164628 on epoch=599
03/02/2022 08:26:05 - INFO - __main__ - save last model!
03/02/2022 08:26:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 08:26:05 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 08:26:05 - INFO - __main__ - Printing 3 examples
03/02/2022 08:26:05 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 08:26:05 - INFO - __main__ - ['No']
03/02/2022 08:26:05 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 08:26:05 - INFO - __main__ - ['Yes']
03/02/2022 08:26:05 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 08:26:05 - INFO - __main__ - ['Yes']
03/02/2022 08:26:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:26:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:26:14 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 08:26:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:26:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:26:19 - INFO - __main__ - Starting training!
03/02/2022 08:29:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_21_0.2_8_predictions.txt
03/02/2022 08:29:47 - INFO - __main__ - Classification-F1 on test data: 0.1120
03/02/2022 08:29:50 - INFO - __main__ - prefix=circa_16_21, lr=0.2, bsz=8, dev_performance=0.3442850661135598, test_performance=0.11197512264011732
03/02/2022 08:29:50 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.5, bsz=8 ...
03/02/2022 08:29:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:29:51 - INFO - __main__ - Printing 3 examples
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:29:51 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:29:51 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:29:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:29:51 - INFO - __main__ - Printing 3 examples
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 08:29:51 - INFO - __main__ - ['Yes']
03/02/2022 08:29:51 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:29:51 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:29:51 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:30:03 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:30:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:30:04 - INFO - __main__ - Starting training!
03/02/2022 08:30:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=1
03/02/2022 08:30:09 - INFO - __main__ - Step 20 Global step 20 Train loss 2.28 on epoch=3
03/02/2022 08:30:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.49 on epoch=5
03/02/2022 08:30:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=7
03/02/2022 08:30:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.87 on epoch=9
03/02/2022 08:30:18 - INFO - __main__ - Global step 50 Train loss 1.82 Classification-F1 0.08777929776561788 on epoch=9
03/02/2022 08:30:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08777929776561788 on epoch=9, global_step=50
03/02/2022 08:30:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.73 on epoch=11
03/02/2022 08:30:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.68 on epoch=13
03/02/2022 08:30:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=15
03/02/2022 08:30:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.68 on epoch=17
03/02/2022 08:30:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=19
03/02/2022 08:30:31 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.0768170426065163 on epoch=19
03/02/2022 08:30:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=21
03/02/2022 08:30:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=23
03/02/2022 08:30:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=25
03/02/2022 08:30:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=27
03/02/2022 08:30:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=29
03/02/2022 08:30:44 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 08:30:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=31
03/02/2022 08:30:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=33
03/02/2022 08:30:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=35
03/02/2022 08:30:53 - INFO - __main__ - Step 190 Global step 190 Train loss 2.46 on epoch=37
03/02/2022 08:30:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=39
03/02/2022 08:30:56 - INFO - __main__ - Global step 200 Train loss 1.01 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 08:30:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=41
03/02/2022 08:31:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=43
03/02/2022 08:31:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=45
03/02/2022 08:31:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=47
03/02/2022 08:31:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=49
03/02/2022 08:31:09 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.06588235294117648 on epoch=49
03/02/2022 08:31:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=51
03/02/2022 08:31:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=53
03/02/2022 08:31:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 08:31:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=57
03/02/2022 08:31:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=59
03/02/2022 08:31:22 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.06436781609195402 on epoch=59
03/02/2022 08:31:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=61
03/02/2022 08:31:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=63
03/02/2022 08:31:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=65
03/02/2022 08:31:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=67
03/02/2022 08:31:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 08:31:35 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.056140350877192984 on epoch=69
03/02/2022 08:31:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=71
03/02/2022 08:31:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=73
03/02/2022 08:31:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=75
03/02/2022 08:31:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=77
03/02/2022 08:31:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=79
03/02/2022 08:31:48 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.0991869918699187 on epoch=79
03/02/2022 08:31:48 - INFO - __main__ - Saving model with best Classification-F1: 0.08777929776561788 -> 0.0991869918699187 on epoch=79, global_step=400
03/02/2022 08:31:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=81
03/02/2022 08:31:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=83
03/02/2022 08:31:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=85
03/02/2022 08:31:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=87
03/02/2022 08:32:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=89
03/02/2022 08:32:02 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.2259493670886076 on epoch=89
03/02/2022 08:32:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0991869918699187 -> 0.2259493670886076 on epoch=89, global_step=450
03/02/2022 08:32:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=91
03/02/2022 08:32:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=93
03/02/2022 08:32:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=95
03/02/2022 08:32:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 08:32:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=99
03/02/2022 08:32:14 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.17890124705089314 on epoch=99
03/02/2022 08:32:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=101
03/02/2022 08:32:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=103
03/02/2022 08:32:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=105
03/02/2022 08:32:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=107
03/02/2022 08:32:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=109
03/02/2022 08:32:28 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.28968660968660964 on epoch=109
03/02/2022 08:32:28 - INFO - __main__ - Saving model with best Classification-F1: 0.2259493670886076 -> 0.28968660968660964 on epoch=109, global_step=550
03/02/2022 08:32:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=111
03/02/2022 08:32:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=113
03/02/2022 08:32:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=115
03/02/2022 08:32:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=117
03/02/2022 08:32:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=119
03/02/2022 08:32:42 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.24158730158730157 on epoch=119
03/02/2022 08:32:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=121
03/02/2022 08:32:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=123
03/02/2022 08:32:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=125
03/02/2022 08:32:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=127
03/02/2022 08:32:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=129
03/02/2022 08:32:55 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.27515151515151515 on epoch=129
03/02/2022 08:32:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=131
03/02/2022 08:33:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=133
03/02/2022 08:33:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=135
03/02/2022 08:33:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=137
03/02/2022 08:33:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=139
03/02/2022 08:33:09 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.27456179113271467 on epoch=139
03/02/2022 08:33:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=141
03/02/2022 08:33:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=143
03/02/2022 08:33:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=145
03/02/2022 08:33:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=147
03/02/2022 08:33:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=149
03/02/2022 08:33:23 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3104829039642455 on epoch=149
03/02/2022 08:33:23 - INFO - __main__ - Saving model with best Classification-F1: 0.28968660968660964 -> 0.3104829039642455 on epoch=149, global_step=750
03/02/2022 08:33:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=151
03/02/2022 08:33:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=153
03/02/2022 08:33:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=155
03/02/2022 08:33:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=157
03/02/2022 08:33:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=159
03/02/2022 08:33:36 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.22826475849731662 on epoch=159
03/02/2022 08:33:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=161
03/02/2022 08:33:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=163
03/02/2022 08:33:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=165
03/02/2022 08:33:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=167
03/02/2022 08:33:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=169
03/02/2022 08:33:50 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.25641877856252593 on epoch=169
03/02/2022 08:33:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=171
03/02/2022 08:33:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=173
03/02/2022 08:33:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=175
03/02/2022 08:33:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=177
03/02/2022 08:34:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=179
03/02/2022 08:34:04 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.2735483870967742 on epoch=179
03/02/2022 08:34:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=181
03/02/2022 08:34:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=183
03/02/2022 08:34:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=185
03/02/2022 08:34:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=187
03/02/2022 08:34:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=189
03/02/2022 08:34:17 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.27999999999999997 on epoch=189
03/02/2022 08:34:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=191
03/02/2022 08:34:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=193
03/02/2022 08:34:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=195
03/02/2022 08:34:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=197
03/02/2022 08:34:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=199
03/02/2022 08:34:31 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.28624975183641055 on epoch=199
03/02/2022 08:34:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=201
03/02/2022 08:34:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=203
03/02/2022 08:34:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=205
03/02/2022 08:34:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=207
03/02/2022 08:34:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=209
03/02/2022 08:34:45 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.3099120234604106 on epoch=209
03/02/2022 08:34:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=211
03/02/2022 08:34:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=213
03/02/2022 08:34:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=215
03/02/2022 08:34:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=217
03/02/2022 08:34:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=219
03/02/2022 08:34:59 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.41061274509803924 on epoch=219
03/02/2022 08:34:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3104829039642455 -> 0.41061274509803924 on epoch=219, global_step=1100
03/02/2022 08:35:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=221
03/02/2022 08:35:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=223
03/02/2022 08:35:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=225
03/02/2022 08:35:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=227
03/02/2022 08:35:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=229
03/02/2022 08:35:13 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.29408369408369406 on epoch=229
03/02/2022 08:35:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=231
03/02/2022 08:35:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=233
03/02/2022 08:35:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=235
03/02/2022 08:35:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=237
03/02/2022 08:35:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=239
03/02/2022 08:35:27 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.3955761953904369 on epoch=239
03/02/2022 08:35:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=241
03/02/2022 08:35:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=243
03/02/2022 08:35:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=245
03/02/2022 08:35:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=247
03/02/2022 08:35:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=249
03/02/2022 08:35:40 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.2970635386119257 on epoch=249
03/02/2022 08:35:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=251
03/02/2022 08:35:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=253
03/02/2022 08:35:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=255
03/02/2022 08:35:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=257
03/02/2022 08:35:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=259
03/02/2022 08:35:54 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.45925925925925926 on epoch=259
03/02/2022 08:35:54 - INFO - __main__ - Saving model with best Classification-F1: 0.41061274509803924 -> 0.45925925925925926 on epoch=259, global_step=1300
03/02/2022 08:35:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=261
03/02/2022 08:35:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=263
03/02/2022 08:36:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=265
03/02/2022 08:36:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=267
03/02/2022 08:36:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=269
03/02/2022 08:36:08 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.3177477477477478 on epoch=269
03/02/2022 08:36:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=271
03/02/2022 08:36:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=273
03/02/2022 08:36:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=275
03/02/2022 08:36:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=277
03/02/2022 08:36:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=279
03/02/2022 08:36:22 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.449961229702609 on epoch=279
03/02/2022 08:36:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=281
03/02/2022 08:36:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=283
03/02/2022 08:36:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=285
03/02/2022 08:36:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=287
03/02/2022 08:36:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=289
03/02/2022 08:36:36 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.41121942110177406 on epoch=289
03/02/2022 08:36:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=291
03/02/2022 08:36:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=293
03/02/2022 08:36:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=295
03/02/2022 08:36:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=297
03/02/2022 08:36:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=299
03/02/2022 08:36:50 - INFO - __main__ - Global step 1500 Train loss 0.25 Classification-F1 0.47416190894451765 on epoch=299
03/02/2022 08:36:50 - INFO - __main__ - Saving model with best Classification-F1: 0.45925925925925926 -> 0.47416190894451765 on epoch=299, global_step=1500
03/02/2022 08:36:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=301
03/02/2022 08:36:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=303
03/02/2022 08:36:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=305
03/02/2022 08:36:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=307
03/02/2022 08:37:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=309
03/02/2022 08:37:04 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.5545212785150866 on epoch=309
03/02/2022 08:37:04 - INFO - __main__ - Saving model with best Classification-F1: 0.47416190894451765 -> 0.5545212785150866 on epoch=309, global_step=1550
03/02/2022 08:37:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=311
03/02/2022 08:37:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=313
03/02/2022 08:37:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=315
03/02/2022 08:37:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=317
03/02/2022 08:37:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=319
03/02/2022 08:37:18 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.5207306944331902 on epoch=319
03/02/2022 08:37:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=321
03/02/2022 08:37:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=323
03/02/2022 08:37:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=325
03/02/2022 08:37:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=327
03/02/2022 08:37:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=329
03/02/2022 08:37:32 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.46168905472636823 on epoch=329
03/02/2022 08:37:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=331
03/02/2022 08:37:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=333
03/02/2022 08:37:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=335
03/02/2022 08:37:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=337
03/02/2022 08:37:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=339
03/02/2022 08:37:47 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.4470034595034595 on epoch=339
03/02/2022 08:37:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=341
03/02/2022 08:37:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=343
03/02/2022 08:37:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=345
03/02/2022 08:37:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=347
03/02/2022 08:37:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=349
03/02/2022 08:38:01 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.3946713511930903 on epoch=349
03/02/2022 08:38:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=351
03/02/2022 08:38:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=353
03/02/2022 08:38:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=355
03/02/2022 08:38:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=357
03/02/2022 08:38:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=359
03/02/2022 08:38:15 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.5592736369910283 on epoch=359
03/02/2022 08:38:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5545212785150866 -> 0.5592736369910283 on epoch=359, global_step=1800
03/02/2022 08:38:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=361
03/02/2022 08:38:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=363
03/02/2022 08:38:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=365
03/02/2022 08:38:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=367
03/02/2022 08:38:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=369
03/02/2022 08:38:29 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.49662054476963197 on epoch=369
03/02/2022 08:38:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=371
03/02/2022 08:38:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=373
03/02/2022 08:38:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=375
03/02/2022 08:38:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=377
03/02/2022 08:38:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=379
03/02/2022 08:38:43 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.47674369747899165 on epoch=379
03/02/2022 08:38:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=381
03/02/2022 08:38:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=383
03/02/2022 08:38:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=385
03/02/2022 08:38:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=387
03/02/2022 08:38:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=389
03/02/2022 08:38:57 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.36673427991886415 on epoch=389
03/02/2022 08:38:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=391
03/02/2022 08:39:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=393
03/02/2022 08:39:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=395
03/02/2022 08:39:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=397
03/02/2022 08:39:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=399
03/02/2022 08:39:10 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.37850871191388435 on epoch=399
03/02/2022 08:39:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=401
03/02/2022 08:39:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=403
03/02/2022 08:39:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=405
03/02/2022 08:39:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=407
03/02/2022 08:39:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=409
03/02/2022 08:39:24 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.41303535353535353 on epoch=409
03/02/2022 08:39:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=411
03/02/2022 08:39:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=413
03/02/2022 08:39:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=415
03/02/2022 08:39:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=417
03/02/2022 08:39:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=419
03/02/2022 08:39:38 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.4167370297401258 on epoch=419
03/02/2022 08:39:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=421
03/02/2022 08:39:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=423
03/02/2022 08:39:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=425
03/02/2022 08:39:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=427
03/02/2022 08:39:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=429
03/02/2022 08:39:52 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.4600806138306139 on epoch=429
03/02/2022 08:39:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=431
03/02/2022 08:39:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=433
03/02/2022 08:39:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 08:40:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=437
03/02/2022 08:40:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=439
03/02/2022 08:40:06 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.5665040876997398 on epoch=439
03/02/2022 08:40:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5592736369910283 -> 0.5665040876997398 on epoch=439, global_step=2200
03/02/2022 08:40:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=441
03/02/2022 08:40:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=443
03/02/2022 08:40:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=445
03/02/2022 08:40:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=447
03/02/2022 08:40:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=449
03/02/2022 08:40:20 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.48094600441802743 on epoch=449
03/02/2022 08:40:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=451
03/02/2022 08:40:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=453
03/02/2022 08:40:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=455
03/02/2022 08:40:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=457
03/02/2022 08:40:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=459
03/02/2022 08:40:34 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.4490519227361333 on epoch=459
03/02/2022 08:40:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=461
03/02/2022 08:40:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=463
03/02/2022 08:40:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=465
03/02/2022 08:40:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=467
03/02/2022 08:40:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=469
03/02/2022 08:40:48 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.5986907256257101 on epoch=469
03/02/2022 08:40:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5665040876997398 -> 0.5986907256257101 on epoch=469, global_step=2350
03/02/2022 08:40:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=471
03/02/2022 08:40:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=473
03/02/2022 08:40:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=475
03/02/2022 08:40:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=477
03/02/2022 08:40:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=479
03/02/2022 08:41:02 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.5065138429109017 on epoch=479
03/02/2022 08:41:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=481
03/02/2022 08:41:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=483
03/02/2022 08:41:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=485
03/02/2022 08:41:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=487
03/02/2022 08:41:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=489
03/02/2022 08:41:16 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.4022578267143485 on epoch=489
03/02/2022 08:41:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=491
03/02/2022 08:41:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 08:41:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=495
03/02/2022 08:41:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 08:41:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=499
03/02/2022 08:41:30 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.5093730563286807 on epoch=499
03/02/2022 08:41:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=501
03/02/2022 08:41:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=503
03/02/2022 08:41:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=505
03/02/2022 08:41:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=507
03/02/2022 08:41:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=509
03/02/2022 08:41:44 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.509033315612263 on epoch=509
03/02/2022 08:41:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=511
03/02/2022 08:41:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=513
03/02/2022 08:41:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 08:41:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=517
03/02/2022 08:41:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=519
03/02/2022 08:41:58 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5880091234929944 on epoch=519
03/02/2022 08:42:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=521
03/02/2022 08:42:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=523
03/02/2022 08:42:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=525
03/02/2022 08:42:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 08:42:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=529
03/02/2022 08:42:12 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5237387822102545 on epoch=529
03/02/2022 08:42:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 08:42:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=533
03/02/2022 08:42:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=535
03/02/2022 08:42:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=537
03/02/2022 08:42:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=539
03/02/2022 08:42:25 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6143527506952358 on epoch=539
03/02/2022 08:42:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5986907256257101 -> 0.6143527506952358 on epoch=539, global_step=2700
03/02/2022 08:42:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=541
03/02/2022 08:42:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=543
03/02/2022 08:42:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 08:42:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=547
03/02/2022 08:42:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=549
03/02/2022 08:42:39 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6149019607843137 on epoch=549
03/02/2022 08:42:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6143527506952358 -> 0.6149019607843137 on epoch=549, global_step=2750
03/02/2022 08:42:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 08:42:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 08:42:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=555
03/02/2022 08:42:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=557
03/02/2022 08:42:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=559
03/02/2022 08:42:53 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6853175431691445 on epoch=559
03/02/2022 08:42:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6149019607843137 -> 0.6853175431691445 on epoch=559, global_step=2800
03/02/2022 08:42:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=561
03/02/2022 08:42:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=563
03/02/2022 08:43:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=565
03/02/2022 08:43:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=567
03/02/2022 08:43:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=569
03/02/2022 08:43:07 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.5191433075394055 on epoch=569
03/02/2022 08:43:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=571
03/02/2022 08:43:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=573
03/02/2022 08:43:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 08:43:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=577
03/02/2022 08:43:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=579
03/02/2022 08:43:21 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.45026667348095917 on epoch=579
03/02/2022 08:43:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 08:43:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=583
03/02/2022 08:43:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=585
03/02/2022 08:43:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=587
03/02/2022 08:43:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=589
03/02/2022 08:43:35 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5971814649613512 on epoch=589
03/02/2022 08:43:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 08:43:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=593
03/02/2022 08:43:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=595
03/02/2022 08:43:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=597
03/02/2022 08:43:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=599
03/02/2022 08:43:47 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:43:47 - INFO - __main__ - Printing 3 examples
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 08:43:47 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:43:47 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:43:47 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:43:47 - INFO - __main__ - Printing 3 examples
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 08:43:47 - INFO - __main__ - ['Yes']
03/02/2022 08:43:47 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:43:47 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:43:48 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:43:49 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.521174624614547 on epoch=599
03/02/2022 08:43:49 - INFO - __main__ - save last model!
03/02/2022 08:43:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 08:43:49 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 08:43:49 - INFO - __main__ - Printing 3 examples
03/02/2022 08:43:49 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 08:43:49 - INFO - __main__ - ['No']
03/02/2022 08:43:49 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 08:43:49 - INFO - __main__ - ['Yes']
03/02/2022 08:43:49 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 08:43:49 - INFO - __main__ - ['Yes']
03/02/2022 08:43:49 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:43:52 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:43:58 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 08:44:01 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:44:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:44:02 - INFO - __main__ - Starting training!
03/02/2022 08:48:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_42_0.5_8_predictions.txt
03/02/2022 08:48:11 - INFO - __main__ - Classification-F1 on test data: 0.1596
03/02/2022 08:48:11 - INFO - __main__ - prefix=circa_16_42, lr=0.5, bsz=8, dev_performance=0.6853175431691445, test_performance=0.159640712867425
03/02/2022 08:48:11 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.4, bsz=8 ...
03/02/2022 08:48:12 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:48:12 - INFO - __main__ - Printing 3 examples
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 08:48:12 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:48:12 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 08:48:12 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 08:48:12 - INFO - __main__ - Printing 3 examples
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 08:48:12 - INFO - __main__ - ['Yes']
03/02/2022 08:48:12 - INFO - __main__ - Tokenizing Input ...
03/02/2022 08:48:12 - INFO - __main__ - Tokenizing Output ...
03/02/2022 08:48:12 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 08:48:26 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 08:48:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 08:48:27 - INFO - __main__ - Starting training!
03/02/2022 08:48:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.71 on epoch=1
03/02/2022 08:48:32 - INFO - __main__ - Step 20 Global step 20 Train loss 2.57 on epoch=3
03/02/2022 08:48:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.92 on epoch=5
03/02/2022 08:48:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.50 on epoch=7
03/02/2022 08:48:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.20 on epoch=9
03/02/2022 08:48:41 - INFO - __main__ - Global step 50 Train loss 2.18 Classification-F1 0.10606451612903225 on epoch=9
03/02/2022 08:48:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10606451612903225 on epoch=9, global_step=50
03/02/2022 08:48:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=11
03/02/2022 08:48:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.82 on epoch=13
03/02/2022 08:48:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.71 on epoch=15
03/02/2022 08:48:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=17
03/02/2022 08:48:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=19
03/02/2022 08:48:54 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 08:48:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=21
03/02/2022 08:48:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=23
03/02/2022 08:49:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=25
03/02/2022 08:49:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=27
03/02/2022 08:49:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=29
03/02/2022 08:49:07 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.09124324324324325 on epoch=29
03/02/2022 08:49:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=31
03/02/2022 08:49:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=33
03/02/2022 08:49:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=35
03/02/2022 08:49:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=37
03/02/2022 08:49:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=39
03/02/2022 08:49:20 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.09662337662337663 on epoch=39
03/02/2022 08:49:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=41
03/02/2022 08:49:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=43
03/02/2022 08:49:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=45
03/02/2022 08:49:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=47
03/02/2022 08:49:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=49
03/02/2022 08:49:32 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 08:49:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=51
03/02/2022 08:49:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=53
03/02/2022 08:49:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=55
03/02/2022 08:49:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=57
03/02/2022 08:49:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=59
03/02/2022 08:49:46 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=59
03/02/2022 08:49:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=61
03/02/2022 08:49:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=63
03/02/2022 08:49:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=65
03/02/2022 08:49:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=67
03/02/2022 08:49:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=69
03/02/2022 08:49:59 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.08591749644381223 on epoch=69
03/02/2022 08:50:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=71
03/02/2022 08:50:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=73
03/02/2022 08:50:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=75
03/02/2022 08:50:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=77
03/02/2022 08:50:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=79
03/02/2022 08:50:12 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.17745974955277283 on epoch=79
03/02/2022 08:50:12 - INFO - __main__ - Saving model with best Classification-F1: 0.10606451612903225 -> 0.17745974955277283 on epoch=79, global_step=400
03/02/2022 08:50:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=81
03/02/2022 08:50:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=83
03/02/2022 08:50:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=85
03/02/2022 08:50:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=87
03/02/2022 08:50:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=89
03/02/2022 08:50:26 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.20600089565606808 on epoch=89
03/02/2022 08:50:26 - INFO - __main__ - Saving model with best Classification-F1: 0.17745974955277283 -> 0.20600089565606808 on epoch=89, global_step=450
03/02/2022 08:50:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=91
03/02/2022 08:50:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=93
03/02/2022 08:50:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=95
03/02/2022 08:50:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=97
03/02/2022 08:50:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=99
03/02/2022 08:50:40 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.2361625216888375 on epoch=99
03/02/2022 08:50:40 - INFO - __main__ - Saving model with best Classification-F1: 0.20600089565606808 -> 0.2361625216888375 on epoch=99, global_step=500
03/02/2022 08:50:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=101
03/02/2022 08:50:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=103
03/02/2022 08:50:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=105
03/02/2022 08:50:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=107
03/02/2022 08:50:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=109
03/02/2022 08:50:53 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.26830466830466826 on epoch=109
03/02/2022 08:50:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2361625216888375 -> 0.26830466830466826 on epoch=109, global_step=550
03/02/2022 08:50:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=111
03/02/2022 08:50:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=113
03/02/2022 08:51:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=115
03/02/2022 08:51:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=117
03/02/2022 08:51:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=119
03/02/2022 08:51:07 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.33917656556337217 on epoch=119
03/02/2022 08:51:07 - INFO - __main__ - Saving model with best Classification-F1: 0.26830466830466826 -> 0.33917656556337217 on epoch=119, global_step=600
03/02/2022 08:51:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=121
03/02/2022 08:51:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=123
03/02/2022 08:51:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=125
03/02/2022 08:51:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=127
03/02/2022 08:51:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=129
03/02/2022 08:51:21 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.3480023450985338 on epoch=129
03/02/2022 08:51:21 - INFO - __main__ - Saving model with best Classification-F1: 0.33917656556337217 -> 0.3480023450985338 on epoch=129, global_step=650
03/02/2022 08:51:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=131
03/02/2022 08:51:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=133
03/02/2022 08:51:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=135
03/02/2022 08:51:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=137
03/02/2022 08:51:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=139
03/02/2022 08:51:35 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.4360077112519682 on epoch=139
03/02/2022 08:51:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3480023450985338 -> 0.4360077112519682 on epoch=139, global_step=700
03/02/2022 08:51:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=141
03/02/2022 08:51:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=143
03/02/2022 08:51:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=145
03/02/2022 08:51:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=147
03/02/2022 08:51:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=149
03/02/2022 08:51:49 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.36684770682466533 on epoch=149
03/02/2022 08:51:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=151
03/02/2022 08:51:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=153
03/02/2022 08:51:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=155
03/02/2022 08:51:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=157
03/02/2022 08:52:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=159
03/02/2022 08:52:03 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.3780824639841033 on epoch=159
03/02/2022 08:52:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=161
03/02/2022 08:52:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=163
03/02/2022 08:52:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=165
03/02/2022 08:52:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=167
03/02/2022 08:52:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=169
03/02/2022 08:52:16 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.3925003123644889 on epoch=169
03/02/2022 08:52:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=171
03/02/2022 08:52:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=173
03/02/2022 08:52:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=175
03/02/2022 08:52:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=177
03/02/2022 08:52:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=179
03/02/2022 08:52:30 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.4446428571428571 on epoch=179
03/02/2022 08:52:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4360077112519682 -> 0.4446428571428571 on epoch=179, global_step=900
03/02/2022 08:52:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=181
03/02/2022 08:52:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=183
03/02/2022 08:52:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=185
03/02/2022 08:52:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=187
03/02/2022 08:52:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=189
03/02/2022 08:52:44 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.3653656126482213 on epoch=189
03/02/2022 08:52:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=191
03/02/2022 08:52:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=193
03/02/2022 08:52:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=195
03/02/2022 08:52:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=197
03/02/2022 08:52:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=199
03/02/2022 08:52:57 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.4004850893650637 on epoch=199
03/02/2022 08:53:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=201
03/02/2022 08:53:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=203
03/02/2022 08:53:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=205
03/02/2022 08:53:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=207
03/02/2022 08:53:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=209
03/02/2022 08:53:11 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.4292567917815321 on epoch=209
03/02/2022 08:53:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=211
03/02/2022 08:53:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=213
03/02/2022 08:53:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=215
03/02/2022 08:53:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=217
03/02/2022 08:53:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=219
03/02/2022 08:53:25 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.3668440122044241 on epoch=219
03/02/2022 08:53:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=221
03/02/2022 08:53:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=223
03/02/2022 08:53:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=225
03/02/2022 08:53:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=227
03/02/2022 08:53:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=229
03/02/2022 08:53:39 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.34000721500721504 on epoch=229
03/02/2022 08:53:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=231
03/02/2022 08:53:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=233
03/02/2022 08:53:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=235
03/02/2022 08:53:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=237
03/02/2022 08:53:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=239
03/02/2022 08:53:52 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.37180270491440703 on epoch=239
03/02/2022 08:53:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=241
03/02/2022 08:53:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=243
03/02/2022 08:53:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=245
03/02/2022 08:54:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=247
03/02/2022 08:54:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=249
03/02/2022 08:54:06 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.45485429834425534 on epoch=249
03/02/2022 08:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4446428571428571 -> 0.45485429834425534 on epoch=249, global_step=1250
03/02/2022 08:54:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=251
03/02/2022 08:54:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=253
03/02/2022 08:54:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=255
03/02/2022 08:54:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=257
03/02/2022 08:54:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=259
03/02/2022 08:54:20 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.45760249554367205 on epoch=259
03/02/2022 08:54:20 - INFO - __main__ - Saving model with best Classification-F1: 0.45485429834425534 -> 0.45760249554367205 on epoch=259, global_step=1300
03/02/2022 08:54:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=261
03/02/2022 08:54:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=263
03/02/2022 08:54:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=265
03/02/2022 08:54:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=267
03/02/2022 08:54:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=269
03/02/2022 08:54:34 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.44572301729278474 on epoch=269
03/02/2022 08:54:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=271
03/02/2022 08:54:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=273
03/02/2022 08:54:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=275
03/02/2022 08:54:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=277
03/02/2022 08:54:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=279
03/02/2022 08:54:48 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.4873613998613998 on epoch=279
03/02/2022 08:54:48 - INFO - __main__ - Saving model with best Classification-F1: 0.45760249554367205 -> 0.4873613998613998 on epoch=279, global_step=1400
03/02/2022 08:54:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=281
03/02/2022 08:54:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=283
03/02/2022 08:54:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=285
03/02/2022 08:54:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=287
03/02/2022 08:54:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=289
03/02/2022 08:55:01 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.3944775132275132 on epoch=289
03/02/2022 08:55:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=291
03/02/2022 08:55:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=293
03/02/2022 08:55:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=295
03/02/2022 08:55:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=297
03/02/2022 08:55:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=299
03/02/2022 08:55:15 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.42476851851851843 on epoch=299
03/02/2022 08:55:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=301
03/02/2022 08:55:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=303
03/02/2022 08:55:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=305
03/02/2022 08:55:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=307
03/02/2022 08:55:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=309
03/02/2022 08:55:29 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.44913274449564766 on epoch=309
03/02/2022 08:55:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=311
03/02/2022 08:55:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=313
03/02/2022 08:55:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=315
03/02/2022 08:55:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=317
03/02/2022 08:55:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=319
03/02/2022 08:55:43 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.5246228786926461 on epoch=319
03/02/2022 08:55:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4873613998613998 -> 0.5246228786926461 on epoch=319, global_step=1600
03/02/2022 08:55:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=321
03/02/2022 08:55:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=323
03/02/2022 08:55:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=325
03/02/2022 08:55:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=327
03/02/2022 08:55:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=329
03/02/2022 08:55:57 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.4401431059061505 on epoch=329
03/02/2022 08:55:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=331
03/02/2022 08:56:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=333
03/02/2022 08:56:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=335
03/02/2022 08:56:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=337
03/02/2022 08:56:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=339
03/02/2022 08:56:11 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.43728639781271356 on epoch=339
03/02/2022 08:56:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=341
03/02/2022 08:56:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=343
03/02/2022 08:56:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=345
03/02/2022 08:56:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=347
03/02/2022 08:56:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=349
03/02/2022 08:56:25 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.49890121164744966 on epoch=349
03/02/2022 08:56:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=351
03/02/2022 08:56:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=353
03/02/2022 08:56:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=355
03/02/2022 08:56:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=357
03/02/2022 08:56:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=359
03/02/2022 08:56:39 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.43139122315592904 on epoch=359
03/02/2022 08:56:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=361
03/02/2022 08:56:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=363
03/02/2022 08:56:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=365
03/02/2022 08:56:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=367
03/02/2022 08:56:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=369
03/02/2022 08:56:52 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.30629659832246037 on epoch=369
03/02/2022 08:56:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=371
03/02/2022 08:56:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=373
03/02/2022 08:56:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=375
03/02/2022 08:57:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=377
03/02/2022 08:57:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=379
03/02/2022 08:57:05 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.27667250007675537 on epoch=379
03/02/2022 08:57:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=381
03/02/2022 08:57:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=383
03/02/2022 08:57:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=385
03/02/2022 08:57:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=387
03/02/2022 08:57:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=389
03/02/2022 08:57:19 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.24393500643500643 on epoch=389
03/02/2022 08:57:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=391
03/02/2022 08:57:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=393
03/02/2022 08:57:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=395
03/02/2022 08:57:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=397
03/02/2022 08:57:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=399
03/02/2022 08:57:33 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.22840585801112115 on epoch=399
03/02/2022 08:57:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=401
03/02/2022 08:57:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=403
03/02/2022 08:57:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=405
03/02/2022 08:57:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=407
03/02/2022 08:57:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=409
03/02/2022 08:57:46 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.320209614946457 on epoch=409
03/02/2022 08:57:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=411
03/02/2022 08:57:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=413
03/02/2022 08:57:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=415
03/02/2022 08:57:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=417
03/02/2022 08:57:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=419
03/02/2022 08:58:00 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.3164593345881378 on epoch=419
03/02/2022 08:58:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=421
03/02/2022 08:58:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=423
03/02/2022 08:58:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=425
03/02/2022 08:58:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=427
03/02/2022 08:58:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=429
03/02/2022 08:58:14 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.3995452616142271 on epoch=429
03/02/2022 08:58:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=431
03/02/2022 08:58:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=433
03/02/2022 08:58:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 08:58:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=437
03/02/2022 08:58:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=439
03/02/2022 08:58:27 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.4516042780748663 on epoch=439
03/02/2022 08:58:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=441
03/02/2022 08:58:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=443
03/02/2022 08:58:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=445
03/02/2022 08:58:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=447
03/02/2022 08:58:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=449
03/02/2022 08:58:41 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.34804292929292935 on epoch=449
03/02/2022 08:58:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=451
03/02/2022 08:58:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=453
03/02/2022 08:58:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=455
03/02/2022 08:58:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=457
03/02/2022 08:58:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=459
03/02/2022 08:58:54 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.39050385704521046 on epoch=459
03/02/2022 08:58:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=461
03/02/2022 08:58:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=463
03/02/2022 08:59:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=465
03/02/2022 08:59:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=467
03/02/2022 08:59:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=469
03/02/2022 08:59:08 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.45464289839289834 on epoch=469
03/02/2022 08:59:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=471
03/02/2022 08:59:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=473
03/02/2022 08:59:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=475
03/02/2022 08:59:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=477
03/02/2022 08:59:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=479
03/02/2022 08:59:21 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.3408640612587981 on epoch=479
03/02/2022 08:59:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=481
03/02/2022 08:59:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=483
03/02/2022 08:59:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=485
03/02/2022 08:59:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=487
03/02/2022 08:59:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=489
03/02/2022 08:59:35 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.2957190957190957 on epoch=489
03/02/2022 08:59:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=491
03/02/2022 08:59:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=493
03/02/2022 08:59:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=495
03/02/2022 08:59:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=497
03/02/2022 08:59:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=499
03/02/2022 08:59:48 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.4003088077336198 on epoch=499
03/02/2022 08:59:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=501
03/02/2022 08:59:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=503
03/02/2022 08:59:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=505
03/02/2022 08:59:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=507
03/02/2022 09:00:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=509
03/02/2022 09:00:02 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.22466170270637834 on epoch=509
03/02/2022 09:00:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=511
03/02/2022 09:00:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=513
03/02/2022 09:00:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 09:00:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=517
03/02/2022 09:00:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=519
03/02/2022 09:00:15 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.47866900344367136 on epoch=519
03/02/2022 09:00:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=521
03/02/2022 09:00:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=523
03/02/2022 09:00:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=525
03/02/2022 09:00:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=527
03/02/2022 09:00:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=529
03/02/2022 09:00:29 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.357732858192417 on epoch=529
03/02/2022 09:00:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 09:00:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=533
03/02/2022 09:00:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=535
03/02/2022 09:00:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=537
03/02/2022 09:00:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=539
03/02/2022 09:00:43 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.2746709740244223 on epoch=539
03/02/2022 09:00:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=541
03/02/2022 09:00:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=543
03/02/2022 09:00:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=545
03/02/2022 09:00:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=547
03/02/2022 09:00:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 09:00:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.33760492286751365 on epoch=549
03/02/2022 09:00:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 09:01:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 09:01:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=555
03/02/2022 09:01:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=557
03/02/2022 09:01:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=559
03/02/2022 09:01:10 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2413230648524766 on epoch=559
03/02/2022 09:01:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=561
03/02/2022 09:01:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 09:01:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=565
03/02/2022 09:01:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=567
03/02/2022 09:01:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=569
03/02/2022 09:01:23 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3069662294553495 on epoch=569
03/02/2022 09:01:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=571
03/02/2022 09:01:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=573
03/02/2022 09:01:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 09:01:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=577
03/02/2022 09:01:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=579
03/02/2022 09:01:37 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.281994114559904 on epoch=579
03/02/2022 09:01:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 09:01:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=583
03/02/2022 09:01:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 09:01:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 09:01:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=589
03/02/2022 09:01:51 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2895808633530932 on epoch=589
03/02/2022 09:01:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 09:01:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 09:01:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=595
03/02/2022 09:02:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=597
03/02/2022 09:02:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 09:02:04 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:02:04 - INFO - __main__ - Printing 3 examples
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 09:02:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:02:04 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:02:04 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:02:04 - INFO - __main__ - Printing 3 examples
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 09:02:04 - INFO - __main__ - ['Yes']
03/02/2022 09:02:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:02:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:02:04 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:02:05 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.47691755083996457 on epoch=599
03/02/2022 09:02:05 - INFO - __main__ - save last model!
03/02/2022 09:02:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 09:02:05 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 09:02:05 - INFO - __main__ - Printing 3 examples
03/02/2022 09:02:05 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 09:02:05 - INFO - __main__ - ['No']
03/02/2022 09:02:05 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 09:02:05 - INFO - __main__ - ['Yes']
03/02/2022 09:02:05 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 09:02:05 - INFO - __main__ - ['Yes']
03/02/2022 09:02:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:02:08 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:02:14 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 09:02:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:02:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:02:19 - INFO - __main__ - Starting training!
03/02/2022 09:05:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_42_0.4_8_predictions.txt
03/02/2022 09:05:45 - INFO - __main__ - Classification-F1 on test data: 0.1820
03/02/2022 09:05:45 - INFO - __main__ - prefix=circa_16_42, lr=0.4, bsz=8, dev_performance=0.5246228786926461, test_performance=0.18203842109748733
03/02/2022 09:05:45 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.3, bsz=8 ...
03/02/2022 09:05:46 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:05:46 - INFO - __main__ - Printing 3 examples
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:05:46 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:05:46 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:05:46 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:05:46 - INFO - __main__ - Printing 3 examples
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 09:05:46 - INFO - __main__ - ['Yes']
03/02/2022 09:05:46 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:05:46 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:05:46 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:06:00 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:06:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:06:01 - INFO - __main__ - Starting training!
03/02/2022 09:06:04 - INFO - __main__ - Step 10 Global step 10 Train loss 3.72 on epoch=1
03/02/2022 09:06:06 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=3
03/02/2022 09:06:08 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=5
03/02/2022 09:06:11 - INFO - __main__ - Step 40 Global step 40 Train loss 1.75 on epoch=7
03/02/2022 09:06:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.44 on epoch=9
03/02/2022 09:06:15 - INFO - __main__ - Global step 50 Train loss 2.42 Classification-F1 0.09089783281733746 on epoch=9
03/02/2022 09:06:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09089783281733746 on epoch=9, global_step=50
03/02/2022 09:06:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=11
03/02/2022 09:06:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=13
03/02/2022 09:06:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=15
03/02/2022 09:06:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.72 on epoch=17
03/02/2022 09:06:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.71 on epoch=19
03/02/2022 09:06:28 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.056140350877192984 on epoch=19
03/02/2022 09:06:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=21
03/02/2022 09:06:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=23
03/02/2022 09:06:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=25
03/02/2022 09:06:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.61 on epoch=27
03/02/2022 09:06:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=29
03/02/2022 09:06:40 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 09:06:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.64 on epoch=31
03/02/2022 09:06:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=33
03/02/2022 09:06:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=35
03/02/2022 09:06:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=37
03/02/2022 09:06:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=39
03/02/2022 09:06:54 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.05128205128205127 on epoch=39
03/02/2022 09:06:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=41
03/02/2022 09:06:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=43
03/02/2022 09:07:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=45
03/02/2022 09:07:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=47
03/02/2022 09:07:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=49
03/02/2022 09:07:07 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.056140350877192984 on epoch=49
03/02/2022 09:07:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=51
03/02/2022 09:07:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=53
03/02/2022 09:07:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=55
03/02/2022 09:07:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=57
03/02/2022 09:07:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=59
03/02/2022 09:07:21 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.1753460730934842 on epoch=59
03/02/2022 09:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.09089783281733746 -> 0.1753460730934842 on epoch=59, global_step=300
03/02/2022 09:07:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=61
03/02/2022 09:07:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=63
03/02/2022 09:07:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=65
03/02/2022 09:07:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=67
03/02/2022 09:07:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 09:07:34 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.1802020202020202 on epoch=69
03/02/2022 09:07:34 - INFO - __main__ - Saving model with best Classification-F1: 0.1753460730934842 -> 0.1802020202020202 on epoch=69, global_step=350
03/02/2022 09:07:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=71
03/02/2022 09:07:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=73
03/02/2022 09:07:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=75
03/02/2022 09:07:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=77
03/02/2022 09:07:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=79
03/02/2022 09:07:47 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.06292134831460675 on epoch=79
03/02/2022 09:07:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=81
03/02/2022 09:07:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=83
03/02/2022 09:07:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=85
03/02/2022 09:07:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=87
03/02/2022 09:07:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=89
03/02/2022 09:08:00 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.20156321839080463 on epoch=89
03/02/2022 09:08:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1802020202020202 -> 0.20156321839080463 on epoch=89, global_step=450
03/02/2022 09:08:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=91
03/02/2022 09:08:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=93
03/02/2022 09:08:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=95
03/02/2022 09:08:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 09:08:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=99
03/02/2022 09:08:13 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.15803600654664485 on epoch=99
03/02/2022 09:08:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=101
03/02/2022 09:08:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=103
03/02/2022 09:08:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=105
03/02/2022 09:08:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=107
03/02/2022 09:08:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=109
03/02/2022 09:08:26 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.26644388749651904 on epoch=109
03/02/2022 09:08:26 - INFO - __main__ - Saving model with best Classification-F1: 0.20156321839080463 -> 0.26644388749651904 on epoch=109, global_step=550
03/02/2022 09:08:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=111
03/02/2022 09:08:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=113
03/02/2022 09:08:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=115
03/02/2022 09:08:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=117
03/02/2022 09:08:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=119
03/02/2022 09:08:40 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.25999999999999995 on epoch=119
03/02/2022 09:08:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=121
03/02/2022 09:08:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=123
03/02/2022 09:08:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=125
03/02/2022 09:08:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=127
03/02/2022 09:08:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=129
03/02/2022 09:08:54 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.23214285714285712 on epoch=129
03/02/2022 09:08:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=131
03/02/2022 09:08:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=133
03/02/2022 09:09:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=135
03/02/2022 09:09:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=137
03/02/2022 09:09:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=139
03/02/2022 09:09:07 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.24438003220611915 on epoch=139
03/02/2022 09:09:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=141
03/02/2022 09:09:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=143
03/02/2022 09:09:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=145
03/02/2022 09:09:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=147
03/02/2022 09:09:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=149
03/02/2022 09:09:21 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.22943201376936315 on epoch=149
03/02/2022 09:09:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=151
03/02/2022 09:09:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=153
03/02/2022 09:09:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=155
03/02/2022 09:09:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=157
03/02/2022 09:09:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=159
03/02/2022 09:09:35 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.2725607327757865 on epoch=159
03/02/2022 09:09:35 - INFO - __main__ - Saving model with best Classification-F1: 0.26644388749651904 -> 0.2725607327757865 on epoch=159, global_step=800
03/02/2022 09:09:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=161
03/02/2022 09:09:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=163
03/02/2022 09:09:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=165
03/02/2022 09:09:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=167
03/02/2022 09:09:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=169
03/02/2022 09:09:49 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.27999999999999997 on epoch=169
03/02/2022 09:09:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2725607327757865 -> 0.27999999999999997 on epoch=169, global_step=850
03/02/2022 09:09:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=171
03/02/2022 09:09:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=173
03/02/2022 09:09:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=175
03/02/2022 09:09:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=177
03/02/2022 09:10:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=179
03/02/2022 09:10:02 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.2735483870967742 on epoch=179
03/02/2022 09:10:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=181
03/02/2022 09:10:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=183
03/02/2022 09:10:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=185
03/02/2022 09:10:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=187
03/02/2022 09:10:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=189
03/02/2022 09:10:16 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.45094969253048467 on epoch=189
03/02/2022 09:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.27999999999999997 -> 0.45094969253048467 on epoch=189, global_step=950
03/02/2022 09:10:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=191
03/02/2022 09:10:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=193
03/02/2022 09:10:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=195
03/02/2022 09:10:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=197
03/02/2022 09:10:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=199
03/02/2022 09:10:30 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.34249956940035925 on epoch=199
03/02/2022 09:10:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=201
03/02/2022 09:10:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=203
03/02/2022 09:10:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=205
03/02/2022 09:10:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=207
03/02/2022 09:10:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=209
03/02/2022 09:10:44 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.3610140215403373 on epoch=209
03/02/2022 09:10:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=211
03/02/2022 09:10:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=213
03/02/2022 09:10:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=215
03/02/2022 09:10:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.26 on epoch=217
03/02/2022 09:10:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=219
03/02/2022 09:10:58 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.3425106609808103 on epoch=219
03/02/2022 09:11:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=221
03/02/2022 09:11:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=223
03/02/2022 09:11:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=225
03/02/2022 09:11:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=227
03/02/2022 09:11:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=229
03/02/2022 09:11:11 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.32683463796477497 on epoch=229
03/02/2022 09:11:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=231
03/02/2022 09:11:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=233
03/02/2022 09:11:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=235
03/02/2022 09:11:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=237
03/02/2022 09:11:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=239
03/02/2022 09:11:25 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.3770927601809955 on epoch=239
03/02/2022 09:11:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=241
03/02/2022 09:11:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=243
03/02/2022 09:11:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=245
03/02/2022 09:11:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=247
03/02/2022 09:11:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=249
03/02/2022 09:11:39 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.36474867724867727 on epoch=249
03/02/2022 09:11:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=251
03/02/2022 09:11:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=253
03/02/2022 09:11:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=255
03/02/2022 09:11:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=257
03/02/2022 09:11:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=259
03/02/2022 09:11:52 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.3953396029258099 on epoch=259
03/02/2022 09:11:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=261
03/02/2022 09:11:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=263
03/02/2022 09:11:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=265
03/02/2022 09:12:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=267
03/02/2022 09:12:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=269
03/02/2022 09:12:05 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.37176197458455523 on epoch=269
03/02/2022 09:12:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=271
03/02/2022 09:12:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=273
03/02/2022 09:12:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=275
03/02/2022 09:12:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=277
03/02/2022 09:12:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=279
03/02/2022 09:12:19 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.3880254154447703 on epoch=279
03/02/2022 09:12:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=281
03/02/2022 09:12:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=283
03/02/2022 09:12:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=285
03/02/2022 09:12:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=287
03/02/2022 09:12:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=289
03/02/2022 09:12:32 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.39137096774193547 on epoch=289
03/02/2022 09:12:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=291
03/02/2022 09:12:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=293
03/02/2022 09:12:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=295
03/02/2022 09:12:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=297
03/02/2022 09:12:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=299
03/02/2022 09:12:46 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.3506168831168831 on epoch=299
03/02/2022 09:12:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=301
03/02/2022 09:12:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=303
03/02/2022 09:12:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=305
03/02/2022 09:12:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=307
03/02/2022 09:12:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=309
03/02/2022 09:12:59 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.36604406130268197 on epoch=309
03/02/2022 09:13:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=311
03/02/2022 09:13:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=313
03/02/2022 09:13:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=315
03/02/2022 09:13:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=317
03/02/2022 09:13:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=319
03/02/2022 09:13:12 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.37723942723942727 on epoch=319
03/02/2022 09:13:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=321
03/02/2022 09:13:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=323
03/02/2022 09:13:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=325
03/02/2022 09:13:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=327
03/02/2022 09:13:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=329
03/02/2022 09:13:25 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.4067971750196179 on epoch=329
03/02/2022 09:13:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=331
03/02/2022 09:13:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=333
03/02/2022 09:13:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=335
03/02/2022 09:13:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=337
03/02/2022 09:13:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=339
03/02/2022 09:13:39 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.3511586452762924 on epoch=339
03/02/2022 09:13:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=341
03/02/2022 09:13:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=343
03/02/2022 09:13:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=345
03/02/2022 09:13:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=347
03/02/2022 09:13:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=349
03/02/2022 09:13:52 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.33960592784122196 on epoch=349
03/02/2022 09:13:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=351
03/02/2022 09:13:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=353
03/02/2022 09:13:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=355
03/02/2022 09:14:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=357
03/02/2022 09:14:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=359
03/02/2022 09:14:05 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.39719355719355715 on epoch=359
03/02/2022 09:14:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=361
03/02/2022 09:14:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=363
03/02/2022 09:14:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=365
03/02/2022 09:14:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=367
03/02/2022 09:14:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=369
03/02/2022 09:14:19 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.34134199134199134 on epoch=369
03/02/2022 09:14:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=371
03/02/2022 09:14:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=373
03/02/2022 09:14:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=375
03/02/2022 09:14:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=377
03/02/2022 09:14:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=379
03/02/2022 09:14:33 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.31784147869674184 on epoch=379
03/02/2022 09:14:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=381
03/02/2022 09:14:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=383
03/02/2022 09:14:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=385
03/02/2022 09:14:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=387
03/02/2022 09:14:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=389
03/02/2022 09:14:46 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.41157331832254435 on epoch=389
03/02/2022 09:14:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=391
03/02/2022 09:14:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=393
03/02/2022 09:14:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=395
03/02/2022 09:14:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=397
03/02/2022 09:14:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=399
03/02/2022 09:14:59 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.3757626389205337 on epoch=399
03/02/2022 09:15:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=401
03/02/2022 09:15:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=403
03/02/2022 09:15:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=405
03/02/2022 09:15:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=407
03/02/2022 09:15:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=409
03/02/2022 09:15:12 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.3404917522564581 on epoch=409
03/02/2022 09:15:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=411
03/02/2022 09:15:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=413
03/02/2022 09:15:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=415
03/02/2022 09:15:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=417
03/02/2022 09:15:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=419
03/02/2022 09:15:26 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.39096530920060335 on epoch=419
03/02/2022 09:15:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=421
03/02/2022 09:15:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=423
03/02/2022 09:15:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=425
03/02/2022 09:15:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=427
03/02/2022 09:15:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=429
03/02/2022 09:15:40 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.3700480720829558 on epoch=429
03/02/2022 09:15:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=431
03/02/2022 09:15:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=433
03/02/2022 09:15:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=435
03/02/2022 09:15:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=437
03/02/2022 09:15:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=439
03/02/2022 09:15:53 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.3409261534261534 on epoch=439
03/02/2022 09:15:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=441
03/02/2022 09:15:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=443
03/02/2022 09:16:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=445
03/02/2022 09:16:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=447
03/02/2022 09:16:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=449
03/02/2022 09:16:07 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.32514054572878104 on epoch=449
03/02/2022 09:16:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=451
03/02/2022 09:16:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=453
03/02/2022 09:16:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=455
03/02/2022 09:16:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=457
03/02/2022 09:16:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=459
03/02/2022 09:16:21 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.26550709346408274 on epoch=459
03/02/2022 09:16:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=461
03/02/2022 09:16:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=463
03/02/2022 09:16:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=465
03/02/2022 09:16:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=467
03/02/2022 09:16:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=469
03/02/2022 09:16:34 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.2979315769369949 on epoch=469
03/02/2022 09:16:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=471
03/02/2022 09:16:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=473
03/02/2022 09:16:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=475
03/02/2022 09:16:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=477
03/02/2022 09:16:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=479
03/02/2022 09:16:48 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.3018170162151279 on epoch=479
03/02/2022 09:16:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=481
03/02/2022 09:16:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=483
03/02/2022 09:16:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=485
03/02/2022 09:16:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=487
03/02/2022 09:16:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=489
03/02/2022 09:17:02 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.4257099015719706 on epoch=489
03/02/2022 09:17:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=491
03/02/2022 09:17:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=493
03/02/2022 09:17:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=495
03/02/2022 09:17:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=497
03/02/2022 09:17:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=499
03/02/2022 09:17:16 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.4168363849036118 on epoch=499
03/02/2022 09:17:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=501
03/02/2022 09:17:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=503
03/02/2022 09:17:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=505
03/02/2022 09:17:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=507
03/02/2022 09:17:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=509
03/02/2022 09:17:30 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.33060273676740565 on epoch=509
03/02/2022 09:17:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=511
03/02/2022 09:17:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=513
03/02/2022 09:17:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=515
03/02/2022 09:17:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=517
03/02/2022 09:17:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=519
03/02/2022 09:17:44 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.3875768489213867 on epoch=519
03/02/2022 09:17:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=521
03/02/2022 09:17:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=523
03/02/2022 09:17:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=525
03/02/2022 09:17:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=527
03/02/2022 09:17:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=529
03/02/2022 09:17:57 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.345546673126615 on epoch=529
03/02/2022 09:18:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=531
03/02/2022 09:18:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=533
03/02/2022 09:18:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=535
03/02/2022 09:18:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=537
03/02/2022 09:18:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=539
03/02/2022 09:18:11 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.2984978145349662 on epoch=539
03/02/2022 09:18:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=541
03/02/2022 09:18:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=543
03/02/2022 09:18:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=545
03/02/2022 09:18:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=547
03/02/2022 09:18:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 09:18:25 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.3902056277056277 on epoch=549
03/02/2022 09:18:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=551
03/02/2022 09:18:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 09:18:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=555
03/02/2022 09:18:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=557
03/02/2022 09:18:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=559
03/02/2022 09:18:39 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.48061678477747344 on epoch=559
03/02/2022 09:18:39 - INFO - __main__ - Saving model with best Classification-F1: 0.45094969253048467 -> 0.48061678477747344 on epoch=559, global_step=2800
03/02/2022 09:18:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=561
03/02/2022 09:18:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=563
03/02/2022 09:18:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=565
03/02/2022 09:18:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 09:18:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=569
03/02/2022 09:18:53 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.4770514312374529 on epoch=569
03/02/2022 09:18:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=571
03/02/2022 09:18:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=573
03/02/2022 09:19:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=575
03/02/2022 09:19:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=577
03/02/2022 09:19:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=579
03/02/2022 09:19:07 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6159307359307359 on epoch=579
03/02/2022 09:19:07 - INFO - __main__ - Saving model with best Classification-F1: 0.48061678477747344 -> 0.6159307359307359 on epoch=579, global_step=2900
03/02/2022 09:19:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=581
03/02/2022 09:19:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=583
03/02/2022 09:19:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 09:19:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=587
03/02/2022 09:19:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=589
03/02/2022 09:19:21 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.503459595959596 on epoch=589
03/02/2022 09:19:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 09:19:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=593
03/02/2022 09:19:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=595
03/02/2022 09:19:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 09:19:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 09:19:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:19:33 - INFO - __main__ - Printing 3 examples
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 09:19:33 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:19:33 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:19:33 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:19:33 - INFO - __main__ - Printing 3 examples
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 09:19:33 - INFO - __main__ - ['Yes']
03/02/2022 09:19:33 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:19:33 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:19:33 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:19:35 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.4158290805851781 on epoch=599
03/02/2022 09:19:35 - INFO - __main__ - save last model!
03/02/2022 09:19:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 09:19:35 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 09:19:35 - INFO - __main__ - Printing 3 examples
03/02/2022 09:19:35 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 09:19:35 - INFO - __main__ - ['No']
03/02/2022 09:19:35 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 09:19:35 - INFO - __main__ - ['Yes']
03/02/2022 09:19:35 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 09:19:35 - INFO - __main__ - ['Yes']
03/02/2022 09:19:35 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:19:38 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:19:44 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 09:19:47 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:19:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:19:48 - INFO - __main__ - Starting training!
03/02/2022 09:23:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_42_0.3_8_predictions.txt
03/02/2022 09:23:55 - INFO - __main__ - Classification-F1 on test data: 0.0773
03/02/2022 09:23:56 - INFO - __main__ - prefix=circa_16_42, lr=0.3, bsz=8, dev_performance=0.6159307359307359, test_performance=0.07731837053896559
03/02/2022 09:23:56 - INFO - __main__ - Running ... prefix=circa_16_42, lr=0.2, bsz=8 ...
03/02/2022 09:23:57 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:23:57 - INFO - __main__ - Printing 3 examples
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you watching any new TV shows? [SEP] answer Y: I'm really enjoying Portlandia right now.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Did you have a good flight? [SEP] answer Y: The plane was pretty empty, so it was great.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Are you in the mood for ramen? [SEP] answer Y: I'm always in the mood for it.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:23:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:23:57 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:23:57 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:23:57 - INFO - __main__ - Printing 3 examples
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Have you been to the 360 amphitheater? [SEP] answer Y: Several times, actually.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Would you like to go see live music? [SEP] answer Y: A concert sounds perfect right now.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ -  [circa] context: Y has just travelled from a different city to meet X. [SEP] question X: Are you trying to keep to a budget? [SEP] answer Y: I can spend $100 a day.
03/02/2022 09:23:57 - INFO - __main__ - ['Yes']
03/02/2022 09:23:57 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:23:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:23:57 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:24:11 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:24:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:24:12 - INFO - __main__ - Starting training!
03/02/2022 09:24:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.88 on epoch=1
03/02/2022 09:24:17 - INFO - __main__ - Step 20 Global step 20 Train loss 3.23 on epoch=3
03/02/2022 09:24:19 - INFO - __main__ - Step 30 Global step 30 Train loss 2.70 on epoch=5
03/02/2022 09:24:21 - INFO - __main__ - Step 40 Global step 40 Train loss 2.23 on epoch=7
03/02/2022 09:24:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.86 on epoch=9
03/02/2022 09:24:25 - INFO - __main__ - Global step 50 Train loss 2.78 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 09:24:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 09:24:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.48 on epoch=11
03/02/2022 09:24:29 - INFO - __main__ - Step 70 Global step 70 Train loss 1.33 on epoch=13
03/02/2022 09:24:31 - INFO - __main__ - Step 80 Global step 80 Train loss 1.19 on epoch=15
03/02/2022 09:24:34 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=17
03/02/2022 09:24:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=19
03/02/2022 09:24:38 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.06073153899240857 on epoch=19
03/02/2022 09:24:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=21
03/02/2022 09:24:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=23
03/02/2022 09:24:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=25
03/02/2022 09:24:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.72 on epoch=27
03/02/2022 09:24:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.71 on epoch=29
03/02/2022 09:24:52 - INFO - __main__ - Global step 150 Train loss 0.78 Classification-F1 0.05319148936170213 on epoch=29
03/02/2022 09:24:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=31
03/02/2022 09:24:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=33
03/02/2022 09:24:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=35
03/02/2022 09:25:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=37
03/02/2022 09:25:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=39
03/02/2022 09:25:05 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.09555555555555556 on epoch=39
03/02/2022 09:25:05 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.09555555555555556 on epoch=39, global_step=200
03/02/2022 09:25:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=41
03/02/2022 09:25:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=43
03/02/2022 09:25:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=45
03/02/2022 09:25:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=47
03/02/2022 09:25:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=49
03/02/2022 09:25:18 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 09:25:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=51
03/02/2022 09:25:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=53
03/02/2022 09:25:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=55
03/02/2022 09:25:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=57
03/02/2022 09:25:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=59
03/02/2022 09:25:32 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.14664484451718493 on epoch=59
03/02/2022 09:25:32 - INFO - __main__ - Saving model with best Classification-F1: 0.09555555555555556 -> 0.14664484451718493 on epoch=59, global_step=300
03/02/2022 09:25:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=61
03/02/2022 09:25:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=63
03/02/2022 09:25:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=65
03/02/2022 09:25:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=67
03/02/2022 09:25:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=69
03/02/2022 09:25:45 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.13859020310633213 on epoch=69
03/02/2022 09:25:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=71
03/02/2022 09:25:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=73
03/02/2022 09:25:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=75
03/02/2022 09:25:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=77
03/02/2022 09:25:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=79
03/02/2022 09:25:59 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.1202797202797203 on epoch=79
03/02/2022 09:26:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=81
03/02/2022 09:26:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=83
03/02/2022 09:26:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=85
03/02/2022 09:26:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=87
03/02/2022 09:26:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=89
03/02/2022 09:26:13 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.09921063564603241 on epoch=89
03/02/2022 09:26:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=91
03/02/2022 09:26:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=93
03/02/2022 09:26:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=95
03/02/2022 09:26:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=97
03/02/2022 09:26:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=99
03/02/2022 09:26:27 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.09365853658536585 on epoch=99
03/02/2022 09:26:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=101
03/02/2022 09:26:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=103
03/02/2022 09:26:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=105
03/02/2022 09:26:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=107
03/02/2022 09:26:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=109
03/02/2022 09:26:40 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.13419819819819817 on epoch=109
03/02/2022 09:26:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=111
03/02/2022 09:26:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=113
03/02/2022 09:26:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=115
03/02/2022 09:26:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=117
03/02/2022 09:26:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=119
03/02/2022 09:26:53 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.11945945945945946 on epoch=119
03/02/2022 09:26:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=121
03/02/2022 09:26:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=123
03/02/2022 09:27:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=125
03/02/2022 09:27:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=127
03/02/2022 09:27:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=129
03/02/2022 09:27:07 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.23340819581421082 on epoch=129
03/02/2022 09:27:07 - INFO - __main__ - Saving model with best Classification-F1: 0.14664484451718493 -> 0.23340819581421082 on epoch=129, global_step=650
03/02/2022 09:27:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=131
03/02/2022 09:27:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=133
03/02/2022 09:27:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=135
03/02/2022 09:27:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=137
03/02/2022 09:27:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=139
03/02/2022 09:27:21 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.2526963562753036 on epoch=139
03/02/2022 09:27:21 - INFO - __main__ - Saving model with best Classification-F1: 0.23340819581421082 -> 0.2526963562753036 on epoch=139, global_step=700
03/02/2022 09:27:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=141
03/02/2022 09:27:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=143
03/02/2022 09:27:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=145
03/02/2022 09:27:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=147
03/02/2022 09:27:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=149
03/02/2022 09:27:34 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16281920326864147 on epoch=149
03/02/2022 09:27:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=151
03/02/2022 09:27:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=153
03/02/2022 09:27:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=155
03/02/2022 09:27:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=157
03/02/2022 09:27:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=159
03/02/2022 09:27:48 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.2553846153846154 on epoch=159
03/02/2022 09:27:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2526963562753036 -> 0.2553846153846154 on epoch=159, global_step=800
03/02/2022 09:27:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=161
03/02/2022 09:27:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=163
03/02/2022 09:27:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=165
03/02/2022 09:27:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=167
03/02/2022 09:27:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=169
03/02/2022 09:28:02 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.23774725274725275 on epoch=169
03/02/2022 09:28:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=171
03/02/2022 09:28:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=173
03/02/2022 09:28:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=175
03/02/2022 09:28:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=177
03/02/2022 09:28:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=179
03/02/2022 09:28:16 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.28896314496314496 on epoch=179
03/02/2022 09:28:16 - INFO - __main__ - Saving model with best Classification-F1: 0.2553846153846154 -> 0.28896314496314496 on epoch=179, global_step=900
03/02/2022 09:28:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=181
03/02/2022 09:28:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=183
03/02/2022 09:28:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=185
03/02/2022 09:28:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=187
03/02/2022 09:28:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=189
03/02/2022 09:28:29 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.33560117302052783 on epoch=189
03/02/2022 09:28:29 - INFO - __main__ - Saving model with best Classification-F1: 0.28896314496314496 -> 0.33560117302052783 on epoch=189, global_step=950
03/02/2022 09:28:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=191
03/02/2022 09:28:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=193
03/02/2022 09:28:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=195
03/02/2022 09:28:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=197
03/02/2022 09:28:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=199
03/02/2022 09:28:43 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.309628024557602 on epoch=199
03/02/2022 09:28:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=201
03/02/2022 09:28:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=203
03/02/2022 09:28:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=205
03/02/2022 09:28:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=207
03/02/2022 09:28:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=209
03/02/2022 09:28:57 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.30050965250965256 on epoch=209
03/02/2022 09:28:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=211
03/02/2022 09:29:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=213
03/02/2022 09:29:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=215
03/02/2022 09:29:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=217
03/02/2022 09:29:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=219
03/02/2022 09:29:11 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.31587878787878787 on epoch=219
03/02/2022 09:29:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=221
03/02/2022 09:29:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=223
03/02/2022 09:29:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=225
03/02/2022 09:29:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=227
03/02/2022 09:29:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=229
03/02/2022 09:29:24 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.3699089253187614 on epoch=229
03/02/2022 09:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.33560117302052783 -> 0.3699089253187614 on epoch=229, global_step=1150
03/02/2022 09:29:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=231
03/02/2022 09:29:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=233
03/02/2022 09:29:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=235
03/02/2022 09:29:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=237
03/02/2022 09:29:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=239
03/02/2022 09:29:38 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.26753246753246757 on epoch=239
03/02/2022 09:29:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=241
03/02/2022 09:29:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=243
03/02/2022 09:29:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=245
03/02/2022 09:29:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=247
03/02/2022 09:29:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=249
03/02/2022 09:29:52 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.3364705882352941 on epoch=249
03/02/2022 09:29:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=251
03/02/2022 09:29:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=253
03/02/2022 09:29:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=255
03/02/2022 09:30:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=257
03/02/2022 09:30:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=259
03/02/2022 09:30:06 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.34626670626670625 on epoch=259
03/02/2022 09:30:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=261
03/02/2022 09:30:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=263
03/02/2022 09:30:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=265
03/02/2022 09:30:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=267
03/02/2022 09:30:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=269
03/02/2022 09:30:19 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.3526726942857571 on epoch=269
03/02/2022 09:30:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=271
03/02/2022 09:30:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=273
03/02/2022 09:30:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=275
03/02/2022 09:30:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=277
03/02/2022 09:30:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=279
03/02/2022 09:30:33 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.31650623885918006 on epoch=279
03/02/2022 09:30:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=281
03/02/2022 09:30:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=283
03/02/2022 09:30:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=285
03/02/2022 09:30:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=287
03/02/2022 09:30:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=289
03/02/2022 09:30:47 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.3736794700527469 on epoch=289
03/02/2022 09:30:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3699089253187614 -> 0.3736794700527469 on epoch=289, global_step=1450
03/02/2022 09:30:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=291
03/02/2022 09:30:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=293
03/02/2022 09:30:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=295
03/02/2022 09:30:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=297
03/02/2022 09:30:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=299
03/02/2022 09:31:01 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.34993167010964565 on epoch=299
03/02/2022 09:31:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=301
03/02/2022 09:31:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=303
03/02/2022 09:31:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=305
03/02/2022 09:31:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=307
03/02/2022 09:31:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=309
03/02/2022 09:31:15 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.3809066227889758 on epoch=309
03/02/2022 09:31:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3736794700527469 -> 0.3809066227889758 on epoch=309, global_step=1550
03/02/2022 09:31:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=311
03/02/2022 09:31:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=313
03/02/2022 09:31:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=315
03/02/2022 09:31:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=317
03/02/2022 09:31:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=319
03/02/2022 09:31:29 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.3374625712353907 on epoch=319
03/02/2022 09:31:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=321
03/02/2022 09:31:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=323
03/02/2022 09:31:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=325
03/02/2022 09:31:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=327
03/02/2022 09:31:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=329
03/02/2022 09:31:43 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.35648926237161527 on epoch=329
03/02/2022 09:31:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=331
03/02/2022 09:31:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=333
03/02/2022 09:31:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=335
03/02/2022 09:31:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=337
03/02/2022 09:31:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=339
03/02/2022 09:31:57 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.43617810760667897 on epoch=339
03/02/2022 09:31:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3809066227889758 -> 0.43617810760667897 on epoch=339, global_step=1700
03/02/2022 09:31:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=341
03/02/2022 09:32:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=343
03/02/2022 09:32:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=345
03/02/2022 09:32:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=347
03/02/2022 09:32:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=349
03/02/2022 09:32:11 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.3993464052287582 on epoch=349
03/02/2022 09:32:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=351
03/02/2022 09:32:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=353
03/02/2022 09:32:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=355
03/02/2022 09:32:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=357
03/02/2022 09:32:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=359
03/02/2022 09:32:24 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.38159663865546223 on epoch=359
03/02/2022 09:32:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=361
03/02/2022 09:32:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=363
03/02/2022 09:32:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=365
03/02/2022 09:32:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=367
03/02/2022 09:32:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=369
03/02/2022 09:32:38 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.4246699705523235 on epoch=369
03/02/2022 09:32:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=371
03/02/2022 09:32:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=373
03/02/2022 09:32:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=375
03/02/2022 09:32:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=377
03/02/2022 09:32:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=379
03/02/2022 09:32:52 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.3754497354497355 on epoch=379
03/02/2022 09:32:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=381
03/02/2022 09:32:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=383
03/02/2022 09:32:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=385
03/02/2022 09:33:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=387
03/02/2022 09:33:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=389
03/02/2022 09:33:05 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.3807843137254902 on epoch=389
03/02/2022 09:33:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=391
03/02/2022 09:33:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=393
03/02/2022 09:33:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=395
03/02/2022 09:33:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=397
03/02/2022 09:33:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=399
03/02/2022 09:33:19 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.4178904665314402 on epoch=399
03/02/2022 09:33:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=401
03/02/2022 09:33:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=403
03/02/2022 09:33:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=405
03/02/2022 09:33:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=407
03/02/2022 09:33:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=409
03/02/2022 09:33:33 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.442235294117647 on epoch=409
03/02/2022 09:33:33 - INFO - __main__ - Saving model with best Classification-F1: 0.43617810760667897 -> 0.442235294117647 on epoch=409, global_step=2050
03/02/2022 09:33:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=411
03/02/2022 09:33:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=413
03/02/2022 09:33:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=415
03/02/2022 09:33:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=417
03/02/2022 09:33:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=419
03/02/2022 09:33:47 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.3974235189169334 on epoch=419
03/02/2022 09:33:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=421
03/02/2022 09:33:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=423
03/02/2022 09:33:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=425
03/02/2022 09:33:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=427
03/02/2022 09:33:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=429
03/02/2022 09:34:01 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.3676646859429878 on epoch=429
03/02/2022 09:34:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=431
03/02/2022 09:34:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.15 on epoch=433
03/02/2022 09:34:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=435
03/02/2022 09:34:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=437
03/02/2022 09:34:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=439
03/02/2022 09:34:14 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.4037908496732026 on epoch=439
03/02/2022 09:34:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=441
03/02/2022 09:34:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=443
03/02/2022 09:34:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=445
03/02/2022 09:34:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=447
03/02/2022 09:34:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=449
03/02/2022 09:34:28 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.4442432385069221 on epoch=449
03/02/2022 09:34:28 - INFO - __main__ - Saving model with best Classification-F1: 0.442235294117647 -> 0.4442432385069221 on epoch=449, global_step=2250
03/02/2022 09:34:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=451
03/02/2022 09:34:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=453
03/02/2022 09:34:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=455
03/02/2022 09:34:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=457
03/02/2022 09:34:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=459
03/02/2022 09:34:41 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.3718716577540107 on epoch=459
03/02/2022 09:34:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=461
03/02/2022 09:34:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=463
03/02/2022 09:34:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=465
03/02/2022 09:34:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=467
03/02/2022 09:34:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=469
03/02/2022 09:34:55 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.37946336429308564 on epoch=469
03/02/2022 09:34:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=471
03/02/2022 09:34:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=473
03/02/2022 09:35:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=475
03/02/2022 09:35:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=477
03/02/2022 09:35:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=479
03/02/2022 09:35:09 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.4076717640804328 on epoch=479
03/02/2022 09:35:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=481
03/02/2022 09:35:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=483
03/02/2022 09:35:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=485
03/02/2022 09:35:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=487
03/02/2022 09:35:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=489
03/02/2022 09:35:23 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.29257925392379175 on epoch=489
03/02/2022 09:35:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=491
03/02/2022 09:35:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=493
03/02/2022 09:35:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=495
03/02/2022 09:35:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=497
03/02/2022 09:35:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=499
03/02/2022 09:35:37 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.46314121608239256 on epoch=499
03/02/2022 09:35:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4442432385069221 -> 0.46314121608239256 on epoch=499, global_step=2500
03/02/2022 09:35:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=501
03/02/2022 09:35:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=503
03/02/2022 09:35:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=505
03/02/2022 09:35:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=507
03/02/2022 09:35:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=509
03/02/2022 09:35:51 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.4842352941176471 on epoch=509
03/02/2022 09:35:51 - INFO - __main__ - Saving model with best Classification-F1: 0.46314121608239256 -> 0.4842352941176471 on epoch=509, global_step=2550
03/02/2022 09:35:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=511
03/02/2022 09:35:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=513
03/02/2022 09:35:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=515
03/02/2022 09:36:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=517
03/02/2022 09:36:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=519
03/02/2022 09:36:05 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.4080274214949138 on epoch=519
03/02/2022 09:36:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=521
03/02/2022 09:36:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=523
03/02/2022 09:36:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=525
03/02/2022 09:36:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=527
03/02/2022 09:36:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=529
03/02/2022 09:36:19 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.36595496212588335 on epoch=529
03/02/2022 09:36:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=531
03/02/2022 09:36:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=533
03/02/2022 09:36:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=535
03/02/2022 09:36:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=537
03/02/2022 09:36:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=539
03/02/2022 09:36:33 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.4281627360172792 on epoch=539
03/02/2022 09:36:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=541
03/02/2022 09:36:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=543
03/02/2022 09:36:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=545
03/02/2022 09:36:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=547
03/02/2022 09:36:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=549
03/02/2022 09:36:47 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.43377624095890355 on epoch=549
03/02/2022 09:36:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=551
03/02/2022 09:36:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=553
03/02/2022 09:36:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=555
03/02/2022 09:36:56 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=557
03/02/2022 09:36:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=559
03/02/2022 09:37:00 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.37922803019512885 on epoch=559
03/02/2022 09:37:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=561
03/02/2022 09:37:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=563
03/02/2022 09:37:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=565
03/02/2022 09:37:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=567
03/02/2022 09:37:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=569
03/02/2022 09:37:14 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.3803499894581489 on epoch=569
03/02/2022 09:37:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=571
03/02/2022 09:37:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=573
03/02/2022 09:37:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=575
03/02/2022 09:37:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=577
03/02/2022 09:37:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=579
03/02/2022 09:37:27 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.4350161636303212 on epoch=579
03/02/2022 09:37:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=581
03/02/2022 09:37:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=583
03/02/2022 09:37:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=585
03/02/2022 09:37:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=587
03/02/2022 09:37:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=589
03/02/2022 09:37:41 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.33404077793323844 on epoch=589
03/02/2022 09:37:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=591
03/02/2022 09:37:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=593
03/02/2022 09:37:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=595
03/02/2022 09:37:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=597
03/02/2022 09:37:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=599
03/02/2022 09:37:54 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:37:54 - INFO - __main__ - Printing 3 examples
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 09:37:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:37:54 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:37:54 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:37:54 - INFO - __main__ - Printing 3 examples
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 09:37:54 - INFO - __main__ - ['No']
03/02/2022 09:37:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:37:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:37:54 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:37:55 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.31840324646605056 on epoch=599
03/02/2022 09:37:55 - INFO - __main__ - save last model!
03/02/2022 09:37:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 09:37:55 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 09:37:55 - INFO - __main__ - Printing 3 examples
03/02/2022 09:37:55 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 09:37:55 - INFO - __main__ - ['No']
03/02/2022 09:37:55 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 09:37:55 - INFO - __main__ - ['Yes']
03/02/2022 09:37:55 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 09:37:55 - INFO - __main__ - ['Yes']
03/02/2022 09:37:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:37:58 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:38:05 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 09:38:08 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:38:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:38:09 - INFO - __main__ - Starting training!
03/02/2022 09:42:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_42_0.2_8_predictions.txt
03/02/2022 09:42:00 - INFO - __main__ - Classification-F1 on test data: 0.1145
03/02/2022 09:42:00 - INFO - __main__ - prefix=circa_16_42, lr=0.2, bsz=8, dev_performance=0.4842352941176471, test_performance=0.11448219653603395
03/02/2022 09:42:00 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.5, bsz=8 ...
03/02/2022 09:42:01 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:42:01 - INFO - __main__ - Printing 3 examples
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:42:01 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:42:01 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:42:01 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:42:01 - INFO - __main__ - Printing 3 examples
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 09:42:01 - INFO - __main__ - ['No']
03/02/2022 09:42:01 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:42:01 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:42:01 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:42:15 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:42:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:42:16 - INFO - __main__ - Starting training!
03/02/2022 09:42:24 - INFO - __main__ - Step 10 Global step 10 Train loss 3.62 on epoch=1
03/02/2022 09:42:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.28 on epoch=3
03/02/2022 09:42:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=5
03/02/2022 09:42:30 - INFO - __main__ - Step 40 Global step 40 Train loss 1.22 on epoch=7
03/02/2022 09:42:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=9
03/02/2022 09:42:35 - INFO - __main__ - Global step 50 Train loss 1.90 Classification-F1 0.06451612903225808 on epoch=9
03/02/2022 09:42:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06451612903225808 on epoch=9, global_step=50
03/02/2022 09:42:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.74 on epoch=11
03/02/2022 09:42:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=13
03/02/2022 09:42:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=15
03/02/2022 09:42:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=17
03/02/2022 09:42:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=19
03/02/2022 09:42:49 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.0910394265232975 on epoch=19
03/02/2022 09:42:49 - INFO - __main__ - Saving model with best Classification-F1: 0.06451612903225808 -> 0.0910394265232975 on epoch=19, global_step=100
03/02/2022 09:42:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=21
03/02/2022 09:42:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=23
03/02/2022 09:42:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=25
03/02/2022 09:42:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=27
03/02/2022 09:43:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=29
03/02/2022 09:43:01 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.1481670929241262 on epoch=29
03/02/2022 09:43:01 - INFO - __main__ - Saving model with best Classification-F1: 0.0910394265232975 -> 0.1481670929241262 on epoch=29, global_step=150
03/02/2022 09:43:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=31
03/02/2022 09:43:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=33
03/02/2022 09:43:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=35
03/02/2022 09:43:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=37
03/02/2022 09:43:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=39
03/02/2022 09:43:14 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.06808510638297872 on epoch=39
03/02/2022 09:43:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=41
03/02/2022 09:43:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=43
03/02/2022 09:43:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=45
03/02/2022 09:43:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=47
03/02/2022 09:43:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=49
03/02/2022 09:43:28 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.06666666666666668 on epoch=49
03/02/2022 09:43:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=51
03/02/2022 09:43:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=53
03/02/2022 09:43:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=55
03/02/2022 09:43:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=57
03/02/2022 09:43:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=59
03/02/2022 09:43:41 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.1075098814229249 on epoch=59
03/02/2022 09:43:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=61
03/02/2022 09:43:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=63
03/02/2022 09:43:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=65
03/02/2022 09:43:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=67
03/02/2022 09:43:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=69
03/02/2022 09:43:54 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.14528013527135564 on epoch=69
03/02/2022 09:43:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=71
03/02/2022 09:43:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=73
03/02/2022 09:44:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=75
03/02/2022 09:44:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=77
03/02/2022 09:44:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=79
03/02/2022 09:44:07 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.17741360089186176 on epoch=79
03/02/2022 09:44:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1481670929241262 -> 0.17741360089186176 on epoch=79, global_step=400
03/02/2022 09:44:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=81
03/02/2022 09:44:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=83
03/02/2022 09:44:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=85
03/02/2022 09:44:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=87
03/02/2022 09:44:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=89
03/02/2022 09:44:20 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.1522222222222222 on epoch=89
03/02/2022 09:44:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=91
03/02/2022 09:44:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=93
03/02/2022 09:44:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=95
03/02/2022 09:44:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=97
03/02/2022 09:44:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=99
03/02/2022 09:44:34 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.24839328587454523 on epoch=99
03/02/2022 09:44:34 - INFO - __main__ - Saving model with best Classification-F1: 0.17741360089186176 -> 0.24839328587454523 on epoch=99, global_step=500
03/02/2022 09:44:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=101
03/02/2022 09:44:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=103
03/02/2022 09:44:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=105
03/02/2022 09:44:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=107
03/02/2022 09:44:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=109
03/02/2022 09:44:48 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.28925312235657064 on epoch=109
03/02/2022 09:44:48 - INFO - __main__ - Saving model with best Classification-F1: 0.24839328587454523 -> 0.28925312235657064 on epoch=109, global_step=550
03/02/2022 09:44:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=111
03/02/2022 09:44:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=113
03/02/2022 09:44:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=115
03/02/2022 09:44:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=117
03/02/2022 09:44:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=119
03/02/2022 09:45:01 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.32511784511784503 on epoch=119
03/02/2022 09:45:01 - INFO - __main__ - Saving model with best Classification-F1: 0.28925312235657064 -> 0.32511784511784503 on epoch=119, global_step=600
03/02/2022 09:45:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=121
03/02/2022 09:45:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=123
03/02/2022 09:45:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=125
03/02/2022 09:45:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=127
03/02/2022 09:45:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=129
03/02/2022 09:45:15 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.31048971312662993 on epoch=129
03/02/2022 09:45:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=131
03/02/2022 09:45:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=133
03/02/2022 09:45:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=135
03/02/2022 09:45:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=137
03/02/2022 09:45:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=139
03/02/2022 09:45:29 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.3215034441449536 on epoch=139
03/02/2022 09:45:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=141
03/02/2022 09:45:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=143
03/02/2022 09:45:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=145
03/02/2022 09:45:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=147
03/02/2022 09:45:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=149
03/02/2022 09:45:43 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.38270569734970367 on epoch=149
03/02/2022 09:45:43 - INFO - __main__ - Saving model with best Classification-F1: 0.32511784511784503 -> 0.38270569734970367 on epoch=149, global_step=750
03/02/2022 09:45:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=151
03/02/2022 09:45:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=153
03/02/2022 09:45:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=155
03/02/2022 09:45:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=157
03/02/2022 09:45:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=159
03/02/2022 09:45:57 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3249026391883535 on epoch=159
03/02/2022 09:45:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=161
03/02/2022 09:46:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=163
03/02/2022 09:46:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=165
03/02/2022 09:46:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=167
03/02/2022 09:46:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=169
03/02/2022 09:46:12 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.4226878771860271 on epoch=169
03/02/2022 09:46:12 - INFO - __main__ - Saving model with best Classification-F1: 0.38270569734970367 -> 0.4226878771860271 on epoch=169, global_step=850
03/02/2022 09:46:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=171
03/02/2022 09:46:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=173
03/02/2022 09:46:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=175
03/02/2022 09:46:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=177
03/02/2022 09:46:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=179
03/02/2022 09:46:26 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.35050132625994695 on epoch=179
03/02/2022 09:46:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=181
03/02/2022 09:46:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=183
03/02/2022 09:46:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=185
03/02/2022 09:46:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=187
03/02/2022 09:46:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=189
03/02/2022 09:46:40 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3216337285902503 on epoch=189
03/02/2022 09:46:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=191
03/02/2022 09:46:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=193
03/02/2022 09:46:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=195
03/02/2022 09:46:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=197
03/02/2022 09:46:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=199
03/02/2022 09:46:54 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.22388476388476386 on epoch=199
03/02/2022 09:46:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=201
03/02/2022 09:46:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=203
03/02/2022 09:47:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=205
03/02/2022 09:47:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=207
03/02/2022 09:47:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=209
03/02/2022 09:47:08 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3656419102276819 on epoch=209
03/02/2022 09:47:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=211
03/02/2022 09:47:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=213
03/02/2022 09:47:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=215
03/02/2022 09:47:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=217
03/02/2022 09:47:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=219
03/02/2022 09:47:22 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.28997668997668996 on epoch=219
03/02/2022 09:47:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=221
03/02/2022 09:47:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=223
03/02/2022 09:47:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=225
03/02/2022 09:47:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=227
03/02/2022 09:47:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=229
03/02/2022 09:47:37 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.30999096688751865 on epoch=229
03/02/2022 09:47:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=231
03/02/2022 09:47:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=233
03/02/2022 09:47:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=235
03/02/2022 09:47:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=237
03/02/2022 09:47:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=239
03/02/2022 09:47:51 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.2996052966641202 on epoch=239
03/02/2022 09:47:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=241
03/02/2022 09:47:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=243
03/02/2022 09:47:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=245
03/02/2022 09:48:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=247
03/02/2022 09:48:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=249
03/02/2022 09:48:05 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.34911062652998137 on epoch=249
03/02/2022 09:48:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=251
03/02/2022 09:48:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=253
03/02/2022 09:48:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=255
03/02/2022 09:48:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=257
03/02/2022 09:48:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=259
03/02/2022 09:48:20 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.33459812321501425 on epoch=259
03/02/2022 09:48:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=261
03/02/2022 09:48:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=263
03/02/2022 09:48:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=265
03/02/2022 09:48:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=267
03/02/2022 09:48:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=269
03/02/2022 09:48:34 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.2446316749452638 on epoch=269
03/02/2022 09:48:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=271
03/02/2022 09:48:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=273
03/02/2022 09:48:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=275
03/02/2022 09:48:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=277
03/02/2022 09:48:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=279
03/02/2022 09:48:49 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.25808113455172277 on epoch=279
03/02/2022 09:48:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=281
03/02/2022 09:48:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=283
03/02/2022 09:48:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=285
03/02/2022 09:48:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=287
03/02/2022 09:49:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=289
03/02/2022 09:49:03 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.3390599237110865 on epoch=289
03/02/2022 09:49:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=291
03/02/2022 09:49:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=293
03/02/2022 09:49:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=295
03/02/2022 09:49:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=297
03/02/2022 09:49:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=299
03/02/2022 09:49:18 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.3529608932907284 on epoch=299
03/02/2022 09:49:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=301
03/02/2022 09:49:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=303
03/02/2022 09:49:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=305
03/02/2022 09:49:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=307
03/02/2022 09:49:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=309
03/02/2022 09:49:32 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.22344081844081845 on epoch=309
03/02/2022 09:49:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=311
03/02/2022 09:49:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=313
03/02/2022 09:49:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=315
03/02/2022 09:49:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=317
03/02/2022 09:49:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=319
03/02/2022 09:49:47 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.20887146862939057 on epoch=319
03/02/2022 09:49:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=321
03/02/2022 09:49:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=323
03/02/2022 09:49:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=325
03/02/2022 09:49:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=327
03/02/2022 09:49:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=329
03/02/2022 09:50:01 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.22676418715634405 on epoch=329
03/02/2022 09:50:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=331
03/02/2022 09:50:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=333
03/02/2022 09:50:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=335
03/02/2022 09:50:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=337
03/02/2022 09:50:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=339
03/02/2022 09:50:16 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.1865406067951137 on epoch=339
03/02/2022 09:50:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=341
03/02/2022 09:50:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=343
03/02/2022 09:50:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=345
03/02/2022 09:50:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=347
03/02/2022 09:50:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=349
03/02/2022 09:50:30 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.29176406520156517 on epoch=349
03/02/2022 09:50:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=351
03/02/2022 09:50:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=353
03/02/2022 09:50:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=355
03/02/2022 09:50:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=357
03/02/2022 09:50:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=359
03/02/2022 09:50:44 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.17294533423565683 on epoch=359
03/02/2022 09:50:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=361
03/02/2022 09:50:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=363
03/02/2022 09:50:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=365
03/02/2022 09:50:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=367
03/02/2022 09:50:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=369
03/02/2022 09:50:58 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.22532260767554885 on epoch=369
03/02/2022 09:51:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=371
03/02/2022 09:51:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=373
03/02/2022 09:51:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=375
03/02/2022 09:51:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=377
03/02/2022 09:51:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=379
03/02/2022 09:51:13 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.17422562076939957 on epoch=379
03/02/2022 09:51:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=381
03/02/2022 09:51:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=383
03/02/2022 09:51:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=385
03/02/2022 09:51:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=387
03/02/2022 09:51:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=389
03/02/2022 09:51:27 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.1604942676371248 on epoch=389
03/02/2022 09:51:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=391
03/02/2022 09:51:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=393
03/02/2022 09:51:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=395
03/02/2022 09:51:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=397
03/02/2022 09:51:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=399
03/02/2022 09:51:41 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.13592885872297636 on epoch=399
03/02/2022 09:51:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=401
03/02/2022 09:51:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=403
03/02/2022 09:51:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=405
03/02/2022 09:51:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=407
03/02/2022 09:51:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=409
03/02/2022 09:51:55 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.32576546003016593 on epoch=409
03/02/2022 09:51:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=411
03/02/2022 09:52:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=413
03/02/2022 09:52:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=415
03/02/2022 09:52:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=417
03/02/2022 09:52:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=419
03/02/2022 09:52:10 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.26850254668930396 on epoch=419
03/02/2022 09:52:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=421
03/02/2022 09:52:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=423
03/02/2022 09:52:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=425
03/02/2022 09:52:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=427
03/02/2022 09:52:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=429
03/02/2022 09:52:24 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.3044101522187822 on epoch=429
03/02/2022 09:52:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=431
03/02/2022 09:52:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=433
03/02/2022 09:52:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=435
03/02/2022 09:52:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=437
03/02/2022 09:52:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=439
03/02/2022 09:52:38 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.22858722894192005 on epoch=439
03/02/2022 09:52:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=441
03/02/2022 09:52:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=443
03/02/2022 09:52:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=445
03/02/2022 09:52:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=447
03/02/2022 09:52:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=449
03/02/2022 09:52:53 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.17413435509365743 on epoch=449
03/02/2022 09:52:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=451
03/02/2022 09:52:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=453
03/02/2022 09:52:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=455
03/02/2022 09:53:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=457
03/02/2022 09:53:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=459
03/02/2022 09:53:07 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.277067155067155 on epoch=459
03/02/2022 09:53:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=461
03/02/2022 09:53:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=463
03/02/2022 09:53:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=465
03/02/2022 09:53:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=467
03/02/2022 09:53:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=469
03/02/2022 09:53:22 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.21888985984679762 on epoch=469
03/02/2022 09:53:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=471
03/02/2022 09:53:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=473
03/02/2022 09:53:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=475
03/02/2022 09:53:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=477
03/02/2022 09:53:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=479
03/02/2022 09:53:36 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.33465184587247976 on epoch=479
03/02/2022 09:53:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=481
03/02/2022 09:53:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=483
03/02/2022 09:53:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=485
03/02/2022 09:53:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=487
03/02/2022 09:53:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=489
03/02/2022 09:53:51 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.23375331564986734 on epoch=489
03/02/2022 09:53:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=491
03/02/2022 09:53:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 09:53:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=495
03/02/2022 09:54:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=497
03/02/2022 09:54:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=499
03/02/2022 09:54:05 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.18163678426836322 on epoch=499
03/02/2022 09:54:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=501
03/02/2022 09:54:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=503
03/02/2022 09:54:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=505
03/02/2022 09:54:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=507
03/02/2022 09:54:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=509
03/02/2022 09:54:19 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3074605679021429 on epoch=509
03/02/2022 09:54:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=511
03/02/2022 09:54:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=513
03/02/2022 09:54:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=515
03/02/2022 09:54:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=517
03/02/2022 09:54:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=519
03/02/2022 09:54:34 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.2831874296538928 on epoch=519
03/02/2022 09:54:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=521
03/02/2022 09:54:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=523
03/02/2022 09:54:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=525
03/02/2022 09:54:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=527
03/02/2022 09:54:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=529
03/02/2022 09:54:48 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.26901897927704377 on epoch=529
03/02/2022 09:54:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=531
03/02/2022 09:54:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=533
03/02/2022 09:54:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=535
03/02/2022 09:54:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=537
03/02/2022 09:54:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=539
03/02/2022 09:55:03 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5885194413682786 on epoch=539
03/02/2022 09:55:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4226878771860271 -> 0.5885194413682786 on epoch=539, global_step=2700
03/02/2022 09:55:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=541
03/02/2022 09:55:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=543
03/02/2022 09:55:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=545
03/02/2022 09:55:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=547
03/02/2022 09:55:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 09:55:18 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3227467462849771 on epoch=549
03/02/2022 09:55:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 09:55:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 09:55:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=555
03/02/2022 09:55:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=557
03/02/2022 09:55:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=559
03/02/2022 09:55:32 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.41018457562236366 on epoch=559
03/02/2022 09:55:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=561
03/02/2022 09:55:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 09:55:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=565
03/02/2022 09:55:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 09:55:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=569
03/02/2022 09:55:47 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.46680133727388967 on epoch=569
03/02/2022 09:55:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=571
03/02/2022 09:55:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=573
03/02/2022 09:55:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=575
03/02/2022 09:55:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=577
03/02/2022 09:55:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=579
03/02/2022 09:56:01 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3751931287512175 on epoch=579
03/02/2022 09:56:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 09:56:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=583
03/02/2022 09:56:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=585
03/02/2022 09:56:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 09:56:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=589
03/02/2022 09:56:16 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.3571454635494044 on epoch=589
03/02/2022 09:56:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 09:56:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=593
03/02/2022 09:56:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=595
03/02/2022 09:56:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=597
03/02/2022 09:56:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=599
03/02/2022 09:56:28 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:56:28 - INFO - __main__ - Printing 3 examples
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 09:56:28 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:56:28 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 09:56:28 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 09:56:28 - INFO - __main__ - Printing 3 examples
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 09:56:28 - INFO - __main__ - ['No']
03/02/2022 09:56:28 - INFO - __main__ - Tokenizing Input ...
03/02/2022 09:56:28 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:56:29 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 09:56:30 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.30241839381624325 on epoch=599
03/02/2022 09:56:30 - INFO - __main__ - save last model!
03/02/2022 09:56:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 09:56:30 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 09:56:30 - INFO - __main__ - Printing 3 examples
03/02/2022 09:56:30 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 09:56:30 - INFO - __main__ - ['No']
03/02/2022 09:56:30 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 09:56:30 - INFO - __main__ - ['Yes']
03/02/2022 09:56:30 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 09:56:30 - INFO - __main__ - ['Yes']
03/02/2022 09:56:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 09:56:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 09:56:40 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 09:56:43 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 09:56:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 09:56:43 - INFO - __main__ - Starting training!
03/02/2022 10:01:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_87_0.5_8_predictions.txt
03/02/2022 10:01:47 - INFO - __main__ - Classification-F1 on test data: 0.0756
03/02/2022 10:01:50 - INFO - __main__ - prefix=circa_16_87, lr=0.5, bsz=8, dev_performance=0.5885194413682786, test_performance=0.0755962868189462
03/02/2022 10:01:50 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.4, bsz=8 ...
03/02/2022 10:01:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:01:51 - INFO - __main__ - Printing 3 examples
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:01:51 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:01:51 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 10:01:51 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:01:51 - INFO - __main__ - Printing 3 examples
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 10:01:51 - INFO - __main__ - ['No']
03/02/2022 10:01:51 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:01:51 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:01:51 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 10:02:03 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:02:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:02:04 - INFO - __main__ - Starting training!
03/02/2022 10:02:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.70 on epoch=1
03/02/2022 10:02:09 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=3
03/02/2022 10:02:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.89 on epoch=5
03/02/2022 10:02:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.35 on epoch=7
03/02/2022 10:02:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=9
03/02/2022 10:02:18 - INFO - __main__ - Global step 50 Train loss 2.13 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 10:02:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 10:02:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=11
03/02/2022 10:02:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.69 on epoch=13
03/02/2022 10:02:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=15
03/02/2022 10:02:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=17
03/02/2022 10:02:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=19
03/02/2022 10:02:32 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 10:02:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=21
03/02/2022 10:02:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=23
03/02/2022 10:02:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=25
03/02/2022 10:02:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.65 on epoch=27
03/02/2022 10:02:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=29
03/02/2022 10:02:45 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 10:02:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=31
03/02/2022 10:02:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=33
03/02/2022 10:02:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=35
03/02/2022 10:02:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=37
03/02/2022 10:02:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=39
03/02/2022 10:02:58 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 10:03:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=41
03/02/2022 10:03:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=43
03/02/2022 10:03:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=45
03/02/2022 10:03:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=47
03/02/2022 10:03:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=49
03/02/2022 10:03:11 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.14666666666666667 on epoch=49
03/02/2022 10:03:11 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.14666666666666667 on epoch=49, global_step=250
03/02/2022 10:03:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=51
03/02/2022 10:03:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=53
03/02/2022 10:03:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=55
03/02/2022 10:03:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=57
03/02/2022 10:03:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=59
03/02/2022 10:03:24 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.06315789473684211 on epoch=59
03/02/2022 10:03:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=61
03/02/2022 10:03:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=63
03/02/2022 10:03:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=65
03/02/2022 10:03:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=67
03/02/2022 10:03:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=69
03/02/2022 10:03:38 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 10:03:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=71
03/02/2022 10:03:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=73
03/02/2022 10:03:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=75
03/02/2022 10:03:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=77
03/02/2022 10:03:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=79
03/02/2022 10:03:51 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.11252955082742315 on epoch=79
03/02/2022 10:03:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=81
03/02/2022 10:03:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=83
03/02/2022 10:03:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=85
03/02/2022 10:04:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=87
03/02/2022 10:04:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=89
03/02/2022 10:04:05 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.15320648343904159 on epoch=89
03/02/2022 10:04:05 - INFO - __main__ - Saving model with best Classification-F1: 0.14666666666666667 -> 0.15320648343904159 on epoch=89, global_step=450
03/02/2022 10:04:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=91
03/02/2022 10:04:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=93
03/02/2022 10:04:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=95
03/02/2022 10:04:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=97
03/02/2022 10:04:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=99
03/02/2022 10:04:18 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16556776556776556 on epoch=99
03/02/2022 10:04:18 - INFO - __main__ - Saving model with best Classification-F1: 0.15320648343904159 -> 0.16556776556776556 on epoch=99, global_step=500
03/02/2022 10:04:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=101
03/02/2022 10:04:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=103
03/02/2022 10:04:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=105
03/02/2022 10:04:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=107
03/02/2022 10:04:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=109
03/02/2022 10:04:32 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.2714231437255753 on epoch=109
03/02/2022 10:04:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16556776556776556 -> 0.2714231437255753 on epoch=109, global_step=550
03/02/2022 10:04:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=111
03/02/2022 10:04:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=113
03/02/2022 10:04:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=115
03/02/2022 10:04:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=117
03/02/2022 10:04:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=119
03/02/2022 10:04:46 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.30524089306698 on epoch=119
03/02/2022 10:04:46 - INFO - __main__ - Saving model with best Classification-F1: 0.2714231437255753 -> 0.30524089306698 on epoch=119, global_step=600
03/02/2022 10:04:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=121
03/02/2022 10:04:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=123
03/02/2022 10:04:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=125
03/02/2022 10:04:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=127
03/02/2022 10:04:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=129
03/02/2022 10:05:00 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.3601313628899836 on epoch=129
03/02/2022 10:05:00 - INFO - __main__ - Saving model with best Classification-F1: 0.30524089306698 -> 0.3601313628899836 on epoch=129, global_step=650
03/02/2022 10:05:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=131
03/02/2022 10:05:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=133
03/02/2022 10:05:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=135
03/02/2022 10:05:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=137
03/02/2022 10:05:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=139
03/02/2022 10:05:14 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.39360639360639366 on epoch=139
03/02/2022 10:05:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3601313628899836 -> 0.39360639360639366 on epoch=139, global_step=700
03/02/2022 10:05:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=141
03/02/2022 10:05:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=143
03/02/2022 10:05:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=145
03/02/2022 10:05:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=147
03/02/2022 10:05:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=149
03/02/2022 10:05:27 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.33508417508417504 on epoch=149
03/02/2022 10:05:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=151
03/02/2022 10:05:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=153
03/02/2022 10:05:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=155
03/02/2022 10:05:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=157
03/02/2022 10:05:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=159
03/02/2022 10:05:41 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.4277075847909345 on epoch=159
03/02/2022 10:05:41 - INFO - __main__ - Saving model with best Classification-F1: 0.39360639360639366 -> 0.4277075847909345 on epoch=159, global_step=800
03/02/2022 10:05:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=161
03/02/2022 10:05:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=163
03/02/2022 10:05:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=165
03/02/2022 10:05:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=167
03/02/2022 10:05:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=169
03/02/2022 10:05:55 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.45683500206826777 on epoch=169
03/02/2022 10:05:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4277075847909345 -> 0.45683500206826777 on epoch=169, global_step=850
03/02/2022 10:05:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=171
03/02/2022 10:06:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=173
03/02/2022 10:06:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=175
03/02/2022 10:06:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=177
03/02/2022 10:06:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=179
03/02/2022 10:06:09 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.3596171802054155 on epoch=179
03/02/2022 10:06:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=181
03/02/2022 10:06:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=183
03/02/2022 10:06:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=185
03/02/2022 10:06:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=187
03/02/2022 10:06:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=189
03/02/2022 10:06:23 - INFO - __main__ - Global step 950 Train loss 0.25 Classification-F1 0.3714339701075147 on epoch=189
03/02/2022 10:06:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=191
03/02/2022 10:06:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=193
03/02/2022 10:06:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=195
03/02/2022 10:06:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=197
03/02/2022 10:06:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=199
03/02/2022 10:06:37 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.33304928989139515 on epoch=199
03/02/2022 10:06:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=201
03/02/2022 10:06:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=203
03/02/2022 10:06:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=205
03/02/2022 10:06:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=207
03/02/2022 10:06:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=209
03/02/2022 10:06:51 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.24911184210526316 on epoch=209
03/02/2022 10:06:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=211
03/02/2022 10:06:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=213
03/02/2022 10:06:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=215
03/02/2022 10:07:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=217
03/02/2022 10:07:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=219
03/02/2022 10:07:06 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.23796296296296296 on epoch=219
03/02/2022 10:07:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=221
03/02/2022 10:07:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=223
03/02/2022 10:07:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=225
03/02/2022 10:07:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=227
03/02/2022 10:07:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=229
03/02/2022 10:07:20 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.2822222222222222 on epoch=229
03/02/2022 10:07:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=231
03/02/2022 10:07:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=233
03/02/2022 10:07:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=235
03/02/2022 10:07:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=237
03/02/2022 10:07:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=239
03/02/2022 10:07:34 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.24217796092796093 on epoch=239
03/02/2022 10:07:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=241
03/02/2022 10:07:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=243
03/02/2022 10:07:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=245
03/02/2022 10:07:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=247
03/02/2022 10:07:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=249
03/02/2022 10:07:48 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.24949996804160515 on epoch=249
03/02/2022 10:07:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=251
03/02/2022 10:07:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=253
03/02/2022 10:07:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=255
03/02/2022 10:07:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=257
03/02/2022 10:07:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=259
03/02/2022 10:08:02 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.2514323911382735 on epoch=259
03/02/2022 10:08:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=261
03/02/2022 10:08:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=263
03/02/2022 10:08:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=265
03/02/2022 10:08:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=267
03/02/2022 10:08:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=269
03/02/2022 10:08:16 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.19810574229691877 on epoch=269
03/02/2022 10:08:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=271
03/02/2022 10:08:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=273
03/02/2022 10:08:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=275
03/02/2022 10:08:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=277
03/02/2022 10:08:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=279
03/02/2022 10:08:30 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.2119624419816235 on epoch=279
03/02/2022 10:08:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=281
03/02/2022 10:08:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=283
03/02/2022 10:08:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=285
03/02/2022 10:08:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=287
03/02/2022 10:08:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=289
03/02/2022 10:08:44 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.29117036947960195 on epoch=289
03/02/2022 10:08:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=291
03/02/2022 10:08:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=293
03/02/2022 10:08:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=295
03/02/2022 10:08:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=297
03/02/2022 10:08:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=299
03/02/2022 10:08:58 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.31450886278472484 on epoch=299
03/02/2022 10:09:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=301
03/02/2022 10:09:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=303
03/02/2022 10:09:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=305
03/02/2022 10:09:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=307
03/02/2022 10:09:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=309
03/02/2022 10:09:13 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.21987638533674342 on epoch=309
03/02/2022 10:09:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=311
03/02/2022 10:09:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=313
03/02/2022 10:09:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=315
03/02/2022 10:09:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=317
03/02/2022 10:09:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=319
03/02/2022 10:09:27 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.16458065793929172 on epoch=319
03/02/2022 10:09:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=321
03/02/2022 10:09:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=323
03/02/2022 10:09:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=325
03/02/2022 10:09:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=327
03/02/2022 10:09:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=329
03/02/2022 10:09:41 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.23920539730134932 on epoch=329
03/02/2022 10:09:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=331
03/02/2022 10:09:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=333
03/02/2022 10:09:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=335
03/02/2022 10:09:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=337
03/02/2022 10:09:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=339
03/02/2022 10:09:55 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.23259964173757278 on epoch=339
03/02/2022 10:09:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=341
03/02/2022 10:10:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=343
03/02/2022 10:10:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=345
03/02/2022 10:10:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=347
03/02/2022 10:10:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=349
03/02/2022 10:10:09 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.2197180923651512 on epoch=349
03/02/2022 10:10:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=351
03/02/2022 10:10:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=353
03/02/2022 10:10:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=355
03/02/2022 10:10:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=357
03/02/2022 10:10:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=359
03/02/2022 10:10:24 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.23080163454747252 on epoch=359
03/02/2022 10:10:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=361
03/02/2022 10:10:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=363
03/02/2022 10:10:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=365
03/02/2022 10:10:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=367
03/02/2022 10:10:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=369
03/02/2022 10:10:38 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.2370174429188121 on epoch=369
03/02/2022 10:10:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=371
03/02/2022 10:10:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=373
03/02/2022 10:10:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=375
03/02/2022 10:10:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=377
03/02/2022 10:10:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=379
03/02/2022 10:10:52 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.19553449770841075 on epoch=379
03/02/2022 10:10:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=381
03/02/2022 10:10:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=383
03/02/2022 10:10:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=385
03/02/2022 10:11:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=387
03/02/2022 10:11:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=389
03/02/2022 10:11:06 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.15598739495798317 on epoch=389
03/02/2022 10:11:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=391
03/02/2022 10:11:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=393
03/02/2022 10:11:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=395
03/02/2022 10:11:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=397
03/02/2022 10:11:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=399
03/02/2022 10:11:20 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.15428571428571428 on epoch=399
03/02/2022 10:11:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=401
03/02/2022 10:11:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=403
03/02/2022 10:11:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=405
03/02/2022 10:11:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=407
03/02/2022 10:11:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=409
03/02/2022 10:11:34 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.22053763440860216 on epoch=409
03/02/2022 10:11:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=411
03/02/2022 10:11:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=413
03/02/2022 10:11:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=415
03/02/2022 10:11:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=417
03/02/2022 10:11:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=419
03/02/2022 10:11:49 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.2533080712391057 on epoch=419
03/02/2022 10:11:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=421
03/02/2022 10:11:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=423
03/02/2022 10:11:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=425
03/02/2022 10:11:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=427
03/02/2022 10:12:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=429
03/02/2022 10:12:03 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.24592592592592594 on epoch=429
03/02/2022 10:12:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=431
03/02/2022 10:12:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=433
03/02/2022 10:12:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=435
03/02/2022 10:12:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=437
03/02/2022 10:12:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=439
03/02/2022 10:12:17 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.31646887430108156 on epoch=439
03/02/2022 10:12:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=441
03/02/2022 10:12:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=443
03/02/2022 10:12:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=445
03/02/2022 10:12:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=447
03/02/2022 10:12:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=449
03/02/2022 10:12:32 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.2559958089673852 on epoch=449
03/02/2022 10:12:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=451
03/02/2022 10:12:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=453
03/02/2022 10:12:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=455
03/02/2022 10:12:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=457
03/02/2022 10:12:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=459
03/02/2022 10:12:46 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.20418470418470414 on epoch=459
03/02/2022 10:12:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=461
03/02/2022 10:12:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=463
03/02/2022 10:12:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=465
03/02/2022 10:12:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=467
03/02/2022 10:12:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=469
03/02/2022 10:13:00 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.18096583551129003 on epoch=469
03/02/2022 10:13:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=471
03/02/2022 10:13:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=473
03/02/2022 10:13:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=475
03/02/2022 10:13:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=477
03/02/2022 10:13:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=479
03/02/2022 10:13:14 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.27590891316381516 on epoch=479
03/02/2022 10:13:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=481
03/02/2022 10:13:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=483
03/02/2022 10:13:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=485
03/02/2022 10:13:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=487
03/02/2022 10:13:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=489
03/02/2022 10:13:29 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.24900691900691901 on epoch=489
03/02/2022 10:13:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=491
03/02/2022 10:13:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 10:13:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=495
03/02/2022 10:13:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=497
03/02/2022 10:13:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=499
03/02/2022 10:13:43 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.2190070429865038 on epoch=499
03/02/2022 10:13:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=501
03/02/2022 10:13:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=503
03/02/2022 10:13:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=505
03/02/2022 10:13:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=507
03/02/2022 10:13:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=509
03/02/2022 10:13:57 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.2641975308641975 on epoch=509
03/02/2022 10:14:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=511
03/02/2022 10:14:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=513
03/02/2022 10:14:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=515
03/02/2022 10:14:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=517
03/02/2022 10:14:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=519
03/02/2022 10:14:12 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.23894660894660893 on epoch=519
03/02/2022 10:14:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=521
03/02/2022 10:14:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=523
03/02/2022 10:14:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=525
03/02/2022 10:14:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=527
03/02/2022 10:14:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=529
03/02/2022 10:14:26 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.22808080808080808 on epoch=529
03/02/2022 10:14:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=531
03/02/2022 10:14:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=533
03/02/2022 10:14:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=535
03/02/2022 10:14:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=537
03/02/2022 10:14:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=539
03/02/2022 10:14:41 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.26115140415140414 on epoch=539
03/02/2022 10:14:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=541
03/02/2022 10:14:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=543
03/02/2022 10:14:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=545
03/02/2022 10:14:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=547
03/02/2022 10:14:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=549
03/02/2022 10:14:55 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2826154893406356 on epoch=549
03/02/2022 10:14:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=551
03/02/2022 10:15:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=553
03/02/2022 10:15:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=555
03/02/2022 10:15:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=557
03/02/2022 10:15:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=559
03/02/2022 10:15:10 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.38929131069895095 on epoch=559
03/02/2022 10:15:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=561
03/02/2022 10:15:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=563
03/02/2022 10:15:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=565
03/02/2022 10:15:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=567
03/02/2022 10:15:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=569
03/02/2022 10:15:25 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.4625009956192752 on epoch=569
03/02/2022 10:15:25 - INFO - __main__ - Saving model with best Classification-F1: 0.45683500206826777 -> 0.4625009956192752 on epoch=569, global_step=2850
03/02/2022 10:15:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=571
03/02/2022 10:15:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=573
03/02/2022 10:15:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=575
03/02/2022 10:15:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=577
03/02/2022 10:15:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=579
03/02/2022 10:15:39 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2551092383981584 on epoch=579
03/02/2022 10:15:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 10:15:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=583
03/02/2022 10:15:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=585
03/02/2022 10:15:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=587
03/02/2022 10:15:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=589
03/02/2022 10:15:54 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3136696298281664 on epoch=589
03/02/2022 10:15:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 10:15:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 10:16:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=595
03/02/2022 10:16:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 10:16:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=599
03/02/2022 10:16:07 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:16:07 - INFO - __main__ - Printing 3 examples
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 10:16:07 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:16:07 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 10:16:07 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:16:07 - INFO - __main__ - Printing 3 examples
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 10:16:07 - INFO - __main__ - ['No']
03/02/2022 10:16:07 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:16:07 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:16:07 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 10:16:09 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.25015831774725056 on epoch=599
03/02/2022 10:16:09 - INFO - __main__ - save last model!
03/02/2022 10:16:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 10:16:09 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 10:16:09 - INFO - __main__ - Printing 3 examples
03/02/2022 10:16:09 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 10:16:09 - INFO - __main__ - ['No']
03/02/2022 10:16:09 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 10:16:09 - INFO - __main__ - ['Yes']
03/02/2022 10:16:09 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 10:16:09 - INFO - __main__ - ['Yes']
03/02/2022 10:16:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:16:12 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:16:19 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 10:16:20 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:16:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:16:21 - INFO - __main__ - Starting training!
03/02/2022 10:20:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_87_0.4_8_predictions.txt
03/02/2022 10:20:54 - INFO - __main__ - Classification-F1 on test data: 0.0353
03/02/2022 10:20:54 - INFO - __main__ - prefix=circa_16_87, lr=0.4, bsz=8, dev_performance=0.4625009956192752, test_performance=0.03529815222973613
03/02/2022 10:20:54 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.3, bsz=8 ...
03/02/2022 10:20:55 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:20:55 - INFO - __main__ - Printing 3 examples
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:20:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:20:55 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 10:20:55 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:20:55 - INFO - __main__ - Printing 3 examples
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 10:20:55 - INFO - __main__ - ['No']
03/02/2022 10:20:55 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:20:55 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:20:55 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 10:21:07 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:21:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:21:08 - INFO - __main__ - Starting training!
03/02/2022 10:21:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.82 on epoch=1
03/02/2022 10:21:13 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=3
03/02/2022 10:21:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.18 on epoch=5
03/02/2022 10:21:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.64 on epoch=7
03/02/2022 10:21:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.28 on epoch=9
03/02/2022 10:21:23 - INFO - __main__ - Global step 50 Train loss 2.35 Classification-F1 0.059523809523809514 on epoch=9
03/02/2022 10:21:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.059523809523809514 on epoch=9, global_step=50
03/02/2022 10:21:25 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=11
03/02/2022 10:21:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=13
03/02/2022 10:21:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=15
03/02/2022 10:21:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.73 on epoch=17
03/02/2022 10:21:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=19
03/02/2022 10:21:37 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.0833167495854063 on epoch=19
03/02/2022 10:21:37 - INFO - __main__ - Saving model with best Classification-F1: 0.059523809523809514 -> 0.0833167495854063 on epoch=19, global_step=100
03/02/2022 10:21:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.65 on epoch=21
03/02/2022 10:21:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=23
03/02/2022 10:21:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=25
03/02/2022 10:21:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=27
03/02/2022 10:21:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=29
03/02/2022 10:21:49 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.06666666666666668 on epoch=29
03/02/2022 10:21:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=31
03/02/2022 10:21:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=33
03/02/2022 10:21:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=35
03/02/2022 10:21:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=37
03/02/2022 10:22:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=39
03/02/2022 10:22:02 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.09089783281733746 on epoch=39
03/02/2022 10:22:02 - INFO - __main__ - Saving model with best Classification-F1: 0.0833167495854063 -> 0.09089783281733746 on epoch=39, global_step=200
03/02/2022 10:22:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=41
03/02/2022 10:22:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=43
03/02/2022 10:22:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=45
03/02/2022 10:22:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=47
03/02/2022 10:22:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=49
03/02/2022 10:22:15 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.05057471264367815 on epoch=49
03/02/2022 10:22:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=51
03/02/2022 10:22:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=53
03/02/2022 10:22:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=55
03/02/2022 10:22:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=57
03/02/2022 10:22:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=59
03/02/2022 10:22:29 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.09030732860520094 on epoch=59
03/02/2022 10:22:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=61
03/02/2022 10:22:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=63
03/02/2022 10:22:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=65
03/02/2022 10:22:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=67
03/02/2022 10:22:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=69
03/02/2022 10:22:42 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.12431289640591965 on epoch=69
03/02/2022 10:22:42 - INFO - __main__ - Saving model with best Classification-F1: 0.09089783281733746 -> 0.12431289640591965 on epoch=69, global_step=350
03/02/2022 10:22:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=71
03/02/2022 10:22:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=73
03/02/2022 10:22:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=75
03/02/2022 10:22:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=77
03/02/2022 10:22:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=79
03/02/2022 10:22:55 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.178755980861244 on epoch=79
03/02/2022 10:22:55 - INFO - __main__ - Saving model with best Classification-F1: 0.12431289640591965 -> 0.178755980861244 on epoch=79, global_step=400
03/02/2022 10:22:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=81
03/02/2022 10:23:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=83
03/02/2022 10:23:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=85
03/02/2022 10:23:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=87
03/02/2022 10:23:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=89
03/02/2022 10:23:08 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1616260162601626 on epoch=89
03/02/2022 10:23:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=91
03/02/2022 10:23:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=93
03/02/2022 10:23:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=95
03/02/2022 10:23:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 10:23:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=99
03/02/2022 10:23:22 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.19441295546558704 on epoch=99
03/02/2022 10:23:22 - INFO - __main__ - Saving model with best Classification-F1: 0.178755980861244 -> 0.19441295546558704 on epoch=99, global_step=500
03/02/2022 10:23:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=101
03/02/2022 10:23:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=103
03/02/2022 10:23:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=105
03/02/2022 10:23:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=107
03/02/2022 10:23:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=109
03/02/2022 10:23:36 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.1875686502552174 on epoch=109
03/02/2022 10:23:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=111
03/02/2022 10:23:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=113
03/02/2022 10:23:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=115
03/02/2022 10:23:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=117
03/02/2022 10:23:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=119
03/02/2022 10:23:50 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.2813055657045427 on epoch=119
03/02/2022 10:23:50 - INFO - __main__ - Saving model with best Classification-F1: 0.19441295546558704 -> 0.2813055657045427 on epoch=119, global_step=600
03/02/2022 10:23:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=121
03/02/2022 10:23:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=123
03/02/2022 10:23:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=125
03/02/2022 10:23:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=127
03/02/2022 10:24:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=129
03/02/2022 10:24:03 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.39789399624765476 on epoch=129
03/02/2022 10:24:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2813055657045427 -> 0.39789399624765476 on epoch=129, global_step=650
03/02/2022 10:24:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=131
03/02/2022 10:24:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=133
03/02/2022 10:24:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=135
03/02/2022 10:24:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=137
03/02/2022 10:24:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=139
03/02/2022 10:24:17 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.30500683994528044 on epoch=139
03/02/2022 10:24:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=141
03/02/2022 10:24:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=143
03/02/2022 10:24:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=145
03/02/2022 10:24:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=147
03/02/2022 10:24:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=149
03/02/2022 10:24:30 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.26407347876004594 on epoch=149
03/02/2022 10:24:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=151
03/02/2022 10:24:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=153
03/02/2022 10:24:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=155
03/02/2022 10:24:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=157
03/02/2022 10:24:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=159
03/02/2022 10:24:44 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.3018323586744639 on epoch=159
03/02/2022 10:24:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=161
03/02/2022 10:24:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=163
03/02/2022 10:24:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=165
03/02/2022 10:24:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=167
03/02/2022 10:24:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=169
03/02/2022 10:24:58 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.3724867724867725 on epoch=169
03/02/2022 10:25:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=171
03/02/2022 10:25:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=173
03/02/2022 10:25:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=175
03/02/2022 10:25:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=177
03/02/2022 10:25:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=179
03/02/2022 10:25:11 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.3424813838927713 on epoch=179
03/02/2022 10:25:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=181
03/02/2022 10:25:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=183
03/02/2022 10:25:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=185
03/02/2022 10:25:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=187
03/02/2022 10:25:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=189
03/02/2022 10:25:25 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.31356253819187596 on epoch=189
03/02/2022 10:25:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=191
03/02/2022 10:25:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=193
03/02/2022 10:25:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=195
03/02/2022 10:25:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=197
03/02/2022 10:25:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=199
03/02/2022 10:25:38 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.26685277088502896 on epoch=199
03/02/2022 10:25:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=201
03/02/2022 10:25:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=203
03/02/2022 10:25:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=205
03/02/2022 10:25:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=207
03/02/2022 10:25:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.28 on epoch=209
03/02/2022 10:25:51 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.3038461538461538 on epoch=209
03/02/2022 10:25:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=211
03/02/2022 10:25:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=213
03/02/2022 10:25:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=215
03/02/2022 10:26:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=217
03/02/2022 10:26:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=219
03/02/2022 10:26:05 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.3193180498005654 on epoch=219
03/02/2022 10:26:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=221
03/02/2022 10:26:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=223
03/02/2022 10:26:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=225
03/02/2022 10:26:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=227
03/02/2022 10:26:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=229
03/02/2022 10:26:19 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.2002484334755699 on epoch=229
03/02/2022 10:26:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=231
03/02/2022 10:26:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=233
03/02/2022 10:26:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=235
03/02/2022 10:26:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=237
03/02/2022 10:26:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=239
03/02/2022 10:26:32 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.23036487322201613 on epoch=239
03/02/2022 10:26:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=241
03/02/2022 10:26:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=243
03/02/2022 10:26:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=245
03/02/2022 10:26:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=247
03/02/2022 10:26:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=249
03/02/2022 10:26:46 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.21304491177259197 on epoch=249
03/02/2022 10:26:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=251
03/02/2022 10:26:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=253
03/02/2022 10:26:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=255
03/02/2022 10:26:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=257
03/02/2022 10:26:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=259
03/02/2022 10:27:00 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.23539193455159843 on epoch=259
03/02/2022 10:27:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=261
03/02/2022 10:27:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=263
03/02/2022 10:27:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=265
03/02/2022 10:27:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=267
03/02/2022 10:27:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=269
03/02/2022 10:27:14 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.273717216355141 on epoch=269
03/02/2022 10:27:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=271
03/02/2022 10:27:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=273
03/02/2022 10:27:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=275
03/02/2022 10:27:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=277
03/02/2022 10:27:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=279
03/02/2022 10:27:28 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.24298082869511442 on epoch=279
03/02/2022 10:27:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=281
03/02/2022 10:27:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=283
03/02/2022 10:27:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=285
03/02/2022 10:27:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=287
03/02/2022 10:27:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=289
03/02/2022 10:27:42 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.21313628899835793 on epoch=289
03/02/2022 10:27:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=291
03/02/2022 10:27:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=293
03/02/2022 10:27:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=295
03/02/2022 10:27:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=297
03/02/2022 10:27:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=299
03/02/2022 10:27:56 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.2787759254620739 on epoch=299
03/02/2022 10:27:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=301
03/02/2022 10:28:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=303
03/02/2022 10:28:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=305
03/02/2022 10:28:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=307
03/02/2022 10:28:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=309
03/02/2022 10:28:10 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.32444206180765317 on epoch=309
03/02/2022 10:28:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=311
03/02/2022 10:28:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=313
03/02/2022 10:28:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=315
03/02/2022 10:28:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=317
03/02/2022 10:28:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=319
03/02/2022 10:28:24 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.22509969505043395 on epoch=319
03/02/2022 10:28:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=321
03/02/2022 10:28:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=323
03/02/2022 10:28:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=325
03/02/2022 10:28:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=327
03/02/2022 10:28:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=329
03/02/2022 10:28:39 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.2490563335390922 on epoch=329
03/02/2022 10:28:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=331
03/02/2022 10:28:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=333
03/02/2022 10:28:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=335
03/02/2022 10:28:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=337
03/02/2022 10:28:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=339
03/02/2022 10:28:53 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.30300766469609036 on epoch=339
03/02/2022 10:28:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=341
03/02/2022 10:28:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=343
03/02/2022 10:29:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=345
03/02/2022 10:29:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=347
03/02/2022 10:29:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=349
03/02/2022 10:29:07 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.19277857787680888 on epoch=349
03/02/2022 10:29:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=351
03/02/2022 10:29:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=353
03/02/2022 10:29:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=355
03/02/2022 10:29:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=357
03/02/2022 10:29:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=359
03/02/2022 10:29:21 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.28450409629790224 on epoch=359
03/02/2022 10:29:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=361
03/02/2022 10:29:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=363
03/02/2022 10:29:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=365
03/02/2022 10:29:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=367
03/02/2022 10:29:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=369
03/02/2022 10:29:36 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.2770765457607331 on epoch=369
03/02/2022 10:29:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=371
03/02/2022 10:29:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=373
03/02/2022 10:29:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=375
03/02/2022 10:29:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=377
03/02/2022 10:29:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=379
03/02/2022 10:29:50 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.31543460295995795 on epoch=379
03/02/2022 10:29:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=381
03/02/2022 10:29:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=383
03/02/2022 10:29:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=385
03/02/2022 10:29:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=387
03/02/2022 10:30:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=389
03/02/2022 10:30:04 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.25444740221767626 on epoch=389
03/02/2022 10:30:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=391
03/02/2022 10:30:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=393
03/02/2022 10:30:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=395
03/02/2022 10:30:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=397
03/02/2022 10:30:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=399
03/02/2022 10:30:18 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.2910444637625165 on epoch=399
03/02/2022 10:30:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=401
03/02/2022 10:30:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=403
03/02/2022 10:30:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=405
03/02/2022 10:30:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=407
03/02/2022 10:30:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=409
03/02/2022 10:30:32 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.28555174896559504 on epoch=409
03/02/2022 10:30:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=411
03/02/2022 10:30:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=413
03/02/2022 10:30:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=415
03/02/2022 10:30:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=417
03/02/2022 10:30:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=419
03/02/2022 10:30:46 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.271270587649898 on epoch=419
03/02/2022 10:30:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=421
03/02/2022 10:30:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=423
03/02/2022 10:30:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=425
03/02/2022 10:30:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=427
03/02/2022 10:30:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=429
03/02/2022 10:31:00 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.2162965054269402 on epoch=429
03/02/2022 10:31:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=431
03/02/2022 10:31:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=433
03/02/2022 10:31:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=435
03/02/2022 10:31:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=437
03/02/2022 10:31:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=439
03/02/2022 10:31:14 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.15783363729390715 on epoch=439
03/02/2022 10:31:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=441
03/02/2022 10:31:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=443
03/02/2022 10:31:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=445
03/02/2022 10:31:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=447
03/02/2022 10:31:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=449
03/02/2022 10:31:29 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.17234858020271765 on epoch=449
03/02/2022 10:31:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=451
03/02/2022 10:31:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=453
03/02/2022 10:31:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=455
03/02/2022 10:31:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=457
03/02/2022 10:31:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=459
03/02/2022 10:31:43 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.23310443168599196 on epoch=459
03/02/2022 10:31:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=461
03/02/2022 10:31:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=463
03/02/2022 10:31:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=465
03/02/2022 10:31:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=467
03/02/2022 10:31:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=469
03/02/2022 10:31:57 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.2549409588302932 on epoch=469
03/02/2022 10:31:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=471
03/02/2022 10:32:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=473
03/02/2022 10:32:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=475
03/02/2022 10:32:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=477
03/02/2022 10:32:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=479
03/02/2022 10:32:11 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.1909793442149357 on epoch=479
03/02/2022 10:32:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=481
03/02/2022 10:32:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=483
03/02/2022 10:32:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=485
03/02/2022 10:32:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=487
03/02/2022 10:32:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=489
03/02/2022 10:32:24 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.21524795543571132 on epoch=489
03/02/2022 10:32:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=491
03/02/2022 10:32:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=493
03/02/2022 10:32:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=495
03/02/2022 10:32:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=497
03/02/2022 10:32:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=499
03/02/2022 10:32:38 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.18374719909246762 on epoch=499
03/02/2022 10:32:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=501
03/02/2022 10:32:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=503
03/02/2022 10:32:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=505
03/02/2022 10:32:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=507
03/02/2022 10:32:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=509
03/02/2022 10:32:53 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.23147940366734193 on epoch=509
03/02/2022 10:32:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=511
03/02/2022 10:32:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=513
03/02/2022 10:32:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=515
03/02/2022 10:33:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=517
03/02/2022 10:33:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=519
03/02/2022 10:33:07 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.14878708159361181 on epoch=519
03/02/2022 10:33:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=521
03/02/2022 10:33:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=523
03/02/2022 10:33:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=525
03/02/2022 10:33:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=527
03/02/2022 10:33:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=529
03/02/2022 10:33:21 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.21137333306620565 on epoch=529
03/02/2022 10:33:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=531
03/02/2022 10:33:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=533
03/02/2022 10:33:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=535
03/02/2022 10:33:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=537
03/02/2022 10:33:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=539
03/02/2022 10:33:35 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.2355874011046425 on epoch=539
03/02/2022 10:33:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=541
03/02/2022 10:33:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=543
03/02/2022 10:33:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=545
03/02/2022 10:33:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=547
03/02/2022 10:33:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=549
03/02/2022 10:33:49 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.1884132698172107 on epoch=549
03/02/2022 10:33:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=551
03/02/2022 10:33:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=553
03/02/2022 10:33:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=555
03/02/2022 10:33:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=557
03/02/2022 10:34:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=559
03/02/2022 10:34:04 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.2286504799548278 on epoch=559
03/02/2022 10:34:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=561
03/02/2022 10:34:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=563
03/02/2022 10:34:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=565
03/02/2022 10:34:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=567
03/02/2022 10:34:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=569
03/02/2022 10:34:18 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.25421489621489624 on epoch=569
03/02/2022 10:34:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=571
03/02/2022 10:34:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=573
03/02/2022 10:34:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=575
03/02/2022 10:34:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=577
03/02/2022 10:34:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=579
03/02/2022 10:34:32 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.30811766864170764 on epoch=579
03/02/2022 10:34:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=581
03/02/2022 10:34:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=583
03/02/2022 10:34:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=585
03/02/2022 10:34:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=587
03/02/2022 10:34:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=589
03/02/2022 10:34:47 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2928937328937329 on epoch=589
03/02/2022 10:34:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=591
03/02/2022 10:34:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=593
03/02/2022 10:34:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=595
03/02/2022 10:34:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=597
03/02/2022 10:34:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=599
03/02/2022 10:34:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:34:59 - INFO - __main__ - Printing 3 examples
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 10:34:59 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:34:59 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 10:34:59 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:34:59 - INFO - __main__ - Printing 3 examples
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 10:34:59 - INFO - __main__ - ['No']
03/02/2022 10:34:59 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:34:59 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:34:59 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 10:35:01 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.2160705960705961 on epoch=599
03/02/2022 10:35:01 - INFO - __main__ - save last model!
03/02/2022 10:35:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 10:35:01 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 10:35:01 - INFO - __main__ - Printing 3 examples
03/02/2022 10:35:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 10:35:01 - INFO - __main__ - ['No']
03/02/2022 10:35:01 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 10:35:01 - INFO - __main__ - ['Yes']
03/02/2022 10:35:01 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 10:35:01 - INFO - __main__ - ['Yes']
03/02/2022 10:35:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:35:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:35:11 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 10:35:13 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:35:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:35:14 - INFO - __main__ - Starting training!
03/02/2022 10:39:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_87_0.3_8_predictions.txt
03/02/2022 10:39:33 - INFO - __main__ - Classification-F1 on test data: 0.0167
03/02/2022 10:39:33 - INFO - __main__ - prefix=circa_16_87, lr=0.3, bsz=8, dev_performance=0.39789399624765476, test_performance=0.016692521741944574
03/02/2022 10:39:33 - INFO - __main__ - Running ... prefix=circa_16_87, lr=0.2, bsz=8 ...
03/02/2022 10:39:34 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:39:34 - INFO - __main__ - Printing 3 examples
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: X wants to know what activities Y likes to do during weekends. [SEP] question X: Have you ever been water skiing? [SEP] answer Y: I went tubing one summer.
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Would you like to go to the vegan restaurant? [SEP] answer Y: I love eating dairy.
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: X and Y are colleagues who are leaving work on a Friday at the same time. [SEP] question X: Are you coming to work tomorrow? [SEP] answer Y: I never come to work on Saturdays!
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:39:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:39:34 - INFO - __main__ - Loaded 80 examples from train data
use DistributedSampler
03/02/2022 10:39:34 - INFO - __main__ - Start tokenizing ... 80 instances
03/02/2022 10:39:34 - INFO - __main__ - Printing 3 examples
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Is it within walking distance of work? [SEP] answer Y: It's a subway ride to most places.
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: X wants to know about Y's music preferences. [SEP] question X: Do you still like listening to blues? [SEP] answer Y: I don't really anymore
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ -  [circa] context: Y has just told X that he/she is thinking of buying a flat in New York. [SEP] question X: Will you be on the top floor? [SEP] answer Y: The flat is located on the first floor.
03/02/2022 10:39:34 - INFO - __main__ - ['No']
03/02/2022 10:39:34 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:39:34 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:39:34 - INFO - __main__ - Loaded 80 examples from dev data
03/02/2022 10:39:48 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:39:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:39:49 - INFO - __main__ - Starting training!
03/02/2022 10:39:54 - INFO - __main__ - Step 10 Global step 10 Train loss 4.03 on epoch=1
03/02/2022 10:39:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.33 on epoch=3
03/02/2022 10:39:59 - INFO - __main__ - Step 30 Global step 30 Train loss 2.81 on epoch=5
03/02/2022 10:40:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.38 on epoch=7
03/02/2022 10:40:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.97 on epoch=9
03/02/2022 10:40:04 - INFO - __main__ - Global step 50 Train loss 2.90 Classification-F1 0.06666666666666668 on epoch=9
03/02/2022 10:40:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.06666666666666668 on epoch=9, global_step=50
03/02/2022 10:40:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.62 on epoch=11
03/02/2022 10:40:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.40 on epoch=13
03/02/2022 10:40:11 - INFO - __main__ - Step 80 Global step 80 Train loss 1.23 on epoch=15
03/02/2022 10:40:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=17
03/02/2022 10:40:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=19
03/02/2022 10:40:18 - INFO - __main__ - Global step 100 Train loss 1.27 Classification-F1 0.06666666666666668 on epoch=19
03/02/2022 10:40:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=21
03/02/2022 10:40:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=23
03/02/2022 10:40:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.75 on epoch=25
03/02/2022 10:40:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=27
03/02/2022 10:40:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.68 on epoch=29
03/02/2022 10:40:32 - INFO - __main__ - Global step 150 Train loss 0.76 Classification-F1 0.10151515151515152 on epoch=29
03/02/2022 10:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.06666666666666668 -> 0.10151515151515152 on epoch=29, global_step=150
03/02/2022 10:40:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=31
03/02/2022 10:40:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=33
03/02/2022 10:40:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=35
03/02/2022 10:40:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=37
03/02/2022 10:40:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=39
03/02/2022 10:40:46 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.06666666666666668 on epoch=39
03/02/2022 10:40:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.60 on epoch=41
03/02/2022 10:40:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=43
03/02/2022 10:40:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=45
03/02/2022 10:40:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=47
03/02/2022 10:40:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=49
03/02/2022 10:41:00 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.13111111111111112 on epoch=49
03/02/2022 10:41:00 - INFO - __main__ - Saving model with best Classification-F1: 0.10151515151515152 -> 0.13111111111111112 on epoch=49, global_step=250
03/02/2022 10:41:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=51
03/02/2022 10:41:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=53
03/02/2022 10:41:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=55
03/02/2022 10:41:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=57
03/02/2022 10:41:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=59
03/02/2022 10:41:14 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.05977011494252874 on epoch=59
03/02/2022 10:41:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=61
03/02/2022 10:41:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=63
03/02/2022 10:41:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=65
03/02/2022 10:41:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=67
03/02/2022 10:41:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=69
03/02/2022 10:41:27 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.06666666666666668 on epoch=69
03/02/2022 10:41:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=71
03/02/2022 10:41:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=73
03/02/2022 10:41:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=75
03/02/2022 10:41:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=77
03/02/2022 10:41:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=79
03/02/2022 10:41:41 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.1049062049062049 on epoch=79
03/02/2022 10:41:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=81
03/02/2022 10:41:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=83
03/02/2022 10:41:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=85
03/02/2022 10:41:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=87
03/02/2022 10:41:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=89
03/02/2022 10:41:55 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.08126696832579186 on epoch=89
03/02/2022 10:41:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=91
03/02/2022 10:41:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=93
03/02/2022 10:42:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=95
03/02/2022 10:42:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=97
03/02/2022 10:42:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=99
03/02/2022 10:42:08 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.13725752508361205 on epoch=99
03/02/2022 10:42:08 - INFO - __main__ - Saving model with best Classification-F1: 0.13111111111111112 -> 0.13725752508361205 on epoch=99, global_step=500
03/02/2022 10:42:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=101
03/02/2022 10:42:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=103
03/02/2022 10:42:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=105
03/02/2022 10:42:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=107
03/02/2022 10:42:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=109
03/02/2022 10:42:22 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.1855632183908046 on epoch=109
03/02/2022 10:42:22 - INFO - __main__ - Saving model with best Classification-F1: 0.13725752508361205 -> 0.1855632183908046 on epoch=109, global_step=550
03/02/2022 10:42:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=111
03/02/2022 10:42:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=113
03/02/2022 10:42:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=115
03/02/2022 10:42:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=117
03/02/2022 10:42:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=119
03/02/2022 10:42:35 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.21875862068965515 on epoch=119
03/02/2022 10:42:35 - INFO - __main__ - Saving model with best Classification-F1: 0.1855632183908046 -> 0.21875862068965515 on epoch=119, global_step=600
03/02/2022 10:42:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=121
03/02/2022 10:42:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=123
03/02/2022 10:42:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=125
03/02/2022 10:42:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=127
03/02/2022 10:42:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=129
03/02/2022 10:42:49 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.13386645341863254 on epoch=129
03/02/2022 10:42:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=131
03/02/2022 10:42:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=133
03/02/2022 10:42:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=135
03/02/2022 10:42:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=137
03/02/2022 10:43:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=139
03/02/2022 10:43:03 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.16436918201624084 on epoch=139
03/02/2022 10:43:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=141
03/02/2022 10:43:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=143
03/02/2022 10:43:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=145
03/02/2022 10:43:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=147
03/02/2022 10:43:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=149
03/02/2022 10:43:17 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2725346325139637 on epoch=149
03/02/2022 10:43:17 - INFO - __main__ - Saving model with best Classification-F1: 0.21875862068965515 -> 0.2725346325139637 on epoch=149, global_step=750
03/02/2022 10:43:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=151
03/02/2022 10:43:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=153
03/02/2022 10:43:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=155
03/02/2022 10:43:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=157
03/02/2022 10:43:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=159
03/02/2022 10:43:31 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.17460570618465357 on epoch=159
03/02/2022 10:43:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=161
03/02/2022 10:43:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=163
03/02/2022 10:43:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=165
03/02/2022 10:43:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=167
03/02/2022 10:43:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=169
03/02/2022 10:43:45 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.16931818181818184 on epoch=169
03/02/2022 10:43:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=171
03/02/2022 10:43:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=173
03/02/2022 10:43:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=175
03/02/2022 10:43:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=177
03/02/2022 10:43:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=179
03/02/2022 10:43:59 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.16756756756756758 on epoch=179
03/02/2022 10:44:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=181
03/02/2022 10:44:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=183
03/02/2022 10:44:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=185
03/02/2022 10:44:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=187
03/02/2022 10:44:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=189
03/02/2022 10:44:14 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.2202870189171559 on epoch=189
03/02/2022 10:44:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=191
03/02/2022 10:44:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=193
03/02/2022 10:44:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=195
03/02/2022 10:44:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=197
03/02/2022 10:44:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=199
03/02/2022 10:44:28 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.2885714285714286 on epoch=199
03/02/2022 10:44:28 - INFO - __main__ - Saving model with best Classification-F1: 0.2725346325139637 -> 0.2885714285714286 on epoch=199, global_step=1000
03/02/2022 10:44:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=201
03/02/2022 10:44:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=203
03/02/2022 10:44:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=205
03/02/2022 10:44:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=207
03/02/2022 10:44:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=209
03/02/2022 10:44:42 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.26845816657137417 on epoch=209
03/02/2022 10:44:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=211
03/02/2022 10:44:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=213
03/02/2022 10:44:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=215
03/02/2022 10:44:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=217
03/02/2022 10:44:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=219
03/02/2022 10:44:56 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.2786956521739131 on epoch=219
03/02/2022 10:44:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=221
03/02/2022 10:45:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=223
03/02/2022 10:45:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=225
03/02/2022 10:45:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=227
03/02/2022 10:45:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=229
03/02/2022 10:45:09 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.28942446339554234 on epoch=229
03/02/2022 10:45:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2885714285714286 -> 0.28942446339554234 on epoch=229, global_step=1150
03/02/2022 10:45:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=231
03/02/2022 10:45:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=233
03/02/2022 10:45:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=235
03/02/2022 10:45:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=237
03/02/2022 10:45:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=239
03/02/2022 10:45:22 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.3359307359307359 on epoch=239
03/02/2022 10:45:23 - INFO - __main__ - Saving model with best Classification-F1: 0.28942446339554234 -> 0.3359307359307359 on epoch=239, global_step=1200
03/02/2022 10:45:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=241
03/02/2022 10:45:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=243
03/02/2022 10:45:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=245
03/02/2022 10:45:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=247
03/02/2022 10:45:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=249
03/02/2022 10:45:36 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.3524662529588638 on epoch=249
03/02/2022 10:45:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3359307359307359 -> 0.3524662529588638 on epoch=249, global_step=1250
03/02/2022 10:45:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=251
03/02/2022 10:45:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=253
03/02/2022 10:45:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=255
03/02/2022 10:45:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=257
03/02/2022 10:45:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=259
03/02/2022 10:45:50 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.37759856630824373 on epoch=259
03/02/2022 10:45:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3524662529588638 -> 0.37759856630824373 on epoch=259, global_step=1300
03/02/2022 10:45:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=261
03/02/2022 10:45:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=263
03/02/2022 10:45:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=265
03/02/2022 10:45:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=267
03/02/2022 10:46:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=269
03/02/2022 10:46:04 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.24755920408094323 on epoch=269
03/02/2022 10:46:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=271
03/02/2022 10:46:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=273
03/02/2022 10:46:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=275
03/02/2022 10:46:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=277
03/02/2022 10:46:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=279
03/02/2022 10:46:17 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.23762337581209395 on epoch=279
03/02/2022 10:46:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=281
03/02/2022 10:46:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=283
03/02/2022 10:46:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=285
03/02/2022 10:46:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=287
03/02/2022 10:46:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=289
03/02/2022 10:46:31 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.274325860948668 on epoch=289
03/02/2022 10:46:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=291
03/02/2022 10:46:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=293
03/02/2022 10:46:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=295
03/02/2022 10:46:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=297
03/02/2022 10:46:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=299
03/02/2022 10:46:45 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.2509618705671623 on epoch=299
03/02/2022 10:46:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=301
03/02/2022 10:46:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=303
03/02/2022 10:46:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=305
03/02/2022 10:46:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=307
03/02/2022 10:46:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=309
03/02/2022 10:46:58 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.27555555555555555 on epoch=309
03/02/2022 10:47:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=311
03/02/2022 10:47:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=313
03/02/2022 10:47:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=315
03/02/2022 10:47:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=317
03/02/2022 10:47:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=319
03/02/2022 10:47:11 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.25338472590627764 on epoch=319
03/02/2022 10:47:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=321
03/02/2022 10:47:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=323
03/02/2022 10:47:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=325
03/02/2022 10:47:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=327
03/02/2022 10:47:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=329
03/02/2022 10:47:25 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.2562850729517396 on epoch=329
03/02/2022 10:47:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=331
03/02/2022 10:47:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=333
03/02/2022 10:47:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=335
03/02/2022 10:47:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=337
03/02/2022 10:47:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=339
03/02/2022 10:47:38 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.18429824561403507 on epoch=339
03/02/2022 10:47:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=341
03/02/2022 10:47:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=343
03/02/2022 10:47:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=345
03/02/2022 10:47:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=347
03/02/2022 10:47:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=349
03/02/2022 10:47:52 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.23097396335583412 on epoch=349
03/02/2022 10:47:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=351
03/02/2022 10:47:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=353
03/02/2022 10:47:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=355
03/02/2022 10:48:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=357
03/02/2022 10:48:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=359
03/02/2022 10:48:05 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.21905902773533478 on epoch=359
03/02/2022 10:48:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=361
03/02/2022 10:48:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=363
03/02/2022 10:48:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=365
03/02/2022 10:48:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=367
03/02/2022 10:48:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=369
03/02/2022 10:48:19 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.18521634615384613 on epoch=369
03/02/2022 10:48:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=371
03/02/2022 10:48:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=373
03/02/2022 10:48:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=375
03/02/2022 10:48:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=377
03/02/2022 10:48:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=379
03/02/2022 10:48:33 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.27476239976239974 on epoch=379
03/02/2022 10:48:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=381
03/02/2022 10:48:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=383
03/02/2022 10:48:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=385
03/02/2022 10:48:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=387
03/02/2022 10:48:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=389
03/02/2022 10:48:47 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.17957219251336898 on epoch=389
03/02/2022 10:48:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=391
03/02/2022 10:48:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=393
03/02/2022 10:48:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=395
03/02/2022 10:48:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=397
03/02/2022 10:48:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=399
03/02/2022 10:49:01 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.25347300274243584 on epoch=399
03/02/2022 10:49:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=401
03/02/2022 10:49:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=403
03/02/2022 10:49:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=405
03/02/2022 10:49:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=407
03/02/2022 10:49:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=409
03/02/2022 10:49:15 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.191954736031488 on epoch=409
03/02/2022 10:49:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=411
03/02/2022 10:49:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=413
03/02/2022 10:49:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=415
03/02/2022 10:49:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=417
03/02/2022 10:49:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=419
03/02/2022 10:49:29 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.21840659340659338 on epoch=419
03/02/2022 10:49:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=421
03/02/2022 10:49:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=423
03/02/2022 10:49:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=425
03/02/2022 10:49:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=427
03/02/2022 10:49:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=429
03/02/2022 10:49:43 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.17786822231266677 on epoch=429
03/02/2022 10:49:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=431
03/02/2022 10:49:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=433
03/02/2022 10:49:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=435
03/02/2022 10:49:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=437
03/02/2022 10:49:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=439
03/02/2022 10:49:57 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.2563563887093299 on epoch=439
03/02/2022 10:49:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=441
03/02/2022 10:50:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=443
03/02/2022 10:50:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=445
03/02/2022 10:50:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=447
03/02/2022 10:50:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=449
03/02/2022 10:50:11 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.19635789307431098 on epoch=449
03/02/2022 10:50:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=451
03/02/2022 10:50:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=453
03/02/2022 10:50:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=455
03/02/2022 10:50:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=457
03/02/2022 10:50:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=459
03/02/2022 10:50:25 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.19169973544973545 on epoch=459
03/02/2022 10:50:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=461
03/02/2022 10:50:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=463
03/02/2022 10:50:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=465
03/02/2022 10:50:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=467
03/02/2022 10:50:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=469
03/02/2022 10:50:39 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.12424242424242425 on epoch=469
03/02/2022 10:50:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=471
03/02/2022 10:50:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=473
03/02/2022 10:50:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=475
03/02/2022 10:50:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=477
03/02/2022 10:50:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=479
03/02/2022 10:50:53 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.1703185703185703 on epoch=479
03/02/2022 10:50:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=481
03/02/2022 10:50:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=483
03/02/2022 10:51:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=485
03/02/2022 10:51:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=487
03/02/2022 10:51:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=489
03/02/2022 10:51:07 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.17211900425015178 on epoch=489
03/02/2022 10:51:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=491
03/02/2022 10:51:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=493
03/02/2022 10:51:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=495
03/02/2022 10:51:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=497
03/02/2022 10:51:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=499
03/02/2022 10:51:21 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.1640372536278969 on epoch=499
03/02/2022 10:51:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=501
03/02/2022 10:51:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=503
03/02/2022 10:51:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=505
03/02/2022 10:51:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=507
03/02/2022 10:51:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=509
03/02/2022 10:51:36 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.17032217380043463 on epoch=509
03/02/2022 10:51:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=511
03/02/2022 10:51:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=513
03/02/2022 10:51:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=515
03/02/2022 10:51:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=517
03/02/2022 10:51:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=519
03/02/2022 10:51:50 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.22002566399118123 on epoch=519
03/02/2022 10:51:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=521
03/02/2022 10:51:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=523
03/02/2022 10:51:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=525
03/02/2022 10:51:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=527
03/02/2022 10:52:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=529
03/02/2022 10:52:05 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.1312021312021312 on epoch=529
03/02/2022 10:52:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=531
03/02/2022 10:52:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=533
03/02/2022 10:52:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=535
03/02/2022 10:52:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=537
03/02/2022 10:52:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=539
03/02/2022 10:52:19 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.17810484433896412 on epoch=539
03/02/2022 10:52:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=541
03/02/2022 10:52:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=543
03/02/2022 10:52:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=545
03/02/2022 10:52:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=547
03/02/2022 10:52:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=549
03/02/2022 10:52:33 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.16503842096045993 on epoch=549
03/02/2022 10:52:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=551
03/02/2022 10:52:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=553
03/02/2022 10:52:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=555
03/02/2022 10:52:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=557
03/02/2022 10:52:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=559
03/02/2022 10:52:47 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.1545877192982456 on epoch=559
03/02/2022 10:52:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=561
03/02/2022 10:52:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=563
03/02/2022 10:52:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=565
03/02/2022 10:52:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=567
03/02/2022 10:52:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=569
03/02/2022 10:53:02 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.19025186636024077 on epoch=569
03/02/2022 10:53:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=571
03/02/2022 10:53:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=573
03/02/2022 10:53:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=575
03/02/2022 10:53:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=577
03/02/2022 10:53:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=579
03/02/2022 10:53:16 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.1598742236024845 on epoch=579
03/02/2022 10:53:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=581
03/02/2022 10:53:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=583
03/02/2022 10:53:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=585
03/02/2022 10:53:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=587
03/02/2022 10:53:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=589
03/02/2022 10:53:31 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.1634625447812261 on epoch=589
03/02/2022 10:53:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=591
03/02/2022 10:53:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=593
03/02/2022 10:53:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=595
03/02/2022 10:53:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=597
03/02/2022 10:53:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=599
03/02/2022 10:53:45 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.14597360311646027 on epoch=599
03/02/2022 10:53:45 - INFO - __main__ - save last model!
03/02/2022 10:53:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 10:53:45 - INFO - __main__ - Start tokenizing ... 6700 instances
03/02/2022 10:53:45 - INFO - __main__ - Printing 3 examples
03/02/2022 10:53:45 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: Do you like kids? [SEP] answer Y: Kids are pretty awful.
03/02/2022 10:53:45 - INFO - __main__ - ['No']
03/02/2022 10:53:45 - INFO - __main__ -  [circa] context: X wants to know about Y's food preferences. [SEP] question X: Do you like buffets? [SEP] answer Y: They're one of my favorite types of restaurants.
03/02/2022 10:53:45 - INFO - __main__ - ['Yes']
03/02/2022 10:53:45 - INFO - __main__ -  [circa] context: Y has just told X that he/she is considering switching his/her job. [SEP] question X: You know when you starting your new job? [SEP] answer Y: I think its next week
03/02/2022 10:53:45 - INFO - __main__ - ['Yes']
03/02/2022 10:53:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:53:48 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:53:55 - INFO - __main__ - Loaded 6700 examples from test data
03/02/2022 10:58:23 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-circa/circa_16_87_0.2_8_predictions.txt
03/02/2022 10:58:24 - INFO - __main__ - Classification-F1 on test data: 0.0128
03/02/2022 10:58:24 - INFO - __main__ - prefix=circa_16_87, lr=0.2, bsz=8, dev_performance=0.37759856630824373, test_performance=0.012809175510985734
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.002719879150390625 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "3459", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 22117, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "3460", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 22117, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 22117, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (3484): No such process
kill: write error: Disk quota exceeded
Task: wiki_split, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_0/1/error.json
03/02/2022 10:58:29 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
Output directory () already exists and is not empty.
03/02/2022 10:58:29 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 10:58:29 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 10:58:29 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 10:58:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 10:58:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 10:58:29 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/02/2022 10:58:29 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/02/2022 10:58:29 - INFO - __main__ - args.device: cuda:0
03/02/2022 10:58:29 - INFO - __main__ - args.device: cuda:1
03/02/2022 10:58:29 - INFO - __main__ - Using 2 gpus
03/02/2022 10:58:29 - INFO - __main__ - Using 2 gpus
03/02/2022 10:58:30 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
03/02/2022 10:58:30 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/02/2022 10:58:34 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.5, bsz=8 ...
03/02/2022 10:58:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 10:58:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 10:58:35 - INFO - __main__ - Printing 3 examples
03/02/2022 10:58:35 - INFO - __main__ - Printing 3 examples
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 10:58:35 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 10:58:35 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 10:58:35 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 10:58:35 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 10:58:35 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 10:58:35 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:58:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 10:58:35 - INFO - __main__ - Loaded 32 examples from train data
03/02/2022 10:58:35 - INFO - __main__ - Start tokenizing ... 32 instances
use DistributedSampler
03/02/2022 10:58:35 - INFO - __main__ - Printing 3 examples
03/02/2022 10:58:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 10:58:35 - INFO - __main__ - Printing 3 examples
03/02/2022 10:58:35 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 10:58:35 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 10:58:35 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 10:58:35 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 10:58:35 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 10:58:35 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:58:35 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Input ...
03/02/2022 10:58:35 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:58:36 - INFO - __main__ - Tokenizing Output ...
03/02/2022 10:58:36 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 10:58:36 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 10:58:50 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:58:50 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 10:58:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:58:52 - INFO - __main__ - Starting training!
03/02/2022 10:58:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 10:58:57 - INFO - __main__ - Starting training!
03/02/2022 10:59:00 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=4
03/02/2022 10:59:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.71 on epoch=9
03/02/2022 10:59:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=14
03/02/2022 10:59:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=19
03/02/2022 10:59:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/02/2022 10:59:20 - INFO - __main__ - Global step 50 Train loss 0.68 Rouge-L 0.7781286102544258 on epoch=24
03/02/2022 10:59:20 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.7781286102544258 on epoch=24, global_step=50
03/02/2022 10:59:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=29
03/02/2022 10:59:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
03/02/2022 10:59:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=39
03/02/2022 10:59:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
03/02/2022 10:59:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
03/02/2022 10:59:43 - INFO - __main__ - Global step 100 Train loss 0.54 Rouge-L 0.8538303344628733 on epoch=49
03/02/2022 10:59:43 - INFO - __main__ - Saving model with best Rouge-L: 0.7781286102544258 -> 0.8538303344628733 on epoch=49, global_step=100
03/02/2022 10:59:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
03/02/2022 10:59:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
03/02/2022 10:59:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
03/02/2022 10:59:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=69
03/02/2022 10:59:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
03/02/2022 11:00:06 - INFO - __main__ - Global step 150 Train loss 0.48 Rouge-L 0.8642392035516904 on epoch=74
03/02/2022 11:00:06 - INFO - __main__ - Saving model with best Rouge-L: 0.8538303344628733 -> 0.8642392035516904 on epoch=74, global_step=150
03/02/2022 11:00:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
03/02/2022 11:00:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
03/02/2022 11:00:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
03/02/2022 11:00:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
03/02/2022 11:00:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
03/02/2022 11:00:28 - INFO - __main__ - Global step 200 Train loss 0.44 Rouge-L 0.8741078704048544 on epoch=99
03/02/2022 11:00:28 - INFO - __main__ - Saving model with best Rouge-L: 0.8642392035516904 -> 0.8741078704048544 on epoch=99, global_step=200
03/02/2022 11:00:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
03/02/2022 11:00:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
03/02/2022 11:00:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=114
03/02/2022 11:00:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
03/02/2022 11:00:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
03/02/2022 11:00:50 - INFO - __main__ - Global step 250 Train loss 0.40 Rouge-L 0.8657543208994521 on epoch=124
03/02/2022 11:00:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
03/02/2022 11:00:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
03/02/2022 11:00:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/02/2022 11:00:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
03/02/2022 11:01:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/02/2022 11:01:12 - INFO - __main__ - Global step 300 Train loss 0.37 Rouge-L 0.8726547057813338 on epoch=149
03/02/2022 11:01:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
03/02/2022 11:01:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
03/02/2022 11:01:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
03/02/2022 11:01:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
03/02/2022 11:01:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
03/02/2022 11:01:35 - INFO - __main__ - Global step 350 Train loss 0.34 Rouge-L 0.8632374286056811 on epoch=174
03/02/2022 11:01:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=179
03/02/2022 11:01:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
03/02/2022 11:01:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=189
03/02/2022 11:01:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
03/02/2022 11:01:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
03/02/2022 11:01:57 - INFO - __main__ - Global step 400 Train loss 0.32 Rouge-L 0.8777851518906259 on epoch=199
03/02/2022 11:01:57 - INFO - __main__ - Saving model with best Rouge-L: 0.8741078704048544 -> 0.8777851518906259 on epoch=199, global_step=400
03/02/2022 11:01:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=204
03/02/2022 11:02:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
03/02/2022 11:02:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
03/02/2022 11:02:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=219
03/02/2022 11:02:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=224
03/02/2022 11:02:19 - INFO - __main__ - Global step 450 Train loss 0.29 Rouge-L 0.8662379627530266 on epoch=224
03/02/2022 11:02:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
03/02/2022 11:02:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
03/02/2022 11:02:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/02/2022 11:02:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
03/02/2022 11:02:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/02/2022 11:02:42 - INFO - __main__ - Global step 500 Train loss 0.27 Rouge-L 0.8635126243339849 on epoch=249
03/02/2022 11:02:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
03/02/2022 11:02:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/02/2022 11:02:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/02/2022 11:02:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/02/2022 11:02:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/02/2022 11:03:04 - INFO - __main__ - Global step 550 Train loss 0.26 Rouge-L 0.8720432620087791 on epoch=274
03/02/2022 11:03:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=279
03/02/2022 11:03:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/02/2022 11:03:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/02/2022 11:03:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
03/02/2022 11:03:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/02/2022 11:03:25 - INFO - __main__ - Global step 600 Train loss 0.24 Rouge-L 0.8476942703705852 on epoch=299
03/02/2022 11:03:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
03/02/2022 11:03:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/02/2022 11:03:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
03/02/2022 11:03:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
03/02/2022 11:03:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=324
03/02/2022 11:03:48 - INFO - __main__ - Global step 650 Train loss 0.23 Rouge-L 0.8680862915380182 on epoch=324
03/02/2022 11:03:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
03/02/2022 11:03:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
03/02/2022 11:03:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
03/02/2022 11:03:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
03/02/2022 11:03:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
03/02/2022 11:04:09 - INFO - __main__ - Global step 700 Train loss 0.21 Rouge-L 0.8543279804409483 on epoch=349
03/02/2022 11:04:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=354
03/02/2022 11:04:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=359
03/02/2022 11:04:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
03/02/2022 11:04:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
03/02/2022 11:04:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
03/02/2022 11:04:32 - INFO - __main__ - Global step 750 Train loss 0.21 Rouge-L 0.8706292499207532 on epoch=374
03/02/2022 11:04:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
03/02/2022 11:04:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/02/2022 11:04:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
03/02/2022 11:04:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
03/02/2022 11:04:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
03/02/2022 11:04:55 - INFO - __main__ - Global step 800 Train loss 0.20 Rouge-L 0.8585620742591544 on epoch=399
03/02/2022 11:04:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
03/02/2022 11:04:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=409
03/02/2022 11:05:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
03/02/2022 11:05:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=419
03/02/2022 11:05:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
03/02/2022 11:05:17 - INFO - __main__ - Global step 850 Train loss 0.18 Rouge-L 0.8702837935832088 on epoch=424
03/02/2022 11:05:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
03/02/2022 11:05:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
03/02/2022 11:05:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/02/2022 11:05:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=444
03/02/2022 11:05:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=449
03/02/2022 11:05:39 - INFO - __main__ - Global step 900 Train loss 0.17 Rouge-L 0.8726947553576883 on epoch=449
03/02/2022 11:05:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
03/02/2022 11:05:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
03/02/2022 11:05:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=464
03/02/2022 11:05:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
03/02/2022 11:05:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/02/2022 11:06:02 - INFO - __main__ - Global step 950 Train loss 0.17 Rouge-L 0.8793965007064838 on epoch=474
03/02/2022 11:06:02 - INFO - __main__ - Saving model with best Rouge-L: 0.8777851518906259 -> 0.8793965007064838 on epoch=474, global_step=950
03/02/2022 11:06:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
03/02/2022 11:06:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
03/02/2022 11:06:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=489
03/02/2022 11:06:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
03/02/2022 11:06:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
03/02/2022 11:06:24 - INFO - __main__ - Global step 1000 Train loss 0.16 Rouge-L 0.8765402925804338 on epoch=499
03/02/2022 11:06:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
03/02/2022 11:06:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
03/02/2022 11:06:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=514
03/02/2022 11:06:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
03/02/2022 11:06:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=524
03/02/2022 11:06:47 - INFO - __main__ - Global step 1050 Train loss 0.15 Rouge-L 0.8796073939007538 on epoch=524
03/02/2022 11:06:47 - INFO - __main__ - Saving model with best Rouge-L: 0.8793965007064838 -> 0.8796073939007538 on epoch=524, global_step=1050
03/02/2022 11:06:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=529
03/02/2022 11:06:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
03/02/2022 11:06:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=539
03/02/2022 11:06:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=544
03/02/2022 11:06:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
03/02/2022 11:07:10 - INFO - __main__ - Global step 1100 Train loss 0.15 Rouge-L 0.870766816533314 on epoch=549
03/02/2022 11:07:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=554
03/02/2022 11:07:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=559
03/02/2022 11:07:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=564
03/02/2022 11:07:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=569
03/02/2022 11:07:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=574
03/02/2022 11:07:32 - INFO - __main__ - Global step 1150 Train loss 0.14 Rouge-L 0.8742813926478841 on epoch=574
03/02/2022 11:07:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=579
03/02/2022 11:07:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=584
03/02/2022 11:07:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
03/02/2022 11:07:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
03/02/2022 11:07:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=599
03/02/2022 11:07:55 - INFO - __main__ - Global step 1200 Train loss 0.13 Rouge-L 0.8703243321315595 on epoch=599
03/02/2022 11:07:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
03/02/2022 11:07:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=609
03/02/2022 11:08:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=614
03/02/2022 11:08:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=619
03/02/2022 11:08:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=624
03/02/2022 11:08:17 - INFO - __main__ - Global step 1250 Train loss 0.13 Rouge-L 0.8810300663949966 on epoch=624
03/02/2022 11:08:17 - INFO - __main__ - Saving model with best Rouge-L: 0.8796073939007538 -> 0.8810300663949966 on epoch=624, global_step=1250
03/02/2022 11:08:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=629
03/02/2022 11:08:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=634
03/02/2022 11:08:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=639
03/02/2022 11:08:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=644
03/02/2022 11:08:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=649
03/02/2022 11:08:40 - INFO - __main__ - Global step 1300 Train loss 0.12 Rouge-L 0.8777935641404695 on epoch=649
03/02/2022 11:08:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=654
03/02/2022 11:08:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=659
03/02/2022 11:08:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=664
03/02/2022 11:08:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=669
03/02/2022 11:08:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=674
03/02/2022 11:09:02 - INFO - __main__ - Global step 1350 Train loss 0.11 Rouge-L 0.8629073472006705 on epoch=674
03/02/2022 11:09:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=679
03/02/2022 11:09:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=684
03/02/2022 11:09:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=689
03/02/2022 11:09:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=694
03/02/2022 11:09:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=699
03/02/2022 11:09:25 - INFO - __main__ - Global step 1400 Train loss 0.11 Rouge-L 0.8709948117432789 on epoch=699
03/02/2022 11:09:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=704
03/02/2022 11:09:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=709
03/02/2022 11:09:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=714
03/02/2022 11:09:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=719
03/02/2022 11:09:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=724
03/02/2022 11:09:47 - INFO - __main__ - Global step 1450 Train loss 0.10 Rouge-L 0.8779954322247983 on epoch=724
03/02/2022 11:09:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=729
03/02/2022 11:09:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=734
03/02/2022 11:09:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=739
03/02/2022 11:09:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
03/02/2022 11:09:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
03/02/2022 11:10:10 - INFO - __main__ - Global step 1500 Train loss 0.09 Rouge-L 0.8554228422578504 on epoch=749
03/02/2022 11:10:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=754
03/02/2022 11:10:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
03/02/2022 11:10:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=764
03/02/2022 11:10:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=769
03/02/2022 11:10:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
03/02/2022 11:10:34 - INFO - __main__ - Global step 1550 Train loss 0.09 Rouge-L 0.8747773271856951 on epoch=774
03/02/2022 11:10:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
03/02/2022 11:10:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=784
03/02/2022 11:10:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
03/02/2022 11:10:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=794
03/02/2022 11:10:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=799
03/02/2022 11:10:56 - INFO - __main__ - Global step 1600 Train loss 0.09 Rouge-L 0.882081281674053 on epoch=799
03/02/2022 11:10:56 - INFO - __main__ - Saving model with best Rouge-L: 0.8810300663949966 -> 0.882081281674053 on epoch=799, global_step=1600
03/02/2022 11:10:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=804
03/02/2022 11:11:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=809
03/02/2022 11:11:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
03/02/2022 11:11:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
03/02/2022 11:11:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
03/02/2022 11:11:20 - INFO - __main__ - Global step 1650 Train loss 0.09 Rouge-L 0.8907172199243032 on epoch=824
03/02/2022 11:11:20 - INFO - __main__ - Saving model with best Rouge-L: 0.882081281674053 -> 0.8907172199243032 on epoch=824, global_step=1650
03/02/2022 11:11:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
03/02/2022 11:11:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=834
03/02/2022 11:11:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
03/02/2022 11:11:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/02/2022 11:11:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=849
03/02/2022 11:11:44 - INFO - __main__ - Global step 1700 Train loss 0.08 Rouge-L 0.890183197145211 on epoch=849
03/02/2022 11:11:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=854
03/02/2022 11:11:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/02/2022 11:11:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
03/02/2022 11:11:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
03/02/2022 11:11:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
03/02/2022 11:12:06 - INFO - __main__ - Global step 1750 Train loss 0.08 Rouge-L 0.8845109089968147 on epoch=874
03/02/2022 11:12:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
03/02/2022 11:12:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
03/02/2022 11:12:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
03/02/2022 11:12:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
03/02/2022 11:12:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/02/2022 11:12:30 - INFO - __main__ - Global step 1800 Train loss 0.07 Rouge-L 0.8797404508794329 on epoch=899
03/02/2022 11:12:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
03/02/2022 11:12:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
03/02/2022 11:12:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
03/02/2022 11:12:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=919
03/02/2022 11:12:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=924
03/02/2022 11:12:53 - INFO - __main__ - Global step 1850 Train loss 0.07 Rouge-L 0.8678642422289469 on epoch=924
03/02/2022 11:12:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=929
03/02/2022 11:12:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=934
03/02/2022 11:13:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=939
03/02/2022 11:13:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/02/2022 11:13:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
03/02/2022 11:13:16 - INFO - __main__ - Global step 1900 Train loss 0.07 Rouge-L 0.8739520008317885 on epoch=949
03/02/2022 11:13:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/02/2022 11:13:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=959
03/02/2022 11:13:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
03/02/2022 11:13:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
03/02/2022 11:13:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
03/02/2022 11:13:41 - INFO - __main__ - Global step 1950 Train loss 0.07 Rouge-L 0.8862140875257328 on epoch=974
03/02/2022 11:13:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=979
03/02/2022 11:13:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=984
03/02/2022 11:13:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
03/02/2022 11:13:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
03/02/2022 11:13:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/02/2022 11:13:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:13:54 - INFO - __main__ - Printing 3 examples
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 11:13:54 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 11:13:54 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 11:13:54 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 11:13:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 11:13:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:13:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 11:13:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:13:54 - INFO - __main__ - Printing 3 examples
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 11:13:54 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 11:13:54 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 11:13:54 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 11:13:54 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 11:13:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 11:13:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:13:54 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 11:14:04 - INFO - __main__ - Global step 2000 Train loss 0.07 Rouge-L 0.8806234568388156 on epoch=999
03/02/2022 11:14:04 - INFO - __main__ - save last model!
03/02/2022 11:14:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 11:14:04 - INFO - __main__ - Start tokenizing ... 5000 instances
03/02/2022 11:14:04 - INFO - __main__ - Printing 3 examples
03/02/2022 11:14:04 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/02/2022 11:14:04 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/02/2022 11:14:04 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/02/2022 11:14:04 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/02/2022 11:14:04 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/02/2022 11:14:04 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/02/2022 11:14:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 11:14:06 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 11:14:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 11:14:07 - INFO - __main__ - Starting training!
03/02/2022 11:14:07 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:14:12 - INFO - __main__ - Loaded 5000 examples from test data
03/02/2022 11:43:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.5_8_predictions.txt
03/02/2022 11:43:52 - INFO - __main__ - Rouge-L on test data: 0.8703
03/02/2022 11:43:52 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.5, bsz=8, dev_performance=0.8907172199243032, test_performance=0.8702633735700505
03/02/2022 11:43:52 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.4, bsz=8 ...
03/02/2022 11:43:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:43:53 - INFO - __main__ - Printing 3 examples
03/02/2022 11:43:53 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 11:43:53 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 11:43:53 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 11:43:53 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 11:43:53 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 11:43:53 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 11:43:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 11:43:53 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:43:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 11:43:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:43:53 - INFO - __main__ - Printing 3 examples
03/02/2022 11:43:54 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 11:43:54 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 11:43:54 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 11:43:54 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 11:43:54 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 11:43:54 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 11:43:54 - INFO - __main__ - Tokenizing Input ...
03/02/2022 11:43:54 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:43:54 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 11:44:08 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 11:44:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 11:44:08 - INFO - __main__ - Starting training!
03/02/2022 11:44:11 - INFO - __main__ - Step 10 Global step 10 Train loss 0.82 on epoch=4
03/02/2022 11:44:14 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=9
03/02/2022 11:44:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=14
03/02/2022 11:44:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=19
03/02/2022 11:44:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
03/02/2022 11:44:33 - INFO - __main__ - Global step 50 Train loss 0.67 Rouge-L 0.7875387293812833 on epoch=24
03/02/2022 11:44:33 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.7875387293812833 on epoch=24, global_step=50
03/02/2022 11:44:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=29
03/02/2022 11:44:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
03/02/2022 11:44:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
03/02/2022 11:44:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
03/02/2022 11:44:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=49
03/02/2022 11:44:56 - INFO - __main__ - Global step 100 Train loss 0.55 Rouge-L 0.8586240157470528 on epoch=49
03/02/2022 11:44:56 - INFO - __main__ - Saving model with best Rouge-L: 0.7875387293812833 -> 0.8586240157470528 on epoch=49, global_step=100
03/02/2022 11:44:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
03/02/2022 11:45:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=59
03/02/2022 11:45:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
03/02/2022 11:45:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
03/02/2022 11:45:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
03/02/2022 11:45:20 - INFO - __main__ - Global step 150 Train loss 0.50 Rouge-L 0.8559061549559362 on epoch=74
03/02/2022 11:45:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
03/02/2022 11:45:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=84
03/02/2022 11:45:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=89
03/02/2022 11:45:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=94
03/02/2022 11:45:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
03/02/2022 11:45:41 - INFO - __main__ - Global step 200 Train loss 0.46 Rouge-L 0.8560781747128157 on epoch=99
03/02/2022 11:45:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
03/02/2022 11:45:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
03/02/2022 11:45:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=114
03/02/2022 11:45:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
03/02/2022 11:45:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
03/02/2022 11:46:04 - INFO - __main__ - Global step 250 Train loss 0.43 Rouge-L 0.8729491387977737 on epoch=124
03/02/2022 11:46:04 - INFO - __main__ - Saving model with best Rouge-L: 0.8586240157470528 -> 0.8729491387977737 on epoch=124, global_step=250
03/02/2022 11:46:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
03/02/2022 11:46:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=134
03/02/2022 11:46:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=139
03/02/2022 11:46:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
03/02/2022 11:46:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
03/02/2022 11:46:26 - INFO - __main__ - Global step 300 Train loss 0.39 Rouge-L 0.8714349175782756 on epoch=149
03/02/2022 11:46:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/02/2022 11:46:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
03/02/2022 11:46:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
03/02/2022 11:46:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=169
03/02/2022 11:46:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
03/02/2022 11:46:48 - INFO - __main__ - Global step 350 Train loss 0.37 Rouge-L 0.8574464774223701 on epoch=174
03/02/2022 11:46:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
03/02/2022 11:46:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=184
03/02/2022 11:46:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
03/02/2022 11:46:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
03/02/2022 11:46:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=199
03/02/2022 11:47:10 - INFO - __main__ - Global step 400 Train loss 0.35 Rouge-L 0.8689568306234412 on epoch=199
03/02/2022 11:47:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
03/02/2022 11:47:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
03/02/2022 11:47:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
03/02/2022 11:47:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
03/02/2022 11:47:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
03/02/2022 11:47:32 - INFO - __main__ - Global step 450 Train loss 0.33 Rouge-L 0.8748200531835366 on epoch=224
03/02/2022 11:47:32 - INFO - __main__ - Saving model with best Rouge-L: 0.8729491387977737 -> 0.8748200531835366 on epoch=224, global_step=450
03/02/2022 11:47:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=229
03/02/2022 11:47:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
03/02/2022 11:47:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=239
03/02/2022 11:47:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
03/02/2022 11:47:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
03/02/2022 11:47:55 - INFO - __main__ - Global step 500 Train loss 0.31 Rouge-L 0.8669077704130745 on epoch=249
03/02/2022 11:47:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
03/02/2022 11:47:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=259
03/02/2022 11:48:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=264
03/02/2022 11:48:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=269
03/02/2022 11:48:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
03/02/2022 11:48:18 - INFO - __main__ - Global step 550 Train loss 0.30 Rouge-L 0.8778378009294665 on epoch=274
03/02/2022 11:48:18 - INFO - __main__ - Saving model with best Rouge-L: 0.8748200531835366 -> 0.8778378009294665 on epoch=274, global_step=550
03/02/2022 11:48:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
03/02/2022 11:48:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
03/02/2022 11:48:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=289
03/02/2022 11:48:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
03/02/2022 11:48:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
03/02/2022 11:48:41 - INFO - __main__ - Global step 600 Train loss 0.28 Rouge-L 0.8640347506038931 on epoch=299
03/02/2022 11:48:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=304
03/02/2022 11:48:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=309
03/02/2022 11:48:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
03/02/2022 11:48:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
03/02/2022 11:48:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
03/02/2022 11:49:04 - INFO - __main__ - Global step 650 Train loss 0.27 Rouge-L 0.8717537916268548 on epoch=324
03/02/2022 11:49:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
03/02/2022 11:49:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
03/02/2022 11:49:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=339
03/02/2022 11:49:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/02/2022 11:49:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/02/2022 11:49:26 - INFO - __main__ - Global step 700 Train loss 0.26 Rouge-L 0.8610735579430746 on epoch=349
03/02/2022 11:49:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
03/02/2022 11:49:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/02/2022 11:49:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/02/2022 11:49:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/02/2022 11:49:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/02/2022 11:49:49 - INFO - __main__ - Global step 750 Train loss 0.24 Rouge-L 0.8607345119211891 on epoch=374
03/02/2022 11:49:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/02/2022 11:49:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/02/2022 11:49:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
03/02/2022 11:49:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/02/2022 11:50:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
03/02/2022 11:50:12 - INFO - __main__ - Global step 800 Train loss 0.23 Rouge-L 0.865809674147316 on epoch=399
03/02/2022 11:50:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
03/02/2022 11:50:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/02/2022 11:50:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
03/02/2022 11:50:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
03/02/2022 11:50:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=424
03/02/2022 11:50:35 - INFO - __main__ - Global step 850 Train loss 0.22 Rouge-L 0.8786743388961471 on epoch=424
03/02/2022 11:50:36 - INFO - __main__ - Saving model with best Rouge-L: 0.8778378009294665 -> 0.8786743388961471 on epoch=424, global_step=850
03/02/2022 11:50:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
03/02/2022 11:50:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
03/02/2022 11:50:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/02/2022 11:50:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/02/2022 11:50:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
03/02/2022 11:50:59 - INFO - __main__ - Global step 900 Train loss 0.22 Rouge-L 0.882064520344348 on epoch=449
03/02/2022 11:50:59 - INFO - __main__ - Saving model with best Rouge-L: 0.8786743388961471 -> 0.882064520344348 on epoch=449, global_step=900
03/02/2022 11:51:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
03/02/2022 11:51:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
03/02/2022 11:51:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/02/2022 11:51:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
03/02/2022 11:51:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
03/02/2022 11:51:22 - INFO - __main__ - Global step 950 Train loss 0.21 Rouge-L 0.8774693342228399 on epoch=474
03/02/2022 11:51:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=479
03/02/2022 11:51:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
03/02/2022 11:51:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=489
03/02/2022 11:51:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=494
03/02/2022 11:51:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/02/2022 11:51:45 - INFO - __main__ - Global step 1000 Train loss 0.20 Rouge-L 0.8649560045995071 on epoch=499
03/02/2022 11:51:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=504
03/02/2022 11:51:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
03/02/2022 11:51:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=514
03/02/2022 11:51:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=519
03/02/2022 11:51:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=524
03/02/2022 11:52:07 - INFO - __main__ - Global step 1050 Train loss 0.19 Rouge-L 0.8581223010979814 on epoch=524
03/02/2022 11:52:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=529
03/02/2022 11:52:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=534
03/02/2022 11:52:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
03/02/2022 11:52:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=544
03/02/2022 11:52:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
03/02/2022 11:52:30 - INFO - __main__ - Global step 1100 Train loss 0.19 Rouge-L 0.8711108462961412 on epoch=549
03/02/2022 11:52:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=554
03/02/2022 11:52:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=559
03/02/2022 11:52:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=564
03/02/2022 11:52:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=569
03/02/2022 11:52:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=574
03/02/2022 11:52:53 - INFO - __main__ - Global step 1150 Train loss 0.17 Rouge-L 0.8671876990094731 on epoch=574
03/02/2022 11:52:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=579
03/02/2022 11:52:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
03/02/2022 11:53:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
03/02/2022 11:53:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
03/02/2022 11:53:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=599
03/02/2022 11:53:16 - INFO - __main__ - Global step 1200 Train loss 0.17 Rouge-L 0.846029000625618 on epoch=599
03/02/2022 11:53:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=604
03/02/2022 11:53:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
03/02/2022 11:53:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=614
03/02/2022 11:53:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
03/02/2022 11:53:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=624
03/02/2022 11:53:39 - INFO - __main__ - Global step 1250 Train loss 0.17 Rouge-L 0.87615258133478 on epoch=624
03/02/2022 11:53:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=629
03/02/2022 11:53:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=634
03/02/2022 11:53:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
03/02/2022 11:53:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=644
03/02/2022 11:53:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=649
03/02/2022 11:54:04 - INFO - __main__ - Global step 1300 Train loss 0.15 Rouge-L 0.8703298089753528 on epoch=649
03/02/2022 11:54:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=654
03/02/2022 11:54:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=659
03/02/2022 11:54:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=664
03/02/2022 11:54:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=669
03/02/2022 11:54:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/02/2022 11:54:29 - INFO - __main__ - Global step 1350 Train loss 0.15 Rouge-L 0.8667129024117586 on epoch=674
03/02/2022 11:54:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=679
03/02/2022 11:54:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/02/2022 11:54:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
03/02/2022 11:54:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=694
03/02/2022 11:54:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
03/02/2022 11:54:52 - INFO - __main__ - Global step 1400 Train loss 0.15 Rouge-L 0.8650120761800646 on epoch=699
03/02/2022 11:54:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
03/02/2022 11:54:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/02/2022 11:54:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
03/02/2022 11:55:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=719
03/02/2022 11:55:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
03/02/2022 11:55:15 - INFO - __main__ - Global step 1450 Train loss 0.15 Rouge-L 0.8656909145062417 on epoch=724
03/02/2022 11:55:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=729
03/02/2022 11:55:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=734
03/02/2022 11:55:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=739
03/02/2022 11:55:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=744
03/02/2022 11:55:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/02/2022 11:55:38 - INFO - __main__ - Global step 1500 Train loss 0.14 Rouge-L 0.8546507647358703 on epoch=749
03/02/2022 11:55:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=754
03/02/2022 11:55:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
03/02/2022 11:55:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
03/02/2022 11:55:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
03/02/2022 11:55:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=774
03/02/2022 11:56:02 - INFO - __main__ - Global step 1550 Train loss 0.13 Rouge-L 0.8541546330261862 on epoch=774
03/02/2022 11:56:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/02/2022 11:56:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=784
03/02/2022 11:56:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/02/2022 11:56:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
03/02/2022 11:56:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=799
03/02/2022 11:56:29 - INFO - __main__ - Global step 1600 Train loss 0.13 Rouge-L 0.8538531921165384 on epoch=799
03/02/2022 11:56:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=804
03/02/2022 11:56:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=809
03/02/2022 11:56:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/02/2022 11:56:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=819
03/02/2022 11:56:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
03/02/2022 11:56:51 - INFO - __main__ - Global step 1650 Train loss 0.13 Rouge-L 0.8686114427580847 on epoch=824
03/02/2022 11:56:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
03/02/2022 11:56:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/02/2022 11:56:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=839
03/02/2022 11:57:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=844
03/02/2022 11:57:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/02/2022 11:57:14 - INFO - __main__ - Global step 1700 Train loss 0.12 Rouge-L 0.8603269657974021 on epoch=849
03/02/2022 11:57:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=854
03/02/2022 11:57:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
03/02/2022 11:57:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/02/2022 11:57:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/02/2022 11:57:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=874
03/02/2022 11:57:38 - INFO - __main__ - Global step 1750 Train loss 0.11 Rouge-L 0.8666548312063961 on epoch=874
03/02/2022 11:57:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/02/2022 11:57:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=884
03/02/2022 11:57:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
03/02/2022 11:57:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=894
03/02/2022 11:57:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
03/02/2022 11:58:00 - INFO - __main__ - Global step 1800 Train loss 0.11 Rouge-L 0.8770753620538285 on epoch=899
03/02/2022 11:58:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=904
03/02/2022 11:58:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/02/2022 11:58:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=914
03/02/2022 11:58:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=919
03/02/2022 11:58:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/02/2022 11:58:26 - INFO - __main__ - Global step 1850 Train loss 0.10 Rouge-L 0.8662054743528288 on epoch=924
03/02/2022 11:58:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=929
03/02/2022 11:58:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/02/2022 11:58:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
03/02/2022 11:58:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/02/2022 11:58:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=949
03/02/2022 11:58:48 - INFO - __main__ - Global step 1900 Train loss 0.10 Rouge-L 0.8639375268877503 on epoch=949
03/02/2022 11:58:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/02/2022 11:58:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
03/02/2022 11:58:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
03/02/2022 11:58:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
03/02/2022 11:59:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=974
03/02/2022 11:59:12 - INFO - __main__ - Global step 1950 Train loss 0.09 Rouge-L 0.8695753637474208 on epoch=974
03/02/2022 11:59:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=979
03/02/2022 11:59:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
03/02/2022 11:59:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/02/2022 11:59:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/02/2022 11:59:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
03/02/2022 11:59:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:59:24 - INFO - __main__ - Printing 3 examples
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 11:59:24 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 11:59:24 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 11:59:24 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 11:59:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 11:59:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:59:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 11:59:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 11:59:24 - INFO - __main__ - Printing 3 examples
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 11:59:24 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 11:59:24 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 11:59:24 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 11:59:24 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 11:59:24 - INFO - __main__ - Tokenizing Input ...
03/02/2022 11:59:24 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:59:25 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 11:59:34 - INFO - __main__ - Global step 2000 Train loss 0.09 Rouge-L 0.8693945158742733 on epoch=999
03/02/2022 11:59:34 - INFO - __main__ - save last model!
03/02/2022 11:59:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 11:59:34 - INFO - __main__ - Start tokenizing ... 5000 instances
03/02/2022 11:59:34 - INFO - __main__ - Printing 3 examples
03/02/2022 11:59:34 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/02/2022 11:59:34 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/02/2022 11:59:34 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/02/2022 11:59:34 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/02/2022 11:59:34 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/02/2022 11:59:34 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/02/2022 11:59:34 - INFO - __main__ - Tokenizing Input ...
03/02/2022 11:59:37 - INFO - __main__ - Tokenizing Output ...
03/02/2022 11:59:39 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 11:59:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 11:59:40 - INFO - __main__ - Starting training!
03/02/2022 11:59:42 - INFO - __main__ - Loaded 5000 examples from test data
03/02/2022 12:28:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.4_8_predictions.txt
03/02/2022 12:28:26 - INFO - __main__ - Rouge-L on test data: 0.8707
03/02/2022 12:28:26 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.4, bsz=8, dev_performance=0.882064520344348, test_performance=0.8706791290964444
03/02/2022 12:28:26 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.3, bsz=8 ...
03/02/2022 12:28:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 12:28:27 - INFO - __main__ - Printing 3 examples
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 12:28:27 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 12:28:27 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 12:28:27 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 12:28:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 12:28:27 - INFO - __main__ - Tokenizing Output ...
03/02/2022 12:28:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 12:28:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 12:28:27 - INFO - __main__ - Printing 3 examples
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 12:28:27 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 12:28:27 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 12:28:27 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 12:28:27 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 12:28:27 - INFO - __main__ - Tokenizing Input ...
03/02/2022 12:28:27 - INFO - __main__ - Tokenizing Output ...
03/02/2022 12:28:27 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 12:28:41 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 12:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 12:28:42 - INFO - __main__ - Starting training!
03/02/2022 12:28:45 - INFO - __main__ - Step 10 Global step 10 Train loss 0.85 on epoch=4
03/02/2022 12:28:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=9
03/02/2022 12:28:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=14
03/02/2022 12:28:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=19
03/02/2022 12:28:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
03/02/2022 12:29:05 - INFO - __main__ - Global step 50 Train loss 0.71 Rouge-L 0.6703325722708713 on epoch=24
03/02/2022 12:29:05 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6703325722708713 on epoch=24, global_step=50
03/02/2022 12:29:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
03/02/2022 12:29:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
03/02/2022 12:29:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=39
03/02/2022 12:29:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
03/02/2022 12:29:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
03/02/2022 12:29:28 - INFO - __main__ - Global step 100 Train loss 0.57 Rouge-L 0.8379783155956653 on epoch=49
03/02/2022 12:29:28 - INFO - __main__ - Saving model with best Rouge-L: 0.6703325722708713 -> 0.8379783155956653 on epoch=49, global_step=100
03/02/2022 12:29:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
03/02/2022 12:29:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=59
03/02/2022 12:29:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
03/02/2022 12:29:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=69
03/02/2022 12:29:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=74
03/02/2022 12:29:51 - INFO - __main__ - Global step 150 Train loss 0.53 Rouge-L 0.8575625991161984 on epoch=74
03/02/2022 12:29:51 - INFO - __main__ - Saving model with best Rouge-L: 0.8379783155956653 -> 0.8575625991161984 on epoch=74, global_step=150
03/02/2022 12:29:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=79
03/02/2022 12:29:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=84
03/02/2022 12:29:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=89
03/02/2022 12:30:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=94
03/02/2022 12:30:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=99
03/02/2022 12:30:16 - INFO - __main__ - Global step 200 Train loss 0.50 Rouge-L 0.8577389373418454 on epoch=99
03/02/2022 12:30:16 - INFO - __main__ - Saving model with best Rouge-L: 0.8575625991161984 -> 0.8577389373418454 on epoch=99, global_step=200
03/02/2022 12:30:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=104
03/02/2022 12:30:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=109
03/02/2022 12:30:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=114
03/02/2022 12:30:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=119
03/02/2022 12:30:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
03/02/2022 12:30:39 - INFO - __main__ - Global step 250 Train loss 0.47 Rouge-L 0.8562694228958173 on epoch=124
03/02/2022 12:30:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=129
03/02/2022 12:30:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
03/02/2022 12:30:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
03/02/2022 12:30:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=144
03/02/2022 12:30:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=149
03/02/2022 12:31:03 - INFO - __main__ - Global step 300 Train loss 0.45 Rouge-L 0.8706854936599691 on epoch=149
03/02/2022 12:31:03 - INFO - __main__ - Saving model with best Rouge-L: 0.8577389373418454 -> 0.8706854936599691 on epoch=149, global_step=300
03/02/2022 12:31:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=154
03/02/2022 12:31:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=159
03/02/2022 12:31:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
03/02/2022 12:31:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
03/02/2022 12:31:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
03/02/2022 12:31:27 - INFO - __main__ - Global step 350 Train loss 0.42 Rouge-L 0.8736456346265205 on epoch=174
03/02/2022 12:31:27 - INFO - __main__ - Saving model with best Rouge-L: 0.8706854936599691 -> 0.8736456346265205 on epoch=174, global_step=350
03/02/2022 12:31:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=179
03/02/2022 12:31:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
03/02/2022 12:31:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=189
03/02/2022 12:31:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=194
03/02/2022 12:31:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
03/02/2022 12:31:50 - INFO - __main__ - Global step 400 Train loss 0.40 Rouge-L 0.8608427202567708 on epoch=199
03/02/2022 12:31:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=204
03/02/2022 12:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
03/02/2022 12:31:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
03/02/2022 12:31:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/02/2022 12:32:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
03/02/2022 12:32:14 - INFO - __main__ - Global step 450 Train loss 0.38 Rouge-L 0.8602764349476679 on epoch=224
03/02/2022 12:32:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=229
03/02/2022 12:32:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
03/02/2022 12:32:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
03/02/2022 12:32:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
03/02/2022 12:32:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=249
03/02/2022 12:32:37 - INFO - __main__ - Global step 500 Train loss 0.37 Rouge-L 0.8609840802008331 on epoch=249
03/02/2022 12:32:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/02/2022 12:32:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=259
03/02/2022 12:32:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
03/02/2022 12:32:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
03/02/2022 12:32:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
03/02/2022 12:33:00 - INFO - __main__ - Global step 550 Train loss 0.35 Rouge-L 0.8592322201298236 on epoch=274
03/02/2022 12:33:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
03/02/2022 12:33:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=284
03/02/2022 12:33:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=289
03/02/2022 12:33:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=294
03/02/2022 12:33:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=299
03/02/2022 12:33:24 - INFO - __main__ - Global step 600 Train loss 0.33 Rouge-L 0.8564343710891305 on epoch=299
03/02/2022 12:33:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
03/02/2022 12:33:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
03/02/2022 12:33:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=314
03/02/2022 12:33:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=319
03/02/2022 12:33:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=324
03/02/2022 12:33:47 - INFO - __main__ - Global step 650 Train loss 0.31 Rouge-L 0.8615583861579461 on epoch=324
03/02/2022 12:33:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
03/02/2022 12:33:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=334
03/02/2022 12:33:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=339
03/02/2022 12:33:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=344
03/02/2022 12:33:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=349
03/02/2022 12:34:11 - INFO - __main__ - Global step 700 Train loss 0.30 Rouge-L 0.8657239116197821 on epoch=349
03/02/2022 12:34:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
03/02/2022 12:34:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
03/02/2022 12:34:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
03/02/2022 12:34:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=369
03/02/2022 12:34:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
03/02/2022 12:34:34 - INFO - __main__ - Global step 750 Train loss 0.29 Rouge-L 0.874322890126205 on epoch=374
03/02/2022 12:34:34 - INFO - __main__ - Saving model with best Rouge-L: 0.8736456346265205 -> 0.874322890126205 on epoch=374, global_step=750
03/02/2022 12:34:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
03/02/2022 12:34:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=384
03/02/2022 12:34:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=389
03/02/2022 12:34:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=394
03/02/2022 12:34:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=399
03/02/2022 12:34:58 - INFO - __main__ - Global step 800 Train loss 0.28 Rouge-L 0.8714501663335025 on epoch=399
03/02/2022 12:35:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
03/02/2022 12:35:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
03/02/2022 12:35:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
03/02/2022 12:35:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=419
03/02/2022 12:35:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
03/02/2022 12:35:21 - INFO - __main__ - Global step 850 Train loss 0.27 Rouge-L 0.8691017735280061 on epoch=424
03/02/2022 12:35:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=429
03/02/2022 12:35:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/02/2022 12:35:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=439
03/02/2022 12:35:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
03/02/2022 12:35:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
03/02/2022 12:35:44 - INFO - __main__ - Global step 900 Train loss 0.26 Rouge-L 0.8589428101351071 on epoch=449
03/02/2022 12:35:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/02/2022 12:35:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
03/02/2022 12:35:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=464
03/02/2022 12:35:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
03/02/2022 12:35:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
03/02/2022 12:36:07 - INFO - __main__ - Global step 950 Train loss 0.26 Rouge-L 0.8561298829975514 on epoch=474
03/02/2022 12:36:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=479
03/02/2022 12:36:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
03/02/2022 12:36:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
03/02/2022 12:36:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
03/02/2022 12:36:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
03/02/2022 12:36:31 - INFO - __main__ - Global step 1000 Train loss 0.25 Rouge-L 0.8902086325135342 on epoch=499
03/02/2022 12:36:31 - INFO - __main__ - Saving model with best Rouge-L: 0.874322890126205 -> 0.8902086325135342 on epoch=499, global_step=1000
03/02/2022 12:36:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=504
03/02/2022 12:36:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/02/2022 12:36:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=514
03/02/2022 12:36:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
03/02/2022 12:36:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/02/2022 12:36:54 - INFO - __main__ - Global step 1050 Train loss 0.23 Rouge-L 0.8801406716523484 on epoch=524
03/02/2022 12:36:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
03/02/2022 12:36:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
03/02/2022 12:37:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=539
03/02/2022 12:37:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/02/2022 12:37:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
03/02/2022 12:37:17 - INFO - __main__ - Global step 1100 Train loss 0.23 Rouge-L 0.882680091620462 on epoch=549
03/02/2022 12:37:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
03/02/2022 12:37:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/02/2022 12:37:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/02/2022 12:37:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
03/02/2022 12:37:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=574
03/02/2022 12:37:40 - INFO - __main__ - Global step 1150 Train loss 0.22 Rouge-L 0.8723348514661294 on epoch=574
03/02/2022 12:37:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=579
03/02/2022 12:37:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
03/02/2022 12:37:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=589
03/02/2022 12:37:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/02/2022 12:37:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/02/2022 12:38:04 - INFO - __main__ - Global step 1200 Train loss 0.21 Rouge-L 0.8794352755116706 on epoch=599
03/02/2022 12:38:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/02/2022 12:38:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=609
03/02/2022 12:38:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=614
03/02/2022 12:38:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
03/02/2022 12:38:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
03/02/2022 12:38:27 - INFO - __main__ - Global step 1250 Train loss 0.21 Rouge-L 0.8782213899179967 on epoch=624
03/02/2022 12:38:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=629
03/02/2022 12:38:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/02/2022 12:38:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/02/2022 12:38:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=644
03/02/2022 12:38:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=649
03/02/2022 12:38:50 - INFO - __main__ - Global step 1300 Train loss 0.20 Rouge-L 0.8611688927690472 on epoch=649
03/02/2022 12:38:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/02/2022 12:38:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=659
03/02/2022 12:38:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/02/2022 12:38:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/02/2022 12:39:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
03/02/2022 12:39:13 - INFO - __main__ - Global step 1350 Train loss 0.20 Rouge-L 0.8679885128685927 on epoch=674
03/02/2022 12:39:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
03/02/2022 12:39:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=684
03/02/2022 12:39:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=689
03/02/2022 12:39:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
03/02/2022 12:39:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/02/2022 12:39:36 - INFO - __main__ - Global step 1400 Train loss 0.19 Rouge-L 0.8702048853540544 on epoch=699
03/02/2022 12:39:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=704
03/02/2022 12:39:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/02/2022 12:39:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/02/2022 12:39:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=719
03/02/2022 12:39:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/02/2022 12:39:59 - INFO - __main__ - Global step 1450 Train loss 0.18 Rouge-L 0.8495710405673885 on epoch=724
03/02/2022 12:40:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/02/2022 12:40:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/02/2022 12:40:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/02/2022 12:40:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
03/02/2022 12:40:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=749
03/02/2022 12:40:23 - INFO - __main__ - Global step 1500 Train loss 0.18 Rouge-L 0.8587021298705381 on epoch=749
03/02/2022 12:40:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/02/2022 12:40:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=759
03/02/2022 12:40:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=764
03/02/2022 12:40:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=769
03/02/2022 12:40:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
03/02/2022 12:40:46 - INFO - __main__ - Global step 1550 Train loss 0.17 Rouge-L 0.8531892199315451 on epoch=774
03/02/2022 12:40:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=779
03/02/2022 12:40:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=784
03/02/2022 12:40:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=789
03/02/2022 12:40:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=794
03/02/2022 12:40:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/02/2022 12:41:09 - INFO - __main__ - Global step 1600 Train loss 0.16 Rouge-L 0.8707788575602984 on epoch=799
03/02/2022 12:41:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
03/02/2022 12:41:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=809
03/02/2022 12:41:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
03/02/2022 12:41:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/02/2022 12:41:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
03/02/2022 12:41:32 - INFO - __main__ - Global step 1650 Train loss 0.16 Rouge-L 0.8611901903332222 on epoch=824
03/02/2022 12:41:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
03/02/2022 12:41:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=834
03/02/2022 12:41:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
03/02/2022 12:41:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=844
03/02/2022 12:41:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=849
03/02/2022 12:41:55 - INFO - __main__ - Global step 1700 Train loss 0.15 Rouge-L 0.868878162315733 on epoch=849
03/02/2022 12:41:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=854
03/02/2022 12:42:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=859
03/02/2022 12:42:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
03/02/2022 12:42:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=869
03/02/2022 12:42:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/02/2022 12:42:18 - INFO - __main__ - Global step 1750 Train loss 0.15 Rouge-L 0.8604285251484705 on epoch=874
03/02/2022 12:42:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/02/2022 12:42:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/02/2022 12:42:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=889
03/02/2022 12:42:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
03/02/2022 12:42:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=899
03/02/2022 12:42:42 - INFO - __main__ - Global step 1800 Train loss 0.14 Rouge-L 0.8558655711346017 on epoch=899
03/02/2022 12:42:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/02/2022 12:42:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=909
03/02/2022 12:42:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/02/2022 12:42:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/02/2022 12:42:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=924
03/02/2022 12:43:05 - INFO - __main__ - Global step 1850 Train loss 0.14 Rouge-L 0.8781082314035675 on epoch=924
03/02/2022 12:43:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/02/2022 12:43:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/02/2022 12:43:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=939
03/02/2022 12:43:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/02/2022 12:43:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=949
03/02/2022 12:43:28 - INFO - __main__ - Global step 1900 Train loss 0.13 Rouge-L 0.853573338602069 on epoch=949
03/02/2022 12:43:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
03/02/2022 12:43:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
03/02/2022 12:43:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=964
03/02/2022 12:43:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/02/2022 12:43:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
03/02/2022 12:43:51 - INFO - __main__ - Global step 1950 Train loss 0.13 Rouge-L 0.8734796721818749 on epoch=974
03/02/2022 12:43:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
03/02/2022 12:43:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=984
03/02/2022 12:43:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
03/02/2022 12:44:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=994
03/02/2022 12:44:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
03/02/2022 12:44:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 12:44:04 - INFO - __main__ - Printing 3 examples
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 12:44:04 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 12:44:04 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 12:44:04 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 12:44:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 12:44:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 12:44:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 12:44:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 12:44:04 - INFO - __main__ - Printing 3 examples
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 12:44:04 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 12:44:04 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 12:44:04 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 12:44:04 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 12:44:04 - INFO - __main__ - Tokenizing Input ...
03/02/2022 12:44:04 - INFO - __main__ - Tokenizing Output ...
03/02/2022 12:44:04 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 12:44:15 - INFO - __main__ - Global step 2000 Train loss 0.12 Rouge-L 0.8610201734986361 on epoch=999
03/02/2022 12:44:15 - INFO - __main__ - save last model!
03/02/2022 12:44:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 12:44:15 - INFO - __main__ - Start tokenizing ... 5000 instances
03/02/2022 12:44:15 - INFO - __main__ - Printing 3 examples
03/02/2022 12:44:15 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
03/02/2022 12:44:15 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
03/02/2022 12:44:15 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
03/02/2022 12:44:15 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
03/02/2022 12:44:15 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
03/02/2022 12:44:15 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
03/02/2022 12:44:15 - INFO - __main__ - Tokenizing Input ...
03/02/2022 12:44:17 - INFO - __main__ - Tokenizing Output ...
03/02/2022 12:44:18 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 12:44:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 12:44:19 - INFO - __main__ - Starting training!
03/02/2022 12:44:23 - INFO - __main__ - Loaded 5000 examples from test data
03/02/2022 13:14:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.3_8_predictions.txt
03/02/2022 13:14:05 - INFO - __main__ - Rouge-L on test data: 0.8720
03/02/2022 13:14:05 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.3, bsz=8, dev_performance=0.8902086325135342, test_performance=0.8719526484565373
03/02/2022 13:14:05 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.2, bsz=8 ...
03/02/2022 13:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 13:14:06 - INFO - __main__ - Printing 3 examples
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
03/02/2022 13:14:06 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
03/02/2022 13:14:06 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
03/02/2022 13:14:06 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
03/02/2022 13:14:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 13:14:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 13:14:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 13:14:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 13:14:06 - INFO - __main__ - Printing 3 examples
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
03/02/2022 13:14:06 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
03/02/2022 13:14:06 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
03/02/2022 13:14:06 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
03/02/2022 13:14:06 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
03/02/2022 13:14:06 - INFO - __main__ - Tokenizing Input ...
03/02/2022 13:14:06 - INFO - __main__ - Tokenizing Output ...
03/02/2022 13:14:06 - INFO - __main__ - Loaded 32 examples from dev data
[E ProcessGroupNCCL.cpp:566] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800532 milliseconds before timing out.
03/02/2022 13:14:21 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 13:14:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 13:14:21 - INFO - __main__ - Starting training!
[E ProcessGroupNCCL.cpp:325] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1800532 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 3492) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_1/1/error.json
Output directory () already exists and is not empty.
03/02/2022 13:19:24 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 13:19:24 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 13:19:24 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 13:19:24 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 13:19:24 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 13:19:24 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 13:19:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:19:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:19:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:19:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:19:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:19:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:20:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:21:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:22:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:23:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:24:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:25:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:26:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:27:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:28:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:29:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:30:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:31:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:32:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:33:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:34:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:35:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:36:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:37:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:38:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:39:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:40:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:41:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:42:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:43:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:16 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:26 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:36 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:46 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:44:56 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:06 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:45:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:46:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:47:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:48:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:49:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:49:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:49:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/02/2022 13:49:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
    main()  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>

  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()    torch.distributed.init_process_group(backend="nccl")
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    torch.distributed.init_process_group(backend="nccl")    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    _store_based_barrier(rank, store, timeout)    raise RuntimeError(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier

    raise RuntimeError(RuntimeError: 
RuntimeErrorTimed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3515) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_2/1/error.json
Output directory () already exists and is not empty.
03/02/2022 13:49:31 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 13:49:31 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 13:49:31 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 13:49:31 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 13:49:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 13:49:31 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 13:49:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:49:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:49:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:49:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:50:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:51:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:52:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:53:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:54:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:55:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:56:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 13:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:04:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:05:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:06:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:07:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:08:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:09:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:10:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:11:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:12:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:43 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:13:53 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:03 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:13 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:23 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:33 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:14:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:15:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:16:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:17:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:18:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/02/2022 14:19:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main

  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")    torch.distributed.init_process_group(backend="nccl")

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(    raise RuntimeError(
RuntimeError
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00): Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3527) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ft3dgtmp/none_hlr9gkrx/attempt_3/1/error.json
Output directory () already exists and is not empty.
03/02/2022 14:19:37 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 14:19:37 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 14:19:37 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 14:19:37 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
03/02/2022 14:19:38 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 14:19:38 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 14:19:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:19:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:19:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:19:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:20:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:21:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:22:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:23:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:24:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:25:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:26:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:27:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:28:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:29:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:30:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:31:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:32:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:33:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:34:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:35:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:36:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:37:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:38:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:39:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:40:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:41:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:42:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:43:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:44:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:45:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:46:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:47:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:40 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:48:50 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:00 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:10 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:20 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/02/2022 14:49:30 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
        main()main()

  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)    _store_based_barrier(rank, store, timeout)

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(    raise RuntimeError(
RuntimeError
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00): Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3539) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0023887157440185547 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "3539", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 13875, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "3540", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 13875, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13875, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 3539 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     singletask_from_meta.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-03-02_14:49:43
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 3539)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-03-02_14:49:43
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 3540)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (3552): No such process
kill: write error: Disk quota exceeded
Task: break-QDMR, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_prb2155u/none_fkpt0qp4
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/02/2022 14:49:46 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 14:49:46 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
03/02/2022 14:49:46 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/02/2022 14:49:46 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
03/02/2022 14:49:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/02/2022 14:49:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/02/2022 14:49:46 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/02/2022 14:49:46 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/02/2022 14:49:46 - INFO - __main__ - args.device: cuda:0
03/02/2022 14:49:46 - INFO - __main__ - args.device: cuda:1
03/02/2022 14:49:46 - INFO - __main__ - Using 2 gpus
03/02/2022 14:49:46 - INFO - __main__ - Using 2 gpus
03/02/2022 14:49:46 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
03/02/2022 14:49:46 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/02/2022 14:49:55 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.5, bsz=8 ...
03/02/2022 14:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 14:49:56 - INFO - __main__ - Printing 3 examples
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/02/2022 14:49:56 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/02/2022 14:49:56 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/02/2022 14:49:56 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 14:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 14:49:56 - INFO - __main__ - Printing 3 examples
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/02/2022 14:49:56 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/02/2022 14:49:56 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/02/2022 14:49:56 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 14:49:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 14:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 14:49:56 - INFO - __main__ - Printing 3 examples
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/02/2022 14:49:56 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/02/2022 14:49:56 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/02/2022 14:49:56 - INFO - __main__ - Loaded 32 examples from train data
03/02/2022 14:49:56 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
use DistributedSampler
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 14:49:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 14:49:56 - INFO - __main__ - Printing 3 examples
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/02/2022 14:49:56 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/02/2022 14:49:56 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/02/2022 14:49:56 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 14:49:56 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Input ...
03/02/2022 14:49:56 - INFO - __main__ - Tokenizing Output ...
03/02/2022 14:49:56 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 14:49:56 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 14:50:11 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 14:50:11 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 14:50:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 14:50:12 - INFO - __main__ - Starting training!
03/02/2022 14:50:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 14:50:18 - INFO - __main__ - Starting training!
03/02/2022 14:50:23 - INFO - __main__ - Step 10 Global step 10 Train loss 2.94 on epoch=4
03/02/2022 14:50:25 - INFO - __main__ - Step 20 Global step 20 Train loss 1.97 on epoch=9
03/02/2022 14:50:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
03/02/2022 14:50:30 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=19
03/02/2022 14:50:32 - INFO - __main__ - Step 50 Global step 50 Train loss 1.20 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/02/2022 14:50:37 - INFO - __main__ - Global step 50 Train loss 1.81 EM 0.03125 on epoch=24
03/02/2022 14:50:37 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/02/2022 14:50:40 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=29
03/02/2022 14:50:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=34
03/02/2022 14:50:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=39
03/02/2022 14:50:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=44
03/02/2022 14:50:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=49
03/02/2022 14:50:53 - INFO - __main__ - Global step 100 Train loss 1.02 EM 0.0 on epoch=49
03/02/2022 14:50:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=54
03/02/2022 14:50:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=59
03/02/2022 14:51:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=64
03/02/2022 14:51:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=69
03/02/2022 14:51:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=74
03/02/2022 14:51:09 - INFO - __main__ - Global step 150 Train loss 0.84 EM 0.03125 on epoch=74
03/02/2022 14:51:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=79
03/02/2022 14:51:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=84
03/02/2022 14:51:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=89
03/02/2022 14:51:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=94
03/02/2022 14:51:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=99
03/02/2022 14:51:25 - INFO - __main__ - Global step 200 Train loss 0.73 EM 0.09375 on epoch=99
03/02/2022 14:51:25 - INFO - __main__ - Saving model with best EM: 0.03125 -> 0.09375 on epoch=99, global_step=200
03/02/2022 14:51:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.67 on epoch=104
03/02/2022 14:51:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=109
03/02/2022 14:51:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=114
03/02/2022 14:51:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=119
03/02/2022 14:51:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=124
03/02/2022 14:51:42 - INFO - __main__ - Global step 250 Train loss 0.66 EM 0.125 on epoch=124
03/02/2022 14:51:42 - INFO - __main__ - Saving model with best EM: 0.09375 -> 0.125 on epoch=124, global_step=250
03/02/2022 14:51:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=129
03/02/2022 14:51:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=134
03/02/2022 14:51:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=139
03/02/2022 14:51:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=144
03/02/2022 14:51:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=149
03/02/2022 14:51:58 - INFO - __main__ - Global step 300 Train loss 0.60 EM 0.0625 on epoch=149
03/02/2022 14:52:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=154
03/02/2022 14:52:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=159
03/02/2022 14:52:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=164
03/02/2022 14:52:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=169
03/02/2022 14:52:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=174
03/02/2022 14:52:13 - INFO - __main__ - Global step 350 Train loss 0.56 EM 0.0625 on epoch=174
03/02/2022 14:52:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=179
03/02/2022 14:52:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=184
03/02/2022 14:52:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=189
03/02/2022 14:52:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=194
03/02/2022 14:52:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=199
03/02/2022 14:52:28 - INFO - __main__ - Global step 400 Train loss 0.52 EM 0.09375 on epoch=199
03/02/2022 14:52:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=204
03/02/2022 14:52:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=209
03/02/2022 14:52:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=214
03/02/2022 14:52:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=219
03/02/2022 14:52:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=224
03/02/2022 14:52:44 - INFO - __main__ - Global step 450 Train loss 0.49 EM 0.0625 on epoch=224
03/02/2022 14:52:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=229
03/02/2022 14:52:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=234
03/02/2022 14:52:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=239
03/02/2022 14:52:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=244
03/02/2022 14:52:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=249
03/02/2022 14:52:59 - INFO - __main__ - Global step 500 Train loss 0.47 EM 0.0625 on epoch=249
03/02/2022 14:53:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=254
03/02/2022 14:53:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=259
03/02/2022 14:53:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=264
03/02/2022 14:53:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=269
03/02/2022 14:53:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
03/02/2022 14:53:15 - INFO - __main__ - Global step 550 Train loss 0.44 EM 0.0625 on epoch=274
03/02/2022 14:53:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=279
03/02/2022 14:53:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=284
03/02/2022 14:53:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=289
03/02/2022 14:53:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=294
03/02/2022 14:53:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=299
03/02/2022 14:53:31 - INFO - __main__ - Global step 600 Train loss 0.42 EM 0.03125 on epoch=299
03/02/2022 14:53:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
03/02/2022 14:53:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=309
03/02/2022 14:53:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=314
03/02/2022 14:53:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=319
03/02/2022 14:53:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=324
03/02/2022 14:53:47 - INFO - __main__ - Global step 650 Train loss 0.40 EM 0.0625 on epoch=324
03/02/2022 14:53:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/02/2022 14:53:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/02/2022 14:53:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=339
03/02/2022 14:53:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/02/2022 14:53:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=349
03/02/2022 14:54:03 - INFO - __main__ - Global step 700 Train loss 0.39 EM 0.03125 on epoch=349
03/02/2022 14:54:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
03/02/2022 14:54:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/02/2022 14:54:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=364
03/02/2022 14:54:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/02/2022 14:54:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/02/2022 14:54:19 - INFO - __main__ - Global step 750 Train loss 0.37 EM 0.0625 on epoch=374
03/02/2022 14:54:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=379
03/02/2022 14:54:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/02/2022 14:54:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=389
03/02/2022 14:54:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
03/02/2022 14:54:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
03/02/2022 14:54:36 - INFO - __main__ - Global step 800 Train loss 0.35 EM 0.03125 on epoch=399
03/02/2022 14:54:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=404
03/02/2022 14:54:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=409
03/02/2022 14:54:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=414
03/02/2022 14:54:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=419
03/02/2022 14:54:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=424
03/02/2022 14:54:52 - INFO - __main__ - Global step 850 Train loss 0.35 EM 0.03125 on epoch=424
03/02/2022 14:54:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=429
03/02/2022 14:54:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=434
03/02/2022 14:54:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
03/02/2022 14:55:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/02/2022 14:55:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=449
03/02/2022 14:55:09 - INFO - __main__ - Global step 900 Train loss 0.33 EM 0.03125 on epoch=449
03/02/2022 14:55:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=454
03/02/2022 14:55:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
03/02/2022 14:55:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=464
03/02/2022 14:55:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=469
03/02/2022 14:55:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=474
03/02/2022 14:55:26 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.03125 on epoch=474
03/02/2022 14:55:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/02/2022 14:55:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/02/2022 14:55:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
03/02/2022 14:55:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/02/2022 14:55:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=499
03/02/2022 14:55:42 - INFO - __main__ - Global step 1000 Train loss 0.32 EM 0.03125 on epoch=499
03/02/2022 14:55:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=504
03/02/2022 14:55:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=509
03/02/2022 14:55:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
03/02/2022 14:55:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=519
03/02/2022 14:55:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=524
03/02/2022 14:55:59 - INFO - __main__ - Global step 1050 Train loss 0.30 EM 0.0625 on epoch=524
03/02/2022 14:56:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=529
03/02/2022 14:56:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=534
03/02/2022 14:56:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=539
03/02/2022 14:56:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/02/2022 14:56:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=549
03/02/2022 14:56:16 - INFO - __main__ - Global step 1100 Train loss 0.30 EM 0.09375 on epoch=549
03/02/2022 14:56:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=554
03/02/2022 14:56:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
03/02/2022 14:56:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
03/02/2022 14:56:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
03/02/2022 14:56:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/02/2022 14:56:33 - INFO - __main__ - Global step 1150 Train loss 0.29 EM 0.03125 on epoch=574
03/02/2022 14:56:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=579
03/02/2022 14:56:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
03/02/2022 14:56:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=589
03/02/2022 14:56:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
03/02/2022 14:56:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
03/02/2022 14:56:50 - INFO - __main__ - Global step 1200 Train loss 0.27 EM 0.03125 on epoch=599
03/02/2022 14:56:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=604
03/02/2022 14:56:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=609
03/02/2022 14:56:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
03/02/2022 14:56:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=619
03/02/2022 14:57:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=624
03/02/2022 14:57:07 - INFO - __main__ - Global step 1250 Train loss 0.28 EM 0.03125 on epoch=624
03/02/2022 14:57:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/02/2022 14:57:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/02/2022 14:57:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=639
03/02/2022 14:57:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=644
03/02/2022 14:57:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/02/2022 14:57:24 - INFO - __main__ - Global step 1300 Train loss 0.26 EM 0.03125 on epoch=649
03/02/2022 14:57:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
03/02/2022 14:57:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=659
03/02/2022 14:57:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/02/2022 14:57:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/02/2022 14:57:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=674
03/02/2022 14:57:41 - INFO - __main__ - Global step 1350 Train loss 0.26 EM 0.03125 on epoch=674
03/02/2022 14:57:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/02/2022 14:57:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
03/02/2022 14:57:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=689
03/02/2022 14:57:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=694
03/02/2022 14:57:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
03/02/2022 14:57:59 - INFO - __main__ - Global step 1400 Train loss 0.26 EM 0.0625 on epoch=699
03/02/2022 14:58:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.26 on epoch=704
03/02/2022 14:58:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=709
03/02/2022 14:58:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
03/02/2022 14:58:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/02/2022 14:58:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=724
03/02/2022 14:58:15 - INFO - __main__ - Global step 1450 Train loss 0.25 EM 0.0625 on epoch=724
03/02/2022 14:58:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
03/02/2022 14:58:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
03/02/2022 14:58:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=739
03/02/2022 14:58:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=744
03/02/2022 14:58:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
03/02/2022 14:58:32 - INFO - __main__ - Global step 1500 Train loss 0.24 EM 0.0625 on epoch=749
03/02/2022 14:58:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/02/2022 14:58:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/02/2022 14:58:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=764
03/02/2022 14:58:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
03/02/2022 14:58:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
03/02/2022 14:58:48 - INFO - __main__ - Global step 1550 Train loss 0.23 EM 0.03125 on epoch=774
03/02/2022 14:58:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/02/2022 14:58:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/02/2022 14:58:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/02/2022 14:58:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/02/2022 14:58:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
03/02/2022 14:59:05 - INFO - __main__ - Global step 1600 Train loss 0.23 EM 0.0625 on epoch=799
03/02/2022 14:59:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
03/02/2022 14:59:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/02/2022 14:59:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
03/02/2022 14:59:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
03/02/2022 14:59:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/02/2022 14:59:23 - INFO - __main__ - Global step 1650 Train loss 0.22 EM 0.03125 on epoch=824
03/02/2022 14:59:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
03/02/2022 14:59:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
03/02/2022 14:59:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=839
03/02/2022 14:59:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
03/02/2022 14:59:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=849
03/02/2022 14:59:40 - INFO - __main__ - Global step 1700 Train loss 0.22 EM 0.0625 on epoch=849
03/02/2022 14:59:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/02/2022 14:59:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=859
03/02/2022 14:59:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/02/2022 14:59:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
03/02/2022 14:59:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=874
03/02/2022 14:59:58 - INFO - __main__ - Global step 1750 Train loss 0.21 EM 0.03125 on epoch=874
03/02/2022 15:00:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
03/02/2022 15:00:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/02/2022 15:00:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=889
03/02/2022 15:00:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
03/02/2022 15:00:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/02/2022 15:00:14 - INFO - __main__ - Global step 1800 Train loss 0.20 EM 0.0625 on epoch=899
03/02/2022 15:00:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
03/02/2022 15:00:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=909
03/02/2022 15:00:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/02/2022 15:00:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
03/02/2022 15:00:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=924
03/02/2022 15:00:31 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0625 on epoch=924
03/02/2022 15:00:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
03/02/2022 15:00:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=934
03/02/2022 15:00:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=939
03/02/2022 15:00:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=944
03/02/2022 15:00:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
03/02/2022 15:00:48 - INFO - __main__ - Global step 1900 Train loss 0.20 EM 0.0625 on epoch=949
03/02/2022 15:00:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=954
03/02/2022 15:00:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=959
03/02/2022 15:00:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
03/02/2022 15:00:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=969
03/02/2022 15:00:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
03/02/2022 15:01:06 - INFO - __main__ - Global step 1950 Train loss 0.19 EM 0.0625 on epoch=974
03/02/2022 15:01:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
03/02/2022 15:01:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/02/2022 15:01:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=989
03/02/2022 15:01:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
03/02/2022 15:01:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=999
03/02/2022 15:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 15:01:19 - INFO - __main__ - Printing 3 examples
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/02/2022 15:01:19 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/02/2022 15:01:19 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/02/2022 15:01:19 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/02/2022 15:01:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 15:01:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 15:01:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 15:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 15:01:19 - INFO - __main__ - Printing 3 examples
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/02/2022 15:01:19 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/02/2022 15:01:19 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/02/2022 15:01:19 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/02/2022 15:01:19 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/02/2022 15:01:19 - INFO - __main__ - Tokenizing Input ...
03/02/2022 15:01:19 - INFO - __main__ - Tokenizing Output ...
03/02/2022 15:01:19 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 15:01:22 - INFO - __main__ - Global step 2000 Train loss 0.19 EM 0.03125 on epoch=999
03/02/2022 15:01:22 - INFO - __main__ - save last model!
03/02/2022 15:01:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/02/2022 15:01:22 - INFO - __main__ - Start tokenizing ... 7760 instances
03/02/2022 15:01:22 - INFO - __main__ - Printing 3 examples
03/02/2022 15:01:22 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
03/02/2022 15:01:22 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
03/02/2022 15:01:22 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
03/02/2022 15:01:22 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
03/02/2022 15:01:22 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
03/02/2022 15:01:22 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
03/02/2022 15:01:22 - INFO - __main__ - Tokenizing Input ...
03/02/2022 15:01:25 - INFO - __main__ - Tokenizing Output ...
03/02/2022 15:01:31 - INFO - __main__ - load prompt embedding from ckpt
03/02/2022 15:01:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/02/2022 15:01:32 - INFO - __main__ - Starting training!
03/02/2022 15:01:33 - INFO - __main__ - Loaded 7760 examples from test data
03/02/2022 15:26:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR/break-QDMR_32_100_0.5_8_predictions.txt
03/02/2022 15:26:55 - INFO - __main__ - EM on test data: 0.0329
03/02/2022 15:26:56 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.5, bsz=8, dev_performance=0.125, test_performance=0.03286082474226804
03/02/2022 15:26:56 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.4, bsz=8 ...
03/02/2022 15:26:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 15:26:57 - INFO - __main__ - Printing 3 examples
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
03/02/2022 15:26:57 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
03/02/2022 15:26:57 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
03/02/2022 15:26:57 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
03/02/2022 15:26:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/02/2022 15:26:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 15:26:57 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/02/2022 15:26:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/02/2022 15:26:57 - INFO - __main__ - Printing 3 examples
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
03/02/2022 15:26:57 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
03/02/2022 15:26:57 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
03/02/2022 15:26:57 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
03/02/2022 15:26:57 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
03/02/2022 15:26:57 - INFO - __main__ - Tokenizing Input ...
03/02/2022 15:26:57 - INFO - __main__ - Tokenizing Output ...
03/02/2022 15:26:57 - INFO - __main__ - Loaded 32 examples from dev data
03/02/2022 15:27:09 - INFO - __main__ - load prompt embedding from ckpt
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/utils/versions.py", line 105, in require_version
  File "/opt/conda/envs/meta/lib/python3.9/importlib/metadata.py", line 551, in version
    return distribution(distribution_name).version
  File "/opt/conda/envs/meta/lib/python3.9/importlib/metadata.py", line 524, in distribution
    return Distribution.from_name(distribution_name)
  File "/opt/conda/envs/meta/lib/python3.9/importlib/metadata.py", line 187, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: torch

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/run_singletask_ddp_prompt.py", line 88, in run
    optimizer = OSS(params=filter(lambda p: p.requires_grad, model.parameters()), optim=optimizer,
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/fairscale/optim/oss.py", line 165, in __init__
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/fairscale/optim/oss.py", line 475, in refresh_trainable
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/optimization.py", line 463, in __init__
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/utils/versions.py", line 107, in require_version
importlib.metadata.PackageNotFoundError: The 'torch>=1.5.0' distribution was not found and is required by this application. 
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3559) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_1/1/error.json
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>

    import numpy as np
ModuleNotFoundError: No module named 'numpy'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6858) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_2/1/error.json
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>

    import numpy as np
ModuleNotFoundError: No module named 'numpy'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6864) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_prb2155u/none_fkpt0qp4/attempt_3/1/error.json
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>
    import numpy as np
ModuleNotFoundError: Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 26, in <module>
No module named 'numpy'
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6866) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.002607583999633789 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "6866", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 2263, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "6867", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 2263, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 2263, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 6866 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     singletask_from_meta.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-03-02_15:27:27
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 6866)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-03-02_15:27:27
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 6867)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (6872): No such process
kill: write error: Disk quota exceeded
Task: freebase_qa, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6878): No such process
kill: write error: Disk quota exceeded
Task: glue-qnli, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6884): No such process
kill: write error: Disk quota exceeded
Task: hatexplain, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6890): No such process
kill: write error: Disk quota exceeded
Task: circa, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6896): No such process
kill: write error: Disk quota exceeded
Task: wiki_split, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6902): No such process
kill: write error: Disk quota exceeded
Task: break-QDMR, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6908): No such process
kill: write error: Disk quota exceeded
Task: freebase_qa, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6914): No such process
kill: write error: Disk quota exceeded
Task: glue-qnli, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6920): No such process
kill: write error: Disk quota exceeded
Task: hatexplain, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6926): No such process
kill: write error: Disk quota exceeded
Task: circa, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6932): No such process
kill: write error: Disk quota exceeded
Task: wiki_split, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6938): No such process
kill: write error: Disk quota exceeded
Task: break-QDMR, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/bin/python: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
++++++++++++++++++++++++++++++
kill: (6944): No such process
kill: write error: Disk quota exceeded
