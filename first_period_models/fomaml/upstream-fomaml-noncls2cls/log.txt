01/05/2022 13:33:21 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', identifier='large', output_dir='models/upstream-fomaml-noncls2cls', do_train=True, do_predict=False, inner_bsz=4, inner_lr=3e-05, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=4, num_train_epochs=120.0, warmup_steps=360, total_steps=2500, wait_step=10000000000, verbose=False, eval_period=10, prefix='', debug=False, seed=42, custom_tasks_splits='./dataloader/custom_tasks_splits/train_nonclassification_test_classification.json', cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=-1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='3')
01/05/2022 13:33:21 - INFO - __main__ - models/upstream-fomaml-noncls2cls
01/05/2022 13:33:21 - INFO - __main__ - args.device: cuda
01/05/2022 13:33:21 - INFO - __main__ - Using 1 gpus
01/05/2022 13:33:22 - INFO - __main__ - Training on the following tasks: ['ade_corpus_v2-dosage', 'art', 'biomrc', 'blimp-anaphor_number_agreement', 'blimp-ellipsis_n_bar_2', 'blimp-sentential_negation_npi_licensor_present', 'blimp-sentential_negation_npi_scope', 'break-QDMR-high-level', 'commonsense_qa', 'crows_pairs', 'dream', 'duorc', 'eli5-asks', 'eli5-eli5', 'freebase_qa', 'gigaword', 'hellaswag', 'hotpot_qa', 'kilt_ay2', 'kilt_hotpotqa', 'kilt_trex', 'kilt_zsre', 'lama-conceptnet', 'lama-google_re', 'lama-squad', 'math_qa', 'numer_sense', 'openbookqa', 'piqa', 'proto_qa', 'qa_srl', 'quarel', 'quartz-no_knowledge', 'race-high', 'reddit_tifu-title', 'reddit_tifu-tldr', 'ropes', 'sciq', 'social_i_qa', 'spider', 'superglue-multirc', 'wiki_bio', 'wikisql', 'xsum', 'yelp_review_full']
01/05/2022 13:33:27 - INFO - __main__ - Start tokenizing ... 225 instances
01/05/2022 13:33:27 - INFO - __main__ - Printing 3 examples
01/05/2022 13:33:27 - INFO - __main__ -  [ade_corpus_v2-dosage] A small initial dose of prazosin ranging from 0.5 to 1 mg has been recommended to avoid the first-dose phenomenon characterized by a sudden and severe drop in blood pressure after the administration of the first dose of prazosin. [SEP] prazosin
01/05/2022 13:33:27 - INFO - __main__ -  0.5 to 1 mg
01/05/2022 13:33:27 - INFO - __main__ -  [ade_corpus_v2-dosage] High-dose intravenous mannitol infusion in various clinical settings may result in acute renal failure (ARF). [SEP] mannitol
01/05/2022 13:33:27 - INFO - __main__ -  High-dose
01/05/2022 13:33:27 - INFO - __main__ -  [ade_corpus_v2-dosage] This paper reports on a 6.9-year-old autistic male who developed repeated episodes of acute dystonic reactions associated with pimozide administration at the doses of 0.096 mg/kg/day and 0.032 mg/kg/day and 32 hours following pimozide withdrawal, as well as during subsequent thioridazine administration. [SEP] pimozide
01/05/2022 13:33:27 - INFO - __main__ -  0.032 mg/kg/day
01/05/2022 13:33:27 - INFO - __main__ - Tokenizing Train Input ...
01/05/2022 13:33:32 - INFO - __main__ - Tokenizing Train Output ...
01/05/2022 13:33:36 - INFO - __main__ - Tokenizing Dev Input ...
01/05/2022 13:33:42 - INFO - __main__ - Tokenizing Dev Output ...
01/05/2022 13:33:55 - INFO - __main__ - Loaded 225 examples from train data
01/05/2022 13:34:11 - INFO - __main__ - try to initialize prompt embeddings
01/05/2022 13:34:16 - INFO - __main__ - Starting training!
01/05/2022 13:34:46 - INFO - __main__ - global step: 10; train loss: 4.834815979003906; dev loss: 4.725239276885986
01/05/2022 13:35:15 - INFO - __main__ - global step: 20; train loss: 4.888472080230713; dev loss: 4.695133209228516
01/05/2022 13:35:45 - INFO - __main__ - global step: 30; train loss: 4.0526885986328125; dev loss: 4.308211326599121
01/05/2022 13:36:14 - INFO - __main__ - global step: 40; train loss: 4.334166526794434; dev loss: 4.385439872741699
01/05/2022 13:36:44 - INFO - __main__ - global step: 50; train loss: 5.004183292388916; dev loss: 5.200080394744873
01/05/2022 13:37:13 - INFO - __main__ - global step: 60; train loss: 5.0153656005859375; dev loss: 4.759551048278809
01/05/2022 13:37:43 - INFO - __main__ - global step: 70; train loss: 4.538379669189453; dev loss: 4.642645835876465
01/05/2022 13:38:13 - INFO - __main__ - global step: 80; train loss: 5.043776988983154; dev loss: 4.878776550292969
01/05/2022 13:38:42 - INFO - __main__ - global step: 90; train loss: 5.674260139465332; dev loss: 5.647654056549072
01/05/2022 13:39:11 - INFO - __main__ - global step: 100; train loss: 5.080416202545166; dev loss: 5.218934059143066
01/05/2022 13:39:41 - INFO - __main__ - global step: 110; train loss: 4.607611656188965; dev loss: 4.759591102600098
01/05/2022 13:40:11 - INFO - __main__ - global step: 120; train loss: 5.7125115394592285; dev loss: 5.820974349975586
01/05/2022 13:40:41 - INFO - __main__ - global step: 130; train loss: 5.908072471618652; dev loss: 5.740704536437988
01/05/2022 13:41:11 - INFO - __main__ - global step: 140; train loss: 5.0157976150512695; dev loss: 5.127560615539551
01/05/2022 13:41:42 - INFO - __main__ - global step: 150; train loss: 4.995297908782959; dev loss: 5.22284460067749
01/05/2022 13:42:13 - INFO - __main__ - global step: 160; train loss: 5.326957702636719; dev loss: 5.142056941986084
01/05/2022 13:42:46 - INFO - __main__ - global step: 170; train loss: 4.904341697692871; dev loss: 4.894758701324463
01/05/2022 13:43:18 - INFO - __main__ - global step: 180; train loss: 4.6726884841918945; dev loss: 4.584712505340576
01/05/2022 13:43:51 - INFO - __main__ - global step: 190; train loss: 4.160459995269775; dev loss: 4.248405456542969
01/05/2022 13:44:23 - INFO - __main__ - global step: 200; train loss: 4.990866661071777; dev loss: 4.74558162689209
01/05/2022 13:44:55 - INFO - __main__ - global step: 210; train loss: 4.765812873840332; dev loss: 4.395961761474609
01/05/2022 13:45:26 - INFO - __main__ - global step: 220; train loss: 4.828404426574707; dev loss: 4.916197776794434
01/05/2022 13:45:58 - INFO - __main__ - global step: 230; train loss: 4.735394477844238; dev loss: 4.700413703918457
01/05/2022 13:46:30 - INFO - __main__ - global step: 240; train loss: 4.523165702819824; dev loss: 4.6081647872924805
01/05/2022 13:47:01 - INFO - __main__ - global step: 250; train loss: 4.736447334289551; dev loss: 4.777327060699463
01/05/2022 13:47:34 - INFO - __main__ - global step: 260; train loss: 4.245652675628662; dev loss: 4.251766204833984
01/05/2022 13:48:06 - INFO - __main__ - global step: 270; train loss: 4.7568488121032715; dev loss: 4.403112888336182
01/05/2022 13:48:38 - INFO - __main__ - global step: 280; train loss: 4.547968864440918; dev loss: 4.447149753570557
01/05/2022 13:49:08 - INFO - __main__ - global step: 290; train loss: 4.067961692810059; dev loss: 4.341937065124512
01/05/2022 13:49:39 - INFO - __main__ - global step: 300; train loss: 4.395695209503174; dev loss: 4.285479545593262
01/05/2022 13:50:11 - INFO - __main__ - global step: 310; train loss: 4.539931774139404; dev loss: 4.419989109039307
01/05/2022 13:50:42 - INFO - __main__ - global step: 320; train loss: 4.2310919761657715; dev loss: 4.203643798828125
01/05/2022 13:51:13 - INFO - __main__ - global step: 330; train loss: 4.370211601257324; dev loss: 4.377996921539307
01/05/2022 13:51:46 - INFO - __main__ - global step: 340; train loss: 4.287144660949707; dev loss: 4.2220892906188965
01/05/2022 13:52:18 - INFO - __main__ - global step: 350; train loss: 4.441656112670898; dev loss: 4.289137363433838
01/05/2022 13:52:50 - INFO - __main__ - global step: 360; train loss: 4.620171070098877; dev loss: 4.57949686050415
01/05/2022 13:53:22 - INFO - __main__ - global step: 370; train loss: 4.35338020324707; dev loss: 4.276810169219971
01/05/2022 13:53:55 - INFO - __main__ - global step: 380; train loss: 4.320609092712402; dev loss: 4.2753143310546875
01/05/2022 13:54:26 - INFO - __main__ - global step: 390; train loss: 4.404420375823975; dev loss: 4.322142601013184
01/05/2022 13:54:57 - INFO - __main__ - global step: 400; train loss: 4.069867134094238; dev loss: 4.242081165313721
01/05/2022 13:55:28 - INFO - __main__ - global step: 410; train loss: 4.231463432312012; dev loss: 4.165334224700928
01/05/2022 13:56:00 - INFO - __main__ - global step: 420; train loss: 4.191593170166016; dev loss: 4.119585990905762
01/05/2022 13:56:31 - INFO - __main__ - global step: 430; train loss: 3.9244072437286377; dev loss: 4.031552314758301
01/05/2022 13:57:02 - INFO - __main__ - global step: 440; train loss: 4.215543746948242; dev loss: 4.390995025634766
01/05/2022 13:57:33 - INFO - __main__ - global step: 450; train loss: 4.787210464477539; dev loss: 4.495755195617676
01/05/2022 13:58:05 - INFO - __main__ - global step: 460; train loss: 4.155754089355469; dev loss: 4.387601852416992
01/05/2022 13:58:36 - INFO - __main__ - global step: 470; train loss: 4.421016216278076; dev loss: 4.584558010101318
01/05/2022 13:59:07 - INFO - __main__ - global step: 480; train loss: 4.940919876098633; dev loss: 4.906308650970459
01/05/2022 13:59:38 - INFO - __main__ - global step: 490; train loss: 4.5949602127075195; dev loss: 4.632969856262207
01/05/2022 14:00:10 - INFO - __main__ - global step: 500; train loss: 4.790687561035156; dev loss: 4.53952693939209
01/05/2022 14:00:41 - INFO - __main__ - global step: 510; train loss: 4.614283561706543; dev loss: 4.563701629638672
01/05/2022 14:01:13 - INFO - __main__ - global step: 520; train loss: 4.430832862854004; dev loss: 4.462819576263428
01/05/2022 14:01:44 - INFO - __main__ - global step: 530; train loss: 4.31273889541626; dev loss: 4.3392791748046875
01/05/2022 14:02:15 - INFO - __main__ - global step: 540; train loss: 4.562966346740723; dev loss: 4.309332847595215
01/05/2022 14:02:47 - INFO - __main__ - global step: 550; train loss: 4.353847503662109; dev loss: 4.6061906814575195
01/05/2022 14:03:19 - INFO - __main__ - global step: 560; train loss: 4.115234851837158; dev loss: 4.340788841247559
01/05/2022 14:03:51 - INFO - __main__ - global step: 570; train loss: 4.084397792816162; dev loss: 4.157289981842041
01/05/2022 14:04:21 - INFO - __main__ - global step: 580; train loss: 4.326308250427246; dev loss: 4.394768714904785
01/05/2022 14:04:53 - INFO - __main__ - global step: 590; train loss: 4.432923793792725; dev loss: 4.293563365936279
01/05/2022 14:05:23 - INFO - __main__ - global step: 600; train loss: 4.441551208496094; dev loss: 4.4713358879089355
01/05/2022 14:05:55 - INFO - __main__ - global step: 610; train loss: 4.079059600830078; dev loss: 4.01096248626709
01/05/2022 14:06:27 - INFO - __main__ - global step: 620; train loss: 4.041110515594482; dev loss: 4.210195064544678
01/05/2022 14:06:58 - INFO - __main__ - global step: 630; train loss: 4.0264434814453125; dev loss: 4.115537166595459
01/05/2022 14:07:29 - INFO - __main__ - global step: 640; train loss: 4.090780735015869; dev loss: 4.099689960479736
01/05/2022 14:08:01 - INFO - __main__ - global step: 650; train loss: 3.9485745429992676; dev loss: 3.9366965293884277
01/05/2022 14:08:32 - INFO - __main__ - global step: 660; train loss: 4.096367835998535; dev loss: 4.105534553527832
01/05/2022 14:09:04 - INFO - __main__ - global step: 670; train loss: 4.04934549331665; dev loss: 4.192692756652832
01/05/2022 14:09:35 - INFO - __main__ - global step: 680; train loss: 3.6290640830993652; dev loss: 3.964261531829834
01/05/2022 14:10:06 - INFO - __main__ - global step: 690; train loss: 4.26686429977417; dev loss: 4.240792274475098
01/05/2022 14:10:37 - INFO - __main__ - global step: 700; train loss: 4.027096748352051; dev loss: 4.1475090980529785
01/05/2022 14:11:09 - INFO - __main__ - global step: 710; train loss: 4.015389442443848; dev loss: 4.010120868682861
01/05/2022 14:11:40 - INFO - __main__ - global step: 720; train loss: 3.8501288890838623; dev loss: 3.9901785850524902
01/05/2022 14:12:11 - INFO - __main__ - global step: 730; train loss: 4.075050354003906; dev loss: 4.063953399658203
01/05/2022 14:12:41 - INFO - __main__ - global step: 740; train loss: 4.259049892425537; dev loss: 4.186563014984131
01/05/2022 14:13:12 - INFO - __main__ - global step: 750; train loss: 4.760862827301025; dev loss: 4.89816951751709
01/05/2022 14:13:42 - INFO - __main__ - global step: 760; train loss: 5.070635795593262; dev loss: 5.477339267730713
01/05/2022 14:14:13 - INFO - __main__ - global step: 770; train loss: 5.302044868469238; dev loss: 5.36466121673584
01/05/2022 14:14:43 - INFO - __main__ - global step: 780; train loss: 5.481351375579834; dev loss: 5.847336769104004
01/05/2022 14:15:14 - INFO - __main__ - global step: 790; train loss: 4.229142189025879; dev loss: 4.253983497619629
01/05/2022 14:15:45 - INFO - __main__ - global step: 800; train loss: 5.622570991516113; dev loss: 5.444794654846191
01/05/2022 14:16:14 - INFO - __main__ - global step: 810; train loss: 5.8113884925842285; dev loss: 5.633143901824951
01/05/2022 14:16:45 - INFO - __main__ - global step: 820; train loss: 5.151715278625488; dev loss: 5.083006858825684
01/05/2022 14:17:15 - INFO - __main__ - global step: 830; train loss: 5.058133602142334; dev loss: 5.194812774658203
01/05/2022 14:17:45 - INFO - __main__ - global step: 840; train loss: 4.712471961975098; dev loss: 4.737183570861816
01/05/2022 14:18:17 - INFO - __main__ - global step: 850; train loss: 5.432546138763428; dev loss: 5.44589900970459
01/05/2022 14:18:49 - INFO - __main__ - global step: 860; train loss: 5.266091823577881; dev loss: 5.2637128829956055
01/05/2022 14:19:21 - INFO - __main__ - global step: 870; train loss: 4.548140048980713; dev loss: 4.391873359680176
01/05/2022 14:19:53 - INFO - __main__ - global step: 880; train loss: 4.876564025878906; dev loss: 4.982295036315918
01/05/2022 14:20:26 - INFO - __main__ - global step: 890; train loss: 5.170285701751709; dev loss: 5.067190647125244
01/05/2022 14:20:58 - INFO - __main__ - global step: 900; train loss: 5.33528995513916; dev loss: 5.337337493896484
01/05/2022 14:21:29 - INFO - __main__ - global step: 910; train loss: 5.755366325378418; dev loss: 5.658801078796387
01/05/2022 14:22:00 - INFO - __main__ - global step: 920; train loss: 6.199723243713379; dev loss: 6.197720527648926
01/05/2022 14:22:31 - INFO - __main__ - global step: 930; train loss: 5.640664100646973; dev loss: 5.4604644775390625
01/05/2022 14:23:03 - INFO - __main__ - global step: 940; train loss: 5.5242018699646; dev loss: 5.5361008644104
01/05/2022 14:23:34 - INFO - __main__ - global step: 950; train loss: 5.481038570404053; dev loss: 5.62264347076416
01/05/2022 14:24:05 - INFO - __main__ - global step: 960; train loss: 5.367804050445557; dev loss: 5.39197301864624
01/05/2022 14:24:36 - INFO - __main__ - global step: 970; train loss: 5.466301918029785; dev loss: 5.716921806335449
01/05/2022 14:25:06 - INFO - __main__ - global step: 980; train loss: 5.508255958557129; dev loss: 5.35589599609375
01/05/2022 14:25:38 - INFO - __main__ - global step: 990; train loss: 5.497762680053711; dev loss: 5.4361891746521
01/05/2022 14:26:10 - INFO - __main__ - global step: 1000; train loss: 5.021214008331299; dev loss: 4.99977970123291
01/05/2022 14:26:42 - INFO - __main__ - global step: 1010; train loss: 5.295290470123291; dev loss: 5.474134922027588
01/05/2022 14:27:13 - INFO - __main__ - global step: 1020; train loss: 5.240210056304932; dev loss: 5.166594505310059
01/05/2022 14:27:44 - INFO - __main__ - global step: 1030; train loss: 5.364506721496582; dev loss: 5.410226821899414
01/05/2022 14:28:14 - INFO - __main__ - global step: 1040; train loss: 5.0185112953186035; dev loss: 5.111374855041504
01/05/2022 14:28:44 - INFO - __main__ - global step: 1050; train loss: 5.045374870300293; dev loss: 5.039297580718994
01/05/2022 14:29:16 - INFO - __main__ - global step: 1060; train loss: 4.95913028717041; dev loss: 4.958406925201416
01/05/2022 14:29:48 - INFO - __main__ - global step: 1070; train loss: 4.942564964294434; dev loss: 5.0893964767456055
01/05/2022 14:30:20 - INFO - __main__ - global step: 1080; train loss: 4.80593204498291; dev loss: 4.577362537384033
01/05/2022 14:30:51 - INFO - __main__ - global step: 1090; train loss: 4.860631465911865; dev loss: 4.921456336975098
01/05/2022 14:31:24 - INFO - __main__ - global step: 1100; train loss: 4.906999588012695; dev loss: 4.917054653167725
01/05/2022 14:31:55 - INFO - __main__ - global step: 1110; train loss: 5.391892433166504; dev loss: 5.268243789672852
01/05/2022 14:32:26 - INFO - __main__ - global step: 1120; train loss: 4.787309169769287; dev loss: 4.831061363220215
01/05/2022 14:32:57 - INFO - __main__ - global step: 1130; train loss: 5.386064052581787; dev loss: 5.27720832824707
01/05/2022 14:33:28 - INFO - __main__ - global step: 1140; train loss: 5.2318339347839355; dev loss: 5.166806221008301
01/05/2022 14:33:59 - INFO - __main__ - global step: 1150; train loss: 4.964201927185059; dev loss: 4.977311611175537
01/05/2022 14:34:32 - INFO - __main__ - global step: 1160; train loss: 5.1587018966674805; dev loss: 4.867409706115723
01/05/2022 14:35:04 - INFO - __main__ - global step: 1170; train loss: 4.769730567932129; dev loss: 5.046123504638672
01/05/2022 14:35:35 - INFO - __main__ - global step: 1180; train loss: 4.93689489364624; dev loss: 4.796712398529053
01/05/2022 14:36:07 - INFO - __main__ - global step: 1190; train loss: 4.719322204589844; dev loss: 4.601222991943359
01/05/2022 14:36:38 - INFO - __main__ - global step: 1200; train loss: 5.316678047180176; dev loss: 4.910229682922363
01/05/2022 14:37:10 - INFO - __main__ - global step: 1210; train loss: 4.727505683898926; dev loss: 4.553122043609619
01/05/2022 14:37:41 - INFO - __main__ - global step: 1220; train loss: 5.194235801696777; dev loss: 5.245021820068359
01/05/2022 14:38:13 - INFO - __main__ - global step: 1230; train loss: 4.925843715667725; dev loss: 4.967827796936035
01/05/2022 14:38:44 - INFO - __main__ - global step: 1240; train loss: 5.294778347015381; dev loss: 5.294326305389404
01/05/2022 14:39:16 - INFO - __main__ - global step: 1250; train loss: 4.8056640625; dev loss: 4.701756954193115
01/05/2022 14:39:47 - INFO - __main__ - global step: 1260; train loss: 4.8737664222717285; dev loss: 4.6175971031188965
01/05/2022 14:40:19 - INFO - __main__ - global step: 1270; train loss: 5.279000282287598; dev loss: 5.231532096862793
01/05/2022 14:40:49 - INFO - __main__ - global step: 1280; train loss: 4.892056465148926; dev loss: 4.982007026672363
01/05/2022 14:41:19 - INFO - __main__ - global step: 1290; train loss: 5.041630268096924; dev loss: 5.0979323387146
01/05/2022 14:41:51 - INFO - __main__ - global step: 1300; train loss: 4.871791362762451; dev loss: 4.877176761627197
01/05/2022 14:42:22 - INFO - __main__ - global step: 1310; train loss: 4.904179573059082; dev loss: 5.244095802307129
01/05/2022 14:42:53 - INFO - __main__ - global step: 1320; train loss: 4.879802227020264; dev loss: 5.045909881591797
01/05/2022 14:43:25 - INFO - __main__ - global step: 1330; train loss: 4.985936164855957; dev loss: 4.935990333557129
01/05/2022 14:43:56 - INFO - __main__ - global step: 1340; train loss: 5.294839382171631; dev loss: 5.210806846618652
01/05/2022 14:44:27 - INFO - __main__ - global step: 1350; train loss: 4.985968589782715; dev loss: 4.990786552429199
01/05/2022 14:44:58 - INFO - __main__ - global step: 1360; train loss: 5.017175197601318; dev loss: 4.992197513580322
01/05/2022 14:45:30 - INFO - __main__ - global step: 1370; train loss: 4.915445327758789; dev loss: 4.80387020111084
01/05/2022 14:46:00 - INFO - __main__ - global step: 1380; train loss: 4.657038688659668; dev loss: 4.656195640563965
01/05/2022 14:46:31 - INFO - __main__ - global step: 1390; train loss: 5.0674028396606445; dev loss: 4.752043724060059
01/05/2022 14:47:02 - INFO - __main__ - global step: 1400; train loss: 5.2684431076049805; dev loss: 5.268588542938232
01/05/2022 14:47:33 - INFO - __main__ - global step: 1410; train loss: 5.043853759765625; dev loss: 4.755904674530029
01/05/2022 14:48:04 - INFO - __main__ - global step: 1420; train loss: 4.772156715393066; dev loss: 4.850848197937012
01/05/2022 14:48:35 - INFO - __main__ - global step: 1430; train loss: 5.168187141418457; dev loss: 5.137495994567871
01/05/2022 14:49:06 - INFO - __main__ - global step: 1440; train loss: 4.931534767150879; dev loss: 4.876547336578369
01/05/2022 14:49:37 - INFO - __main__ - global step: 1450; train loss: 4.88683557510376; dev loss: 4.930245876312256
01/05/2022 14:50:08 - INFO - __main__ - global step: 1460; train loss: 4.6204514503479; dev loss: 5.056391716003418
01/05/2022 14:50:40 - INFO - __main__ - global step: 1470; train loss: 4.9861626625061035; dev loss: 5.034841537475586
01/05/2022 14:51:12 - INFO - __main__ - global step: 1480; train loss: 5.040158748626709; dev loss: 4.9184136390686035
01/05/2022 14:51:44 - INFO - __main__ - global step: 1490; train loss: 4.892124176025391; dev loss: 4.877520561218262
01/05/2022 14:52:16 - INFO - __main__ - global step: 1500; train loss: 5.023035526275635; dev loss: 4.997955322265625
01/05/2022 14:52:48 - INFO - __main__ - global step: 1510; train loss: 4.8931660652160645; dev loss: 5.089592933654785
01/05/2022 14:53:19 - INFO - __main__ - global step: 1520; train loss: 5.006887912750244; dev loss: 4.992592811584473
01/05/2022 14:53:50 - INFO - __main__ - global step: 1530; train loss: 4.825605392456055; dev loss: 4.746359825134277
01/05/2022 14:54:21 - INFO - __main__ - global step: 1540; train loss: 5.057913303375244; dev loss: 4.967203617095947
01/05/2022 14:54:50 - INFO - __main__ - global step: 1550; train loss: 5.0182905197143555; dev loss: 5.159374713897705
01/05/2022 14:55:21 - INFO - __main__ - global step: 1560; train loss: 4.809624671936035; dev loss: 4.663567066192627
01/05/2022 14:55:51 - INFO - __main__ - global step: 1570; train loss: 4.979792594909668; dev loss: 4.8746657371521
01/05/2022 14:56:22 - INFO - __main__ - global step: 1580; train loss: 4.797446250915527; dev loss: 4.770809650421143
01/05/2022 14:56:52 - INFO - __main__ - global step: 1590; train loss: 5.010191917419434; dev loss: 5.102718353271484
01/05/2022 14:57:22 - INFO - __main__ - global step: 1600; train loss: 4.797214508056641; dev loss: 4.580356597900391
01/05/2022 14:57:52 - INFO - __main__ - global step: 1610; train loss: 4.661318302154541; dev loss: 4.729792594909668
01/05/2022 14:58:22 - INFO - __main__ - global step: 1620; train loss: 4.862160682678223; dev loss: 4.690672874450684
01/05/2022 14:58:53 - INFO - __main__ - global step: 1630; train loss: 4.830422401428223; dev loss: 4.985546588897705
01/05/2022 14:59:25 - INFO - __main__ - global step: 1640; train loss: 4.476480960845947; dev loss: 4.626107215881348
01/05/2022 14:59:56 - INFO - __main__ - global step: 1650; train loss: 5.047253131866455; dev loss: 5.078805446624756
01/05/2022 15:00:26 - INFO - __main__ - global step: 1660; train loss: 4.957767486572266; dev loss: 5.078493595123291
01/05/2022 15:00:57 - INFO - __main__ - global step: 1670; train loss: 4.797544956207275; dev loss: 4.904001712799072
01/05/2022 15:01:28 - INFO - __main__ - global step: 1680; train loss: 4.789588928222656; dev loss: 4.657413959503174
01/05/2022 15:01:57 - INFO - __main__ - global step: 1690; train loss: 5.07288932800293; dev loss: 5.052536487579346
01/05/2022 15:02:28 - INFO - __main__ - global step: 1700; train loss: 4.63301944732666; dev loss: 4.79829216003418
01/05/2022 15:02:59 - INFO - __main__ - global step: 1710; train loss: 4.900717735290527; dev loss: 4.982834815979004
01/05/2022 15:03:31 - INFO - __main__ - global step: 1720; train loss: 4.959412097930908; dev loss: 5.0018486976623535
01/05/2022 15:04:02 - INFO - __main__ - global step: 1730; train loss: 4.7875237464904785; dev loss: 4.741203308105469
01/05/2022 15:04:34 - INFO - __main__ - global step: 1740; train loss: 4.466784954071045; dev loss: 4.426693916320801
01/05/2022 15:05:05 - INFO - __main__ - global step: 1750; train loss: 4.676270484924316; dev loss: 4.822454452514648
01/05/2022 15:05:36 - INFO - __main__ - global step: 1760; train loss: 4.619166851043701; dev loss: 4.662192344665527
01/05/2022 15:06:07 - INFO - __main__ - global step: 1770; train loss: 4.894828796386719; dev loss: 4.715183258056641
01/05/2022 15:06:37 - INFO - __main__ - global step: 1780; train loss: 4.8581743240356445; dev loss: 4.569271564483643
01/05/2022 15:07:07 - INFO - __main__ - global step: 1790; train loss: 4.733811378479004; dev loss: 4.819283485412598
01/05/2022 15:07:37 - INFO - __main__ - global step: 1800; train loss: 4.669083595275879; dev loss: 4.885826587677002
01/05/2022 15:08:07 - INFO - __main__ - global step: 1810; train loss: 4.754754066467285; dev loss: 4.909448623657227
01/05/2022 15:08:37 - INFO - __main__ - global step: 1820; train loss: 5.142138481140137; dev loss: 4.7784528732299805
01/05/2022 15:09:08 - INFO - __main__ - global step: 1830; train loss: 4.594282150268555; dev loss: 4.766434669494629
01/05/2022 15:09:37 - INFO - __main__ - global step: 1840; train loss: 4.681894779205322; dev loss: 4.591606616973877
01/05/2022 15:10:08 - INFO - __main__ - global step: 1850; train loss: 4.783563137054443; dev loss: 4.675370693206787
01/05/2022 15:10:39 - INFO - __main__ - global step: 1860; train loss: 4.499821662902832; dev loss: 4.6010422706604
01/05/2022 15:11:11 - INFO - __main__ - global step: 1870; train loss: 5.044187545776367; dev loss: 5.046982288360596
01/05/2022 15:11:43 - INFO - __main__ - global step: 1880; train loss: 4.556283473968506; dev loss: 4.54906702041626
01/05/2022 15:12:14 - INFO - __main__ - global step: 1890; train loss: 4.747469902038574; dev loss: 4.7898406982421875
01/05/2022 15:12:45 - INFO - __main__ - global step: 1900; train loss: 4.34438419342041; dev loss: 4.269864082336426
01/05/2022 15:13:15 - INFO - __main__ - global step: 1910; train loss: 4.877712249755859; dev loss: 4.861807346343994
01/05/2022 15:13:46 - INFO - __main__ - global step: 1920; train loss: 4.6962809562683105; dev loss: 4.755767345428467
01/05/2022 15:14:16 - INFO - __main__ - global step: 1930; train loss: 4.932955741882324; dev loss: 4.878931522369385
01/05/2022 15:14:46 - INFO - __main__ - global step: 1940; train loss: 4.623257637023926; dev loss: 4.80226993560791
01/05/2022 15:15:17 - INFO - __main__ - global step: 1950; train loss: 4.4580841064453125; dev loss: 4.639915466308594
01/05/2022 15:15:48 - INFO - __main__ - global step: 1960; train loss: 4.756739139556885; dev loss: 4.610462665557861
01/05/2022 15:16:18 - INFO - __main__ - global step: 1970; train loss: 4.594046592712402; dev loss: 4.5692853927612305
01/05/2022 15:16:49 - INFO - __main__ - global step: 1980; train loss: 4.854699611663818; dev loss: 4.4459757804870605
01/05/2022 15:17:19 - INFO - __main__ - global step: 1990; train loss: 5.131705284118652; dev loss: 5.161101818084717
01/05/2022 15:17:50 - INFO - __main__ - global step: 2000; train loss: 4.870491981506348; dev loss: 4.7159929275512695
01/05/2022 15:18:20 - INFO - __main__ - global step: 2010; train loss: 4.594944953918457; dev loss: 4.474419116973877
01/05/2022 15:18:52 - INFO - __main__ - global step: 2020; train loss: 4.313287258148193; dev loss: 4.38260555267334
01/05/2022 15:19:23 - INFO - __main__ - global step: 2030; train loss: 4.440881729125977; dev loss: 4.315788269042969
01/05/2022 15:19:55 - INFO - __main__ - global step: 2040; train loss: 4.449315547943115; dev loss: 4.402295112609863
01/05/2022 15:20:27 - INFO - __main__ - global step: 2050; train loss: 4.8470282554626465; dev loss: 4.8780927658081055
01/05/2022 15:20:57 - INFO - __main__ - global step: 2060; train loss: 5.326722621917725; dev loss: 5.52606725692749
01/05/2022 15:21:28 - INFO - __main__ - global step: 2070; train loss: 5.058666229248047; dev loss: 5.4072675704956055
01/05/2022 15:21:59 - INFO - __main__ - global step: 2080; train loss: 4.9129862785339355; dev loss: 4.857247352600098
01/05/2022 15:22:30 - INFO - __main__ - global step: 2090; train loss: 4.945521354675293; dev loss: 4.9034576416015625
01/05/2022 15:23:02 - INFO - __main__ - global step: 2100; train loss: 4.652082443237305; dev loss: 4.913003921508789
01/05/2022 15:23:33 - INFO - __main__ - global step: 2110; train loss: 4.6771440505981445; dev loss: 4.693102836608887
01/05/2022 15:24:05 - INFO - __main__ - global step: 2120; train loss: 4.823025703430176; dev loss: 4.816520690917969
01/05/2022 15:24:37 - INFO - __main__ - global step: 2130; train loss: 4.990075588226318; dev loss: 4.944836616516113
01/05/2022 15:25:10 - INFO - __main__ - global step: 2140; train loss: 4.833517551422119; dev loss: 4.852550029754639
01/05/2022 15:25:42 - INFO - __main__ - global step: 2150; train loss: 4.987865924835205; dev loss: 4.874159336090088
01/05/2022 15:26:13 - INFO - __main__ - global step: 2160; train loss: 5.236433029174805; dev loss: 5.244093418121338
01/05/2022 15:26:45 - INFO - __main__ - global step: 2170; train loss: 5.035147190093994; dev loss: 4.932238578796387
01/05/2022 15:27:16 - INFO - __main__ - global step: 2180; train loss: 4.785212516784668; dev loss: 5.147261619567871
01/05/2022 15:27:47 - INFO - __main__ - global step: 2190; train loss: 4.767703533172607; dev loss: 4.9284987449646
01/05/2022 15:28:19 - INFO - __main__ - global step: 2200; train loss: 4.52679443359375; dev loss: 4.400511741638184
01/05/2022 15:28:50 - INFO - __main__ - global step: 2210; train loss: 4.60439920425415; dev loss: 4.6439385414123535
01/05/2022 15:29:21 - INFO - __main__ - global step: 2220; train loss: 4.872363090515137; dev loss: 4.662989616394043
01/05/2022 15:29:52 - INFO - __main__ - global step: 2230; train loss: 4.409535884857178; dev loss: 4.449369430541992
01/05/2022 15:30:23 - INFO - __main__ - global step: 2240; train loss: 4.8807291984558105; dev loss: 4.772914886474609
01/05/2022 15:30:56 - INFO - __main__ - global step: 2250; train loss: 4.385141849517822; dev loss: 4.501124382019043
01/05/2022 15:31:27 - INFO - __main__ - global step: 2260; train loss: 4.800928592681885; dev loss: 4.5126824378967285
01/05/2022 15:31:58 - INFO - __main__ - global step: 2270; train loss: 4.678206443786621; dev loss: 4.564145565032959
01/05/2022 15:32:29 - INFO - __main__ - global step: 2280; train loss: 4.479902267456055; dev loss: 4.50933837890625
01/05/2022 15:33:01 - INFO - __main__ - global step: 2290; train loss: 4.3237810134887695; dev loss: 4.496021270751953
01/05/2022 15:33:32 - INFO - __main__ - global step: 2300; train loss: 4.9168925285339355; dev loss: 4.718174934387207
01/05/2022 15:34:02 - INFO - __main__ - global step: 2310; train loss: 4.670495986938477; dev loss: 4.39525842666626
01/05/2022 15:34:33 - INFO - __main__ - global step: 2320; train loss: 4.42594051361084; dev loss: 4.258673191070557
01/05/2022 15:35:04 - INFO - __main__ - global step: 2330; train loss: 4.705384254455566; dev loss: 4.827582836151123
01/05/2022 15:35:36 - INFO - __main__ - global step: 2340; train loss: 4.58973503112793; dev loss: 4.667591094970703
01/05/2022 15:36:08 - INFO - __main__ - global step: 2350; train loss: 4.295958995819092; dev loss: 4.288546085357666
01/05/2022 15:36:39 - INFO - __main__ - global step: 2360; train loss: 4.549691200256348; dev loss: 4.381433010101318
01/05/2022 15:37:11 - INFO - __main__ - global step: 2370; train loss: 4.5587592124938965; dev loss: 4.447713851928711
01/05/2022 15:37:42 - INFO - __main__ - global step: 2380; train loss: 4.386570930480957; dev loss: 4.387526512145996
01/05/2022 15:38:14 - INFO - __main__ - global step: 2390; train loss: 4.346865177154541; dev loss: 4.34887170791626
01/05/2022 15:38:44 - INFO - __main__ - global step: 2400; train loss: 4.412066459655762; dev loss: 4.560698509216309
01/05/2022 15:39:16 - INFO - __main__ - global step: 2410; train loss: 4.155087471008301; dev loss: 4.289353370666504
01/05/2022 15:39:47 - INFO - __main__ - global step: 2420; train loss: 4.9568610191345215; dev loss: 4.8389081954956055
01/05/2022 15:40:18 - INFO - __main__ - global step: 2430; train loss: 4.3294172286987305; dev loss: 4.430380821228027
01/05/2022 15:40:48 - INFO - __main__ - global step: 2440; train loss: 4.763730049133301; dev loss: 4.943268775939941
01/05/2022 15:41:21 - INFO - __main__ - global step: 2450; train loss: 4.268886089324951; dev loss: 4.102883338928223
01/05/2022 15:41:54 - INFO - __main__ - global step: 2460; train loss: 4.33306360244751; dev loss: 4.359387397766113
01/05/2022 15:42:26 - INFO - __main__ - global step: 2470; train loss: 4.572281837463379; dev loss: 4.590569019317627
01/05/2022 15:42:58 - INFO - __main__ - global step: 2480; train loss: 4.317699909210205; dev loss: 4.309373378753662
01/05/2022 15:43:29 - INFO - __main__ - global step: 2490; train loss: 4.622442722320557; dev loss: 4.533679008483887
01/05/2022 15:44:00 - INFO - __main__ - global step: 2500; train loss: 4.393012046813965; dev loss: 4.260252952575684
01/05/2022 15:44:00 - INFO - __main__ - save model!
