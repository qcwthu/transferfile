05/21/2022 21:28:28 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:28:28 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo
05/21/2022 21:28:28 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:28:28 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo
05/21/2022 21:28:30 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:28:30 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:28:30 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:28:30 - INFO - __main__ - Using 2 gpus
05/21/2022 21:28:30 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/21/2022 21:28:30 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:28:30 - INFO - __main__ - Using 2 gpus
05/21/2022 21:28:30 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/21/2022 21:28:36 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.5, bsz=8 ...
06/17/2022 13:02:51 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/17/2022 13:02:51 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo
06/17/2022 13:02:51 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/17/2022 13:02:51 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo
06/17/2022 13:02:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/17/2022 13:02:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/17/2022 13:02:53 - INFO - __main__ - args.device: cuda:1
06/17/2022 13:02:53 - INFO - __main__ - args.device: cuda:0
06/17/2022 13:02:53 - INFO - __main__ - Using 2 gpus
06/17/2022 13:02:53 - INFO - __main__ - Using 2 gpus
06/17/2022 13:02:53 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
06/17/2022 13:02:53 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
06/17/2022 13:02:57 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.5, bsz=8 ...
06/17/2022 13:02:58 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:02:58 - INFO - __main__ - Printing 3 examples
06/17/2022 13:02:58 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:02:58 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:02:58 - INFO - __main__ - Printing 3 examples
06/17/2022 13:02:58 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:02:58 - INFO - __main__ - ['others']
06/17/2022 13:02:58 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:02:58 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:02:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:02:59 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:02:59 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:02:59 - INFO - __main__ - Printing 3 examples
06/17/2022 13:02:59 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:02:59 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:02:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:02:59 - INFO - __main__ - Printing 3 examples
06/17/2022 13:02:59 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:02:59 - INFO - __main__ - ['others']
06/17/2022 13:02:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:02:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:02:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:03:00 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:03:00 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:03:18 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:03:18 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:03:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:03:19 - INFO - __main__ - Starting training!
06/17/2022 13:03:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:03:24 - INFO - __main__ - Starting training!
06/17/2022 13:03:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.39 on epoch=0
06/17/2022 13:03:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.23 on epoch=0
06/17/2022 13:03:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.18 on epoch=0
06/17/2022 13:03:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=1
06/17/2022 13:03:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=1
06/17/2022 13:03:44 - INFO - __main__ - Global step 50 Train loss 1.35 Classification-F1 0.14476797088262056 on epoch=1
06/17/2022 13:03:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.14476797088262056 on epoch=1, global_step=50
06/17/2022 13:03:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=1
06/17/2022 13:03:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=2
06/17/2022 13:03:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=2
06/17/2022 13:03:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=2
06/17/2022 13:03:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=3
06/17/2022 13:04:03 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.34792906665679296 on epoch=3
06/17/2022 13:04:03 - INFO - __main__ - Saving model with best Classification-F1: 0.14476797088262056 -> 0.34792906665679296 on epoch=3, global_step=100
06/17/2022 13:04:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.01 on epoch=3
06/17/2022 13:04:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=3
06/17/2022 13:04:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=4
06/17/2022 13:04:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=4
06/17/2022 13:04:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.81 on epoch=4
06/17/2022 13:04:22 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.41646449751138415 on epoch=4
06/17/2022 13:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.34792906665679296 -> 0.41646449751138415 on epoch=4, global_step=150
06/17/2022 13:04:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=4
06/17/2022 13:04:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=5
06/17/2022 13:04:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=5
06/17/2022 13:04:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=5
06/17/2022 13:04:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=6
06/17/2022 13:04:41 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.46341265151638095 on epoch=6
06/17/2022 13:04:41 - INFO - __main__ - Saving model with best Classification-F1: 0.41646449751138415 -> 0.46341265151638095 on epoch=6, global_step=200
06/17/2022 13:04:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=6
06/17/2022 13:04:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=6
06/17/2022 13:04:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=7
06/17/2022 13:04:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.66 on epoch=7
06/17/2022 13:04:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=7
06/17/2022 13:05:00 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.4643819302218573 on epoch=7
06/17/2022 13:05:00 - INFO - __main__ - Saving model with best Classification-F1: 0.46341265151638095 -> 0.4643819302218573 on epoch=7, global_step=250
06/17/2022 13:05:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=8
06/17/2022 13:05:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=8
06/17/2022 13:05:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=8
06/17/2022 13:05:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=9
06/17/2022 13:05:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=9
06/17/2022 13:05:19 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.5761568823294373 on epoch=9
06/17/2022 13:05:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4643819302218573 -> 0.5761568823294373 on epoch=9, global_step=300
06/17/2022 13:05:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=9
06/17/2022 13:05:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.66 on epoch=9
06/17/2022 13:05:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=10
06/17/2022 13:05:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
06/17/2022 13:05:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=10
06/17/2022 13:05:38 - INFO - __main__ - Global step 350 Train loss 0.65 Classification-F1 0.6987325384717562 on epoch=10
06/17/2022 13:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5761568823294373 -> 0.6987325384717562 on epoch=10, global_step=350
06/17/2022 13:05:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=11
06/17/2022 13:05:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=11
06/17/2022 13:05:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.62 on epoch=11
06/17/2022 13:05:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=12
06/17/2022 13:05:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=12
06/17/2022 13:05:56 - INFO - __main__ - Global step 400 Train loss 0.59 Classification-F1 0.6499718199245342 on epoch=12
06/17/2022 13:05:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=12
06/17/2022 13:06:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=13
06/17/2022 13:06:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=13
06/17/2022 13:06:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=13
06/17/2022 13:06:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=14
06/17/2022 13:06:15 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.5866973069221891 on epoch=14
06/17/2022 13:06:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=14
06/17/2022 13:06:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=14
06/17/2022 13:06:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=14
06/17/2022 13:06:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=15
06/17/2022 13:06:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=15
06/17/2022 13:06:34 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.6891269358984088 on epoch=15
06/17/2022 13:06:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=15
06/17/2022 13:06:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=16
06/17/2022 13:06:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=16
06/17/2022 13:06:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=16
06/17/2022 13:06:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=17
06/17/2022 13:06:53 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.6938092601218933 on epoch=17
06/17/2022 13:06:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=17
06/17/2022 13:06:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=17
06/17/2022 13:07:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=18
06/17/2022 13:07:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=18
06/17/2022 13:07:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=18
06/17/2022 13:07:12 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.7292044749206102 on epoch=18
06/17/2022 13:07:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6987325384717562 -> 0.7292044749206102 on epoch=18, global_step=600
06/17/2022 13:07:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=19
06/17/2022 13:07:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=19
06/17/2022 13:07:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=19
06/17/2022 13:07:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=19
06/17/2022 13:07:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=20
06/17/2022 13:07:31 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.767215333979824 on epoch=20
06/17/2022 13:07:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7292044749206102 -> 0.767215333979824 on epoch=20, global_step=650
06/17/2022 13:07:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=20
06/17/2022 13:07:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=20
06/17/2022 13:07:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=21
06/17/2022 13:07:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=21
06/17/2022 13:07:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=21
06/17/2022 13:07:49 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.8001788832778295 on epoch=21
06/17/2022 13:07:49 - INFO - __main__ - Saving model with best Classification-F1: 0.767215333979824 -> 0.8001788832778295 on epoch=21, global_step=700
06/17/2022 13:07:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=22
06/17/2022 13:07:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=22
06/17/2022 13:07:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=22
06/17/2022 13:07:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=23
06/17/2022 13:08:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=23
06/17/2022 13:08:08 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.7214017967363194 on epoch=23
06/17/2022 13:08:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=23
06/17/2022 13:08:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=24
06/17/2022 13:08:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=24
06/17/2022 13:08:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=24
06/17/2022 13:08:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=24
06/17/2022 13:08:27 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.7730879279064415 on epoch=24
06/17/2022 13:08:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=25
06/17/2022 13:08:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=25
06/17/2022 13:08:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=25
06/17/2022 13:08:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=26
06/17/2022 13:08:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=26
06/17/2022 13:08:47 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.7538578600552244 on epoch=26
06/17/2022 13:08:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=26
06/17/2022 13:08:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=27
06/17/2022 13:08:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=27
06/17/2022 13:08:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=27
06/17/2022 13:08:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=28
06/17/2022 13:09:06 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.7047916726535027 on epoch=28
06/17/2022 13:09:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=28
06/17/2022 13:09:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=28
06/17/2022 13:09:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=29
06/17/2022 13:09:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=29
06/17/2022 13:09:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=29
06/17/2022 13:09:24 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.7891112071460031 on epoch=29
06/17/2022 13:09:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=29
06/17/2022 13:09:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=30
06/17/2022 13:09:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=30
06/17/2022 13:09:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=30
06/17/2022 13:09:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=31
06/17/2022 13:09:43 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.7674415572680685 on epoch=31
06/17/2022 13:09:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=31
06/17/2022 13:09:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=31
06/17/2022 13:09:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=32
06/17/2022 13:09:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=32
06/17/2022 13:09:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=32
06/17/2022 13:10:02 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7197803743565583 on epoch=32
06/17/2022 13:10:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=33
06/17/2022 13:10:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=33
06/17/2022 13:10:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=33
06/17/2022 13:10:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=34
06/17/2022 13:10:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=34
06/17/2022 13:10:21 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.7784312992520925 on epoch=34
06/17/2022 13:10:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=34
06/17/2022 13:10:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=34
06/17/2022 13:10:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=35
06/17/2022 13:10:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=35
06/17/2022 13:10:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=35
06/17/2022 13:10:40 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.7595286221128374 on epoch=35
06/17/2022 13:10:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=36
06/17/2022 13:10:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=36
06/17/2022 13:10:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=36
06/17/2022 13:10:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=37
06/17/2022 13:10:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=37
06/17/2022 13:10:59 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.6939783792794813 on epoch=37
06/17/2022 13:11:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=37
06/17/2022 13:11:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=38
06/17/2022 13:11:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=38
06/17/2022 13:11:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=38
06/17/2022 13:11:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=39
06/17/2022 13:11:18 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.7247593074578382 on epoch=39
06/17/2022 13:11:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=39
06/17/2022 13:11:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=39
06/17/2022 13:11:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=39
06/17/2022 13:11:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=40
06/17/2022 13:11:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=40
06/17/2022 13:11:36 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.6812856159285563 on epoch=40
06/17/2022 13:11:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=40
06/17/2022 13:11:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=41
06/17/2022 13:11:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=41
06/17/2022 13:11:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=41
06/17/2022 13:11:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=42
06/17/2022 13:11:55 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.7792211722635701 on epoch=42
06/17/2022 13:11:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=42
06/17/2022 13:12:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=42
06/17/2022 13:12:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=43
06/17/2022 13:12:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=43
06/17/2022 13:12:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=43
06/17/2022 13:12:14 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7394884194256639 on epoch=43
06/17/2022 13:12:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=44
06/17/2022 13:12:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=44
06/17/2022 13:12:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=44
06/17/2022 13:12:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=44
06/17/2022 13:12:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=45
06/17/2022 13:12:33 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7887605035995184 on epoch=45
06/17/2022 13:12:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=45
06/17/2022 13:12:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=45
06/17/2022 13:12:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=46
06/17/2022 13:12:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=46
06/17/2022 13:12:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=46
06/17/2022 13:12:52 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.7536245000606955 on epoch=46
06/17/2022 13:12:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=47
06/17/2022 13:12:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=47
06/17/2022 13:12:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=47
06/17/2022 13:13:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=48
06/17/2022 13:13:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=48
06/17/2022 13:13:11 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.796850490557675 on epoch=48
06/17/2022 13:13:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=48
06/17/2022 13:13:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=49
06/17/2022 13:13:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=49
06/17/2022 13:13:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=49
06/17/2022 13:13:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=49
06/17/2022 13:13:29 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.7778466989945965 on epoch=49
06/17/2022 13:13:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=50
06/17/2022 13:13:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=50
06/17/2022 13:13:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=50
06/17/2022 13:13:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
06/17/2022 13:13:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=51
06/17/2022 13:13:48 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7655139089626891 on epoch=51
06/17/2022 13:13:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=51
06/17/2022 13:13:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
06/17/2022 13:13:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=52
06/17/2022 13:13:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=52
06/17/2022 13:14:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=53
06/17/2022 13:14:07 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.7705748005547934 on epoch=53
06/17/2022 13:14:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=53
06/17/2022 13:14:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=53
06/17/2022 13:14:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=54
06/17/2022 13:14:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=54
06/17/2022 13:14:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=54
06/17/2022 13:14:26 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7574116386206883 on epoch=54
06/17/2022 13:14:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=54
06/17/2022 13:14:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
06/17/2022 13:14:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=55
06/17/2022 13:14:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=55
06/17/2022 13:14:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
06/17/2022 13:14:45 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.7629757874252382 on epoch=56
06/17/2022 13:14:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=56
06/17/2022 13:14:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=56
06/17/2022 13:14:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=57
06/17/2022 13:14:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=57
06/17/2022 13:14:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=57
06/17/2022 13:15:04 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7790524152464451 on epoch=57
06/17/2022 13:15:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=58
06/17/2022 13:15:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=58
06/17/2022 13:15:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=58
06/17/2022 13:15:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=59
06/17/2022 13:15:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=59
06/17/2022 13:15:23 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.7730506666752407 on epoch=59
06/17/2022 13:15:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=59
06/17/2022 13:15:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=59
06/17/2022 13:15:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=60
06/17/2022 13:15:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
06/17/2022 13:15:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=60
06/17/2022 13:15:42 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.772020520293557 on epoch=60
06/17/2022 13:15:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=61
06/17/2022 13:15:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
06/17/2022 13:15:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=61
06/17/2022 13:15:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
06/17/2022 13:15:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=62
06/17/2022 13:16:01 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.747894386587587 on epoch=62
06/17/2022 13:16:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=62
06/17/2022 13:16:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=63
06/17/2022 13:16:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=63
06/17/2022 13:16:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=63
06/17/2022 13:16:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=64
06/17/2022 13:16:20 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7796210637409057 on epoch=64
06/17/2022 13:16:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=64
06/17/2022 13:16:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=64
06/17/2022 13:16:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=64
06/17/2022 13:16:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=65
06/17/2022 13:16:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=65
06/17/2022 13:16:38 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.7476353167529638 on epoch=65
06/17/2022 13:16:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=65
06/17/2022 13:16:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=66
06/17/2022 13:16:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=66
06/17/2022 13:16:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
06/17/2022 13:16:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=67
06/17/2022 13:16:57 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.7883756117547238 on epoch=67
06/17/2022 13:17:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=67
06/17/2022 13:17:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=67
06/17/2022 13:17:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=68
06/17/2022 13:17:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=68
06/17/2022 13:17:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=68
06/17/2022 13:17:16 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.7884544026785406 on epoch=68
06/17/2022 13:17:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=69
06/17/2022 13:17:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=69
06/17/2022 13:17:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=69
06/17/2022 13:17:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=69
06/17/2022 13:17:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=70
06/17/2022 13:17:35 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.8141588776379637 on epoch=70
06/17/2022 13:17:35 - INFO - __main__ - Saving model with best Classification-F1: 0.8001788832778295 -> 0.8141588776379637 on epoch=70, global_step=2250
06/17/2022 13:17:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=70
06/17/2022 13:17:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.31 on epoch=70
06/17/2022 13:17:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=71
06/17/2022 13:17:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=71
06/17/2022 13:17:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=71
06/17/2022 13:17:54 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7818066811389368 on epoch=71
06/17/2022 13:17:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=72
06/17/2022 13:17:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=72
06/17/2022 13:18:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=72
06/17/2022 13:18:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=73
06/17/2022 13:18:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=73
06/17/2022 13:18:13 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.80347738567579 on epoch=73
06/17/2022 13:18:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=73
06/17/2022 13:18:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=74
06/17/2022 13:18:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
06/17/2022 13:18:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=74
06/17/2022 13:18:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=74
06/17/2022 13:18:32 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.8099522584143126 on epoch=74
06/17/2022 13:18:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=75
06/17/2022 13:18:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=75
06/17/2022 13:18:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=75
06/17/2022 13:18:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=76
06/17/2022 13:18:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=76
06/17/2022 13:18:51 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7967717148751632 on epoch=76
06/17/2022 13:18:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=76
06/17/2022 13:18:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=77
06/17/2022 13:18:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=77
06/17/2022 13:19:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=77
06/17/2022 13:19:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
06/17/2022 13:19:09 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7817649656428538 on epoch=78
06/17/2022 13:19:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=78
06/17/2022 13:19:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=78
06/17/2022 13:19:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=79
06/17/2022 13:19:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.20 on epoch=79
06/17/2022 13:19:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=79
06/17/2022 13:19:28 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7523083520435863 on epoch=79
06/17/2022 13:19:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=79
06/17/2022 13:19:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=80
06/17/2022 13:19:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=80
06/17/2022 13:19:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=80
06/17/2022 13:19:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=81
06/17/2022 13:19:47 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.7713591430349086 on epoch=81
06/17/2022 13:19:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=81
06/17/2022 13:19:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=81
06/17/2022 13:19:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=82
06/17/2022 13:19:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=82
06/17/2022 13:19:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
06/17/2022 13:20:06 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.8027355531014123 on epoch=82
06/17/2022 13:20:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=83
06/17/2022 13:20:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=83
06/17/2022 13:20:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
06/17/2022 13:20:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=84
06/17/2022 13:20:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=84
06/17/2022 13:20:25 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.7843127115843402 on epoch=84
06/17/2022 13:20:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=84
06/17/2022 13:20:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=84
06/17/2022 13:20:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=85
06/17/2022 13:20:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=85
06/17/2022 13:20:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=85
06/17/2022 13:20:44 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.7937691932085349 on epoch=85
06/17/2022 13:20:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=86
06/17/2022 13:20:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=86
06/17/2022 13:20:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=86
06/17/2022 13:20:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
06/17/2022 13:20:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=87
06/17/2022 13:21:03 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.7880308323400369 on epoch=87
06/17/2022 13:21:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.21 on epoch=87
06/17/2022 13:21:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=88
06/17/2022 13:21:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
06/17/2022 13:21:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=88
06/17/2022 13:21:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=89
06/17/2022 13:21:22 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.8148995653949699 on epoch=89
06/17/2022 13:21:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8141588776379637 -> 0.8148995653949699 on epoch=89, global_step=2850
06/17/2022 13:21:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
06/17/2022 13:21:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
06/17/2022 13:21:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
06/17/2022 13:21:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=90
06/17/2022 13:21:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=90
06/17/2022 13:21:40 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7899757220842649 on epoch=90
06/17/2022 13:21:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=90
06/17/2022 13:21:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=91
06/17/2022 13:21:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=91
06/17/2022 13:21:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=91
06/17/2022 13:21:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=92
06/17/2022 13:21:59 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.7738907137035489 on epoch=92
06/17/2022 13:22:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=92
06/17/2022 13:22:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.26 on epoch=92
06/17/2022 13:22:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=93
06/17/2022 13:22:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=93
06/17/2022 13:22:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=93
06/17/2022 13:22:13 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:22:13 - INFO - __main__ - Printing 3 examples
06/17/2022 13:22:13 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:22:13 - INFO - __main__ - ['others']
06/17/2022 13:22:13 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:22:13 - INFO - __main__ - ['others']
06/17/2022 13:22:13 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:22:13 - INFO - __main__ - ['others']
06/17/2022 13:22:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:22:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:22:14 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:22:14 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:22:14 - INFO - __main__ - Printing 3 examples
06/17/2022 13:22:14 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:22:14 - INFO - __main__ - ['others']
06/17/2022 13:22:14 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:22:14 - INFO - __main__ - ['others']
06/17/2022 13:22:14 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:22:14 - INFO - __main__ - ['others']
06/17/2022 13:22:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:22:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:22:15 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:22:19 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.6524020323988673 on epoch=93
06/17/2022 13:22:19 - INFO - __main__ - save last model!
06/17/2022 13:22:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 13:22:19 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 13:22:19 - INFO - __main__ - Printing 3 examples
06/17/2022 13:22:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 13:22:19 - INFO - __main__ - ['others']
06/17/2022 13:22:19 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 13:22:19 - INFO - __main__ - ['others']
06/17/2022 13:22:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 13:22:19 - INFO - __main__ - ['others']
06/17/2022 13:22:19 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:22:21 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:22:26 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 13:22:33 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:22:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:22:34 - INFO - __main__ - Starting training!
06/17/2022 13:23:38 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_100_0.5_8_predictions.txt
06/17/2022 13:23:38 - INFO - __main__ - Classification-F1 on test data: 0.3128
06/17/2022 13:23:39 - INFO - __main__ - prefix=emo_128_100, lr=0.5, bsz=8, dev_performance=0.8148995653949699, test_performance=0.3127711405041807
06/17/2022 13:23:39 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.4, bsz=8 ...
06/17/2022 13:23:40 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:23:40 - INFO - __main__ - Printing 3 examples
06/17/2022 13:23:40 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:23:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:23:40 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:23:40 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:23:40 - INFO - __main__ - Printing 3 examples
06/17/2022 13:23:40 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:23:40 - INFO - __main__ - ['others']
06/17/2022 13:23:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:23:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:23:41 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:23:57 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:23:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:23:58 - INFO - __main__ - Starting training!
06/17/2022 13:24:01 - INFO - __main__ - Step 10 Global step 10 Train loss 2.55 on epoch=0
06/17/2022 13:24:04 - INFO - __main__ - Step 20 Global step 20 Train loss 1.42 on epoch=0
06/17/2022 13:24:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=0
06/17/2022 13:24:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=1
06/17/2022 13:24:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=1
06/17/2022 13:24:18 - INFO - __main__ - Global step 50 Train loss 1.42 Classification-F1 0.1378191856452726 on epoch=1
06/17/2022 13:24:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1378191856452726 on epoch=1, global_step=50
06/17/2022 13:24:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=1
06/17/2022 13:24:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=2
06/17/2022 13:24:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.84 on epoch=2
06/17/2022 13:24:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=2
06/17/2022 13:24:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=3
06/17/2022 13:24:38 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.2951609695225951 on epoch=3
06/17/2022 13:24:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1378191856452726 -> 0.2951609695225951 on epoch=3, global_step=100
06/17/2022 13:24:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=3
06/17/2022 13:24:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=3
06/17/2022 13:24:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=4
06/17/2022 13:24:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=4
06/17/2022 13:24:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=4
06/17/2022 13:24:57 - INFO - __main__ - Global step 150 Train loss 0.83 Classification-F1 0.40834981678841453 on epoch=4
06/17/2022 13:24:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2951609695225951 -> 0.40834981678841453 on epoch=4, global_step=150
06/17/2022 13:25:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=4
06/17/2022 13:25:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=5
06/17/2022 13:25:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.67 on epoch=5
06/17/2022 13:25:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=5
06/17/2022 13:25:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
06/17/2022 13:25:17 - INFO - __main__ - Global step 200 Train loss 0.72 Classification-F1 0.4701829081152792 on epoch=6
06/17/2022 13:25:17 - INFO - __main__ - Saving model with best Classification-F1: 0.40834981678841453 -> 0.4701829081152792 on epoch=6, global_step=200
06/17/2022 13:25:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.75 on epoch=6
06/17/2022 13:25:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.68 on epoch=6
06/17/2022 13:25:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=7
06/17/2022 13:25:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=7
06/17/2022 13:25:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=7
06/17/2022 13:25:36 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.3230709357256679 on epoch=7
06/17/2022 13:25:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=8
06/17/2022 13:25:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.68 on epoch=8
06/17/2022 13:25:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=8
06/17/2022 13:25:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=9
06/17/2022 13:25:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=9
06/17/2022 13:25:56 - INFO - __main__ - Global step 300 Train loss 0.67 Classification-F1 0.6796152004502987 on epoch=9
06/17/2022 13:25:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4701829081152792 -> 0.6796152004502987 on epoch=9, global_step=300
06/17/2022 13:25:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=9
06/17/2022 13:26:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.56 on epoch=9
06/17/2022 13:26:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=10
06/17/2022 13:26:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=10
06/17/2022 13:26:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=10
06/17/2022 13:26:15 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.6360066151011908 on epoch=10
06/17/2022 13:26:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=11
06/17/2022 13:26:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=11
06/17/2022 13:26:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=11
06/17/2022 13:26:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=12
06/17/2022 13:26:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.64 on epoch=12
06/17/2022 13:26:34 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.49263323278005927 on epoch=12
06/17/2022 13:26:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=12
06/17/2022 13:26:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=13
06/17/2022 13:26:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=13
06/17/2022 13:26:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=13
06/17/2022 13:26:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=14
06/17/2022 13:26:54 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.6141002492249741 on epoch=14
06/17/2022 13:26:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=14
06/17/2022 13:26:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=14
06/17/2022 13:27:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=14
06/17/2022 13:27:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=15
06/17/2022 13:27:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=15
06/17/2022 13:27:13 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.6366019873035326 on epoch=15
06/17/2022 13:27:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=15
06/17/2022 13:27:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=16
06/17/2022 13:27:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=16
06/17/2022 13:27:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=16
06/17/2022 13:27:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=17
06/17/2022 13:27:33 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.7315530309177506 on epoch=17
06/17/2022 13:27:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6796152004502987 -> 0.7315530309177506 on epoch=17, global_step=550
06/17/2022 13:27:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=17
06/17/2022 13:27:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=17
06/17/2022 13:27:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=18
06/17/2022 13:27:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=18
06/17/2022 13:27:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=18
06/17/2022 13:27:52 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.7318962582801329 on epoch=18
06/17/2022 13:27:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7315530309177506 -> 0.7318962582801329 on epoch=18, global_step=600
06/17/2022 13:27:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=19
06/17/2022 13:27:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=19
06/17/2022 13:28:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
06/17/2022 13:28:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=19
06/17/2022 13:28:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=20
06/17/2022 13:28:11 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.6937515561597006 on epoch=20
06/17/2022 13:28:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=20
06/17/2022 13:28:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=20
06/17/2022 13:28:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=21
06/17/2022 13:28:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=21
06/17/2022 13:28:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=21
06/17/2022 13:28:31 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.7400985740717566 on epoch=21
06/17/2022 13:28:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7318962582801329 -> 0.7400985740717566 on epoch=21, global_step=700
06/17/2022 13:28:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=22
06/17/2022 13:28:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=22
06/17/2022 13:28:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=22
06/17/2022 13:28:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=23
06/17/2022 13:28:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=23
06/17/2022 13:28:50 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.7623573026178768 on epoch=23
06/17/2022 13:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7400985740717566 -> 0.7623573026178768 on epoch=23, global_step=750
06/17/2022 13:28:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=23
06/17/2022 13:28:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=24
06/17/2022 13:28:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=24
06/17/2022 13:29:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=24
06/17/2022 13:29:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=24
06/17/2022 13:29:10 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.7353819153636716 on epoch=24
06/17/2022 13:29:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=25
06/17/2022 13:29:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=25
06/17/2022 13:29:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
06/17/2022 13:29:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=26
06/17/2022 13:29:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=26
06/17/2022 13:29:29 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.7540767784046908 on epoch=26
06/17/2022 13:29:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=26
06/17/2022 13:29:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=27
06/17/2022 13:29:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=27
06/17/2022 13:29:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=27
06/17/2022 13:29:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=28
06/17/2022 13:29:49 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.7362556493083783 on epoch=28
06/17/2022 13:29:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=28
06/17/2022 13:29:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=28
06/17/2022 13:29:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=29
06/17/2022 13:29:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=29
06/17/2022 13:30:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=29
06/17/2022 13:30:08 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.7582648274321536 on epoch=29
06/17/2022 13:30:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=29
06/17/2022 13:30:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=30
06/17/2022 13:30:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=30
06/17/2022 13:30:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=30
06/17/2022 13:30:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=31
06/17/2022 13:30:28 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.7355349327984487 on epoch=31
06/17/2022 13:30:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=31
06/17/2022 13:30:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=31
06/17/2022 13:30:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=32
06/17/2022 13:30:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=32
06/17/2022 13:30:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=32
06/17/2022 13:30:47 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.737830011925731 on epoch=32
06/17/2022 13:30:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
06/17/2022 13:30:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=33
06/17/2022 13:30:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=33
06/17/2022 13:30:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=34
06/17/2022 13:31:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=34
06/17/2022 13:31:07 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.7782834513570557 on epoch=34
06/17/2022 13:31:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7623573026178768 -> 0.7782834513570557 on epoch=34, global_step=1100
06/17/2022 13:31:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=34
06/17/2022 13:31:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=34
06/17/2022 13:31:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=35
06/17/2022 13:31:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=35
06/17/2022 13:31:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=35
06/17/2022 13:31:26 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.7913219480155526 on epoch=35
06/17/2022 13:31:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7782834513570557 -> 0.7913219480155526 on epoch=35, global_step=1150
06/17/2022 13:31:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=36
06/17/2022 13:31:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=36
06/17/2022 13:31:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=36
06/17/2022 13:31:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=37
06/17/2022 13:31:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=37
06/17/2022 13:31:46 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.7717387913787277 on epoch=37
06/17/2022 13:31:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=37
06/17/2022 13:31:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=38
06/17/2022 13:31:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=38
06/17/2022 13:31:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=38
06/17/2022 13:31:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=39
06/17/2022 13:32:05 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.7724255756994829 on epoch=39
06/17/2022 13:32:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=39
06/17/2022 13:32:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=39
06/17/2022 13:32:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=39
06/17/2022 13:32:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=40
06/17/2022 13:32:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=40
06/17/2022 13:32:25 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.7191195299014479 on epoch=40
06/17/2022 13:32:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=40
06/17/2022 13:32:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=41
06/17/2022 13:32:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=41
06/17/2022 13:32:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=41
06/17/2022 13:32:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=42
06/17/2022 13:32:44 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.7783323408301511 on epoch=42
06/17/2022 13:32:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=42
06/17/2022 13:32:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=42
06/17/2022 13:32:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=43
06/17/2022 13:32:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=43
06/17/2022 13:32:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=43
06/17/2022 13:33:04 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7633723213847099 on epoch=43
06/17/2022 13:33:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=44
06/17/2022 13:33:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=44
06/17/2022 13:33:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=44
06/17/2022 13:33:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=44
06/17/2022 13:33:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=45
06/17/2022 13:33:23 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.7780437534206857 on epoch=45
06/17/2022 13:33:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=45
06/17/2022 13:33:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=45
06/17/2022 13:33:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=46
06/17/2022 13:33:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=46
06/17/2022 13:33:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=46
06/17/2022 13:33:43 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.7849224462071196 on epoch=46
06/17/2022 13:33:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=47
06/17/2022 13:33:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
06/17/2022 13:33:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=47
06/17/2022 13:33:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=48
06/17/2022 13:33:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=48
06/17/2022 13:34:02 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7918879870193649 on epoch=48
06/17/2022 13:34:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7913219480155526 -> 0.7918879870193649 on epoch=48, global_step=1550
06/17/2022 13:34:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=48
06/17/2022 13:34:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=49
06/17/2022 13:34:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=49
06/17/2022 13:34:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=49
06/17/2022 13:34:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=49
06/17/2022 13:34:22 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.7981889104312544 on epoch=49
06/17/2022 13:34:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7918879870193649 -> 0.7981889104312544 on epoch=49, global_step=1600
06/17/2022 13:34:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=50
06/17/2022 13:34:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
06/17/2022 13:34:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=50
06/17/2022 13:34:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=51
06/17/2022 13:34:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=51
06/17/2022 13:34:41 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.7957291418259741 on epoch=51
06/17/2022 13:34:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=51
06/17/2022 13:34:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=52
06/17/2022 13:34:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=52
06/17/2022 13:34:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=52
06/17/2022 13:34:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=53
06/17/2022 13:35:01 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.7803714862861871 on epoch=53
06/17/2022 13:35:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=53
06/17/2022 13:35:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=53
06/17/2022 13:35:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=54
06/17/2022 13:35:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=54
06/17/2022 13:35:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=54
06/17/2022 13:35:20 - INFO - __main__ - Global step 1750 Train loss 0.31 Classification-F1 0.8072177364921872 on epoch=54
06/17/2022 13:35:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7981889104312544 -> 0.8072177364921872 on epoch=54, global_step=1750
06/17/2022 13:35:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=54
06/17/2022 13:35:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=55
06/17/2022 13:35:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=55
06/17/2022 13:35:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=55
06/17/2022 13:35:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=56
06/17/2022 13:35:40 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.7881913603614417 on epoch=56
06/17/2022 13:35:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=56
06/17/2022 13:35:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=56
06/17/2022 13:35:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=57
06/17/2022 13:35:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=57
06/17/2022 13:35:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=57
06/17/2022 13:35:59 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.7570272968695733 on epoch=57
06/17/2022 13:36:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=58
06/17/2022 13:36:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
06/17/2022 13:36:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=58
06/17/2022 13:36:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=59
06/17/2022 13:36:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=59
06/17/2022 13:36:19 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.8073870287599436 on epoch=59
06/17/2022 13:36:19 - INFO - __main__ - Saving model with best Classification-F1: 0.8072177364921872 -> 0.8073870287599436 on epoch=59, global_step=1900
06/17/2022 13:36:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=59
06/17/2022 13:36:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=59
06/17/2022 13:36:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=60
06/17/2022 13:36:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=60
06/17/2022 13:36:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=60
06/17/2022 13:36:39 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.7953146617999146 on epoch=60
06/17/2022 13:36:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=61
06/17/2022 13:36:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=61
06/17/2022 13:36:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=61
06/17/2022 13:36:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
06/17/2022 13:36:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=62
06/17/2022 13:36:58 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7970989999594187 on epoch=62
06/17/2022 13:37:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=62
06/17/2022 13:37:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=63
06/17/2022 13:37:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=63
06/17/2022 13:37:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=63
06/17/2022 13:37:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
06/17/2022 13:37:17 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.7556536906839161 on epoch=64
06/17/2022 13:37:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
06/17/2022 13:37:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=64
06/17/2022 13:37:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=64
06/17/2022 13:37:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
06/17/2022 13:37:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=65
06/17/2022 13:37:36 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7668245760018477 on epoch=65
06/17/2022 13:37:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.32 on epoch=65
06/17/2022 13:37:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=66
06/17/2022 13:37:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=66
06/17/2022 13:37:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
06/17/2022 13:37:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=67
06/17/2022 13:37:55 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.7574280510937539 on epoch=67
06/17/2022 13:37:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=67
06/17/2022 13:38:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=67
06/17/2022 13:38:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=68
06/17/2022 13:38:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=68
06/17/2022 13:38:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
06/17/2022 13:38:14 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.7683495230657198 on epoch=68
06/17/2022 13:38:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=69
06/17/2022 13:38:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.25 on epoch=69
06/17/2022 13:38:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=69
06/17/2022 13:38:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=69
06/17/2022 13:38:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=70
06/17/2022 13:38:32 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.7853479931424496 on epoch=70
06/17/2022 13:38:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=70
06/17/2022 13:38:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=70
06/17/2022 13:38:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=71
06/17/2022 13:38:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.31 on epoch=71
06/17/2022 13:38:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=71
06/17/2022 13:38:51 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.8104027111772911 on epoch=71
06/17/2022 13:38:51 - INFO - __main__ - Saving model with best Classification-F1: 0.8073870287599436 -> 0.8104027111772911 on epoch=71, global_step=2300
06/17/2022 13:38:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.22 on epoch=72
06/17/2022 13:38:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=72
06/17/2022 13:38:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=72
06/17/2022 13:39:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=73
06/17/2022 13:39:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=73
06/17/2022 13:39:10 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.8154638607571109 on epoch=73
06/17/2022 13:39:10 - INFO - __main__ - Saving model with best Classification-F1: 0.8104027111772911 -> 0.8154638607571109 on epoch=73, global_step=2350
06/17/2022 13:39:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=73
06/17/2022 13:39:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
06/17/2022 13:39:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=74
06/17/2022 13:39:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.30 on epoch=74
06/17/2022 13:39:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=74
06/17/2022 13:39:29 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.7835194015102812 on epoch=74
06/17/2022 13:39:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=75
06/17/2022 13:39:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=75
06/17/2022 13:39:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=75
06/17/2022 13:39:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=76
06/17/2022 13:39:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.27 on epoch=76
06/17/2022 13:39:48 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.7884274069650122 on epoch=76
06/17/2022 13:39:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=76
06/17/2022 13:39:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=77
06/17/2022 13:39:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=77
06/17/2022 13:39:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=77
06/17/2022 13:40:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=78
06/17/2022 13:40:07 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.7984794762693429 on epoch=78
06/17/2022 13:40:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.23 on epoch=78
06/17/2022 13:40:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=78
06/17/2022 13:40:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=79
06/17/2022 13:40:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=79
06/17/2022 13:40:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.25 on epoch=79
06/17/2022 13:40:26 - INFO - __main__ - Global step 2550 Train loss 0.23 Classification-F1 0.7865617571836919 on epoch=79
06/17/2022 13:40:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=79
06/17/2022 13:40:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=80
06/17/2022 13:40:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=80
06/17/2022 13:40:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=80
06/17/2022 13:40:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=81
06/17/2022 13:40:45 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.780682567738509 on epoch=81
06/17/2022 13:40:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=81
06/17/2022 13:40:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 13:40:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=82
06/17/2022 13:40:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=82
06/17/2022 13:40:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=82
06/17/2022 13:41:04 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.7998237576650383 on epoch=82
06/17/2022 13:41:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=83
06/17/2022 13:41:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.27 on epoch=83
06/17/2022 13:41:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=83
06/17/2022 13:41:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
06/17/2022 13:41:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=84
06/17/2022 13:41:23 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.805197714276507 on epoch=84
06/17/2022 13:41:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=84
06/17/2022 13:41:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=84
06/17/2022 13:41:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=85
06/17/2022 13:41:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=85
06/17/2022 13:41:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=85
06/17/2022 13:41:42 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.7991371305680159 on epoch=85
06/17/2022 13:41:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=86
06/17/2022 13:41:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=86
06/17/2022 13:41:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=86
06/17/2022 13:41:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=87
06/17/2022 13:41:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=87
06/17/2022 13:42:01 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.7845056311956867 on epoch=87
06/17/2022 13:42:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=87
06/17/2022 13:42:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=88
06/17/2022 13:42:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
06/17/2022 13:42:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=88
06/17/2022 13:42:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=89
06/17/2022 13:42:20 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.7778088480049634 on epoch=89
06/17/2022 13:42:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=89
06/17/2022 13:42:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
06/17/2022 13:42:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
06/17/2022 13:42:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
06/17/2022 13:42:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=90
06/17/2022 13:42:39 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.780700991220441 on epoch=90
06/17/2022 13:42:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=90
06/17/2022 13:42:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=91
06/17/2022 13:42:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=91
06/17/2022 13:42:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=91
06/17/2022 13:42:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=92
06/17/2022 13:42:58 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.7782184399253602 on epoch=92
06/17/2022 13:43:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=92
06/17/2022 13:43:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.19 on epoch=92
06/17/2022 13:43:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=93
06/17/2022 13:43:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=93
06/17/2022 13:43:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
06/17/2022 13:43:11 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:43:11 - INFO - __main__ - Printing 3 examples
06/17/2022 13:43:11 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:43:11 - INFO - __main__ - ['others']
06/17/2022 13:43:11 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:43:11 - INFO - __main__ - ['others']
06/17/2022 13:43:11 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:43:11 - INFO - __main__ - ['others']
06/17/2022 13:43:11 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:43:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:43:12 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:43:12 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:43:12 - INFO - __main__ - Printing 3 examples
06/17/2022 13:43:12 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:43:12 - INFO - __main__ - ['others']
06/17/2022 13:43:12 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:43:12 - INFO - __main__ - ['others']
06/17/2022 13:43:12 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:43:12 - INFO - __main__ - ['others']
06/17/2022 13:43:12 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:43:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:43:13 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:43:17 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7783296444594996 on epoch=93
06/17/2022 13:43:17 - INFO - __main__ - save last model!
06/17/2022 13:43:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 13:43:17 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 13:43:17 - INFO - __main__ - Printing 3 examples
06/17/2022 13:43:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 13:43:17 - INFO - __main__ - ['others']
06/17/2022 13:43:17 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 13:43:17 - INFO - __main__ - ['others']
06/17/2022 13:43:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 13:43:17 - INFO - __main__ - ['others']
06/17/2022 13:43:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:43:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:43:24 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 13:43:29 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:43:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:43:30 - INFO - __main__ - Starting training!
06/17/2022 13:44:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_100_0.4_8_predictions.txt
06/17/2022 13:44:36 - INFO - __main__ - Classification-F1 on test data: 0.4551
06/17/2022 13:44:37 - INFO - __main__ - prefix=emo_128_100, lr=0.4, bsz=8, dev_performance=0.8154638607571109, test_performance=0.4550564350725388
06/17/2022 13:44:37 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.3, bsz=8 ...
06/17/2022 13:44:38 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:44:38 - INFO - __main__ - Printing 3 examples
06/17/2022 13:44:38 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 13:44:38 - INFO - __main__ - ['others']
06/17/2022 13:44:38 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 13:44:38 - INFO - __main__ - ['others']
06/17/2022 13:44:38 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 13:44:38 - INFO - __main__ - ['others']
06/17/2022 13:44:38 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:44:38 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:44:39 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 13:44:39 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 13:44:39 - INFO - __main__ - Printing 3 examples
06/17/2022 13:44:39 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 13:44:39 - INFO - __main__ - ['others']
06/17/2022 13:44:39 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 13:44:39 - INFO - __main__ - ['others']
06/17/2022 13:44:39 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 13:44:39 - INFO - __main__ - ['others']
06/17/2022 13:44:39 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:44:39 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:44:39 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 13:44:58 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 13:44:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 13:44:59 - INFO - __main__ - Starting training!
06/17/2022 13:45:02 - INFO - __main__ - Step 10 Global step 10 Train loss 2.82 on epoch=0
06/17/2022 13:45:04 - INFO - __main__ - Step 20 Global step 20 Train loss 1.59 on epoch=0
06/17/2022 13:45:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.31 on epoch=0
06/17/2022 13:45:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=1
06/17/2022 13:45:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=1
06/17/2022 13:45:19 - INFO - __main__ - Global step 50 Train loss 1.52 Classification-F1 0.11958439405600617 on epoch=1
06/17/2022 13:45:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11958439405600617 on epoch=1, global_step=50
06/17/2022 13:45:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
06/17/2022 13:45:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
06/17/2022 13:45:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=2
06/17/2022 13:45:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=2
06/17/2022 13:45:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=3
06/17/2022 13:45:37 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.2505763428659841 on epoch=3
06/17/2022 13:45:37 - INFO - __main__ - Saving model with best Classification-F1: 0.11958439405600617 -> 0.2505763428659841 on epoch=3, global_step=100
06/17/2022 13:45:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=3
06/17/2022 13:45:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=3
06/17/2022 13:45:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=4
06/17/2022 13:45:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=4
06/17/2022 13:45:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=4
06/17/2022 13:45:56 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.22959541540917572 on epoch=4
06/17/2022 13:45:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=4
06/17/2022 13:46:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=5
06/17/2022 13:46:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=5
06/17/2022 13:46:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
06/17/2022 13:46:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.76 on epoch=6
06/17/2022 13:46:15 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.16647366969947613 on epoch=6
06/17/2022 13:46:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=6
06/17/2022 13:46:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=6
06/17/2022 13:46:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=7
06/17/2022 13:46:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=7
06/17/2022 13:46:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.71 on epoch=7
06/17/2022 13:46:34 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.2817894658737982 on epoch=7
06/17/2022 13:46:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2505763428659841 -> 0.2817894658737982 on epoch=7, global_step=250
06/17/2022 13:46:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.74 on epoch=8
06/17/2022 13:46:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=8
06/17/2022 13:46:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=8
06/17/2022 13:46:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=9
06/17/2022 13:46:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.67 on epoch=9
06/17/2022 13:46:52 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.6193074844549193 on epoch=9
06/17/2022 13:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2817894658737982 -> 0.6193074844549193 on epoch=9, global_step=300
06/17/2022 13:46:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=9
06/17/2022 13:46:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=9
06/17/2022 13:47:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=10
06/17/2022 13:47:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.67 on epoch=10
06/17/2022 13:47:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=10
06/17/2022 13:47:11 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.6261072408752739 on epoch=10
06/17/2022 13:47:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6193074844549193 -> 0.6261072408752739 on epoch=10, global_step=350
06/17/2022 13:47:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.68 on epoch=11
06/17/2022 13:47:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=11
06/17/2022 13:47:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=11
06/17/2022 13:47:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=12
06/17/2022 13:47:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.67 on epoch=12
06/17/2022 13:47:30 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.47541637002082304 on epoch=12
06/17/2022 13:47:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=12
06/17/2022 13:47:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=13
06/17/2022 13:47:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.62 on epoch=13
06/17/2022 13:47:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=13
06/17/2022 13:47:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.64 on epoch=14
06/17/2022 13:47:49 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.6047107229324791 on epoch=14
06/17/2022 13:47:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=14
06/17/2022 13:47:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.68 on epoch=14
06/17/2022 13:47:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=14
06/17/2022 13:47:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=15
06/17/2022 13:48:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=15
06/17/2022 13:48:07 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.6336640331256058 on epoch=15
06/17/2022 13:48:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6261072408752739 -> 0.6336640331256058 on epoch=15, global_step=500
06/17/2022 13:48:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=15
06/17/2022 13:48:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=16
06/17/2022 13:48:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=16
06/17/2022 13:48:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=16
06/17/2022 13:48:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=17
06/17/2022 13:48:26 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.4214423961599457 on epoch=17
06/17/2022 13:48:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=17
06/17/2022 13:48:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=17
06/17/2022 13:48:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=18
06/17/2022 13:48:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=18
06/17/2022 13:48:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=18
06/17/2022 13:48:45 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.7605323367549651 on epoch=18
06/17/2022 13:48:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6336640331256058 -> 0.7605323367549651 on epoch=18, global_step=600
06/17/2022 13:48:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=19
06/17/2022 13:48:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=19
06/17/2022 13:48:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=19
06/17/2022 13:48:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=19
06/17/2022 13:48:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=20
06/17/2022 13:49:04 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.7124468131527675 on epoch=20
06/17/2022 13:49:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=20
06/17/2022 13:49:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=20
06/17/2022 13:49:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=21
06/17/2022 13:49:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=21
06/17/2022 13:49:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=21
06/17/2022 13:49:23 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.7337193539779807 on epoch=21
06/17/2022 13:49:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=22
06/17/2022 13:49:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=22
06/17/2022 13:49:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=22
06/17/2022 13:49:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=23
06/17/2022 13:49:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=23
06/17/2022 13:49:41 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.7131795893651042 on epoch=23
06/17/2022 13:49:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=23
06/17/2022 13:49:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=24
06/17/2022 13:49:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=24
06/17/2022 13:49:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=24
06/17/2022 13:49:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=24
06/17/2022 13:50:00 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.7469387113927569 on epoch=24
06/17/2022 13:50:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=25
06/17/2022 13:50:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=25
06/17/2022 13:50:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=25
06/17/2022 13:50:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=26
06/17/2022 13:50:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=26
06/17/2022 13:50:19 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.7154200157723289 on epoch=26
06/17/2022 13:50:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=26
06/17/2022 13:50:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=27
06/17/2022 13:50:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=27
06/17/2022 13:50:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=27
06/17/2022 13:50:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=28
06/17/2022 13:50:38 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.7374641406968246 on epoch=28
06/17/2022 13:50:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=28
06/17/2022 13:50:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=28
06/17/2022 13:50:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=29
06/17/2022 13:50:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=29
06/17/2022 13:50:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=29
06/17/2022 13:50:56 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.7693541787896665 on epoch=29
06/17/2022 13:50:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7605323367549651 -> 0.7693541787896665 on epoch=29, global_step=950
06/17/2022 13:50:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=29
06/17/2022 13:51:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=30
06/17/2022 13:51:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=30
06/17/2022 13:51:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=30
06/17/2022 13:51:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=31
06/17/2022 13:51:15 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.7305875372749978 on epoch=31
06/17/2022 13:51:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=31
06/17/2022 13:51:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=31
06/17/2022 13:51:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=32
06/17/2022 13:51:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=32
06/17/2022 13:51:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=32
06/17/2022 13:51:34 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.6568134109063883 on epoch=32
06/17/2022 13:51:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=33
06/17/2022 13:51:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=33
06/17/2022 13:51:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=33
06/17/2022 13:51:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=34
06/17/2022 13:51:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=34
06/17/2022 13:51:53 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.7780898574118913 on epoch=34
06/17/2022 13:51:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7693541787896665 -> 0.7780898574118913 on epoch=34, global_step=1100
06/17/2022 13:51:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=34
06/17/2022 13:51:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=34
06/17/2022 13:52:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=35
06/17/2022 13:52:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
06/17/2022 13:52:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=35
06/17/2022 13:52:12 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.7689881110555338 on epoch=35
06/17/2022 13:52:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=36
06/17/2022 13:52:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=36
06/17/2022 13:52:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=36
06/17/2022 13:52:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=37
06/17/2022 13:52:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.53 on epoch=37
06/17/2022 13:52:30 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.6535428286104678 on epoch=37
06/17/2022 13:52:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=37
06/17/2022 13:52:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=38
06/17/2022 13:52:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=38
06/17/2022 13:52:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=38
06/17/2022 13:52:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=39
06/17/2022 13:52:49 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.7620612445979681 on epoch=39
06/17/2022 13:52:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=39
06/17/2022 13:52:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
06/17/2022 13:52:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=39
06/17/2022 13:52:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=40
06/17/2022 13:53:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=40
06/17/2022 13:53:08 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.7624764662628083 on epoch=40
06/17/2022 13:53:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=40
06/17/2022 13:53:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=41
06/17/2022 13:53:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=41
06/17/2022 13:53:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=41
06/17/2022 13:53:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=42
06/17/2022 13:53:27 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7817994247507433 on epoch=42
06/17/2022 13:53:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7780898574118913 -> 0.7817994247507433 on epoch=42, global_step=1350
06/17/2022 13:53:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=42
06/17/2022 13:53:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
06/17/2022 13:53:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=43
06/17/2022 13:53:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=43
06/17/2022 13:53:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=43
06/17/2022 13:53:46 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.7658405786333701 on epoch=43
06/17/2022 13:53:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=44
06/17/2022 13:53:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=44
06/17/2022 13:53:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=44
06/17/2022 13:53:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=44
06/17/2022 13:53:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=45
06/17/2022 13:54:05 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.8016508329989103 on epoch=45
06/17/2022 13:54:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7817994247507433 -> 0.8016508329989103 on epoch=45, global_step=1450
06/17/2022 13:54:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=45
06/17/2022 13:54:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=45
06/17/2022 13:54:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=46
06/17/2022 13:54:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=46
06/17/2022 13:54:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
06/17/2022 13:54:24 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.8174978927216487 on epoch=46
06/17/2022 13:54:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8016508329989103 -> 0.8174978927216487 on epoch=46, global_step=1500
06/17/2022 13:54:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=47
06/17/2022 13:54:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=47
06/17/2022 13:54:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=47
06/17/2022 13:54:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.58 on epoch=48
06/17/2022 13:54:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=48
06/17/2022 13:54:43 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.800705430514828 on epoch=48
06/17/2022 13:54:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=48
06/17/2022 13:54:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=49
06/17/2022 13:54:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.60 on epoch=49
06/17/2022 13:54:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.72 on epoch=49
06/17/2022 13:54:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=49
06/17/2022 13:55:02 - INFO - __main__ - Global step 1600 Train loss 0.57 Classification-F1 0.7940147314251114 on epoch=49
06/17/2022 13:55:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=50
06/17/2022 13:55:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=50
06/17/2022 13:55:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=50
06/17/2022 13:55:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=51
06/17/2022 13:55:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=51
06/17/2022 13:55:22 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.7740030923092915 on epoch=51
06/17/2022 13:55:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=51
06/17/2022 13:55:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=52
06/17/2022 13:55:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=52
06/17/2022 13:55:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=52
06/17/2022 13:55:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=53
06/17/2022 13:55:41 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.7828535539450181 on epoch=53
06/17/2022 13:55:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=53
06/17/2022 13:55:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=53
06/17/2022 13:55:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=54
06/17/2022 13:55:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=54
06/17/2022 13:55:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=54
06/17/2022 13:56:00 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.780538963820014 on epoch=54
06/17/2022 13:56:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=54
06/17/2022 13:56:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=55
06/17/2022 13:56:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=55
06/17/2022 13:56:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=55
06/17/2022 13:56:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
06/17/2022 13:56:19 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.7893759181446556 on epoch=56
06/17/2022 13:56:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=56
06/17/2022 13:56:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=56
06/17/2022 13:56:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=57
06/17/2022 13:56:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=57
06/17/2022 13:56:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=57
06/17/2022 13:56:38 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.7644504851407923 on epoch=57
06/17/2022 13:56:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=58
06/17/2022 13:56:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=58
06/17/2022 13:56:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=58
06/17/2022 13:56:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=59
06/17/2022 13:56:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=59
06/17/2022 13:56:58 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.7985622193865554 on epoch=59
06/17/2022 13:57:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=59
06/17/2022 13:57:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=59
06/17/2022 13:57:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.69 on epoch=60
06/17/2022 13:57:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=60
06/17/2022 13:57:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.53 on epoch=60
06/17/2022 13:57:17 - INFO - __main__ - Global step 1950 Train loss 0.49 Classification-F1 0.767129936481635 on epoch=60
06/17/2022 13:57:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=61
06/17/2022 13:57:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=61
06/17/2022 13:57:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
06/17/2022 13:57:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=62
06/17/2022 13:57:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=62
06/17/2022 13:57:36 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.7958596157152112 on epoch=62
06/17/2022 13:57:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=62
06/17/2022 13:57:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=63
06/17/2022 13:57:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=63
06/17/2022 13:57:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=63
06/17/2022 13:57:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=64
06/17/2022 13:57:55 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.7599529343600986 on epoch=64
06/17/2022 13:57:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=64
06/17/2022 13:58:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=64
06/17/2022 13:58:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
06/17/2022 13:58:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=65
06/17/2022 13:58:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=65
06/17/2022 13:58:14 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7681521615214869 on epoch=65
06/17/2022 13:58:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=65
06/17/2022 13:58:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=66
06/17/2022 13:58:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=66
06/17/2022 13:58:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=66
06/17/2022 13:58:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=67
06/17/2022 13:58:33 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.7790837396959381 on epoch=67
06/17/2022 13:58:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=67
06/17/2022 13:58:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=67
06/17/2022 13:58:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=68
06/17/2022 13:58:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=68
06/17/2022 13:58:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=68
06/17/2022 13:58:52 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.7965460492560151 on epoch=68
06/17/2022 13:58:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=69
06/17/2022 13:58:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=69
06/17/2022 13:59:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.34 on epoch=69
06/17/2022 13:59:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=69
06/17/2022 13:59:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=70
06/17/2022 13:59:12 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.8081648151663245 on epoch=70
06/17/2022 13:59:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=70
06/17/2022 13:59:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.30 on epoch=70
06/17/2022 13:59:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=71
06/17/2022 13:59:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.32 on epoch=71
06/17/2022 13:59:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=71
06/17/2022 13:59:31 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.763675402833472 on epoch=71
06/17/2022 13:59:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
06/17/2022 13:59:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.31 on epoch=72
06/17/2022 13:59:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=72
06/17/2022 13:59:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.28 on epoch=73
06/17/2022 13:59:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
06/17/2022 13:59:50 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.8103121765455823 on epoch=73
06/17/2022 13:59:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
06/17/2022 13:59:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.31 on epoch=74
06/17/2022 13:59:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
06/17/2022 14:00:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=74
06/17/2022 14:00:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=74
06/17/2022 14:00:09 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.7937384160810841 on epoch=74
06/17/2022 14:00:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=75
06/17/2022 14:00:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=75
06/17/2022 14:00:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.31 on epoch=75
06/17/2022 14:00:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=76
06/17/2022 14:00:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=76
06/17/2022 14:00:28 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.7475236039926624 on epoch=76
06/17/2022 14:00:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=76
06/17/2022 14:00:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=77
06/17/2022 14:00:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=77
06/17/2022 14:00:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.26 on epoch=77
06/17/2022 14:00:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
06/17/2022 14:00:47 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.7540424946168898 on epoch=78
06/17/2022 14:00:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=78
06/17/2022 14:00:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=78
06/17/2022 14:00:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.29 on epoch=79
06/17/2022 14:00:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=79
06/17/2022 14:01:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=79
06/17/2022 14:01:07 - INFO - __main__ - Global step 2550 Train loss 0.25 Classification-F1 0.793730015651915 on epoch=79
06/17/2022 14:01:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
06/17/2022 14:01:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=80
06/17/2022 14:01:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=80
06/17/2022 14:01:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.26 on epoch=80
06/17/2022 14:01:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=81
06/17/2022 14:01:26 - INFO - __main__ - Global step 2600 Train loss 0.21 Classification-F1 0.7590684474499134 on epoch=81
06/17/2022 14:01:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=81
06/17/2022 14:01:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.25 on epoch=81
06/17/2022 14:01:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=82
06/17/2022 14:01:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=82
06/17/2022 14:01:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=82
06/17/2022 14:01:45 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.7587875241219303 on epoch=82
06/17/2022 14:01:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=83
06/17/2022 14:01:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=83
06/17/2022 14:01:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=83
06/17/2022 14:01:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=84
06/17/2022 14:01:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.27 on epoch=84
06/17/2022 14:02:04 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.771422723795949 on epoch=84
06/17/2022 14:02:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=84
06/17/2022 14:02:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 14:02:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=85
06/17/2022 14:02:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=85
06/17/2022 14:02:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=85
06/17/2022 14:02:23 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.7891447220835166 on epoch=85
06/17/2022 14:02:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=86
06/17/2022 14:02:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.34 on epoch=86
06/17/2022 14:02:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.25 on epoch=86
06/17/2022 14:02:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=87
06/17/2022 14:02:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.26 on epoch=87
06/17/2022 14:02:42 - INFO - __main__ - Global step 2800 Train loss 0.24 Classification-F1 0.7835496982345849 on epoch=87
06/17/2022 14:02:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.35 on epoch=87
06/17/2022 14:02:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.37 on epoch=88
06/17/2022 14:02:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=88
06/17/2022 14:02:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=88
06/17/2022 14:02:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.29 on epoch=89
06/17/2022 14:03:02 - INFO - __main__ - Global step 2850 Train loss 0.30 Classification-F1 0.7990817606487708 on epoch=89
06/17/2022 14:03:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=89
06/17/2022 14:03:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=89
06/17/2022 14:03:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=89
06/17/2022 14:03:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=90
06/17/2022 14:03:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=90
06/17/2022 14:03:21 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.7855640420673089 on epoch=90
06/17/2022 14:03:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=90
06/17/2022 14:03:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.25 on epoch=91
06/17/2022 14:03:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=91
06/17/2022 14:03:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=91
06/17/2022 14:03:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=92
06/17/2022 14:03:40 - INFO - __main__ - Global step 2950 Train loss 0.24 Classification-F1 0.7494146804427348 on epoch=92
06/17/2022 14:03:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.26 on epoch=92
06/17/2022 14:03:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=92
06/17/2022 14:03:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.24 on epoch=93
06/17/2022 14:03:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=93
06/17/2022 14:03:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=93
06/17/2022 14:03:54 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:03:54 - INFO - __main__ - Printing 3 examples
06/17/2022 14:03:54 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:03:54 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:03:54 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:03:54 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:03:54 - INFO - __main__ - Printing 3 examples
06/17/2022 14:03:54 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 14:03:54 - INFO - __main__ - ['others']
06/17/2022 14:03:54 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:03:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:03:55 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:03:59 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.7539645450346841 on epoch=93
06/17/2022 14:03:59 - INFO - __main__ - save last model!
06/17/2022 14:03:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 14:03:59 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 14:03:59 - INFO - __main__ - Printing 3 examples
06/17/2022 14:03:59 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 14:03:59 - INFO - __main__ - ['others']
06/17/2022 14:03:59 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 14:03:59 - INFO - __main__ - ['others']
06/17/2022 14:03:59 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 14:03:59 - INFO - __main__ - ['others']
06/17/2022 14:03:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:04:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:04:07 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 14:04:13 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:04:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:04:14 - INFO - __main__ - Starting training!
06/17/2022 14:05:22 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_100_0.3_8_predictions.txt
06/17/2022 14:05:22 - INFO - __main__ - Classification-F1 on test data: 0.4556
06/17/2022 14:05:22 - INFO - __main__ - prefix=emo_128_100, lr=0.3, bsz=8, dev_performance=0.8174978927216487, test_performance=0.4555822123990768
06/17/2022 14:05:22 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.2, bsz=8 ...
06/17/2022 14:05:23 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:05:23 - INFO - __main__ - Printing 3 examples
06/17/2022 14:05:23 - INFO - __main__ -  [emo] how cause yes am listening
06/17/2022 14:05:23 - INFO - __main__ - ['others']
06/17/2022 14:05:23 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/17/2022 14:05:23 - INFO - __main__ - ['others']
06/17/2022 14:05:23 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/17/2022 14:05:23 - INFO - __main__ - ['others']
06/17/2022 14:05:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:05:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:05:24 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:05:24 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:05:24 - INFO - __main__ - Printing 3 examples
06/17/2022 14:05:24 - INFO - __main__ -  [emo] when when it comes to you never why
06/17/2022 14:05:24 - INFO - __main__ - ['others']
06/17/2022 14:05:24 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
06/17/2022 14:05:24 - INFO - __main__ - ['others']
06/17/2022 14:05:24 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
06/17/2022 14:05:24 - INFO - __main__ - ['others']
06/17/2022 14:05:24 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:05:24 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:05:25 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:05:40 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:05:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:05:41 - INFO - __main__ - Starting training!
06/17/2022 14:05:44 - INFO - __main__ - Step 10 Global step 10 Train loss 3.29 on epoch=0
06/17/2022 14:05:47 - INFO - __main__ - Step 20 Global step 20 Train loss 1.96 on epoch=0
06/17/2022 14:05:49 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=0
06/17/2022 14:05:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=1
06/17/2022 14:05:54 - INFO - __main__ - Step 50 Global step 50 Train loss 1.10 on epoch=1
06/17/2022 14:06:01 - INFO - __main__ - Global step 50 Train loss 1.79 Classification-F1 0.15197600378887544 on epoch=1
06/17/2022 14:06:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15197600378887544 on epoch=1, global_step=50
06/17/2022 14:06:04 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=1
06/17/2022 14:06:06 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=2
06/17/2022 14:06:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=2
06/17/2022 14:06:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
06/17/2022 14:06:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=3
06/17/2022 14:06:20 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.42968773504659213 on epoch=3
06/17/2022 14:06:20 - INFO - __main__ - Saving model with best Classification-F1: 0.15197600378887544 -> 0.42968773504659213 on epoch=3, global_step=100
06/17/2022 14:06:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=3
06/17/2022 14:06:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=3
06/17/2022 14:06:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=4
06/17/2022 14:06:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.95 on epoch=4
06/17/2022 14:06:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=4
06/17/2022 14:06:39 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.17909943889548555 on epoch=4
06/17/2022 14:06:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=4
06/17/2022 14:06:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=5
06/17/2022 14:06:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=5
06/17/2022 14:06:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
06/17/2022 14:06:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=6
06/17/2022 14:06:58 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.36701312221969673 on epoch=6
06/17/2022 14:07:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=6
06/17/2022 14:07:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=6
06/17/2022 14:07:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.73 on epoch=7
06/17/2022 14:07:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=7
06/17/2022 14:07:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=7
06/17/2022 14:07:17 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.2481942539550459 on epoch=7
06/17/2022 14:07:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=8
06/17/2022 14:07:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=8
06/17/2022 14:07:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=8
06/17/2022 14:07:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.64 on epoch=9
06/17/2022 14:07:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=9
06/17/2022 14:07:36 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.6029123772746073 on epoch=9
06/17/2022 14:07:36 - INFO - __main__ - Saving model with best Classification-F1: 0.42968773504659213 -> 0.6029123772746073 on epoch=9, global_step=300
06/17/2022 14:07:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=9
06/17/2022 14:07:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=9
06/17/2022 14:07:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=10
06/17/2022 14:07:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
06/17/2022 14:07:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.77 on epoch=10
06/17/2022 14:07:55 - INFO - __main__ - Global step 350 Train loss 0.70 Classification-F1 0.4897611946244352 on epoch=10
06/17/2022 14:07:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.65 on epoch=11
06/17/2022 14:08:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.72 on epoch=11
06/17/2022 14:08:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=11
06/17/2022 14:08:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.61 on epoch=12
06/17/2022 14:08:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.72 on epoch=12
06/17/2022 14:08:14 - INFO - __main__ - Global step 400 Train loss 0.68 Classification-F1 0.6003009940123157 on epoch=12
06/17/2022 14:08:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=12
06/17/2022 14:08:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=13
06/17/2022 14:08:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=13
06/17/2022 14:08:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=13
06/17/2022 14:08:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=14
06/17/2022 14:08:33 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.5387953492130967 on epoch=14
06/17/2022 14:08:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=14
06/17/2022 14:08:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.70 on epoch=14
06/17/2022 14:08:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=14
06/17/2022 14:08:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=15
06/17/2022 14:08:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=15
06/17/2022 14:08:51 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.5129360449843128 on epoch=15
06/17/2022 14:08:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=15
06/17/2022 14:08:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=16
06/17/2022 14:08:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.66 on epoch=16
06/17/2022 14:09:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=16
06/17/2022 14:09:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=17
06/17/2022 14:09:10 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.6585601969698648 on epoch=17
06/17/2022 14:09:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6029123772746073 -> 0.6585601969698648 on epoch=17, global_step=550
06/17/2022 14:09:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=17
06/17/2022 14:09:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=17
06/17/2022 14:09:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.62 on epoch=18
06/17/2022 14:09:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=18
06/17/2022 14:09:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=18
06/17/2022 14:09:29 - INFO - __main__ - Global step 600 Train loss 0.59 Classification-F1 0.6726643459013184 on epoch=18
06/17/2022 14:09:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6585601969698648 -> 0.6726643459013184 on epoch=18, global_step=600
06/17/2022 14:09:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=19
06/17/2022 14:09:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=19
06/17/2022 14:09:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=19
06/17/2022 14:09:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=19
06/17/2022 14:09:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=20
06/17/2022 14:09:48 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.7347985746017797 on epoch=20
06/17/2022 14:09:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6726643459013184 -> 0.7347985746017797 on epoch=20, global_step=650
06/17/2022 14:09:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=20
06/17/2022 14:09:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=20
06/17/2022 14:09:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=21
06/17/2022 14:09:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.63 on epoch=21
06/17/2022 14:10:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=21
06/17/2022 14:10:07 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.7154850945271934 on epoch=21
06/17/2022 14:10:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=22
06/17/2022 14:10:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=22
06/17/2022 14:10:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=22
06/17/2022 14:10:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=23
06/17/2022 14:10:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.53 on epoch=23
06/17/2022 14:10:26 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.6639955966493399 on epoch=23
06/17/2022 14:10:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=23
06/17/2022 14:10:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=24
06/17/2022 14:10:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=24
06/17/2022 14:10:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=24
06/17/2022 14:10:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=24
06/17/2022 14:10:45 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.7428085204948506 on epoch=24
06/17/2022 14:10:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7347985746017797 -> 0.7428085204948506 on epoch=24, global_step=800
06/17/2022 14:10:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=25
06/17/2022 14:10:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=25
06/17/2022 14:10:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=25
06/17/2022 14:10:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=26
06/17/2022 14:10:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=26
06/17/2022 14:11:04 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.6852628329006547 on epoch=26
06/17/2022 14:11:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=26
06/17/2022 14:11:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=27
06/17/2022 14:11:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=27
06/17/2022 14:11:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=27
06/17/2022 14:11:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=28
06/17/2022 14:11:22 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.588774273073742 on epoch=28
06/17/2022 14:11:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=28
06/17/2022 14:11:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=28
06/17/2022 14:11:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=29
06/17/2022 14:11:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=29
06/17/2022 14:11:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=29
06/17/2022 14:11:41 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.6998455047872596 on epoch=29
06/17/2022 14:11:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=29
06/17/2022 14:11:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=30
06/17/2022 14:11:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=30
06/17/2022 14:11:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=30
06/17/2022 14:11:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=31
06/17/2022 14:12:00 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.6711237878176187 on epoch=31
06/17/2022 14:12:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=31
06/17/2022 14:12:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=31
06/17/2022 14:12:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=32
06/17/2022 14:12:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=32
06/17/2022 14:12:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=32
06/17/2022 14:12:19 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.7421925915507721 on epoch=32
06/17/2022 14:12:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=33
06/17/2022 14:12:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=33
06/17/2022 14:12:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=33
06/17/2022 14:12:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=34
06/17/2022 14:12:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=34
06/17/2022 14:12:37 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.7021185564320104 on epoch=34
06/17/2022 14:12:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=34
06/17/2022 14:12:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=34
06/17/2022 14:12:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=35
06/17/2022 14:12:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=35
06/17/2022 14:12:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.52 on epoch=35
06/17/2022 14:12:56 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.748488580385132 on epoch=35
06/17/2022 14:12:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7428085204948506 -> 0.748488580385132 on epoch=35, global_step=1150
06/17/2022 14:12:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
06/17/2022 14:13:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=36
06/17/2022 14:13:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=36
06/17/2022 14:13:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=37
06/17/2022 14:13:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=37
06/17/2022 14:13:15 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.6599178255372946 on epoch=37
06/17/2022 14:13:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=37
06/17/2022 14:13:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=38
06/17/2022 14:13:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=38
06/17/2022 14:13:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=38
06/17/2022 14:13:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=39
06/17/2022 14:13:34 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.6647378623075776 on epoch=39
06/17/2022 14:13:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=39
06/17/2022 14:13:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=39
06/17/2022 14:13:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=39
06/17/2022 14:13:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=40
06/17/2022 14:13:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=40
06/17/2022 14:13:53 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.6846995797852597 on epoch=40
06/17/2022 14:13:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=40
06/17/2022 14:13:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
06/17/2022 14:14:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=41
06/17/2022 14:14:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=41
06/17/2022 14:14:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=42
06/17/2022 14:14:11 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.7488318319018851 on epoch=42
06/17/2022 14:14:12 - INFO - __main__ - Saving model with best Classification-F1: 0.748488580385132 -> 0.7488318319018851 on epoch=42, global_step=1350
06/17/2022 14:14:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=42
06/17/2022 14:14:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=42
06/17/2022 14:14:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=43
06/17/2022 14:14:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=43
06/17/2022 14:14:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=43
06/17/2022 14:14:30 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.7824279410256175 on epoch=43
06/17/2022 14:14:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7488318319018851 -> 0.7824279410256175 on epoch=43, global_step=1400
06/17/2022 14:14:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=44
06/17/2022 14:14:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=44
06/17/2022 14:14:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=44
06/17/2022 14:14:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=44
06/17/2022 14:14:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=45
06/17/2022 14:14:49 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.7628619787077258 on epoch=45
06/17/2022 14:14:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=45
06/17/2022 14:14:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=45
06/17/2022 14:14:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=46
06/17/2022 14:14:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=46
06/17/2022 14:15:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=46
06/17/2022 14:15:08 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.7193235948547457 on epoch=46
06/17/2022 14:15:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=47
06/17/2022 14:15:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
06/17/2022 14:15:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=47
06/17/2022 14:15:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=48
06/17/2022 14:15:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=48
06/17/2022 14:15:27 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.789296137469589 on epoch=48
06/17/2022 14:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7824279410256175 -> 0.789296137469589 on epoch=48, global_step=1550
06/17/2022 14:15:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=48
06/17/2022 14:15:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=49
06/17/2022 14:15:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=49
06/17/2022 14:15:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=49
06/17/2022 14:15:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=49
06/17/2022 14:15:45 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.7597646557440885 on epoch=49
06/17/2022 14:15:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=50
06/17/2022 14:15:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=50
06/17/2022 14:15:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=50
06/17/2022 14:15:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=51
06/17/2022 14:15:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=51
06/17/2022 14:16:04 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.7586571932399416 on epoch=51
06/17/2022 14:16:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=51
06/17/2022 14:16:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=52
06/17/2022 14:16:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=52
06/17/2022 14:16:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=52
06/17/2022 14:16:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
06/17/2022 14:16:23 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.7508898704731144 on epoch=53
06/17/2022 14:16:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=53
06/17/2022 14:16:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=53
06/17/2022 14:16:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=54
06/17/2022 14:16:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=54
06/17/2022 14:16:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=54
06/17/2022 14:16:42 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.781691013814653 on epoch=54
06/17/2022 14:16:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=54
06/17/2022 14:16:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
06/17/2022 14:16:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=55
06/17/2022 14:16:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=55
06/17/2022 14:16:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=56
06/17/2022 14:17:01 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.7776847996441453 on epoch=56
06/17/2022 14:17:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=56
06/17/2022 14:17:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=56
06/17/2022 14:17:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=57
06/17/2022 14:17:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=57
06/17/2022 14:17:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=57
06/17/2022 14:17:19 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.7775109393530445 on epoch=57
06/17/2022 14:17:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=58
06/17/2022 14:17:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=58
06/17/2022 14:17:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=58
06/17/2022 14:17:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=59
06/17/2022 14:17:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=59
06/17/2022 14:17:38 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.7859276009011373 on epoch=59
06/17/2022 14:17:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=59
06/17/2022 14:17:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=59
06/17/2022 14:17:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=60
06/17/2022 14:17:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=60
06/17/2022 14:17:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=60
06/17/2022 14:17:57 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.7663640100393416 on epoch=60
06/17/2022 14:17:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=61
06/17/2022 14:18:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=61
06/17/2022 14:18:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=61
06/17/2022 14:18:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
06/17/2022 14:18:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=62
06/17/2022 14:18:16 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.7718148084828891 on epoch=62
06/17/2022 14:18:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=62
06/17/2022 14:18:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=63
06/17/2022 14:18:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.34 on epoch=63
06/17/2022 14:18:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=63
06/17/2022 14:18:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=64
06/17/2022 14:18:34 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7641573124547505 on epoch=64
06/17/2022 14:18:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=64
06/17/2022 14:18:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=64
06/17/2022 14:18:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=64
06/17/2022 14:18:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=65
06/17/2022 14:18:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=65
06/17/2022 14:18:53 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7301996894862304 on epoch=65
06/17/2022 14:18:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=65
06/17/2022 14:18:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=66
06/17/2022 14:19:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=66
06/17/2022 14:19:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
06/17/2022 14:19:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=67
06/17/2022 14:19:12 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.7784087996125801 on epoch=67
06/17/2022 14:19:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=67
06/17/2022 14:19:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=67
06/17/2022 14:19:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=68
06/17/2022 14:19:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=68
06/17/2022 14:19:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=68
06/17/2022 14:19:31 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.7710009629291283 on epoch=68
06/17/2022 14:19:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.30 on epoch=69
06/17/2022 14:19:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.30 on epoch=69
06/17/2022 14:19:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=69
06/17/2022 14:19:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=69
06/17/2022 14:19:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=70
06/17/2022 14:19:50 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7965471671884101 on epoch=70
06/17/2022 14:19:50 - INFO - __main__ - Saving model with best Classification-F1: 0.789296137469589 -> 0.7965471671884101 on epoch=70, global_step=2250
06/17/2022 14:19:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=70
06/17/2022 14:19:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=70
06/17/2022 14:19:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=71
06/17/2022 14:19:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=71
06/17/2022 14:20:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=71
06/17/2022 14:20:08 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.7598559329315514 on epoch=71
06/17/2022 14:20:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=72
06/17/2022 14:20:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=72
06/17/2022 14:20:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=72
06/17/2022 14:20:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=73
06/17/2022 14:20:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=73
06/17/2022 14:20:27 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.7506774439999901 on epoch=73
06/17/2022 14:20:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=73
06/17/2022 14:20:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
06/17/2022 14:20:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=74
06/17/2022 14:20:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=74
06/17/2022 14:20:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=74
06/17/2022 14:20:46 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.7885325321041659 on epoch=74
06/17/2022 14:20:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=75
06/17/2022 14:20:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=75
06/17/2022 14:20:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=75
06/17/2022 14:20:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=76
06/17/2022 14:20:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.29 on epoch=76
06/17/2022 14:21:05 - INFO - __main__ - Global step 2450 Train loss 0.25 Classification-F1 0.7637129480839384 on epoch=76
06/17/2022 14:21:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
06/17/2022 14:21:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=77
06/17/2022 14:21:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.23 on epoch=77
06/17/2022 14:21:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=77
06/17/2022 14:21:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=78
06/17/2022 14:21:24 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.7852654365177895 on epoch=78
06/17/2022 14:21:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
06/17/2022 14:21:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=78
06/17/2022 14:21:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=79
06/17/2022 14:21:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=79
06/17/2022 14:21:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=79
06/17/2022 14:21:42 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.7669336956141599 on epoch=79
06/17/2022 14:21:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=79
06/17/2022 14:21:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=80
06/17/2022 14:21:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=80
06/17/2022 14:21:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=80
06/17/2022 14:21:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=81
06/17/2022 14:22:01 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.7640093127474068 on epoch=81
06/17/2022 14:22:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.27 on epoch=81
06/17/2022 14:22:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 14:22:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
06/17/2022 14:22:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=82
06/17/2022 14:22:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=82
06/17/2022 14:22:20 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.7663941522929352 on epoch=82
06/17/2022 14:22:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=83
06/17/2022 14:22:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
06/17/2022 14:22:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=83
06/17/2022 14:22:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
06/17/2022 14:22:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=84
06/17/2022 14:22:39 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.7848100238368755 on epoch=84
06/17/2022 14:22:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=84
06/17/2022 14:22:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=84
06/17/2022 14:22:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=85
06/17/2022 14:22:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=85
06/17/2022 14:22:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=85
06/17/2022 14:22:57 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.7671935778359928 on epoch=85
06/17/2022 14:23:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=86
06/17/2022 14:23:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.30 on epoch=86
06/17/2022 14:23:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=86
06/17/2022 14:23:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=87
06/17/2022 14:23:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.23 on epoch=87
06/17/2022 14:23:16 - INFO - __main__ - Global step 2800 Train loss 0.22 Classification-F1 0.7343983533540704 on epoch=87
06/17/2022 14:23:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=87
06/17/2022 14:23:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=88
06/17/2022 14:23:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=88
06/17/2022 14:23:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
06/17/2022 14:23:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=89
06/17/2022 14:23:35 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.7832931258246827 on epoch=89
06/17/2022 14:23:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=89
06/17/2022 14:23:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.29 on epoch=89
06/17/2022 14:23:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=89
06/17/2022 14:23:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=90
06/17/2022 14:23:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=90
06/17/2022 14:23:54 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.7446178033553452 on epoch=90
06/17/2022 14:23:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=90
06/17/2022 14:23:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=91
06/17/2022 14:24:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.26 on epoch=91
06/17/2022 14:24:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=91
06/17/2022 14:24:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=92
06/17/2022 14:24:13 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.7798567461688224 on epoch=92
06/17/2022 14:24:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=92
06/17/2022 14:24:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=92
06/17/2022 14:24:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=93
06/17/2022 14:24:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.30 on epoch=93
06/17/2022 14:24:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
06/17/2022 14:24:26 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:24:26 - INFO - __main__ - Printing 3 examples
06/17/2022 14:24:26 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 14:24:26 - INFO - __main__ - ['others']
06/17/2022 14:24:26 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 14:24:26 - INFO - __main__ - ['others']
06/17/2022 14:24:26 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 14:24:26 - INFO - __main__ - ['others']
06/17/2022 14:24:26 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:24:26 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:24:27 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:24:27 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:24:27 - INFO - __main__ - Printing 3 examples
06/17/2022 14:24:27 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 14:24:27 - INFO - __main__ - ['others']
06/17/2022 14:24:27 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 14:24:27 - INFO - __main__ - ['others']
06/17/2022 14:24:27 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 14:24:27 - INFO - __main__ - ['others']
06/17/2022 14:24:27 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:24:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:24:27 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:24:31 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.7656056603467796 on epoch=93
06/17/2022 14:24:31 - INFO - __main__ - save last model!
06/17/2022 14:24:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 14:24:31 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 14:24:31 - INFO - __main__ - Printing 3 examples
06/17/2022 14:24:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 14:24:31 - INFO - __main__ - ['others']
06/17/2022 14:24:31 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 14:24:31 - INFO - __main__ - ['others']
06/17/2022 14:24:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 14:24:31 - INFO - __main__ - ['others']
06/17/2022 14:24:31 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:24:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:24:39 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 14:24:45 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:24:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:24:46 - INFO - __main__ - Starting training!
06/17/2022 14:25:51 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_100_0.2_8_predictions.txt
06/17/2022 14:25:51 - INFO - __main__ - Classification-F1 on test data: 0.4574
06/17/2022 14:25:51 - INFO - __main__ - prefix=emo_128_100, lr=0.2, bsz=8, dev_performance=0.7965471671884101, test_performance=0.4573782148535366
06/17/2022 14:25:51 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.5, bsz=8 ...
06/17/2022 14:25:52 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:25:52 - INFO - __main__ - Printing 3 examples
06/17/2022 14:25:52 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 14:25:52 - INFO - __main__ - ['others']
06/17/2022 14:25:52 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 14:25:52 - INFO - __main__ - ['others']
06/17/2022 14:25:52 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 14:25:52 - INFO - __main__ - ['others']
06/17/2022 14:25:52 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:25:53 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:25:53 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:25:53 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:25:53 - INFO - __main__ - Printing 3 examples
06/17/2022 14:25:53 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 14:25:53 - INFO - __main__ - ['others']
06/17/2022 14:25:53 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 14:25:53 - INFO - __main__ - ['others']
06/17/2022 14:25:53 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 14:25:53 - INFO - __main__ - ['others']
06/17/2022 14:25:53 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:25:53 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:25:54 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:26:09 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:26:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:26:10 - INFO - __main__ - Starting training!
06/17/2022 14:26:13 - INFO - __main__ - Step 10 Global step 10 Train loss 2.53 on epoch=0
06/17/2022 14:26:16 - INFO - __main__ - Step 20 Global step 20 Train loss 1.45 on epoch=0
06/17/2022 14:26:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.10 on epoch=0
06/17/2022 14:26:21 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=1
06/17/2022 14:26:23 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=1
06/17/2022 14:26:30 - INFO - __main__ - Global step 50 Train loss 1.44 Classification-F1 0.21761449998566063 on epoch=1
06/17/2022 14:26:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21761449998566063 on epoch=1, global_step=50
06/17/2022 14:26:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=1
06/17/2022 14:26:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=2
06/17/2022 14:26:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=2
06/17/2022 14:26:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=2
06/17/2022 14:26:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=3
06/17/2022 14:26:49 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.2894683400349295 on epoch=3
06/17/2022 14:26:49 - INFO - __main__ - Saving model with best Classification-F1: 0.21761449998566063 -> 0.2894683400349295 on epoch=3, global_step=100
06/17/2022 14:26:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=3
06/17/2022 14:26:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=3
06/17/2022 14:26:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=4
06/17/2022 14:26:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=4
06/17/2022 14:27:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=4
06/17/2022 14:27:08 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.5102893626327856 on epoch=4
06/17/2022 14:27:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2894683400349295 -> 0.5102893626327856 on epoch=4, global_step=150
06/17/2022 14:27:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=4
06/17/2022 14:27:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.76 on epoch=5
06/17/2022 14:27:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=5
06/17/2022 14:27:18 - INFO - __main__ - Step 190 Global step 190 Train loss 1.01 on epoch=5
06/17/2022 14:27:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=6
06/17/2022 14:27:27 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.34194280520284237 on epoch=6
06/17/2022 14:27:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.08 on epoch=6
06/17/2022 14:27:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=6
06/17/2022 14:27:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=7
06/17/2022 14:27:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=7
06/17/2022 14:27:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=7
06/17/2022 14:27:46 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.47859118943840234 on epoch=7
06/17/2022 14:27:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=8
06/17/2022 14:27:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=8
06/17/2022 14:27:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=8
06/17/2022 14:27:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=9
06/17/2022 14:27:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=9
06/17/2022 14:28:05 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.3550113131187016 on epoch=9
06/17/2022 14:28:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=9
06/17/2022 14:28:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=9
06/17/2022 14:28:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=10
06/17/2022 14:28:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=10
06/17/2022 14:28:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=10
06/17/2022 14:28:24 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.5563886759956828 on epoch=10
06/17/2022 14:28:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5102893626327856 -> 0.5563886759956828 on epoch=10, global_step=350
06/17/2022 14:28:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=11
06/17/2022 14:28:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=11
06/17/2022 14:28:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=11
06/17/2022 14:28:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=12
06/17/2022 14:28:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=12
06/17/2022 14:28:44 - INFO - __main__ - Global step 400 Train loss 0.71 Classification-F1 0.563224541547048 on epoch=12
06/17/2022 14:28:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5563886759956828 -> 0.563224541547048 on epoch=12, global_step=400
06/17/2022 14:28:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=12
06/17/2022 14:28:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=13
06/17/2022 14:28:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=13
06/17/2022 14:28:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=13
06/17/2022 14:28:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=14
06/17/2022 14:29:03 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.6029754833003136 on epoch=14
06/17/2022 14:29:03 - INFO - __main__ - Saving model with best Classification-F1: 0.563224541547048 -> 0.6029754833003136 on epoch=14, global_step=450
06/17/2022 14:29:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=14
06/17/2022 14:29:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.71 on epoch=14
06/17/2022 14:29:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=14
06/17/2022 14:29:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.58 on epoch=15
06/17/2022 14:29:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.65 on epoch=15
06/17/2022 14:29:22 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.5124895177764475 on epoch=15
06/17/2022 14:29:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=15
06/17/2022 14:29:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=16
06/17/2022 14:29:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=16
06/17/2022 14:29:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=16
06/17/2022 14:29:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=17
06/17/2022 14:29:41 - INFO - __main__ - Global step 550 Train loss 0.64 Classification-F1 0.6113226880697722 on epoch=17
06/17/2022 14:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6029754833003136 -> 0.6113226880697722 on epoch=17, global_step=550
06/17/2022 14:29:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.73 on epoch=17
06/17/2022 14:29:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.69 on epoch=17
06/17/2022 14:29:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=18
06/17/2022 14:29:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=18
06/17/2022 14:29:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=18
06/17/2022 14:30:00 - INFO - __main__ - Global step 600 Train loss 0.65 Classification-F1 0.7357375478927203 on epoch=18
06/17/2022 14:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6113226880697722 -> 0.7357375478927203 on epoch=18, global_step=600
06/17/2022 14:30:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=19
06/17/2022 14:30:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=19
06/17/2022 14:30:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=19
06/17/2022 14:30:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=19
06/17/2022 14:30:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=20
06/17/2022 14:30:19 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.6314078138730465 on epoch=20
06/17/2022 14:30:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.62 on epoch=20
06/17/2022 14:30:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=20
06/17/2022 14:30:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.63 on epoch=21
06/17/2022 14:30:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=21
06/17/2022 14:30:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=21
06/17/2022 14:30:38 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.6830587092861198 on epoch=21
06/17/2022 14:30:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=22
06/17/2022 14:30:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=22
06/17/2022 14:30:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=22
06/17/2022 14:30:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=23
06/17/2022 14:30:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=23
06/17/2022 14:30:57 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.7278330196312287 on epoch=23
06/17/2022 14:31:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=23
06/17/2022 14:31:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=24
06/17/2022 14:31:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=24
06/17/2022 14:31:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.61 on epoch=24
06/17/2022 14:31:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=24
06/17/2022 14:31:16 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.755206249390244 on epoch=24
06/17/2022 14:31:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7357375478927203 -> 0.755206249390244 on epoch=24, global_step=800
06/17/2022 14:31:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=25
06/17/2022 14:31:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.63 on epoch=25
06/17/2022 14:31:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.60 on epoch=25
06/17/2022 14:31:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=26
06/17/2022 14:31:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.61 on epoch=26
06/17/2022 14:31:35 - INFO - __main__ - Global step 850 Train loss 0.57 Classification-F1 0.6497054932661501 on epoch=26
06/17/2022 14:31:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.57 on epoch=26
06/17/2022 14:31:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=27
06/17/2022 14:31:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.67 on epoch=27
06/17/2022 14:31:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=27
06/17/2022 14:31:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=28
06/17/2022 14:31:55 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.6576185141477012 on epoch=28
06/17/2022 14:31:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.64 on epoch=28
06/17/2022 14:32:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=28
06/17/2022 14:32:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=29
06/17/2022 14:32:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=29
06/17/2022 14:32:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.60 on epoch=29
06/17/2022 14:32:14 - INFO - __main__ - Global step 950 Train loss 0.55 Classification-F1 0.7199373005653921 on epoch=29
06/17/2022 14:32:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=29
06/17/2022 14:32:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=30
06/17/2022 14:32:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=30
06/17/2022 14:32:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=30
06/17/2022 14:32:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=31
06/17/2022 14:32:33 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.7200318056044501 on epoch=31
06/17/2022 14:32:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.60 on epoch=31
06/17/2022 14:32:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=31
06/17/2022 14:32:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=32
06/17/2022 14:32:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=32
06/17/2022 14:32:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=32
06/17/2022 14:32:52 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.634682393294522 on epoch=32
06/17/2022 14:32:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=33
06/17/2022 14:32:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=33
06/17/2022 14:32:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=33
06/17/2022 14:33:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=34
06/17/2022 14:33:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=34
06/17/2022 14:33:11 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.7358235869558107 on epoch=34
06/17/2022 14:33:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=34
06/17/2022 14:33:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
06/17/2022 14:33:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=35
06/17/2022 14:33:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=35
06/17/2022 14:33:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=35
06/17/2022 14:33:30 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.7476430113946155 on epoch=35
06/17/2022 14:33:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=36
06/17/2022 14:33:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=36
06/17/2022 14:33:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=36
06/17/2022 14:33:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=37
06/17/2022 14:33:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=37
06/17/2022 14:33:49 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.75352032809139 on epoch=37
06/17/2022 14:33:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=37
06/17/2022 14:33:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=38
06/17/2022 14:33:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=38
06/17/2022 14:33:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=38
06/17/2022 14:34:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=39
06/17/2022 14:34:08 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.7011841834766193 on epoch=39
06/17/2022 14:34:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=39
06/17/2022 14:34:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.57 on epoch=39
06/17/2022 14:34:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=39
06/17/2022 14:34:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=40
06/17/2022 14:34:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=40
06/17/2022 14:34:27 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.7182461852891424 on epoch=40
06/17/2022 14:34:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=40
06/17/2022 14:34:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
06/17/2022 14:34:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=41
06/17/2022 14:34:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=41
06/17/2022 14:34:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=42
06/17/2022 14:34:46 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.6756420251453892 on epoch=42
06/17/2022 14:34:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=42
06/17/2022 14:34:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=42
06/17/2022 14:34:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=43
06/17/2022 14:34:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=43
06/17/2022 14:34:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=43
06/17/2022 14:35:05 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.7608663709894082 on epoch=43
06/17/2022 14:35:05 - INFO - __main__ - Saving model with best Classification-F1: 0.755206249390244 -> 0.7608663709894082 on epoch=43, global_step=1400
06/17/2022 14:35:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
06/17/2022 14:35:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=44
06/17/2022 14:35:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=44
06/17/2022 14:35:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=44
06/17/2022 14:35:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=45
06/17/2022 14:35:24 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.7424294105827985 on epoch=45
06/17/2022 14:35:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=45
06/17/2022 14:35:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=45
06/17/2022 14:35:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=46
06/17/2022 14:35:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=46
06/17/2022 14:35:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=46
06/17/2022 14:35:44 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.750246574538789 on epoch=46
06/17/2022 14:35:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=47
06/17/2022 14:35:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=47
06/17/2022 14:35:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=47
06/17/2022 14:35:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=48
06/17/2022 14:35:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=48
06/17/2022 14:36:03 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.7455916119808473 on epoch=48
06/17/2022 14:36:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=48
06/17/2022 14:36:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=49
06/17/2022 14:36:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=49
06/17/2022 14:36:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=49
06/17/2022 14:36:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=49
06/17/2022 14:36:22 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.7675352077539763 on epoch=49
06/17/2022 14:36:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7608663709894082 -> 0.7675352077539763 on epoch=49, global_step=1600
06/17/2022 14:36:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=50
06/17/2022 14:36:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=50
06/17/2022 14:36:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=50
06/17/2022 14:36:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=51
06/17/2022 14:36:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=51
06/17/2022 14:36:41 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.7538801054018445 on epoch=51
06/17/2022 14:36:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=51
06/17/2022 14:36:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
06/17/2022 14:36:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=52
06/17/2022 14:36:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=52
06/17/2022 14:36:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=53
06/17/2022 14:37:00 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.7460661032298401 on epoch=53
06/17/2022 14:37:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=53
06/17/2022 14:37:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=53
06/17/2022 14:37:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=54
06/17/2022 14:37:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=54
06/17/2022 14:37:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=54
06/17/2022 14:37:19 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.7461214303745303 on epoch=54
06/17/2022 14:37:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=54
06/17/2022 14:37:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=55
06/17/2022 14:37:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=55
06/17/2022 14:37:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=55
06/17/2022 14:37:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=56
06/17/2022 14:37:38 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.7644137716070626 on epoch=56
06/17/2022 14:37:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=56
06/17/2022 14:37:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=56
06/17/2022 14:37:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=57
06/17/2022 14:37:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=57
06/17/2022 14:37:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=57
06/17/2022 14:37:57 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.6900450584756896 on epoch=57
06/17/2022 14:38:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=58
06/17/2022 14:38:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=58
06/17/2022 14:38:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=58
06/17/2022 14:38:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
06/17/2022 14:38:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=59
06/17/2022 14:38:16 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.7629211234007511 on epoch=59
06/17/2022 14:38:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=59
06/17/2022 14:38:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
06/17/2022 14:38:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=60
06/17/2022 14:38:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=60
06/17/2022 14:38:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=60
06/17/2022 14:38:35 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.7334024117349737 on epoch=60
06/17/2022 14:38:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=61
06/17/2022 14:38:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=61
06/17/2022 14:38:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=61
06/17/2022 14:38:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
06/17/2022 14:38:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=62
06/17/2022 14:38:54 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.7830579319457224 on epoch=62
06/17/2022 14:38:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7675352077539763 -> 0.7830579319457224 on epoch=62, global_step=2000
06/17/2022 14:38:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=62
06/17/2022 14:38:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
06/17/2022 14:39:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.39 on epoch=63
06/17/2022 14:39:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=63
06/17/2022 14:39:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=64
06/17/2022 14:39:13 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.7738012192549082 on epoch=64
06/17/2022 14:39:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=64
06/17/2022 14:39:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=64
06/17/2022 14:39:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=64
06/17/2022 14:39:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
06/17/2022 14:39:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.30 on epoch=65
06/17/2022 14:39:32 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.7268128871299999 on epoch=65
06/17/2022 14:39:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.33 on epoch=65
06/17/2022 14:39:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=66
06/17/2022 14:39:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.44 on epoch=66
06/17/2022 14:39:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=66
06/17/2022 14:39:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=67
06/17/2022 14:39:51 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.7529184090871768 on epoch=67
06/17/2022 14:39:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=67
06/17/2022 14:39:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.33 on epoch=67
06/17/2022 14:39:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=68
06/17/2022 14:40:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=68
06/17/2022 14:40:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.27 on epoch=68
06/17/2022 14:40:10 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.7633921354286618 on epoch=68
06/17/2022 14:40:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=69
06/17/2022 14:40:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=69
06/17/2022 14:40:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=69
06/17/2022 14:40:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=69
06/17/2022 14:40:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=70
06/17/2022 14:40:29 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.7540831582633054 on epoch=70
06/17/2022 14:40:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=70
06/17/2022 14:40:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=70
06/17/2022 14:40:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=71
06/17/2022 14:40:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.35 on epoch=71
06/17/2022 14:40:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=71
06/17/2022 14:40:48 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.784045354081179 on epoch=71
06/17/2022 14:40:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7830579319457224 -> 0.784045354081179 on epoch=71, global_step=2300
06/17/2022 14:40:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
06/17/2022 14:40:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=72
06/17/2022 14:40:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=72
06/17/2022 14:40:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=73
06/17/2022 14:41:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=73
06/17/2022 14:41:08 - INFO - __main__ - Global step 2350 Train loss 0.28 Classification-F1 0.7555183809179864 on epoch=73
06/17/2022 14:41:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=73
06/17/2022 14:41:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=74
06/17/2022 14:41:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.39 on epoch=74
06/17/2022 14:41:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=74
06/17/2022 14:41:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=74
06/17/2022 14:41:27 - INFO - __main__ - Global step 2400 Train loss 0.26 Classification-F1 0.7787887294814854 on epoch=74
06/17/2022 14:41:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=75
06/17/2022 14:41:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=75
06/17/2022 14:41:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=75
06/17/2022 14:41:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=76
06/17/2022 14:41:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=76
06/17/2022 14:41:46 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.7590076788291075 on epoch=76
06/17/2022 14:41:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=76
06/17/2022 14:41:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
06/17/2022 14:41:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.25 on epoch=77
06/17/2022 14:41:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.28 on epoch=77
06/17/2022 14:41:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=78
06/17/2022 14:42:05 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.7299675067351203 on epoch=78
06/17/2022 14:42:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=78
06/17/2022 14:42:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=78
06/17/2022 14:42:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=79
06/17/2022 14:42:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=79
06/17/2022 14:42:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.33 on epoch=79
06/17/2022 14:42:24 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.7704807802764846 on epoch=79
06/17/2022 14:42:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
06/17/2022 14:42:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=80
06/17/2022 14:42:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=80
06/17/2022 14:42:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
06/17/2022 14:42:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
06/17/2022 14:42:43 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.7770137345594068 on epoch=81
06/17/2022 14:42:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.32 on epoch=81
06/17/2022 14:42:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
06/17/2022 14:42:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=82
06/17/2022 14:42:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=82
06/17/2022 14:42:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=82
06/17/2022 14:43:02 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.7651283011213051 on epoch=82
06/17/2022 14:43:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=83
06/17/2022 14:43:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.31 on epoch=83
06/17/2022 14:43:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.30 on epoch=83
06/17/2022 14:43:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=84
06/17/2022 14:43:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=84
06/17/2022 14:43:21 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.7457221532087518 on epoch=84
06/17/2022 14:43:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.29 on epoch=84
06/17/2022 14:43:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=84
06/17/2022 14:43:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=85
06/17/2022 14:43:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=85
06/17/2022 14:43:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=85
06/17/2022 14:43:40 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.7167709874721955 on epoch=85
06/17/2022 14:43:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=86
06/17/2022 14:43:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.25 on epoch=86
06/17/2022 14:43:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=86
06/17/2022 14:43:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=87
06/17/2022 14:43:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=87
06/17/2022 14:43:59 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.7735398819642585 on epoch=87
06/17/2022 14:44:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.25 on epoch=87
06/17/2022 14:44:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=88
06/17/2022 14:44:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.25 on epoch=88
06/17/2022 14:44:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.29 on epoch=88
06/17/2022 14:44:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
06/17/2022 14:44:18 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.7504613515364996 on epoch=89
06/17/2022 14:44:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=89
06/17/2022 14:44:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=89
06/17/2022 14:44:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=89
06/17/2022 14:44:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=90
06/17/2022 14:44:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
06/17/2022 14:44:37 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.7256904471731473 on epoch=90
06/17/2022 14:44:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=90
06/17/2022 14:44:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=91
06/17/2022 14:44:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.22 on epoch=91
06/17/2022 14:44:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=91
06/17/2022 14:44:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=92
06/17/2022 14:44:56 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.7553548846829008 on epoch=92
06/17/2022 14:44:58 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=92
06/17/2022 14:45:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
06/17/2022 14:45:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=93
06/17/2022 14:45:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=93
06/17/2022 14:45:08 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
06/17/2022 14:45:09 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:45:09 - INFO - __main__ - Printing 3 examples
06/17/2022 14:45:09 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 14:45:09 - INFO - __main__ - ['others']
06/17/2022 14:45:09 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 14:45:09 - INFO - __main__ - ['others']
06/17/2022 14:45:09 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 14:45:09 - INFO - __main__ - ['others']
06/17/2022 14:45:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:45:09 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:45:10 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:45:10 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:45:10 - INFO - __main__ - Printing 3 examples
06/17/2022 14:45:10 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 14:45:10 - INFO - __main__ - ['others']
06/17/2022 14:45:10 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 14:45:10 - INFO - __main__ - ['others']
06/17/2022 14:45:10 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 14:45:10 - INFO - __main__ - ['others']
06/17/2022 14:45:10 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:45:10 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:45:11 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:45:15 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.7737172791274768 on epoch=93
06/17/2022 14:45:15 - INFO - __main__ - save last model!
06/17/2022 14:45:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 14:45:15 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 14:45:15 - INFO - __main__ - Printing 3 examples
06/17/2022 14:45:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 14:45:15 - INFO - __main__ - ['others']
06/17/2022 14:45:15 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 14:45:15 - INFO - __main__ - ['others']
06/17/2022 14:45:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 14:45:15 - INFO - __main__ - ['others']
06/17/2022 14:45:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:45:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:45:22 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 14:45:27 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:45:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:45:28 - INFO - __main__ - Starting training!
06/17/2022 14:46:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_13_0.5_8_predictions.txt
06/17/2022 14:46:35 - INFO - __main__ - Classification-F1 on test data: 0.3681
06/17/2022 14:46:35 - INFO - __main__ - prefix=emo_128_13, lr=0.5, bsz=8, dev_performance=0.784045354081179, test_performance=0.36814004104005776
06/17/2022 14:46:35 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.4, bsz=8 ...
06/17/2022 14:46:36 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:46:36 - INFO - __main__ - Printing 3 examples
06/17/2022 14:46:36 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 14:46:36 - INFO - __main__ - ['others']
06/17/2022 14:46:36 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 14:46:36 - INFO - __main__ - ['others']
06/17/2022 14:46:36 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 14:46:36 - INFO - __main__ - ['others']
06/17/2022 14:46:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:46:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:46:37 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 14:46:37 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 14:46:37 - INFO - __main__ - Printing 3 examples
06/17/2022 14:46:37 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 14:46:37 - INFO - __main__ - ['others']
06/17/2022 14:46:37 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 14:46:37 - INFO - __main__ - ['others']
06/17/2022 14:46:37 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 14:46:37 - INFO - __main__ - ['others']
06/17/2022 14:46:37 - INFO - __main__ - Tokenizing Input ...
06/17/2022 14:46:37 - INFO - __main__ - Tokenizing Output ...
06/17/2022 14:46:37 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 14:46:56 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 14:46:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 14:46:57 - INFO - __main__ - Starting training!
06/17/2022 14:47:00 - INFO - __main__ - Step 10 Global step 10 Train loss 2.67 on epoch=0
06/17/2022 14:47:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.53 on epoch=0
06/17/2022 14:47:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.20 on epoch=0
06/17/2022 14:47:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=1
06/17/2022 14:47:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=1
06/17/2022 14:47:16 - INFO - __main__ - Global step 50 Train loss 1.49 Classification-F1 0.17015766941630994 on epoch=1
06/17/2022 14:47:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17015766941630994 on epoch=1, global_step=50
06/17/2022 14:47:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.05 on epoch=1
06/17/2022 14:47:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=2
06/17/2022 14:47:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=2
06/17/2022 14:47:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=2
06/17/2022 14:47:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=3
06/17/2022 14:47:35 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.3143243425144652 on epoch=3
06/17/2022 14:47:35 - INFO - __main__ - Saving model with best Classification-F1: 0.17015766941630994 -> 0.3143243425144652 on epoch=3, global_step=100
06/17/2022 14:47:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=3
06/17/2022 14:47:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=3
06/17/2022 14:47:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=4
06/17/2022 14:47:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=4
06/17/2022 14:47:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
06/17/2022 14:47:54 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.40167599557663164 on epoch=4
06/17/2022 14:47:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3143243425144652 -> 0.40167599557663164 on epoch=4, global_step=150
06/17/2022 14:47:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=4
06/17/2022 14:47:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=5
06/17/2022 14:48:01 - INFO - __main__ - Step 180 Global step 180 Train loss 1.10 on epoch=5
06/17/2022 14:48:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.83 on epoch=5
06/17/2022 14:48:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
06/17/2022 14:48:13 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.28166815750338253 on epoch=6
06/17/2022 14:48:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=6
06/17/2022 14:48:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.79 on epoch=6
06/17/2022 14:48:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.72 on epoch=7
06/17/2022 14:48:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=7
06/17/2022 14:48:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.76 on epoch=7
06/17/2022 14:48:32 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.5419022828251951 on epoch=7
06/17/2022 14:48:32 - INFO - __main__ - Saving model with best Classification-F1: 0.40167599557663164 -> 0.5419022828251951 on epoch=7, global_step=250
06/17/2022 14:48:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.71 on epoch=8
06/17/2022 14:48:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=8
06/17/2022 14:48:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=8
06/17/2022 14:48:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=9
06/17/2022 14:48:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=9
06/17/2022 14:48:51 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.5098243496581093 on epoch=9
06/17/2022 14:48:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.73 on epoch=9
06/17/2022 14:48:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=9
06/17/2022 14:48:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=10
06/17/2022 14:49:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.76 on epoch=10
06/17/2022 14:49:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=10
06/17/2022 14:49:10 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.5896307055241928 on epoch=10
06/17/2022 14:49:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5419022828251951 -> 0.5896307055241928 on epoch=10, global_step=350
06/17/2022 14:49:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=11
06/17/2022 14:49:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.69 on epoch=11
06/17/2022 14:49:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=11
06/17/2022 14:49:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=12
06/17/2022 14:49:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=12
06/17/2022 14:49:28 - INFO - __main__ - Global step 400 Train loss 0.68 Classification-F1 0.6258064560858785 on epoch=12
06/17/2022 14:49:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5896307055241928 -> 0.6258064560858785 on epoch=12, global_step=400
06/17/2022 14:49:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.73 on epoch=12
06/17/2022 14:49:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=13
06/17/2022 14:49:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=13
06/17/2022 14:49:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=13
06/17/2022 14:49:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=14
06/17/2022 14:49:47 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.604887646223566 on epoch=14
06/17/2022 14:49:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=14
06/17/2022 14:49:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=14
06/17/2022 14:49:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=14
06/17/2022 14:49:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=15
06/17/2022 14:49:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=15
06/17/2022 14:50:06 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.5927413239736047 on epoch=15
06/17/2022 14:50:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=15
06/17/2022 14:50:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=16
06/17/2022 14:50:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.70 on epoch=16
06/17/2022 14:50:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=16
06/17/2022 14:50:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=17
06/17/2022 14:50:25 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6776770839158591 on epoch=17
06/17/2022 14:50:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6258064560858785 -> 0.6776770839158591 on epoch=17, global_step=550
06/17/2022 14:50:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.68 on epoch=17
06/17/2022 14:50:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=17
06/17/2022 14:50:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=18
06/17/2022 14:50:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.72 on epoch=18
06/17/2022 14:50:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=18
06/17/2022 14:50:44 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.7354509985265639 on epoch=18
06/17/2022 14:50:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6776770839158591 -> 0.7354509985265639 on epoch=18, global_step=600
06/17/2022 14:50:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=19
06/17/2022 14:50:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.59 on epoch=19
06/17/2022 14:50:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=19
06/17/2022 14:50:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=19
06/17/2022 14:50:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=20
06/17/2022 14:51:03 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.7080898794637003 on epoch=20
06/17/2022 14:51:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=20
06/17/2022 14:51:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=20
06/17/2022 14:51:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=21
06/17/2022 14:51:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=21
06/17/2022 14:51:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=21
06/17/2022 14:51:21 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.7232049912468761 on epoch=21
06/17/2022 14:51:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=22
06/17/2022 14:51:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=22
06/17/2022 14:51:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=22
06/17/2022 14:51:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=23
06/17/2022 14:51:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=23
06/17/2022 14:51:40 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.7495976303394013 on epoch=23
06/17/2022 14:51:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7354509985265639 -> 0.7495976303394013 on epoch=23, global_step=750
06/17/2022 14:51:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=23
06/17/2022 14:51:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.57 on epoch=24
06/17/2022 14:51:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=24
06/17/2022 14:51:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.59 on epoch=24
06/17/2022 14:51:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=24
06/17/2022 14:51:59 - INFO - __main__ - Global step 800 Train loss 0.56 Classification-F1 0.7751462426618766 on epoch=24
06/17/2022 14:51:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7495976303394013 -> 0.7751462426618766 on epoch=24, global_step=800
06/17/2022 14:52:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=25
06/17/2022 14:52:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=25
06/17/2022 14:52:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=25
06/17/2022 14:52:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=26
06/17/2022 14:52:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=26
06/17/2022 14:52:18 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.6728672716429758 on epoch=26
06/17/2022 14:52:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=26
06/17/2022 14:52:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=27
06/17/2022 14:52:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.62 on epoch=27
06/17/2022 14:52:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.50 on epoch=27
06/17/2022 14:52:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=28
06/17/2022 14:52:37 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.7163550077219143 on epoch=28
06/17/2022 14:52:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=28
06/17/2022 14:52:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=28
06/17/2022 14:52:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=29
06/17/2022 14:52:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.61 on epoch=29
06/17/2022 14:52:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.53 on epoch=29
06/17/2022 14:52:56 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.741903929693555 on epoch=29
06/17/2022 14:52:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
06/17/2022 14:53:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=30
06/17/2022 14:53:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=30
06/17/2022 14:53:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.54 on epoch=30
06/17/2022 14:53:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=31
06/17/2022 14:53:15 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.7063562060290132 on epoch=31
06/17/2022 14:53:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.52 on epoch=31
06/17/2022 14:53:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=31
06/17/2022 14:53:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=32
06/17/2022 14:53:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=32
06/17/2022 14:53:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=32
06/17/2022 14:53:34 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.6836114254687913 on epoch=32
06/17/2022 14:53:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=33
06/17/2022 14:53:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=33
06/17/2022 14:53:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=33
06/17/2022 14:53:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=34
06/17/2022 14:53:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=34
06/17/2022 14:53:52 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.7147441336040149 on epoch=34
06/17/2022 14:53:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=34
06/17/2022 14:53:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=34
06/17/2022 14:54:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=35
06/17/2022 14:54:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=35
06/17/2022 14:54:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=35
06/17/2022 14:54:11 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.7063165079682314 on epoch=35
06/17/2022 14:54:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=36
06/17/2022 14:54:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=36
06/17/2022 14:54:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=36
06/17/2022 14:54:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=37
06/17/2022 14:54:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=37
06/17/2022 14:54:30 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.7522603585900526 on epoch=37
06/17/2022 14:54:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=37
06/17/2022 14:54:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=38
06/17/2022 14:54:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=38
06/17/2022 14:54:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=38
06/17/2022 14:54:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=39
06/17/2022 14:54:49 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.6638930031193591 on epoch=39
06/17/2022 14:54:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=39
06/17/2022 14:54:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=39
06/17/2022 14:54:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=39
06/17/2022 14:54:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=40
06/17/2022 14:55:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=40
06/17/2022 14:55:08 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.6644952643295577 on epoch=40
06/17/2022 14:55:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=40
06/17/2022 14:55:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
06/17/2022 14:55:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=41
06/17/2022 14:55:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=41
06/17/2022 14:55:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=42
06/17/2022 14:55:27 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.6701516352639497 on epoch=42
06/17/2022 14:55:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=42
06/17/2022 14:55:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=42
06/17/2022 14:55:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=43
06/17/2022 14:55:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=43
06/17/2022 14:55:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=43
06/17/2022 14:55:46 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.7526167427715956 on epoch=43
06/17/2022 14:55:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=44
06/17/2022 14:55:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=44
06/17/2022 14:55:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=44
06/17/2022 14:55:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.50 on epoch=44
06/17/2022 14:55:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=45
06/17/2022 14:56:05 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.7405331647919117 on epoch=45
06/17/2022 14:56:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
06/17/2022 14:56:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=45
06/17/2022 14:56:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=46
06/17/2022 14:56:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=46
06/17/2022 14:56:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
06/17/2022 14:56:24 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.7642007239741857 on epoch=46
06/17/2022 14:56:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=47
06/17/2022 14:56:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=47
06/17/2022 14:56:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=47
06/17/2022 14:56:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=48
06/17/2022 14:56:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=48
06/17/2022 14:56:42 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.7654284203174037 on epoch=48
06/17/2022 14:56:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=48
06/17/2022 14:56:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=49
06/17/2022 14:56:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=49
06/17/2022 14:56:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=49
06/17/2022 14:56:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=49
06/17/2022 14:57:01 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.7948464251045936 on epoch=49
06/17/2022 14:57:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7751462426618766 -> 0.7948464251045936 on epoch=49, global_step=1600
06/17/2022 14:57:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=50
06/17/2022 14:57:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=50
06/17/2022 14:57:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=50
06/17/2022 14:57:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=51
06/17/2022 14:57:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=51
06/17/2022 14:57:20 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.7431013645765039 on epoch=51
06/17/2022 14:57:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=51
06/17/2022 14:57:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=52
06/17/2022 14:57:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=52
06/17/2022 14:57:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=52
06/17/2022 14:57:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=53
06/17/2022 14:57:39 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7656279587858534 on epoch=53
06/17/2022 14:57:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=53
06/17/2022 14:57:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=53
06/17/2022 14:57:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=54
06/17/2022 14:57:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=54
06/17/2022 14:57:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=54
06/17/2022 14:57:58 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.7564597382322616 on epoch=54
06/17/2022 14:58:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=54
06/17/2022 14:58:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
06/17/2022 14:58:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=55
06/17/2022 14:58:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=55
06/17/2022 14:58:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=56
06/17/2022 14:58:17 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.7600035871222312 on epoch=56
06/17/2022 14:58:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=56
06/17/2022 14:58:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=56
06/17/2022 14:58:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=57
06/17/2022 14:58:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=57
06/17/2022 14:58:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=57
06/17/2022 14:58:36 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.7173322963364404 on epoch=57
06/17/2022 14:58:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=58
06/17/2022 14:58:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=58
06/17/2022 14:58:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=58
06/17/2022 14:58:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=59
06/17/2022 14:58:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=59
06/17/2022 14:58:55 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.7456186612198884 on epoch=59
06/17/2022 14:58:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=59
06/17/2022 14:59:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=59
06/17/2022 14:59:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=60
06/17/2022 14:59:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=60
06/17/2022 14:59:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
06/17/2022 14:59:14 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.7202493663005496 on epoch=60
06/17/2022 14:59:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=61
06/17/2022 14:59:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=61
06/17/2022 14:59:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
06/17/2022 14:59:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
06/17/2022 14:59:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=62
06/17/2022 14:59:33 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.768722058340461 on epoch=62
06/17/2022 14:59:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=62
06/17/2022 14:59:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=63
06/17/2022 14:59:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=63
06/17/2022 14:59:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=63
06/17/2022 14:59:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=64
06/17/2022 14:59:52 - INFO - __main__ - Global step 2050 Train loss 0.27 Classification-F1 0.7383405934184629 on epoch=64
06/17/2022 14:59:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=64
06/17/2022 14:59:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=64
06/17/2022 14:59:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=64
06/17/2022 15:00:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=65
06/17/2022 15:00:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=65
06/17/2022 15:00:10 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.7653070951157611 on epoch=65
06/17/2022 15:00:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=65
06/17/2022 15:00:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=66
06/17/2022 15:00:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=66
06/17/2022 15:00:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.31 on epoch=66
06/17/2022 15:00:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=67
06/17/2022 15:00:29 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.7160589375374102 on epoch=67
06/17/2022 15:00:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.31 on epoch=67
06/17/2022 15:00:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.31 on epoch=67
06/17/2022 15:00:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=68
06/17/2022 15:00:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.33 on epoch=68
06/17/2022 15:00:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=68
06/17/2022 15:00:48 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.7522794302279523 on epoch=68
06/17/2022 15:00:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=69
06/17/2022 15:00:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=69
06/17/2022 15:00:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=69
06/17/2022 15:00:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=69
06/17/2022 15:01:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=70
06/17/2022 15:01:07 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.7644117148394636 on epoch=70
06/17/2022 15:01:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=70
06/17/2022 15:01:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=70
06/17/2022 15:01:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
06/17/2022 15:01:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=71
06/17/2022 15:01:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=71
06/17/2022 15:01:26 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.7597057992410581 on epoch=71
06/17/2022 15:01:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=72
06/17/2022 15:01:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=72
06/17/2022 15:01:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.34 on epoch=72
06/17/2022 15:01:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=73
06/17/2022 15:01:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.26 on epoch=73
06/17/2022 15:01:45 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.7588799663984707 on epoch=73
06/17/2022 15:01:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
06/17/2022 15:01:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
06/17/2022 15:01:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=74
06/17/2022 15:01:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=74
06/17/2022 15:01:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=74
06/17/2022 15:02:04 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.7703419351817159 on epoch=74
06/17/2022 15:02:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=75
06/17/2022 15:02:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=75
06/17/2022 15:02:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=75
06/17/2022 15:02:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=76
06/17/2022 15:02:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=76
06/17/2022 15:02:23 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.7575017357463253 on epoch=76
06/17/2022 15:02:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=76
06/17/2022 15:02:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=77
06/17/2022 15:02:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=77
06/17/2022 15:02:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=77
06/17/2022 15:02:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=78
06/17/2022 15:02:42 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.7631715655098266 on epoch=78
06/17/2022 15:02:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=78
06/17/2022 15:02:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=78
06/17/2022 15:02:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=79
06/17/2022 15:02:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.29 on epoch=79
06/17/2022 15:02:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
06/17/2022 15:03:01 - INFO - __main__ - Global step 2550 Train loss 0.27 Classification-F1 0.736439582359472 on epoch=79
06/17/2022 15:03:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
06/17/2022 15:03:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=80
06/17/2022 15:03:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.30 on epoch=80
06/17/2022 15:03:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.29 on epoch=80
06/17/2022 15:03:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=81
06/17/2022 15:03:20 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.7702086065680466 on epoch=81
06/17/2022 15:03:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=81
06/17/2022 15:03:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 15:03:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=82
06/17/2022 15:03:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=82
06/17/2022 15:03:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=82
06/17/2022 15:03:39 - INFO - __main__ - Global step 2650 Train loss 0.23 Classification-F1 0.7451588812831698 on epoch=82
06/17/2022 15:03:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=83
06/17/2022 15:03:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=83
06/17/2022 15:03:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=83
06/17/2022 15:03:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=84
06/17/2022 15:03:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=84
06/17/2022 15:03:58 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.744669622513632 on epoch=84
06/17/2022 15:04:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=84
06/17/2022 15:04:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=84
06/17/2022 15:04:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
06/17/2022 15:04:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=85
06/17/2022 15:04:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=85
06/17/2022 15:04:17 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.7481413519752955 on epoch=85
06/17/2022 15:04:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=86
06/17/2022 15:04:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.29 on epoch=86
06/17/2022 15:04:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=86
06/17/2022 15:04:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.27 on epoch=87
06/17/2022 15:04:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=87
06/17/2022 15:04:36 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.790885574822911 on epoch=87
06/17/2022 15:04:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=87
06/17/2022 15:04:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.18 on epoch=88
06/17/2022 15:04:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=88
06/17/2022 15:04:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.26 on epoch=88
06/17/2022 15:04:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
06/17/2022 15:04:55 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.7480494611100292 on epoch=89
06/17/2022 15:04:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=89
06/17/2022 15:05:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=89
06/17/2022 15:05:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
06/17/2022 15:05:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=90
06/17/2022 15:05:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=90
06/17/2022 15:05:14 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.7749456406044054 on epoch=90
06/17/2022 15:05:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=90
06/17/2022 15:05:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=91
06/17/2022 15:05:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=91
06/17/2022 15:05:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.23 on epoch=91
06/17/2022 15:05:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=92
06/17/2022 15:05:33 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.7629552972265552 on epoch=92
06/17/2022 15:05:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.26 on epoch=92
06/17/2022 15:05:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=92
06/17/2022 15:05:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
06/17/2022 15:05:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=93
06/17/2022 15:05:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.27 on epoch=93
06/17/2022 15:05:46 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:05:46 - INFO - __main__ - Printing 3 examples
06/17/2022 15:05:46 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 15:05:46 - INFO - __main__ - ['others']
06/17/2022 15:05:46 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 15:05:46 - INFO - __main__ - ['others']
06/17/2022 15:05:46 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 15:05:46 - INFO - __main__ - ['others']
06/17/2022 15:05:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:05:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:05:47 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:05:47 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:05:47 - INFO - __main__ - Printing 3 examples
06/17/2022 15:05:47 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 15:05:47 - INFO - __main__ - ['others']
06/17/2022 15:05:47 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 15:05:47 - INFO - __main__ - ['others']
06/17/2022 15:05:47 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 15:05:47 - INFO - __main__ - ['others']
06/17/2022 15:05:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:05:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:05:48 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:05:52 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.75805619080824 on epoch=93
06/17/2022 15:05:52 - INFO - __main__ - save last model!
06/17/2022 15:05:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 15:05:52 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 15:05:52 - INFO - __main__ - Printing 3 examples
06/17/2022 15:05:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 15:05:52 - INFO - __main__ - ['others']
06/17/2022 15:05:52 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 15:05:52 - INFO - __main__ - ['others']
06/17/2022 15:05:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 15:05:52 - INFO - __main__ - ['others']
06/17/2022 15:05:52 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:05:54 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:06:00 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 15:06:06 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:06:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:06:07 - INFO - __main__ - Starting training!
06/17/2022 15:07:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_13_0.4_8_predictions.txt
06/17/2022 15:07:11 - INFO - __main__ - Classification-F1 on test data: 0.4377
06/17/2022 15:07:12 - INFO - __main__ - prefix=emo_128_13, lr=0.4, bsz=8, dev_performance=0.7948464251045936, test_performance=0.4377329166659739
06/17/2022 15:07:12 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.3, bsz=8 ...
06/17/2022 15:07:13 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:07:13 - INFO - __main__ - Printing 3 examples
06/17/2022 15:07:13 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 15:07:13 - INFO - __main__ - ['others']
06/17/2022 15:07:13 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 15:07:13 - INFO - __main__ - ['others']
06/17/2022 15:07:13 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 15:07:13 - INFO - __main__ - ['others']
06/17/2022 15:07:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:07:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:07:14 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:07:14 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:07:14 - INFO - __main__ - Printing 3 examples
06/17/2022 15:07:14 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 15:07:14 - INFO - __main__ - ['others']
06/17/2022 15:07:14 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 15:07:14 - INFO - __main__ - ['others']
06/17/2022 15:07:14 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 15:07:14 - INFO - __main__ - ['others']
06/17/2022 15:07:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:07:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:07:14 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:07:33 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:07:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:07:34 - INFO - __main__ - Starting training!
06/17/2022 15:07:37 - INFO - __main__ - Step 10 Global step 10 Train loss 2.86 on epoch=0
06/17/2022 15:07:39 - INFO - __main__ - Step 20 Global step 20 Train loss 1.69 on epoch=0
06/17/2022 15:07:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=0
06/17/2022 15:07:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=1
06/17/2022 15:07:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=1
06/17/2022 15:07:54 - INFO - __main__ - Global step 50 Train loss 1.56 Classification-F1 0.1161132970533704 on epoch=1
06/17/2022 15:07:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1161132970533704 on epoch=1, global_step=50
06/17/2022 15:07:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=1
06/17/2022 15:07:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=2
06/17/2022 15:08:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=2
06/17/2022 15:08:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.89 on epoch=2
06/17/2022 15:08:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=3
06/17/2022 15:08:13 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.30685201112353433 on epoch=3
06/17/2022 15:08:13 - INFO - __main__ - Saving model with best Classification-F1: 0.1161132970533704 -> 0.30685201112353433 on epoch=3, global_step=100
06/17/2022 15:08:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=3
06/17/2022 15:08:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=3
06/17/2022 15:08:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=4
06/17/2022 15:08:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=4
06/17/2022 15:08:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=4
06/17/2022 15:08:32 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.37258053523337314 on epoch=4
06/17/2022 15:08:32 - INFO - __main__ - Saving model with best Classification-F1: 0.30685201112353433 -> 0.37258053523337314 on epoch=4, global_step=150
06/17/2022 15:08:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.79 on epoch=4
06/17/2022 15:08:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=5
06/17/2022 15:08:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=5
06/17/2022 15:08:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=5
06/17/2022 15:08:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=6
06/17/2022 15:08:51 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.295395553219755 on epoch=6
06/17/2022 15:08:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=6
06/17/2022 15:08:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=6
06/17/2022 15:08:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.72 on epoch=7
06/17/2022 15:09:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=7
06/17/2022 15:09:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=7
06/17/2022 15:09:10 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.4221384880431175 on epoch=7
06/17/2022 15:09:11 - INFO - __main__ - Saving model with best Classification-F1: 0.37258053523337314 -> 0.4221384880431175 on epoch=7, global_step=250
06/17/2022 15:09:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=8
06/17/2022 15:09:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=8
06/17/2022 15:09:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=8
06/17/2022 15:09:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=9
06/17/2022 15:09:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=9
06/17/2022 15:09:30 - INFO - __main__ - Global step 300 Train loss 0.77 Classification-F1 0.5210605108424547 on epoch=9
06/17/2022 15:09:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4221384880431175 -> 0.5210605108424547 on epoch=9, global_step=300
06/17/2022 15:09:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.67 on epoch=9
06/17/2022 15:09:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=9
06/17/2022 15:09:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=10
06/17/2022 15:09:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.64 on epoch=10
06/17/2022 15:09:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.77 on epoch=10
06/17/2022 15:09:49 - INFO - __main__ - Global step 350 Train loss 0.68 Classification-F1 0.656386764473366 on epoch=10
06/17/2022 15:09:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5210605108424547 -> 0.656386764473366 on epoch=10, global_step=350
06/17/2022 15:09:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=11
06/17/2022 15:09:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=11
06/17/2022 15:09:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=11
06/17/2022 15:09:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=12
06/17/2022 15:10:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.72 on epoch=12
06/17/2022 15:10:08 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.6332340014267843 on epoch=12
06/17/2022 15:10:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=12
06/17/2022 15:10:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=13
06/17/2022 15:10:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=13
06/17/2022 15:10:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=13
06/17/2022 15:10:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=14
06/17/2022 15:10:27 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.5751509305306101 on epoch=14
06/17/2022 15:10:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=14
06/17/2022 15:10:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=14
06/17/2022 15:10:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=14
06/17/2022 15:10:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=15
06/17/2022 15:10:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=15
06/17/2022 15:10:47 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.628506159630235 on epoch=15
06/17/2022 15:10:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=15
06/17/2022 15:10:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=16
06/17/2022 15:10:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=16
06/17/2022 15:10:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.55 on epoch=16
06/17/2022 15:11:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=17
06/17/2022 15:11:07 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.6896357823792278 on epoch=17
06/17/2022 15:11:07 - INFO - __main__ - Saving model with best Classification-F1: 0.656386764473366 -> 0.6896357823792278 on epoch=17, global_step=550
06/17/2022 15:11:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=17
06/17/2022 15:11:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=17
06/17/2022 15:11:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=18
06/17/2022 15:11:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=18
06/17/2022 15:11:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=18
06/17/2022 15:11:26 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.717703702641466 on epoch=18
06/17/2022 15:11:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6896357823792278 -> 0.717703702641466 on epoch=18, global_step=600
06/17/2022 15:11:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=19
06/17/2022 15:11:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=19
06/17/2022 15:11:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.57 on epoch=19
06/17/2022 15:11:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=19
06/17/2022 15:11:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=20
06/17/2022 15:11:46 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.67719971704649 on epoch=20
06/17/2022 15:11:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=20
06/17/2022 15:11:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=20
06/17/2022 15:11:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=21
06/17/2022 15:11:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.68 on epoch=21
06/17/2022 15:11:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=21
06/17/2022 15:12:05 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.7192246797636684 on epoch=21
06/17/2022 15:12:05 - INFO - __main__ - Saving model with best Classification-F1: 0.717703702641466 -> 0.7192246797636684 on epoch=21, global_step=700
06/17/2022 15:12:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=22
06/17/2022 15:12:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=22
06/17/2022 15:12:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=22
06/17/2022 15:12:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=23
06/17/2022 15:12:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=23
06/17/2022 15:12:25 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.7496549086050119 on epoch=23
06/17/2022 15:12:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7192246797636684 -> 0.7496549086050119 on epoch=23, global_step=750
06/17/2022 15:12:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=23
06/17/2022 15:12:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=24
06/17/2022 15:12:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=24
06/17/2022 15:12:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=24
06/17/2022 15:12:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=24
06/17/2022 15:12:45 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.7616549463911619 on epoch=24
06/17/2022 15:12:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7496549086050119 -> 0.7616549463911619 on epoch=24, global_step=800
06/17/2022 15:12:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=25
06/17/2022 15:12:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=25
06/17/2022 15:12:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=25
06/17/2022 15:12:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=26
06/17/2022 15:12:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=26
06/17/2022 15:13:04 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.6915068817520896 on epoch=26
06/17/2022 15:13:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=26
06/17/2022 15:13:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=27
06/17/2022 15:13:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=27
06/17/2022 15:13:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=27
06/17/2022 15:13:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=28
06/17/2022 15:13:24 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.6517586189568859 on epoch=28
06/17/2022 15:13:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.56 on epoch=28
06/17/2022 15:13:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.58 on epoch=28
06/17/2022 15:13:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=29
06/17/2022 15:13:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=29
06/17/2022 15:13:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=29
06/17/2022 15:13:43 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.7499994000266135 on epoch=29
06/17/2022 15:13:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=29
06/17/2022 15:13:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=30
06/17/2022 15:13:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=30
06/17/2022 15:13:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=30
06/17/2022 15:13:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=31
06/17/2022 15:14:03 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.7308026910301255 on epoch=31
06/17/2022 15:14:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=31
06/17/2022 15:14:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=31
06/17/2022 15:14:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=32
06/17/2022 15:14:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=32
06/17/2022 15:14:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=32
06/17/2022 15:14:23 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.6637822704287295 on epoch=32
06/17/2022 15:14:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=33
06/17/2022 15:14:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.62 on epoch=33
06/17/2022 15:14:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=33
06/17/2022 15:14:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=34
06/17/2022 15:14:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=34
06/17/2022 15:14:42 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.7331228615755861 on epoch=34
06/17/2022 15:14:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=34
06/17/2022 15:14:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=34
06/17/2022 15:14:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=35
06/17/2022 15:14:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=35
06/17/2022 15:14:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=35
06/17/2022 15:15:02 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.7551499106692886 on epoch=35
06/17/2022 15:15:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
06/17/2022 15:15:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=36
06/17/2022 15:15:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=36
06/17/2022 15:15:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=37
06/17/2022 15:15:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.62 on epoch=37
06/17/2022 15:15:21 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.7537221595766365 on epoch=37
06/17/2022 15:15:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.75 on epoch=37
06/17/2022 15:15:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=38
06/17/2022 15:15:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=38
06/17/2022 15:15:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=38
06/17/2022 15:15:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=39
06/17/2022 15:15:41 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.743961958311172 on epoch=39
06/17/2022 15:15:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=39
06/17/2022 15:15:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=39
06/17/2022 15:15:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=39
06/17/2022 15:15:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=40
06/17/2022 15:15:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.51 on epoch=40
06/17/2022 15:16:01 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.7215372262098332 on epoch=40
06/17/2022 15:16:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=40
06/17/2022 15:16:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
06/17/2022 15:16:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=41
06/17/2022 15:16:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=41
06/17/2022 15:16:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=42
06/17/2022 15:16:20 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.7467908942842103 on epoch=42
06/17/2022 15:16:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=42
06/17/2022 15:16:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=42
06/17/2022 15:16:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=43
06/17/2022 15:16:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=43
06/17/2022 15:16:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=43
06/17/2022 15:16:40 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.7477388154488335 on epoch=43
06/17/2022 15:16:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=44
06/17/2022 15:16:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=44
06/17/2022 15:16:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=44
06/17/2022 15:16:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=44
06/17/2022 15:16:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=45
06/17/2022 15:17:00 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.7740039259181649 on epoch=45
06/17/2022 15:17:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7616549463911619 -> 0.7740039259181649 on epoch=45, global_step=1450
06/17/2022 15:17:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=45
06/17/2022 15:17:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=45
06/17/2022 15:17:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
06/17/2022 15:17:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=46
06/17/2022 15:17:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
06/17/2022 15:17:19 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.7477692969692813 on epoch=46
06/17/2022 15:17:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=47
06/17/2022 15:17:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=47
06/17/2022 15:17:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=47
06/17/2022 15:17:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
06/17/2022 15:17:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=48
06/17/2022 15:17:38 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.7777321283569208 on epoch=48
06/17/2022 15:17:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7740039259181649 -> 0.7777321283569208 on epoch=48, global_step=1550
06/17/2022 15:17:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=48
06/17/2022 15:17:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=49
06/17/2022 15:17:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=49
06/17/2022 15:17:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=49
06/17/2022 15:17:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=49
06/17/2022 15:17:58 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.7508543290893358 on epoch=49
06/17/2022 15:18:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=50
06/17/2022 15:18:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=50
06/17/2022 15:18:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=50
06/17/2022 15:18:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=51
06/17/2022 15:18:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=51
06/17/2022 15:18:17 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.7544880020092675 on epoch=51
06/17/2022 15:18:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=51
06/17/2022 15:18:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=52
06/17/2022 15:18:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=52
06/17/2022 15:18:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=52
06/17/2022 15:18:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=53
06/17/2022 15:18:36 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.738508973164607 on epoch=53
06/17/2022 15:18:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=53
06/17/2022 15:18:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=53
06/17/2022 15:18:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=54
06/17/2022 15:18:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=54
06/17/2022 15:18:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=54
06/17/2022 15:18:55 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.7589392914341355 on epoch=54
06/17/2022 15:18:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=54
06/17/2022 15:19:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=55
06/17/2022 15:19:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=55
06/17/2022 15:19:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=55
06/17/2022 15:19:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=56
06/17/2022 15:19:15 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.7856230075037565 on epoch=56
06/17/2022 15:19:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7777321283569208 -> 0.7856230075037565 on epoch=56, global_step=1800
06/17/2022 15:19:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=56
06/17/2022 15:19:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=56
06/17/2022 15:19:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=57
06/17/2022 15:19:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=57
06/17/2022 15:19:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=57
06/17/2022 15:19:34 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.7731274576527525 on epoch=57
06/17/2022 15:19:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=58
06/17/2022 15:19:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=58
06/17/2022 15:19:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=58
06/17/2022 15:19:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=59
06/17/2022 15:19:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=59
06/17/2022 15:19:53 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.7453739607541803 on epoch=59
06/17/2022 15:19:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=59
06/17/2022 15:19:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=59
06/17/2022 15:20:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=60
06/17/2022 15:20:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=60
06/17/2022 15:20:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=60
06/17/2022 15:20:13 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.748869252033777 on epoch=60
06/17/2022 15:20:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=61
06/17/2022 15:20:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=61
06/17/2022 15:20:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=61
06/17/2022 15:20:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
06/17/2022 15:20:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=62
06/17/2022 15:20:32 - INFO - __main__ - Global step 2000 Train loss 0.31 Classification-F1 0.7743177838493898 on epoch=62
06/17/2022 15:20:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=62
06/17/2022 15:20:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=63
06/17/2022 15:20:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=63
06/17/2022 15:20:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=63
06/17/2022 15:20:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=64
06/17/2022 15:20:52 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.7295330852558491 on epoch=64
06/17/2022 15:20:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=64
06/17/2022 15:20:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=64
06/17/2022 15:20:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=64
06/17/2022 15:21:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
06/17/2022 15:21:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=65
06/17/2022 15:21:11 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.7633846398237057 on epoch=65
06/17/2022 15:21:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=65
06/17/2022 15:21:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=66
06/17/2022 15:21:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=66
06/17/2022 15:21:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=66
06/17/2022 15:21:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=67
06/17/2022 15:21:30 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.7491797673249376 on epoch=67
06/17/2022 15:21:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.35 on epoch=67
06/17/2022 15:21:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=67
06/17/2022 15:21:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=68
06/17/2022 15:21:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.35 on epoch=68
06/17/2022 15:21:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=68
06/17/2022 15:21:50 - INFO - __main__ - Global step 2200 Train loss 0.33 Classification-F1 0.75950721086136 on epoch=68
06/17/2022 15:21:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=69
06/17/2022 15:21:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=69
06/17/2022 15:21:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=69
06/17/2022 15:22:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=69
06/17/2022 15:22:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.33 on epoch=70
06/17/2022 15:22:09 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.778000599503089 on epoch=70
06/17/2022 15:22:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=70
06/17/2022 15:22:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=70
06/17/2022 15:22:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=71
06/17/2022 15:22:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=71
06/17/2022 15:22:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.38 on epoch=71
06/17/2022 15:22:28 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.784782831188799 on epoch=71
06/17/2022 15:22:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
06/17/2022 15:22:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=72
06/17/2022 15:22:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=72
06/17/2022 15:22:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=73
06/17/2022 15:22:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.32 on epoch=73
06/17/2022 15:22:48 - INFO - __main__ - Global step 2350 Train loss 0.29 Classification-F1 0.8071823418438286 on epoch=73
06/17/2022 15:22:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7856230075037565 -> 0.8071823418438286 on epoch=73, global_step=2350
06/17/2022 15:22:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=73
06/17/2022 15:22:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
06/17/2022 15:22:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=74
06/17/2022 15:22:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=74
06/17/2022 15:23:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
06/17/2022 15:23:07 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.8052718497768112 on epoch=74
06/17/2022 15:23:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=75
06/17/2022 15:23:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=75
06/17/2022 15:23:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=75
06/17/2022 15:23:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=76
06/17/2022 15:23:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=76
06/17/2022 15:23:27 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.7614651686012581 on epoch=76
06/17/2022 15:23:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=76
06/17/2022 15:23:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=77
06/17/2022 15:23:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.28 on epoch=77
06/17/2022 15:23:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=77
06/17/2022 15:23:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
06/17/2022 15:23:46 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.785291392492557 on epoch=78
06/17/2022 15:23:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.30 on epoch=78
06/17/2022 15:23:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=78
06/17/2022 15:23:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=79
06/17/2022 15:23:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=79
06/17/2022 15:23:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
06/17/2022 15:24:05 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.789031524335509 on epoch=79
06/17/2022 15:24:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=79
06/17/2022 15:24:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=80
06/17/2022 15:24:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=80
06/17/2022 15:24:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.30 on epoch=80
06/17/2022 15:24:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=81
06/17/2022 15:24:25 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.7479814471344233 on epoch=81
06/17/2022 15:24:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.33 on epoch=81
06/17/2022 15:24:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.30 on epoch=81
06/17/2022 15:24:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=82
06/17/2022 15:24:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.34 on epoch=82
06/17/2022 15:24:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=82
06/17/2022 15:24:44 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.7578576564754069 on epoch=82
06/17/2022 15:24:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=83
06/17/2022 15:24:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=83
06/17/2022 15:24:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.32 on epoch=83
06/17/2022 15:24:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
06/17/2022 15:24:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=84
06/17/2022 15:25:03 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.7770972816034067 on epoch=84
06/17/2022 15:25:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=84
06/17/2022 15:25:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 15:25:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=85
06/17/2022 15:25:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=85
06/17/2022 15:25:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=85
06/17/2022 15:25:23 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.7616091070623257 on epoch=85
06/17/2022 15:25:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
06/17/2022 15:25:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.29 on epoch=86
06/17/2022 15:25:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
06/17/2022 15:25:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=87
06/17/2022 15:25:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.24 on epoch=87
06/17/2022 15:25:42 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.7929460408997729 on epoch=87
06/17/2022 15:25:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=87
06/17/2022 15:25:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=88
06/17/2022 15:25:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=88
06/17/2022 15:25:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.33 on epoch=88
06/17/2022 15:25:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=89
06/17/2022 15:26:02 - INFO - __main__ - Global step 2850 Train loss 0.28 Classification-F1 0.7680625675355115 on epoch=89
06/17/2022 15:26:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=89
06/17/2022 15:26:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.28 on epoch=89
06/17/2022 15:26:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=89
06/17/2022 15:26:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.27 on epoch=90
06/17/2022 15:26:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.36 on epoch=90
06/17/2022 15:26:21 - INFO - __main__ - Global step 2900 Train loss 0.28 Classification-F1 0.7346185899266038 on epoch=90
06/17/2022 15:26:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.32 on epoch=90
06/17/2022 15:26:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=91
06/17/2022 15:26:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=91
06/17/2022 15:26:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=91
06/17/2022 15:26:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=92
06/17/2022 15:26:40 - INFO - __main__ - Global step 2950 Train loss 0.25 Classification-F1 0.7607768110833201 on epoch=92
06/17/2022 15:26:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.37 on epoch=92
06/17/2022 15:26:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=92
06/17/2022 15:26:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=93
06/17/2022 15:26:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.26 on epoch=93
06/17/2022 15:26:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=93
06/17/2022 15:26:54 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:26:54 - INFO - __main__ - Printing 3 examples
06/17/2022 15:26:54 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 15:26:54 - INFO - __main__ - ['others']
06/17/2022 15:26:54 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 15:26:54 - INFO - __main__ - ['others']
06/17/2022 15:26:54 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 15:26:54 - INFO - __main__ - ['others']
06/17/2022 15:26:54 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:26:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:26:55 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:26:55 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:26:55 - INFO - __main__ - Printing 3 examples
06/17/2022 15:26:55 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 15:26:55 - INFO - __main__ - ['others']
06/17/2022 15:26:55 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 15:26:55 - INFO - __main__ - ['others']
06/17/2022 15:26:55 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 15:26:55 - INFO - __main__ - ['others']
06/17/2022 15:26:55 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:26:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:26:56 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:27:00 - INFO - __main__ - Global step 3000 Train loss 0.28 Classification-F1 0.7712556062385347 on epoch=93
06/17/2022 15:27:00 - INFO - __main__ - save last model!
06/17/2022 15:27:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 15:27:00 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 15:27:00 - INFO - __main__ - Printing 3 examples
06/17/2022 15:27:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 15:27:00 - INFO - __main__ - ['others']
06/17/2022 15:27:00 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 15:27:00 - INFO - __main__ - ['others']
06/17/2022 15:27:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 15:27:00 - INFO - __main__ - ['others']
06/17/2022 15:27:00 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:27:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:27:07 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 15:27:14 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:27:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:27:15 - INFO - __main__ - Starting training!
06/17/2022 15:28:19 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_13_0.3_8_predictions.txt
06/17/2022 15:28:19 - INFO - __main__ - Classification-F1 on test data: 0.4413
06/17/2022 15:28:20 - INFO - __main__ - prefix=emo_128_13, lr=0.3, bsz=8, dev_performance=0.8071823418438286, test_performance=0.4413150952113698
06/17/2022 15:28:20 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.2, bsz=8 ...
06/17/2022 15:28:21 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:28:21 - INFO - __main__ - Printing 3 examples
06/17/2022 15:28:21 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/17/2022 15:28:21 - INFO - __main__ - ['others']
06/17/2022 15:28:21 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/17/2022 15:28:21 - INFO - __main__ - ['others']
06/17/2022 15:28:21 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/17/2022 15:28:21 - INFO - __main__ - ['others']
06/17/2022 15:28:21 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:28:21 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:28:21 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:28:21 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:28:21 - INFO - __main__ - Printing 3 examples
06/17/2022 15:28:21 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
06/17/2022 15:28:21 - INFO - __main__ - ['others']
06/17/2022 15:28:21 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
06/17/2022 15:28:22 - INFO - __main__ - ['others']
06/17/2022 15:28:22 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
06/17/2022 15:28:22 - INFO - __main__ - ['others']
06/17/2022 15:28:22 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:28:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:28:22 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:28:41 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:28:42 - INFO - __main__ - Starting training!
06/17/2022 15:28:45 - INFO - __main__ - Step 10 Global step 10 Train loss 3.14 on epoch=0
06/17/2022 15:28:48 - INFO - __main__ - Step 20 Global step 20 Train loss 1.93 on epoch=0
06/17/2022 15:28:50 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=0
06/17/2022 15:28:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=1
06/17/2022 15:28:55 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=1
06/17/2022 15:29:02 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.1 on epoch=1
06/17/2022 15:29:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 15:29:05 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=1
06/17/2022 15:29:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=2
06/17/2022 15:29:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=2
06/17/2022 15:29:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=2
06/17/2022 15:29:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=3
06/17/2022 15:29:22 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.222142605255692 on epoch=3
06/17/2022 15:29:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.222142605255692 on epoch=3, global_step=100
06/17/2022 15:29:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=3
06/17/2022 15:29:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=3
06/17/2022 15:29:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=4
06/17/2022 15:29:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=4
06/17/2022 15:29:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=4
06/17/2022 15:29:41 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.28316216239145836 on epoch=4
06/17/2022 15:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.222142605255692 -> 0.28316216239145836 on epoch=4, global_step=150
06/17/2022 15:29:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=4
06/17/2022 15:29:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=5
06/17/2022 15:29:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=5
06/17/2022 15:29:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=5
06/17/2022 15:29:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=6
06/17/2022 15:30:01 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.29275442066218643 on epoch=6
06/17/2022 15:30:01 - INFO - __main__ - Saving model with best Classification-F1: 0.28316216239145836 -> 0.29275442066218643 on epoch=6, global_step=200
06/17/2022 15:30:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=6
06/17/2022 15:30:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=6
06/17/2022 15:30:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=7
06/17/2022 15:30:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=7
06/17/2022 15:30:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.79 on epoch=7
06/17/2022 15:30:21 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.40971639893152456 on epoch=7
06/17/2022 15:30:21 - INFO - __main__ - Saving model with best Classification-F1: 0.29275442066218643 -> 0.40971639893152456 on epoch=7, global_step=250
06/17/2022 15:30:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=8
06/17/2022 15:30:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=8
06/17/2022 15:30:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=8
06/17/2022 15:30:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=9
06/17/2022 15:30:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=9
06/17/2022 15:30:40 - INFO - __main__ - Global step 300 Train loss 0.73 Classification-F1 0.39465260912294675 on epoch=9
06/17/2022 15:30:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=9
06/17/2022 15:30:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=9
06/17/2022 15:30:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=10
06/17/2022 15:30:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.76 on epoch=10
06/17/2022 15:30:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=10
06/17/2022 15:31:00 - INFO - __main__ - Global step 350 Train loss 0.74 Classification-F1 0.5345802574509806 on epoch=10
06/17/2022 15:31:00 - INFO - __main__ - Saving model with best Classification-F1: 0.40971639893152456 -> 0.5345802574509806 on epoch=10, global_step=350
06/17/2022 15:31:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=11
06/17/2022 15:31:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=11
06/17/2022 15:31:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=11
06/17/2022 15:31:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=12
06/17/2022 15:31:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=12
06/17/2022 15:31:19 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.5773523811008996 on epoch=12
06/17/2022 15:31:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5345802574509806 -> 0.5773523811008996 on epoch=12, global_step=400
06/17/2022 15:31:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=12
06/17/2022 15:31:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=13
06/17/2022 15:31:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=13
06/17/2022 15:31:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=13
06/17/2022 15:31:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=14
06/17/2022 15:31:39 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.5760133842304855 on epoch=14
06/17/2022 15:31:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=14
06/17/2022 15:31:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=14
06/17/2022 15:31:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=14
06/17/2022 15:31:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=15
06/17/2022 15:31:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=15
06/17/2022 15:31:58 - INFO - __main__ - Global step 500 Train loss 0.66 Classification-F1 0.5934615243970986 on epoch=15
06/17/2022 15:31:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5773523811008996 -> 0.5934615243970986 on epoch=15, global_step=500
06/17/2022 15:32:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.66 on epoch=15
06/17/2022 15:32:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=16
06/17/2022 15:32:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=16
06/17/2022 15:32:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=16
06/17/2022 15:32:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=17
06/17/2022 15:32:18 - INFO - __main__ - Global step 550 Train loss 0.64 Classification-F1 0.6344302025151216 on epoch=17
06/17/2022 15:32:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5934615243970986 -> 0.6344302025151216 on epoch=17, global_step=550
06/17/2022 15:32:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.70 on epoch=17
06/17/2022 15:32:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.66 on epoch=17
06/17/2022 15:32:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=18
06/17/2022 15:32:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.72 on epoch=18
06/17/2022 15:32:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=18
06/17/2022 15:32:38 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.6811809289127837 on epoch=18
06/17/2022 15:32:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6344302025151216 -> 0.6811809289127837 on epoch=18, global_step=600
06/17/2022 15:32:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=19
06/17/2022 15:32:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.70 on epoch=19
06/17/2022 15:32:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=19
06/17/2022 15:32:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=19
06/17/2022 15:32:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=20
06/17/2022 15:32:57 - INFO - __main__ - Global step 650 Train loss 0.60 Classification-F1 0.679817530240314 on epoch=20
06/17/2022 15:33:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.61 on epoch=20
06/17/2022 15:33:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=20
06/17/2022 15:33:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=21
06/17/2022 15:33:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=21
06/17/2022 15:33:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=21
06/17/2022 15:33:17 - INFO - __main__ - Global step 700 Train loss 0.58 Classification-F1 0.6940166164226242 on epoch=21
06/17/2022 15:33:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6811809289127837 -> 0.6940166164226242 on epoch=21, global_step=700
06/17/2022 15:33:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=22
06/17/2022 15:33:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.59 on epoch=22
06/17/2022 15:33:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.64 on epoch=22
06/17/2022 15:33:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=23
06/17/2022 15:33:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.61 on epoch=23
06/17/2022 15:33:36 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.72180022071831 on epoch=23
06/17/2022 15:33:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6940166164226242 -> 0.72180022071831 on epoch=23, global_step=750
06/17/2022 15:33:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=23
06/17/2022 15:33:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=24
06/17/2022 15:33:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=24
06/17/2022 15:33:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=24
06/17/2022 15:33:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=24
06/17/2022 15:33:56 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.6651788047514184 on epoch=24
06/17/2022 15:33:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=25
06/17/2022 15:34:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=25
06/17/2022 15:34:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=25
06/17/2022 15:34:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=26
06/17/2022 15:34:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.66 on epoch=26
06/17/2022 15:34:16 - INFO - __main__ - Global step 850 Train loss 0.54 Classification-F1 0.6426265320492165 on epoch=26
06/17/2022 15:34:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=26
06/17/2022 15:34:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=27
06/17/2022 15:34:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=27
06/17/2022 15:34:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=27
06/17/2022 15:34:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=28
06/17/2022 15:34:35 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.6670760670265826 on epoch=28
06/17/2022 15:34:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.63 on epoch=28
06/17/2022 15:34:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=28
06/17/2022 15:34:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=29
06/17/2022 15:34:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=29
06/17/2022 15:34:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=29
06/17/2022 15:34:55 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.6887411147157588 on epoch=29
06/17/2022 15:34:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=29
06/17/2022 15:35:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=30
06/17/2022 15:35:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=30
06/17/2022 15:35:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=30
06/17/2022 15:35:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=31
06/17/2022 15:35:14 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.6970271553541516 on epoch=31
06/17/2022 15:35:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=31
06/17/2022 15:35:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=31
06/17/2022 15:35:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=32
06/17/2022 15:35:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.54 on epoch=32
06/17/2022 15:35:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=32
06/17/2022 15:35:34 - INFO - __main__ - Global step 1050 Train loss 0.52 Classification-F1 0.6629633748021984 on epoch=32
06/17/2022 15:35:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
06/17/2022 15:35:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.63 on epoch=33
06/17/2022 15:35:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=33
06/17/2022 15:35:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=34
06/17/2022 15:35:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=34
06/17/2022 15:35:54 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.6861950869757456 on epoch=34
06/17/2022 15:35:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.59 on epoch=34
06/17/2022 15:35:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=34
06/17/2022 15:36:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=35
06/17/2022 15:36:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=35
06/17/2022 15:36:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=35
06/17/2022 15:36:13 - INFO - __main__ - Global step 1150 Train loss 0.50 Classification-F1 0.6675617536415568 on epoch=35
06/17/2022 15:36:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=36
06/17/2022 15:36:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.57 on epoch=36
06/17/2022 15:36:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=36
06/17/2022 15:36:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=37
06/17/2022 15:36:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.52 on epoch=37
06/17/2022 15:36:33 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.7135076957102551 on epoch=37
06/17/2022 15:36:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=37
06/17/2022 15:36:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=38
06/17/2022 15:36:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.55 on epoch=38
06/17/2022 15:36:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=38
06/17/2022 15:36:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=39
06/17/2022 15:36:53 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.6882020909652335 on epoch=39
06/17/2022 15:36:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=39
06/17/2022 15:36:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=39
06/17/2022 15:37:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=39
06/17/2022 15:37:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=40
06/17/2022 15:37:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=40
06/17/2022 15:37:13 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.6515455549753222 on epoch=40
06/17/2022 15:37:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=40
06/17/2022 15:37:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=41
06/17/2022 15:37:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=41
06/17/2022 15:37:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=41
06/17/2022 15:37:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=42
06/17/2022 15:37:32 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.7152650948902257 on epoch=42
06/17/2022 15:37:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=42
06/17/2022 15:37:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=42
06/17/2022 15:37:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=43
06/17/2022 15:37:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.61 on epoch=43
06/17/2022 15:37:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=43
06/17/2022 15:37:52 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.7343511691664247 on epoch=43
06/17/2022 15:37:52 - INFO - __main__ - Saving model with best Classification-F1: 0.72180022071831 -> 0.7343511691664247 on epoch=43, global_step=1400
06/17/2022 15:37:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=44
06/17/2022 15:37:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=44
06/17/2022 15:38:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=44
06/17/2022 15:38:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=44
06/17/2022 15:38:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=45
06/17/2022 15:38:11 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.7005182950923317 on epoch=45
06/17/2022 15:38:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=45
06/17/2022 15:38:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=45
06/17/2022 15:38:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=46
06/17/2022 15:38:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=46
06/17/2022 15:38:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=46
06/17/2022 15:38:31 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.7251847973159449 on epoch=46
06/17/2022 15:38:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=47
06/17/2022 15:38:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=47
06/17/2022 15:38:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=47
06/17/2022 15:38:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=48
06/17/2022 15:38:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=48
06/17/2022 15:38:50 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.7538330689656636 on epoch=48
06/17/2022 15:38:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7343511691664247 -> 0.7538330689656636 on epoch=48, global_step=1550
06/17/2022 15:38:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=48
06/17/2022 15:38:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=49
06/17/2022 15:38:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=49
06/17/2022 15:39:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=49
06/17/2022 15:39:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=49
06/17/2022 15:39:10 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.7164014103958877 on epoch=49
06/17/2022 15:39:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.50 on epoch=50
06/17/2022 15:39:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=50
06/17/2022 15:39:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=50
06/17/2022 15:39:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
06/17/2022 15:39:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=51
06/17/2022 15:39:29 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.7270605126338284 on epoch=51
06/17/2022 15:39:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=51
06/17/2022 15:39:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=52
06/17/2022 15:39:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=52
06/17/2022 15:39:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=52
06/17/2022 15:39:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=53
06/17/2022 15:39:48 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.69809413443385 on epoch=53
06/17/2022 15:39:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=53
06/17/2022 15:39:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=53
06/17/2022 15:39:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=54
06/17/2022 15:39:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=54
06/17/2022 15:40:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=54
06/17/2022 15:40:08 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.7504608841450947 on epoch=54
06/17/2022 15:40:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=54
06/17/2022 15:40:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=55
06/17/2022 15:40:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=55
06/17/2022 15:40:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=55
06/17/2022 15:40:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=56
06/17/2022 15:40:27 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.7016998494800218 on epoch=56
06/17/2022 15:40:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=56
06/17/2022 15:40:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=56
06/17/2022 15:40:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=57
06/17/2022 15:40:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=57
06/17/2022 15:40:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=57
06/17/2022 15:40:47 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.7468973179510335 on epoch=57
06/17/2022 15:40:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=58
06/17/2022 15:40:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=58
06/17/2022 15:40:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=58
06/17/2022 15:40:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=59
06/17/2022 15:40:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=59
06/17/2022 15:41:06 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.7493716555449085 on epoch=59
06/17/2022 15:41:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=59
06/17/2022 15:41:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=59
06/17/2022 15:41:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=60
06/17/2022 15:41:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=60
06/17/2022 15:41:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=60
06/17/2022 15:41:25 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.6478556605998502 on epoch=60
06/17/2022 15:41:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=61
06/17/2022 15:41:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=61
06/17/2022 15:41:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=61
06/17/2022 15:41:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=62
06/17/2022 15:41:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=62
06/17/2022 15:41:45 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.7527800859017806 on epoch=62
06/17/2022 15:41:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=62
06/17/2022 15:41:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
06/17/2022 15:41:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=63
06/17/2022 15:41:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=63
06/17/2022 15:41:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=64
06/17/2022 15:42:04 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.7470226388357846 on epoch=64
06/17/2022 15:42:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=64
06/17/2022 15:42:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=64
06/17/2022 15:42:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=64
06/17/2022 15:42:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=65
06/17/2022 15:42:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=65
06/17/2022 15:42:24 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.6520538765084659 on epoch=65
06/17/2022 15:42:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=65
06/17/2022 15:42:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=66
06/17/2022 15:42:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=66
06/17/2022 15:42:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=66
06/17/2022 15:42:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=67
06/17/2022 15:42:43 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.6869260217990727 on epoch=67
06/17/2022 15:42:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=67
06/17/2022 15:42:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=67
06/17/2022 15:42:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=68
06/17/2022 15:42:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=68
06/17/2022 15:42:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.38 on epoch=68
06/17/2022 15:43:02 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.7300004359785566 on epoch=68
06/17/2022 15:43:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=69
06/17/2022 15:43:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=69
06/17/2022 15:43:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=69
06/17/2022 15:43:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=69
06/17/2022 15:43:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=70
06/17/2022 15:43:22 - INFO - __main__ - Global step 2250 Train loss 0.29 Classification-F1 0.7400747045871857 on epoch=70
06/17/2022 15:43:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=70
06/17/2022 15:43:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.32 on epoch=70
06/17/2022 15:43:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=71
06/17/2022 15:43:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.37 on epoch=71
06/17/2022 15:43:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.32 on epoch=71
06/17/2022 15:43:41 - INFO - __main__ - Global step 2300 Train loss 0.30 Classification-F1 0.7347633156001667 on epoch=71
06/17/2022 15:43:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=72
06/17/2022 15:43:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=72
06/17/2022 15:43:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=72
06/17/2022 15:43:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=73
06/17/2022 15:43:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.37 on epoch=73
06/17/2022 15:44:01 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.766725331294455 on epoch=73
06/17/2022 15:44:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7538330689656636 -> 0.766725331294455 on epoch=73, global_step=2350
06/17/2022 15:44:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=73
06/17/2022 15:44:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=74
06/17/2022 15:44:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=74
06/17/2022 15:44:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=74
06/17/2022 15:44:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=74
06/17/2022 15:44:20 - INFO - __main__ - Global step 2400 Train loss 0.29 Classification-F1 0.7448483318398009 on epoch=74
06/17/2022 15:44:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.32 on epoch=75
06/17/2022 15:44:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.32 on epoch=75
06/17/2022 15:44:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=75
06/17/2022 15:44:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
06/17/2022 15:44:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=76
06/17/2022 15:44:40 - INFO - __main__ - Global step 2450 Train loss 0.29 Classification-F1 0.7554158696600557 on epoch=76
06/17/2022 15:44:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=76
06/17/2022 15:44:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=77
06/17/2022 15:44:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.28 on epoch=77
06/17/2022 15:44:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.33 on epoch=77
06/17/2022 15:44:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.20 on epoch=78
06/17/2022 15:44:59 - INFO - __main__ - Global step 2500 Train loss 0.28 Classification-F1 0.7204251832829736 on epoch=78
06/17/2022 15:45:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=78
06/17/2022 15:45:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=78
06/17/2022 15:45:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.28 on epoch=79
06/17/2022 15:45:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=79
06/17/2022 15:45:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=79
06/17/2022 15:45:19 - INFO - __main__ - Global step 2550 Train loss 0.33 Classification-F1 0.7686378898314916 on epoch=79
06/17/2022 15:45:19 - INFO - __main__ - Saving model with best Classification-F1: 0.766725331294455 -> 0.7686378898314916 on epoch=79, global_step=2550
06/17/2022 15:45:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.24 on epoch=79
06/17/2022 15:45:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.28 on epoch=80
06/17/2022 15:45:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.34 on epoch=80
06/17/2022 15:45:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
06/17/2022 15:45:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=81
06/17/2022 15:45:38 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.7677444046009148 on epoch=81
06/17/2022 15:45:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.33 on epoch=81
06/17/2022 15:45:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=81
06/17/2022 15:45:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.23 on epoch=82
06/17/2022 15:45:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.30 on epoch=82
06/17/2022 15:45:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=82
06/17/2022 15:45:57 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.7599587674378376 on epoch=82
06/17/2022 15:46:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.27 on epoch=83
06/17/2022 15:46:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.39 on epoch=83
06/17/2022 15:46:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.33 on epoch=83
06/17/2022 15:46:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
06/17/2022 15:46:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=84
06/17/2022 15:46:17 - INFO - __main__ - Global step 2700 Train loss 0.30 Classification-F1 0.7376438619157474 on epoch=84
06/17/2022 15:46:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.27 on epoch=84
06/17/2022 15:46:22 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=84
06/17/2022 15:46:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.31 on epoch=85
06/17/2022 15:46:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.35 on epoch=85
06/17/2022 15:46:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=85
06/17/2022 15:46:36 - INFO - __main__ - Global step 2750 Train loss 0.28 Classification-F1 0.7229984726765842 on epoch=85
06/17/2022 15:46:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
06/17/2022 15:46:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.46 on epoch=86
06/17/2022 15:46:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=86
06/17/2022 15:46:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=87
06/17/2022 15:46:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.32 on epoch=87
06/17/2022 15:46:56 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.7522369925671812 on epoch=87
06/17/2022 15:46:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=87
06/17/2022 15:47:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=88
06/17/2022 15:47:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=88
06/17/2022 15:47:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.24 on epoch=88
06/17/2022 15:47:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=89
06/17/2022 15:47:15 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.7550364400986638 on epoch=89
06/17/2022 15:47:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.26 on epoch=89
06/17/2022 15:47:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.30 on epoch=89
06/17/2022 15:47:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.28 on epoch=89
06/17/2022 15:47:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=90
06/17/2022 15:47:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=90
06/17/2022 15:47:35 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.756315617558666 on epoch=90
06/17/2022 15:47:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.27 on epoch=90
06/17/2022 15:47:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
06/17/2022 15:47:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=91
06/17/2022 15:47:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=91
06/17/2022 15:47:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.27 on epoch=92
06/17/2022 15:47:54 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.73934108361799 on epoch=92
06/17/2022 15:47:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=92
06/17/2022 15:47:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=92
06/17/2022 15:48:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.26 on epoch=93
06/17/2022 15:48:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.38 on epoch=93
06/17/2022 15:48:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=93
06/17/2022 15:48:08 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:48:08 - INFO - __main__ - Printing 3 examples
06/17/2022 15:48:08 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 15:48:08 - INFO - __main__ - ['sad']
06/17/2022 15:48:08 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 15:48:08 - INFO - __main__ - ['sad']
06/17/2022 15:48:08 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 15:48:08 - INFO - __main__ - ['sad']
06/17/2022 15:48:08 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:48:08 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:48:09 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:48:09 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:48:09 - INFO - __main__ - Printing 3 examples
06/17/2022 15:48:09 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 15:48:09 - INFO - __main__ - ['sad']
06/17/2022 15:48:09 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 15:48:09 - INFO - __main__ - ['sad']
06/17/2022 15:48:09 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 15:48:09 - INFO - __main__ - ['sad']
06/17/2022 15:48:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:48:09 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:48:10 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:48:14 - INFO - __main__ - Global step 3000 Train loss 0.30 Classification-F1 0.7464960084094701 on epoch=93
06/17/2022 15:48:14 - INFO - __main__ - save last model!
06/17/2022 15:48:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 15:48:14 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 15:48:14 - INFO - __main__ - Printing 3 examples
06/17/2022 15:48:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 15:48:14 - INFO - __main__ - ['others']
06/17/2022 15:48:14 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 15:48:14 - INFO - __main__ - ['others']
06/17/2022 15:48:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 15:48:14 - INFO - __main__ - ['others']
06/17/2022 15:48:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:48:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:48:21 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 15:48:27 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:48:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:48:29 - INFO - __main__ - Starting training!
06/17/2022 15:49:34 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_13_0.2_8_predictions.txt
06/17/2022 15:49:34 - INFO - __main__ - Classification-F1 on test data: 0.4090
06/17/2022 15:49:34 - INFO - __main__ - prefix=emo_128_13, lr=0.2, bsz=8, dev_performance=0.7686378898314916, test_performance=0.40897025519756713
06/17/2022 15:49:34 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.5, bsz=8 ...
06/17/2022 15:49:35 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:49:35 - INFO - __main__ - Printing 3 examples
06/17/2022 15:49:35 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 15:49:35 - INFO - __main__ - ['sad']
06/17/2022 15:49:35 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 15:49:35 - INFO - __main__ - ['sad']
06/17/2022 15:49:35 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 15:49:35 - INFO - __main__ - ['sad']
06/17/2022 15:49:35 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:49:35 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:49:36 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 15:49:36 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 15:49:36 - INFO - __main__ - Printing 3 examples
06/17/2022 15:49:36 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 15:49:36 - INFO - __main__ - ['sad']
06/17/2022 15:49:36 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 15:49:36 - INFO - __main__ - ['sad']
06/17/2022 15:49:36 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 15:49:36 - INFO - __main__ - ['sad']
06/17/2022 15:49:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 15:49:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 15:49:36 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 15:49:55 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 15:49:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 15:49:56 - INFO - __main__ - Starting training!
06/17/2022 15:49:59 - INFO - __main__ - Step 10 Global step 10 Train loss 2.55 on epoch=0
06/17/2022 15:50:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=0
06/17/2022 15:50:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.04 on epoch=0
06/17/2022 15:50:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=1
06/17/2022 15:50:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=1
06/17/2022 15:50:17 - INFO - __main__ - Global step 50 Train loss 1.36 Classification-F1 0.13903836992072283 on epoch=1
06/17/2022 15:50:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13903836992072283 on epoch=1, global_step=50
06/17/2022 15:50:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.93 on epoch=1
06/17/2022 15:50:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=2
06/17/2022 15:50:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=2
06/17/2022 15:50:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=2
06/17/2022 15:50:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=3
06/17/2022 15:50:36 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.44629574705622316 on epoch=3
06/17/2022 15:50:36 - INFO - __main__ - Saving model with best Classification-F1: 0.13903836992072283 -> 0.44629574705622316 on epoch=3, global_step=100
06/17/2022 15:50:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=3
06/17/2022 15:50:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=3
06/17/2022 15:50:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=4
06/17/2022 15:50:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=4
06/17/2022 15:50:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.73 on epoch=4
06/17/2022 15:50:56 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.42484458843653133 on epoch=4
06/17/2022 15:50:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=4
06/17/2022 15:51:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=5
06/17/2022 15:51:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=5
06/17/2022 15:51:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=5
06/17/2022 15:51:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=6
06/17/2022 15:51:15 - INFO - __main__ - Global step 200 Train loss 0.76 Classification-F1 0.49447124527913244 on epoch=6
06/17/2022 15:51:15 - INFO - __main__ - Saving model with best Classification-F1: 0.44629574705622316 -> 0.49447124527913244 on epoch=6, global_step=200
06/17/2022 15:51:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=6
06/17/2022 15:51:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=6
06/17/2022 15:51:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=7
06/17/2022 15:51:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=7
06/17/2022 15:51:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=7
06/17/2022 15:51:35 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.6429058479058479 on epoch=7
06/17/2022 15:51:35 - INFO - __main__ - Saving model with best Classification-F1: 0.49447124527913244 -> 0.6429058479058479 on epoch=7, global_step=250
06/17/2022 15:51:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=8
06/17/2022 15:51:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=8
06/17/2022 15:51:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=8
06/17/2022 15:51:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=9
06/17/2022 15:51:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=9
06/17/2022 15:51:54 - INFO - __main__ - Global step 300 Train loss 0.63 Classification-F1 0.6190265031346436 on epoch=9
06/17/2022 15:51:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=9
06/17/2022 15:51:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=9
06/17/2022 15:52:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=10
06/17/2022 15:52:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.63 on epoch=10
06/17/2022 15:52:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=10
06/17/2022 15:52:14 - INFO - __main__ - Global step 350 Train loss 0.64 Classification-F1 0.6732755536923625 on epoch=10
06/17/2022 15:52:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6429058479058479 -> 0.6732755536923625 on epoch=10, global_step=350
06/17/2022 15:52:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=11
06/17/2022 15:52:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=11
06/17/2022 15:52:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=11
06/17/2022 15:52:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=12
06/17/2022 15:52:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=12
06/17/2022 15:52:33 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.6738646260399548 on epoch=12
06/17/2022 15:52:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6732755536923625 -> 0.6738646260399548 on epoch=12, global_step=400
06/17/2022 15:52:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=12
06/17/2022 15:52:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.62 on epoch=13
06/17/2022 15:52:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=13
06/17/2022 15:52:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=13
06/17/2022 15:52:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=14
06/17/2022 15:52:52 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.6461047018848531 on epoch=14
06/17/2022 15:52:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=14
06/17/2022 15:52:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=14
06/17/2022 15:53:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.62 on epoch=14
06/17/2022 15:53:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=15
06/17/2022 15:53:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=15
06/17/2022 15:53:12 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.7256421921277325 on epoch=15
06/17/2022 15:53:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6738646260399548 -> 0.7256421921277325 on epoch=15, global_step=500
06/17/2022 15:53:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=15
06/17/2022 15:53:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=16
06/17/2022 15:53:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=16
06/17/2022 15:53:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=16
06/17/2022 15:53:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=17
06/17/2022 15:53:31 - INFO - __main__ - Global step 550 Train loss 0.57 Classification-F1 0.6846265601762745 on epoch=17
06/17/2022 15:53:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=17
06/17/2022 15:53:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=17
06/17/2022 15:53:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=18
06/17/2022 15:53:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=18
06/17/2022 15:53:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=18
06/17/2022 15:53:51 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.6970563802027687 on epoch=18
06/17/2022 15:53:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=19
06/17/2022 15:53:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=19
06/17/2022 15:53:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=19
06/17/2022 15:54:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=19
06/17/2022 15:54:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=20
06/17/2022 15:54:10 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.5928735030064368 on epoch=20
06/17/2022 15:54:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=20
06/17/2022 15:54:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=20
06/17/2022 15:54:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=21
06/17/2022 15:54:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=21
06/17/2022 15:54:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=21
06/17/2022 15:54:29 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.6605073473725744 on epoch=21
06/17/2022 15:54:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=22
06/17/2022 15:54:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=22
06/17/2022 15:54:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=22
06/17/2022 15:54:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=23
06/17/2022 15:54:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=23
06/17/2022 15:54:49 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.7489408322035441 on epoch=23
06/17/2022 15:54:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7256421921277325 -> 0.7489408322035441 on epoch=23, global_step=750
06/17/2022 15:54:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=23
06/17/2022 15:54:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.61 on epoch=24
06/17/2022 15:54:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=24
06/17/2022 15:54:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=24
06/17/2022 15:55:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=24
06/17/2022 15:55:08 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.711438657705864 on epoch=24
06/17/2022 15:55:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=25
06/17/2022 15:55:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=25
06/17/2022 15:55:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
06/17/2022 15:55:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=26
06/17/2022 15:55:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=26
06/17/2022 15:55:27 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.7445653704920093 on epoch=26
06/17/2022 15:55:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=26
06/17/2022 15:55:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=27
06/17/2022 15:55:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=27
06/17/2022 15:55:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=27
06/17/2022 15:55:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=28
06/17/2022 15:55:47 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.7813290246376253 on epoch=28
06/17/2022 15:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7489408322035441 -> 0.7813290246376253 on epoch=28, global_step=900
06/17/2022 15:55:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=28
06/17/2022 15:55:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=28
06/17/2022 15:55:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=29
06/17/2022 15:55:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=29
06/17/2022 15:55:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=29
06/17/2022 15:56:06 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7271097069789861 on epoch=29
06/17/2022 15:56:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=29
06/17/2022 15:56:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=30
06/17/2022 15:56:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=30
06/17/2022 15:56:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=30
06/17/2022 15:56:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=31
06/17/2022 15:56:25 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.6917657812380145 on epoch=31
06/17/2022 15:56:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=31
06/17/2022 15:56:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=31
06/17/2022 15:56:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=32
06/17/2022 15:56:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=32
06/17/2022 15:56:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=32
06/17/2022 15:56:45 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7762733714511837 on epoch=32
06/17/2022 15:56:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=33
06/17/2022 15:56:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=33
06/17/2022 15:56:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=33
06/17/2022 15:56:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=34
06/17/2022 15:56:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=34
06/17/2022 15:57:04 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.7511163511223351 on epoch=34
06/17/2022 15:57:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=34
06/17/2022 15:57:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=34
06/17/2022 15:57:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=35
06/17/2022 15:57:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=35
06/17/2022 15:57:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=35
06/17/2022 15:57:23 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.7718752744839702 on epoch=35
06/17/2022 15:57:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=36
06/17/2022 15:57:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=36
06/17/2022 15:57:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=36
06/17/2022 15:57:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=37
06/17/2022 15:57:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=37
06/17/2022 15:57:43 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7443081834791963 on epoch=37
06/17/2022 15:57:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=37
06/17/2022 15:57:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=38
06/17/2022 15:57:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=38
06/17/2022 15:57:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=38
06/17/2022 15:57:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=39
06/17/2022 15:58:02 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.75928806668762 on epoch=39
06/17/2022 15:58:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=39
06/17/2022 15:58:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=39
06/17/2022 15:58:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=39
06/17/2022 15:58:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=40
06/17/2022 15:58:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=40
06/17/2022 15:58:21 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7671754830788439 on epoch=40
06/17/2022 15:58:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=40
06/17/2022 15:58:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
06/17/2022 15:58:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=41
06/17/2022 15:58:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=41
06/17/2022 15:58:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=42
06/17/2022 15:58:40 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.778133358430583 on epoch=42
06/17/2022 15:58:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=42
06/17/2022 15:58:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=42
06/17/2022 15:58:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=43
06/17/2022 15:58:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=43
06/17/2022 15:58:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=43
06/17/2022 15:59:00 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7862818080102956 on epoch=43
06/17/2022 15:59:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7813290246376253 -> 0.7862818080102956 on epoch=43, global_step=1400
06/17/2022 15:59:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
06/17/2022 15:59:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=44
06/17/2022 15:59:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=44
06/17/2022 15:59:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=44
06/17/2022 15:59:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=45
06/17/2022 15:59:19 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.7095292814368288 on epoch=45
06/17/2022 15:59:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=45
06/17/2022 15:59:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=45
06/17/2022 15:59:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=46
06/17/2022 15:59:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=46
06/17/2022 15:59:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=46
06/17/2022 15:59:39 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.7195937478565974 on epoch=46
06/17/2022 15:59:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=47
06/17/2022 15:59:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=47
06/17/2022 15:59:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=47
06/17/2022 15:59:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=48
06/17/2022 15:59:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=48
06/17/2022 15:59:58 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.7665995783156316 on epoch=48
06/17/2022 16:00:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=48
06/17/2022 16:00:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=49
06/17/2022 16:00:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=49
06/17/2022 16:00:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=49
06/17/2022 16:00:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=49
06/17/2022 16:00:17 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.7649521684995251 on epoch=49
06/17/2022 16:00:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=50
06/17/2022 16:00:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
06/17/2022 16:00:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=50
06/17/2022 16:00:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=51
06/17/2022 16:00:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=51
06/17/2022 16:00:37 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.7859360723700023 on epoch=51
06/17/2022 16:00:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=51
06/17/2022 16:00:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.34 on epoch=52
06/17/2022 16:00:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=52
06/17/2022 16:00:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=52
06/17/2022 16:00:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=53
06/17/2022 16:00:56 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7973623776990842 on epoch=53
06/17/2022 16:00:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7862818080102956 -> 0.7973623776990842 on epoch=53, global_step=1700
06/17/2022 16:00:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=53
06/17/2022 16:01:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
06/17/2022 16:01:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=54
06/17/2022 16:01:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=54
06/17/2022 16:01:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=54
06/17/2022 16:01:15 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7995728466394935 on epoch=54
06/17/2022 16:01:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7973623776990842 -> 0.7995728466394935 on epoch=54, global_step=1750
06/17/2022 16:01:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
06/17/2022 16:01:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=55
06/17/2022 16:01:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=55
06/17/2022 16:01:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=55
06/17/2022 16:01:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=56
06/17/2022 16:01:34 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.7469289840722972 on epoch=56
06/17/2022 16:01:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=56
06/17/2022 16:01:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=56
06/17/2022 16:01:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=57
06/17/2022 16:01:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=57
06/17/2022 16:01:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=57
06/17/2022 16:01:54 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7555874262684802 on epoch=57
06/17/2022 16:01:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=58
06/17/2022 16:01:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=58
06/17/2022 16:02:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=58
06/17/2022 16:02:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=59
06/17/2022 16:02:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=59
06/17/2022 16:02:13 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.7526163465769676 on epoch=59
06/17/2022 16:02:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=59
06/17/2022 16:02:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=59
06/17/2022 16:02:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=60
06/17/2022 16:02:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=60
06/17/2022 16:02:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=60
06/17/2022 16:02:32 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.7808050690126263 on epoch=60
06/17/2022 16:02:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=61
06/17/2022 16:02:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=61
06/17/2022 16:02:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=61
06/17/2022 16:02:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=62
06/17/2022 16:02:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=62
06/17/2022 16:02:52 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.7814529986949916 on epoch=62
06/17/2022 16:02:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=62
06/17/2022 16:02:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=63
06/17/2022 16:02:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=63
06/17/2022 16:03:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=63
06/17/2022 16:03:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=64
06/17/2022 16:03:11 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.7628248329942551 on epoch=64
06/17/2022 16:03:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=64
06/17/2022 16:03:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=64
06/17/2022 16:03:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
06/17/2022 16:03:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=65
06/17/2022 16:03:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=65
06/17/2022 16:03:30 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7888250544570639 on epoch=65
06/17/2022 16:03:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=65
06/17/2022 16:03:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=66
06/17/2022 16:03:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=66
06/17/2022 16:03:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
06/17/2022 16:03:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=67
06/17/2022 16:03:50 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.7916539149356348 on epoch=67
06/17/2022 16:03:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=67
06/17/2022 16:03:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=67
06/17/2022 16:03:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=68
06/17/2022 16:04:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=68
06/17/2022 16:04:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=68
06/17/2022 16:04:09 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.7818144600820798 on epoch=68
06/17/2022 16:04:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=69
06/17/2022 16:04:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=69
06/17/2022 16:04:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=69
06/17/2022 16:04:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=69
06/17/2022 16:04:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=70
06/17/2022 16:04:28 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.8010995237414882 on epoch=70
06/17/2022 16:04:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7995728466394935 -> 0.8010995237414882 on epoch=70, global_step=2250
06/17/2022 16:04:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
06/17/2022 16:04:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=70
06/17/2022 16:04:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=71
06/17/2022 16:04:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.31 on epoch=71
06/17/2022 16:04:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=71
06/17/2022 16:04:48 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.7427929976965731 on epoch=71
06/17/2022 16:04:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=72
06/17/2022 16:04:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=72
06/17/2022 16:04:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=72
06/17/2022 16:04:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=73
06/17/2022 16:05:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=73
06/17/2022 16:05:07 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.822804832736064 on epoch=73
06/17/2022 16:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8010995237414882 -> 0.822804832736064 on epoch=73, global_step=2350
06/17/2022 16:05:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=73
06/17/2022 16:05:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
06/17/2022 16:05:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=74
06/17/2022 16:05:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
06/17/2022 16:05:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=74
06/17/2022 16:05:26 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.7542985418694771 on epoch=74
06/17/2022 16:05:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=75
06/17/2022 16:05:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=75
06/17/2022 16:05:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=75
06/17/2022 16:05:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=76
06/17/2022 16:05:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=76
06/17/2022 16:05:46 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.8141186586898653 on epoch=76
06/17/2022 16:05:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
06/17/2022 16:05:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=77
06/17/2022 16:05:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
06/17/2022 16:05:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=77
06/17/2022 16:05:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=78
06/17/2022 16:06:05 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.8128909898522909 on epoch=78
06/17/2022 16:06:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=78
06/17/2022 16:06:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=78
06/17/2022 16:06:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=79
06/17/2022 16:06:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=79
06/17/2022 16:06:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.24 on epoch=79
06/17/2022 16:06:24 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7768179671217662 on epoch=79
06/17/2022 16:06:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=79
06/17/2022 16:06:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=80
06/17/2022 16:06:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=80
06/17/2022 16:06:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=80
06/17/2022 16:06:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=81
06/17/2022 16:06:44 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.8059378542562526 on epoch=81
06/17/2022 16:06:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
06/17/2022 16:06:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 16:06:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=82
06/17/2022 16:06:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=82
06/17/2022 16:06:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=82
06/17/2022 16:07:03 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.7996828626169237 on epoch=82
06/17/2022 16:07:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=83
06/17/2022 16:07:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=83
06/17/2022 16:07:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.28 on epoch=83
06/17/2022 16:07:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=84
06/17/2022 16:07:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=84
06/17/2022 16:07:22 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.7733998614979118 on epoch=84
06/17/2022 16:07:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=84
06/17/2022 16:07:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 16:07:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=85
06/17/2022 16:07:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=85
06/17/2022 16:07:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=85
06/17/2022 16:07:42 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.7994912037464875 on epoch=85
06/17/2022 16:07:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.24 on epoch=86
06/17/2022 16:07:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=86
06/17/2022 16:07:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=86
06/17/2022 16:07:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=87
06/17/2022 16:07:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=87
06/17/2022 16:08:01 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.7879022733656647 on epoch=87
06/17/2022 16:08:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=87
06/17/2022 16:08:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=88
06/17/2022 16:08:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=88
06/17/2022 16:08:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
06/17/2022 16:08:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
06/17/2022 16:08:20 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.7843060383016425 on epoch=89
06/17/2022 16:08:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
06/17/2022 16:08:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=89
06/17/2022 16:08:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.20 on epoch=89
06/17/2022 16:08:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
06/17/2022 16:08:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=90
06/17/2022 16:08:39 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.8232159995391238 on epoch=90
06/17/2022 16:08:40 - INFO - __main__ - Saving model with best Classification-F1: 0.822804832736064 -> 0.8232159995391238 on epoch=90, global_step=2900
06/17/2022 16:08:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=90
06/17/2022 16:08:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=91
06/17/2022 16:08:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=91
06/17/2022 16:08:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=91
06/17/2022 16:08:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.29 on epoch=92
06/17/2022 16:08:59 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.7858366183168127 on epoch=92
06/17/2022 16:09:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=92
06/17/2022 16:09:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=92
06/17/2022 16:09:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=93
06/17/2022 16:09:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
06/17/2022 16:09:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.20 on epoch=93
06/17/2022 16:09:12 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:09:12 - INFO - __main__ - Printing 3 examples
06/17/2022 16:09:12 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:09:12 - INFO - __main__ - ['sad']
06/17/2022 16:09:12 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:09:12 - INFO - __main__ - ['sad']
06/17/2022 16:09:12 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:09:12 - INFO - __main__ - ['sad']
06/17/2022 16:09:12 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:09:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:09:13 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:09:13 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:09:13 - INFO - __main__ - Printing 3 examples
06/17/2022 16:09:13 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:09:13 - INFO - __main__ - ['sad']
06/17/2022 16:09:13 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:09:13 - INFO - __main__ - ['sad']
06/17/2022 16:09:13 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:09:13 - INFO - __main__ - ['sad']
06/17/2022 16:09:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:09:13 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:09:14 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:09:18 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.7867290153797984 on epoch=93
06/17/2022 16:09:18 - INFO - __main__ - save last model!
06/17/2022 16:09:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 16:09:18 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 16:09:18 - INFO - __main__ - Printing 3 examples
06/17/2022 16:09:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 16:09:18 - INFO - __main__ - ['others']
06/17/2022 16:09:18 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 16:09:18 - INFO - __main__ - ['others']
06/17/2022 16:09:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 16:09:18 - INFO - __main__ - ['others']
06/17/2022 16:09:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:09:20 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:09:26 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 16:09:31 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:09:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:09:32 - INFO - __main__ - Starting training!
06/17/2022 16:10:37 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_21_0.5_8_predictions.txt
06/17/2022 16:10:37 - INFO - __main__ - Classification-F1 on test data: 0.4763
06/17/2022 16:10:38 - INFO - __main__ - prefix=emo_128_21, lr=0.5, bsz=8, dev_performance=0.8232159995391238, test_performance=0.47634318246338175
06/17/2022 16:10:38 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.4, bsz=8 ...
06/17/2022 16:10:39 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:10:39 - INFO - __main__ - Printing 3 examples
06/17/2022 16:10:39 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:10:39 - INFO - __main__ - ['sad']
06/17/2022 16:10:39 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:10:39 - INFO - __main__ - ['sad']
06/17/2022 16:10:39 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:10:39 - INFO - __main__ - ['sad']
06/17/2022 16:10:39 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:10:39 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:10:40 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:10:40 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:10:40 - INFO - __main__ - Printing 3 examples
06/17/2022 16:10:40 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:10:40 - INFO - __main__ - ['sad']
06/17/2022 16:10:40 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:10:40 - INFO - __main__ - ['sad']
06/17/2022 16:10:40 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:10:40 - INFO - __main__ - ['sad']
06/17/2022 16:10:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:10:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:10:40 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:10:56 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:10:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:10:57 - INFO - __main__ - Starting training!
06/17/2022 16:11:00 - INFO - __main__ - Step 10 Global step 10 Train loss 2.60 on epoch=0
06/17/2022 16:11:03 - INFO - __main__ - Step 20 Global step 20 Train loss 1.52 on epoch=0
06/17/2022 16:11:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.08 on epoch=0
06/17/2022 16:11:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=1
06/17/2022 16:11:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=1
06/17/2022 16:11:17 - INFO - __main__ - Global step 50 Train loss 1.45 Classification-F1 0.1 on epoch=1
06/17/2022 16:11:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 16:11:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=1
06/17/2022 16:11:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=2
06/17/2022 16:11:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=2
06/17/2022 16:11:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=2
06/17/2022 16:11:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=3
06/17/2022 16:11:37 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.3413401091602158 on epoch=3
06/17/2022 16:11:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3413401091602158 on epoch=3, global_step=100
06/17/2022 16:11:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=3
06/17/2022 16:11:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=3
06/17/2022 16:11:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=4
06/17/2022 16:11:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=4
06/17/2022 16:11:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.85 on epoch=4
06/17/2022 16:11:56 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.27113382184664603 on epoch=4
06/17/2022 16:11:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=4
06/17/2022 16:12:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=5
06/17/2022 16:12:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=5
06/17/2022 16:12:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=5
06/17/2022 16:12:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=6
06/17/2022 16:12:16 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.5107823271062011 on epoch=6
06/17/2022 16:12:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3413401091602158 -> 0.5107823271062011 on epoch=6, global_step=200
06/17/2022 16:12:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=6
06/17/2022 16:12:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.79 on epoch=6
06/17/2022 16:12:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=7
06/17/2022 16:12:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.63 on epoch=7
06/17/2022 16:12:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=7
06/17/2022 16:12:36 - INFO - __main__ - Global step 250 Train loss 0.74 Classification-F1 0.6496720211188444 on epoch=7
06/17/2022 16:12:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5107823271062011 -> 0.6496720211188444 on epoch=7, global_step=250
06/17/2022 16:12:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=8
06/17/2022 16:12:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=8
06/17/2022 16:12:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=8
06/17/2022 16:12:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=9
06/17/2022 16:12:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=9
06/17/2022 16:12:55 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.5244830726784953 on epoch=9
06/17/2022 16:12:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=9
06/17/2022 16:13:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=9
06/17/2022 16:13:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=10
06/17/2022 16:13:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=10
06/17/2022 16:13:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=10
06/17/2022 16:13:15 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.5736536039608037 on epoch=10
06/17/2022 16:13:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=11
06/17/2022 16:13:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=11
06/17/2022 16:13:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=11
06/17/2022 16:13:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=12
06/17/2022 16:13:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=12
06/17/2022 16:13:35 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.5566745032200537 on epoch=12
06/17/2022 16:13:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=12
06/17/2022 16:13:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=13
06/17/2022 16:13:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=13
06/17/2022 16:13:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=13
06/17/2022 16:13:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=14
06/17/2022 16:13:54 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.6847853086876737 on epoch=14
06/17/2022 16:13:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6496720211188444 -> 0.6847853086876737 on epoch=14, global_step=450
06/17/2022 16:13:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=14
06/17/2022 16:13:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=14
06/17/2022 16:14:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.64 on epoch=14
06/17/2022 16:14:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=15
06/17/2022 16:14:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=15
06/17/2022 16:14:14 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.727016231529989 on epoch=15
06/17/2022 16:14:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6847853086876737 -> 0.727016231529989 on epoch=15, global_step=500
06/17/2022 16:14:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=15
06/17/2022 16:14:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=16
06/17/2022 16:14:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=16
06/17/2022 16:14:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=16
06/17/2022 16:14:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=17
06/17/2022 16:14:33 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.6693660131200034 on epoch=17
06/17/2022 16:14:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=17
06/17/2022 16:14:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=17
06/17/2022 16:14:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=18
06/17/2022 16:14:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=18
06/17/2022 16:14:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=18
06/17/2022 16:14:52 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.6907054574005824 on epoch=18
06/17/2022 16:14:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=19
06/17/2022 16:14:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=19
06/17/2022 16:15:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
06/17/2022 16:15:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=19
06/17/2022 16:15:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=20
06/17/2022 16:15:12 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.6378025632792533 on epoch=20
06/17/2022 16:15:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=20
06/17/2022 16:15:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=20
06/17/2022 16:15:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=21
06/17/2022 16:15:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=21
06/17/2022 16:15:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=21
06/17/2022 16:15:31 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.7024048628594941 on epoch=21
06/17/2022 16:15:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=22
06/17/2022 16:15:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=22
06/17/2022 16:15:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=22
06/17/2022 16:15:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=23
06/17/2022 16:15:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=23
06/17/2022 16:15:51 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.7549922961550066 on epoch=23
06/17/2022 16:15:51 - INFO - __main__ - Saving model with best Classification-F1: 0.727016231529989 -> 0.7549922961550066 on epoch=23, global_step=750
06/17/2022 16:15:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=23
06/17/2022 16:15:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=24
06/17/2022 16:15:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=24
06/17/2022 16:16:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=24
06/17/2022 16:16:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=24
06/17/2022 16:16:11 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.6784291075658068 on epoch=24
06/17/2022 16:16:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=25
06/17/2022 16:16:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=25
06/17/2022 16:16:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=25
06/17/2022 16:16:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
06/17/2022 16:16:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=26
06/17/2022 16:16:30 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.7556276841969912 on epoch=26
06/17/2022 16:16:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7549922961550066 -> 0.7556276841969912 on epoch=26, global_step=850
06/17/2022 16:16:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=26
06/17/2022 16:16:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=27
06/17/2022 16:16:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=27
06/17/2022 16:16:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=27
06/17/2022 16:16:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=28
06/17/2022 16:16:50 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.7717027413148799 on epoch=28
06/17/2022 16:16:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7556276841969912 -> 0.7717027413148799 on epoch=28, global_step=900
06/17/2022 16:16:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=28
06/17/2022 16:16:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=28
06/17/2022 16:16:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=29
06/17/2022 16:17:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=29
06/17/2022 16:17:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=29
06/17/2022 16:17:09 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.7206860910402384 on epoch=29
06/17/2022 16:17:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=29
06/17/2022 16:17:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=30
06/17/2022 16:17:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=30
06/17/2022 16:17:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=30
06/17/2022 16:17:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=31
06/17/2022 16:17:29 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.6904612457590761 on epoch=31
06/17/2022 16:17:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=31
06/17/2022 16:17:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
06/17/2022 16:17:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=32
06/17/2022 16:17:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=32
06/17/2022 16:17:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=32
06/17/2022 16:17:48 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.7621752922581577 on epoch=32
06/17/2022 16:17:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=33
06/17/2022 16:17:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=33
06/17/2022 16:17:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=33
06/17/2022 16:17:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=34
06/17/2022 16:18:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=34
06/17/2022 16:18:08 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.7491234692889351 on epoch=34
06/17/2022 16:18:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=34
06/17/2022 16:18:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=34
06/17/2022 16:18:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=35
06/17/2022 16:18:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=35
06/17/2022 16:18:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=35
06/17/2022 16:18:27 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.8112700253681595 on epoch=35
06/17/2022 16:18:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7717027413148799 -> 0.8112700253681595 on epoch=35, global_step=1150
06/17/2022 16:18:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=36
06/17/2022 16:18:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=36
06/17/2022 16:18:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=36
06/17/2022 16:18:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=37
06/17/2022 16:18:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
06/17/2022 16:18:47 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.7139026644623061 on epoch=37
06/17/2022 16:18:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=37
06/17/2022 16:18:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=38
06/17/2022 16:18:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=38
06/17/2022 16:18:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=38
06/17/2022 16:18:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=39
06/17/2022 16:19:06 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.7627231054579636 on epoch=39
06/17/2022 16:19:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=39
06/17/2022 16:19:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=39
06/17/2022 16:19:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=39
06/17/2022 16:19:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=40
06/17/2022 16:19:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=40
06/17/2022 16:19:26 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.7854357247734443 on epoch=40
06/17/2022 16:19:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=40
06/17/2022 16:19:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=41
06/17/2022 16:19:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=41
06/17/2022 16:19:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=41
06/17/2022 16:19:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=42
06/17/2022 16:19:45 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7609514852419873 on epoch=42
06/17/2022 16:19:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=42
06/17/2022 16:19:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=42
06/17/2022 16:19:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=43
06/17/2022 16:19:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=43
06/17/2022 16:19:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=43
06/17/2022 16:20:05 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7421919835260438 on epoch=43
06/17/2022 16:20:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=44
06/17/2022 16:20:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=44
06/17/2022 16:20:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=44
06/17/2022 16:20:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=44
06/17/2022 16:20:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=45
06/17/2022 16:20:25 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.6989901572182791 on epoch=45
06/17/2022 16:20:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=45
06/17/2022 16:20:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=45
06/17/2022 16:20:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
06/17/2022 16:20:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=46
06/17/2022 16:20:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
06/17/2022 16:20:46 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.7280863492911317 on epoch=46
06/17/2022 16:20:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=47
06/17/2022 16:20:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=47
06/17/2022 16:20:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=47
06/17/2022 16:20:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=48
06/17/2022 16:20:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=48
06/17/2022 16:21:05 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.7704794885478294 on epoch=48
06/17/2022 16:21:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=48
06/17/2022 16:21:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=49
06/17/2022 16:21:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=49
06/17/2022 16:21:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=49
06/17/2022 16:21:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=49
06/17/2022 16:21:25 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.7018206793206794 on epoch=49
06/17/2022 16:21:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=50
06/17/2022 16:21:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=50
06/17/2022 16:21:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=50
06/17/2022 16:21:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=51
06/17/2022 16:21:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=51
06/17/2022 16:21:44 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.7956944265068946 on epoch=51
06/17/2022 16:21:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=51
06/17/2022 16:21:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=52
06/17/2022 16:21:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=52
06/17/2022 16:21:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=52
06/17/2022 16:21:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=53
06/17/2022 16:22:04 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.7939325392299819 on epoch=53
06/17/2022 16:22:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=53
06/17/2022 16:22:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
06/17/2022 16:22:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=54
06/17/2022 16:22:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=54
06/17/2022 16:22:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=54
06/17/2022 16:22:24 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.7701018243343386 on epoch=54
06/17/2022 16:22:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=54
06/17/2022 16:22:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
06/17/2022 16:22:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=55
06/17/2022 16:22:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=55
06/17/2022 16:22:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=56
06/17/2022 16:22:44 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.7138835451415451 on epoch=56
06/17/2022 16:22:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=56
06/17/2022 16:22:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=56
06/17/2022 16:22:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=57
06/17/2022 16:22:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=57
06/17/2022 16:22:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=57
06/17/2022 16:23:03 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.7946987241657474 on epoch=57
06/17/2022 16:23:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=58
06/17/2022 16:23:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=58
06/17/2022 16:23:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=58
06/17/2022 16:23:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=59
06/17/2022 16:23:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=59
06/17/2022 16:23:23 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.7014379997242901 on epoch=59
06/17/2022 16:23:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=59
06/17/2022 16:23:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=59
06/17/2022 16:23:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=60
06/17/2022 16:23:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=60
06/17/2022 16:23:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=60
06/17/2022 16:23:42 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.7917874465262562 on epoch=60
06/17/2022 16:23:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
06/17/2022 16:23:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=61
06/17/2022 16:23:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=61
06/17/2022 16:23:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=62
06/17/2022 16:23:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=62
06/17/2022 16:24:02 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.737315274936104 on epoch=62
06/17/2022 16:24:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=62
06/17/2022 16:24:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=63
06/17/2022 16:24:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=63
06/17/2022 16:24:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=63
06/17/2022 16:24:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=64
06/17/2022 16:24:21 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.7422188243839436 on epoch=64
06/17/2022 16:24:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=64
06/17/2022 16:24:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=64
06/17/2022 16:24:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=64
06/17/2022 16:24:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=65
06/17/2022 16:24:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=65
06/17/2022 16:24:40 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.8033762396921242 on epoch=65
06/17/2022 16:24:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.32 on epoch=65
06/17/2022 16:24:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=66
06/17/2022 16:24:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=66
06/17/2022 16:24:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
06/17/2022 16:24:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=67
06/17/2022 16:25:00 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.7372418662132322 on epoch=67
06/17/2022 16:25:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=67
06/17/2022 16:25:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=67
06/17/2022 16:25:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=68
06/17/2022 16:25:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=68
06/17/2022 16:25:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=68
06/17/2022 16:25:19 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.6856937182693036 on epoch=68
06/17/2022 16:25:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=69
06/17/2022 16:25:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=69
06/17/2022 16:25:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
06/17/2022 16:25:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.37 on epoch=69
06/17/2022 16:25:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.33 on epoch=70
06/17/2022 16:25:39 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7200073933954865 on epoch=70
06/17/2022 16:25:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=70
06/17/2022 16:25:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=70
06/17/2022 16:25:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=71
06/17/2022 16:25:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=71
06/17/2022 16:25:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=71
06/17/2022 16:25:58 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.767803985895263 on epoch=71
06/17/2022 16:26:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=72
06/17/2022 16:26:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=72
06/17/2022 16:26:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=72
06/17/2022 16:26:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=73
06/17/2022 16:26:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=73
06/17/2022 16:26:18 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.7406801166304152 on epoch=73
06/17/2022 16:26:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.28 on epoch=73
06/17/2022 16:26:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=74
06/17/2022 16:26:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=74
06/17/2022 16:26:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=74
06/17/2022 16:26:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=74
06/17/2022 16:26:38 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.7654412592961981 on epoch=74
06/17/2022 16:26:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=75
06/17/2022 16:26:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=75
06/17/2022 16:26:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=75
06/17/2022 16:26:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
06/17/2022 16:26:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=76
06/17/2022 16:26:58 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.7555479537122693 on epoch=76
06/17/2022 16:27:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=76
06/17/2022 16:27:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=77
06/17/2022 16:27:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=77
06/17/2022 16:27:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=77
06/17/2022 16:27:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.39 on epoch=78
06/17/2022 16:27:18 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.8039763252461849 on epoch=78
06/17/2022 16:27:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
06/17/2022 16:27:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.53 on epoch=78
06/17/2022 16:27:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=79
06/17/2022 16:27:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=79
06/17/2022 16:27:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=79
06/17/2022 16:27:38 - INFO - __main__ - Global step 2550 Train loss 0.32 Classification-F1 0.7628603083687365 on epoch=79
06/17/2022 16:27:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.30 on epoch=79
06/17/2022 16:27:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=80
06/17/2022 16:27:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
06/17/2022 16:27:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=80
06/17/2022 16:27:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=81
06/17/2022 16:27:58 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.7583695407414361 on epoch=81
06/17/2022 16:28:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=81
06/17/2022 16:28:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 16:28:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=82
06/17/2022 16:28:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=82
06/17/2022 16:28:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=82
06/17/2022 16:28:18 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.7715872687646881 on epoch=82
06/17/2022 16:28:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=83
06/17/2022 16:28:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.24 on epoch=83
06/17/2022 16:28:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
06/17/2022 16:28:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=84
06/17/2022 16:28:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=84
06/17/2022 16:28:39 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.7583250995806394 on epoch=84
06/17/2022 16:28:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=84
06/17/2022 16:28:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 16:28:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=85
06/17/2022 16:28:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=85
06/17/2022 16:28:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=85
06/17/2022 16:28:59 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.7751148627154022 on epoch=85
06/17/2022 16:29:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=86
06/17/2022 16:29:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=86
06/17/2022 16:29:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
06/17/2022 16:29:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.30 on epoch=87
06/17/2022 16:29:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=87
06/17/2022 16:29:18 - INFO - __main__ - Global step 2800 Train loss 0.22 Classification-F1 0.7630211729924785 on epoch=87
06/17/2022 16:29:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=87
06/17/2022 16:29:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=88
06/17/2022 16:29:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=88
06/17/2022 16:29:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=88
06/17/2022 16:29:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=89
06/17/2022 16:29:38 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.7705713700084931 on epoch=89
06/17/2022 16:29:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=89
06/17/2022 16:29:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=89
06/17/2022 16:29:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=89
06/17/2022 16:29:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=90
06/17/2022 16:29:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=90
06/17/2022 16:29:58 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.7765634655790385 on epoch=90
06/17/2022 16:30:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=90
06/17/2022 16:30:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=91
06/17/2022 16:30:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=91
06/17/2022 16:30:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=91
06/17/2022 16:30:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.25 on epoch=92
06/17/2022 16:30:19 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.7874285970014703 on epoch=92
06/17/2022 16:30:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=92
06/17/2022 16:30:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=92
06/17/2022 16:30:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
06/17/2022 16:30:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
06/17/2022 16:30:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=93
06/17/2022 16:30:33 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:30:33 - INFO - __main__ - Printing 3 examples
06/17/2022 16:30:33 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:30:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:30:33 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:30:33 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:30:33 - INFO - __main__ - Printing 3 examples
06/17/2022 16:30:33 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:30:33 - INFO - __main__ - ['sad']
06/17/2022 16:30:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:30:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:30:34 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:30:39 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.7682993203580295 on epoch=93
06/17/2022 16:30:39 - INFO - __main__ - save last model!
06/17/2022 16:30:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 16:30:39 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 16:30:39 - INFO - __main__ - Printing 3 examples
06/17/2022 16:30:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 16:30:39 - INFO - __main__ - ['others']
06/17/2022 16:30:39 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 16:30:39 - INFO - __main__ - ['others']
06/17/2022 16:30:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 16:30:39 - INFO - __main__ - ['others']
06/17/2022 16:30:39 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:30:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:30:46 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 16:30:52 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:30:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:30:53 - INFO - __main__ - Starting training!
06/17/2022 16:32:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_21_0.4_8_predictions.txt
06/17/2022 16:32:02 - INFO - __main__ - Classification-F1 on test data: 0.4356
06/17/2022 16:32:03 - INFO - __main__ - prefix=emo_128_21, lr=0.4, bsz=8, dev_performance=0.8112700253681595, test_performance=0.4356240646668115
06/17/2022 16:32:03 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.3, bsz=8 ...
06/17/2022 16:32:03 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:32:03 - INFO - __main__ - Printing 3 examples
06/17/2022 16:32:04 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:32:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:32:04 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:32:04 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:32:04 - INFO - __main__ - Printing 3 examples
06/17/2022 16:32:04 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:32:04 - INFO - __main__ - ['sad']
06/17/2022 16:32:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:32:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:32:05 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:32:24 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:32:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:32:24 - INFO - __main__ - Starting training!
06/17/2022 16:32:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.88 on epoch=0
06/17/2022 16:32:30 - INFO - __main__ - Step 20 Global step 20 Train loss 1.59 on epoch=0
06/17/2022 16:32:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=0
06/17/2022 16:32:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=1
06/17/2022 16:32:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=1
06/17/2022 16:32:44 - INFO - __main__ - Global step 50 Train loss 1.53 Classification-F1 0.1 on epoch=1
06/17/2022 16:32:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 16:32:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=1
06/17/2022 16:32:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=2
06/17/2022 16:32:52 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=2
06/17/2022 16:32:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=2
06/17/2022 16:32:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=3
06/17/2022 16:33:04 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.1 on epoch=3
06/17/2022 16:33:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=3
06/17/2022 16:33:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=3
06/17/2022 16:33:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=4
06/17/2022 16:33:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.79 on epoch=4
06/17/2022 16:33:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.85 on epoch=4
06/17/2022 16:33:24 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.2423483450544831 on epoch=4
06/17/2022 16:33:24 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2423483450544831 on epoch=4, global_step=150
06/17/2022 16:33:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=4
06/17/2022 16:33:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=5
06/17/2022 16:33:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=5
06/17/2022 16:33:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=5
06/17/2022 16:33:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=6
06/17/2022 16:33:43 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.44269538227700655 on epoch=6
06/17/2022 16:33:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2423483450544831 -> 0.44269538227700655 on epoch=6, global_step=200
06/17/2022 16:33:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=6
06/17/2022 16:33:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=6
06/17/2022 16:33:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=7
06/17/2022 16:33:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=7
06/17/2022 16:33:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=7
06/17/2022 16:34:02 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.4854771627430047 on epoch=7
06/17/2022 16:34:02 - INFO - __main__ - Saving model with best Classification-F1: 0.44269538227700655 -> 0.4854771627430047 on epoch=7, global_step=250
06/17/2022 16:34:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.78 on epoch=8
06/17/2022 16:34:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=8
06/17/2022 16:34:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=8
06/17/2022 16:34:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.81 on epoch=9
06/17/2022 16:34:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=9
06/17/2022 16:34:22 - INFO - __main__ - Global step 300 Train loss 0.79 Classification-F1 0.37470276992355556 on epoch=9
06/17/2022 16:34:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=9
06/17/2022 16:34:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=9
06/17/2022 16:34:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=10
06/17/2022 16:34:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.79 on epoch=10
06/17/2022 16:34:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=10
06/17/2022 16:34:41 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.3353575273876602 on epoch=10
06/17/2022 16:34:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.74 on epoch=11
06/17/2022 16:34:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.68 on epoch=11
06/17/2022 16:34:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=11
06/17/2022 16:34:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=12
06/17/2022 16:34:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.69 on epoch=12
06/17/2022 16:35:00 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.48877994324467666 on epoch=12
06/17/2022 16:35:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4854771627430047 -> 0.48877994324467666 on epoch=12, global_step=400
06/17/2022 16:35:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=12
06/17/2022 16:35:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=13
06/17/2022 16:35:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.62 on epoch=13
06/17/2022 16:35:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=13
06/17/2022 16:35:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=14
06/17/2022 16:35:20 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.5265546447148228 on epoch=14
06/17/2022 16:35:20 - INFO - __main__ - Saving model with best Classification-F1: 0.48877994324467666 -> 0.5265546447148228 on epoch=14, global_step=450
06/17/2022 16:35:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=14
06/17/2022 16:35:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=14
06/17/2022 16:35:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.69 on epoch=14
06/17/2022 16:35:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=15
06/17/2022 16:35:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=15
06/17/2022 16:35:39 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.5904239460021987 on epoch=15
06/17/2022 16:35:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5265546447148228 -> 0.5904239460021987 on epoch=15, global_step=500
06/17/2022 16:35:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.68 on epoch=15
06/17/2022 16:35:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.68 on epoch=16
06/17/2022 16:35:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=16
06/17/2022 16:35:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=16
06/17/2022 16:35:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=17
06/17/2022 16:35:59 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.5477968833049078 on epoch=17
06/17/2022 16:36:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=17
06/17/2022 16:36:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=17
06/17/2022 16:36:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.63 on epoch=18
06/17/2022 16:36:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=18
06/17/2022 16:36:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=18
06/17/2022 16:36:18 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.44490852242267137 on epoch=18
06/17/2022 16:36:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=19
06/17/2022 16:36:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=19
06/17/2022 16:36:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=19
06/17/2022 16:36:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=19
06/17/2022 16:36:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=20
06/17/2022 16:36:37 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.4373978627507997 on epoch=20
06/17/2022 16:36:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=20
06/17/2022 16:36:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=20
06/17/2022 16:36:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=21
06/17/2022 16:36:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=21
06/17/2022 16:36:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.63 on epoch=21
06/17/2022 16:36:57 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.6331769071726439 on epoch=21
06/17/2022 16:36:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5904239460021987 -> 0.6331769071726439 on epoch=21, global_step=700
06/17/2022 16:36:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.68 on epoch=22
06/17/2022 16:37:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=22
06/17/2022 16:37:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=22
06/17/2022 16:37:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.56 on epoch=23
06/17/2022 16:37:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=23
06/17/2022 16:37:16 - INFO - __main__ - Global step 750 Train loss 0.55 Classification-F1 0.7015424186401106 on epoch=23
06/17/2022 16:37:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6331769071726439 -> 0.7015424186401106 on epoch=23, global_step=750
06/17/2022 16:37:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=23
06/17/2022 16:37:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=24
06/17/2022 16:37:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=24
06/17/2022 16:37:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=24
06/17/2022 16:37:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.64 on epoch=24
06/17/2022 16:37:36 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.6406739070041312 on epoch=24
06/17/2022 16:37:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=25
06/17/2022 16:37:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=25
06/17/2022 16:37:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=25
06/17/2022 16:37:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=26
06/17/2022 16:37:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=26
06/17/2022 16:37:55 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.6728304372353137 on epoch=26
06/17/2022 16:37:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=26
06/17/2022 16:38:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=27
06/17/2022 16:38:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=27
06/17/2022 16:38:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=27
06/17/2022 16:38:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.55 on epoch=28
06/17/2022 16:38:14 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.7229747325136653 on epoch=28
06/17/2022 16:38:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7015424186401106 -> 0.7229747325136653 on epoch=28, global_step=900
06/17/2022 16:38:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=28
06/17/2022 16:38:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=28
06/17/2022 16:38:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=29
06/17/2022 16:38:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=29
06/17/2022 16:38:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=29
06/17/2022 16:38:34 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.6184532504507723 on epoch=29
06/17/2022 16:38:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=29
06/17/2022 16:38:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=30
06/17/2022 16:38:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=30
06/17/2022 16:38:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=30
06/17/2022 16:38:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=31
06/17/2022 16:38:53 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.5962596580075535 on epoch=31
06/17/2022 16:38:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=31
06/17/2022 16:38:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.52 on epoch=31
06/17/2022 16:39:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=32
06/17/2022 16:39:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=32
06/17/2022 16:39:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=32
06/17/2022 16:39:13 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.6781041580318253 on epoch=32
06/17/2022 16:39:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=33
06/17/2022 16:39:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=33
06/17/2022 16:39:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=33
06/17/2022 16:39:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=34
06/17/2022 16:39:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=34
06/17/2022 16:39:32 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.7068693350260287 on epoch=34
06/17/2022 16:39:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=34
06/17/2022 16:39:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=34
06/17/2022 16:39:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=35
06/17/2022 16:39:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=35
06/17/2022 16:39:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=35
06/17/2022 16:39:51 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.7403607385772518 on epoch=35
06/17/2022 16:39:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7229747325136653 -> 0.7403607385772518 on epoch=35, global_step=1150
06/17/2022 16:39:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.51 on epoch=36
06/17/2022 16:39:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=36
06/17/2022 16:39:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=36
06/17/2022 16:40:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=37
06/17/2022 16:40:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=37
06/17/2022 16:40:11 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.6710471726462618 on epoch=37
06/17/2022 16:40:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=37
06/17/2022 16:40:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=38
06/17/2022 16:40:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=38
06/17/2022 16:40:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=38
06/17/2022 16:40:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=39
06/17/2022 16:40:30 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.7538034947658946 on epoch=39
06/17/2022 16:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7403607385772518 -> 0.7538034947658946 on epoch=39, global_step=1250
06/17/2022 16:40:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=39
06/17/2022 16:40:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=39
06/17/2022 16:40:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=39
06/17/2022 16:40:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=40
06/17/2022 16:40:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=40
06/17/2022 16:40:50 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.7311049606131572 on epoch=40
06/17/2022 16:40:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=40
06/17/2022 16:40:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=41
06/17/2022 16:40:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=41
06/17/2022 16:41:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=41
06/17/2022 16:41:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=42
06/17/2022 16:41:09 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.687861495359948 on epoch=42
06/17/2022 16:41:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=42
06/17/2022 16:41:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=42
06/17/2022 16:41:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=43
06/17/2022 16:41:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=43
06/17/2022 16:41:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=43
06/17/2022 16:41:29 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.7255874763226484 on epoch=43
06/17/2022 16:41:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=44
06/17/2022 16:41:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=44
06/17/2022 16:41:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=44
06/17/2022 16:41:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=44
06/17/2022 16:41:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=45
06/17/2022 16:41:48 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.7138021410114435 on epoch=45
06/17/2022 16:41:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=45
06/17/2022 16:41:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=45
06/17/2022 16:41:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=46
06/17/2022 16:41:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=46
06/17/2022 16:42:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=46
06/17/2022 16:42:08 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.7224626574086872 on epoch=46
06/17/2022 16:42:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=47
06/17/2022 16:42:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=47
06/17/2022 16:42:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=47
06/17/2022 16:42:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=48
06/17/2022 16:42:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=48
06/17/2022 16:42:27 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.783164522018171 on epoch=48
06/17/2022 16:42:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7538034947658946 -> 0.783164522018171 on epoch=48, global_step=1550
06/17/2022 16:42:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=48
06/17/2022 16:42:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=49
06/17/2022 16:42:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=49
06/17/2022 16:42:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=49
06/17/2022 16:42:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=49
06/17/2022 16:42:47 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.5864959066642461 on epoch=49
06/17/2022 16:42:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=50
06/17/2022 16:42:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=50
06/17/2022 16:42:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=50
06/17/2022 16:42:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=51
06/17/2022 16:43:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=51
06/17/2022 16:43:07 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.7387875404731175 on epoch=51
06/17/2022 16:43:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=51
06/17/2022 16:43:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=52
06/17/2022 16:43:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=52
06/17/2022 16:43:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=52
06/17/2022 16:43:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
06/17/2022 16:43:27 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7651378161924788 on epoch=53
06/17/2022 16:43:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.33 on epoch=53
06/17/2022 16:43:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=53
06/17/2022 16:43:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=54
06/17/2022 16:43:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=54
06/17/2022 16:43:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=54
06/17/2022 16:43:46 - INFO - __main__ - Global step 1750 Train loss 0.31 Classification-F1 0.7301827500513618 on epoch=54
06/17/2022 16:43:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=54
06/17/2022 16:43:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=55
06/17/2022 16:43:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=55
06/17/2022 16:43:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=55
06/17/2022 16:43:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=56
06/17/2022 16:44:06 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.7302315353018678 on epoch=56
06/17/2022 16:44:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=56
06/17/2022 16:44:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=56
06/17/2022 16:44:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=57
06/17/2022 16:44:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=57
06/17/2022 16:44:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=57
06/17/2022 16:44:25 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.7474273555313977 on epoch=57
06/17/2022 16:44:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=58
06/17/2022 16:44:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
06/17/2022 16:44:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=58
06/17/2022 16:44:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=59
06/17/2022 16:44:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=59
06/17/2022 16:44:45 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.7116624699783033 on epoch=59
06/17/2022 16:44:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=59
06/17/2022 16:44:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=59
06/17/2022 16:44:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
06/17/2022 16:44:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
06/17/2022 16:44:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=60
06/17/2022 16:45:04 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.7218738910603575 on epoch=60
06/17/2022 16:45:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=61
06/17/2022 16:45:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=61
06/17/2022 16:45:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=61
06/17/2022 16:45:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=62
06/17/2022 16:45:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=62
06/17/2022 16:45:24 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.7014433928021586 on epoch=62
06/17/2022 16:45:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=62
06/17/2022 16:45:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=63
06/17/2022 16:45:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=63
06/17/2022 16:45:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=63
06/17/2022 16:45:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
06/17/2022 16:45:44 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.7723941131747434 on epoch=64
06/17/2022 16:45:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=64
06/17/2022 16:45:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=64
06/17/2022 16:45:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=64
06/17/2022 16:45:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=65
06/17/2022 16:45:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=65
06/17/2022 16:46:03 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7921742111445776 on epoch=65
06/17/2022 16:46:03 - INFO - __main__ - Saving model with best Classification-F1: 0.783164522018171 -> 0.7921742111445776 on epoch=65, global_step=2100
06/17/2022 16:46:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=65
06/17/2022 16:46:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=66
06/17/2022 16:46:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=66
06/17/2022 16:46:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
06/17/2022 16:46:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.29 on epoch=67
06/17/2022 16:46:22 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.7835942811916387 on epoch=67
06/17/2022 16:46:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=67
06/17/2022 16:46:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=67
06/17/2022 16:46:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=68
06/17/2022 16:46:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=68
06/17/2022 16:46:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=68
06/17/2022 16:46:42 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.7911008220112582 on epoch=68
06/17/2022 16:46:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=69
06/17/2022 16:46:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=69
06/17/2022 16:46:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=69
06/17/2022 16:46:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=69
06/17/2022 16:46:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=70
06/17/2022 16:47:02 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.7589538370823776 on epoch=70
06/17/2022 16:47:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=70
06/17/2022 16:47:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=70
06/17/2022 16:47:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=71
06/17/2022 16:47:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=71
06/17/2022 16:47:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.30 on epoch=71
06/17/2022 16:47:21 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.732419784731497 on epoch=71
06/17/2022 16:47:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.22 on epoch=72
06/17/2022 16:47:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=72
06/17/2022 16:47:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=72
06/17/2022 16:47:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=73
06/17/2022 16:47:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=73
06/17/2022 16:47:41 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.7747383663167018 on epoch=73
06/17/2022 16:47:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
06/17/2022 16:47:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=74
06/17/2022 16:47:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=74
06/17/2022 16:47:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=74
06/17/2022 16:47:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.28 on epoch=74
06/17/2022 16:48:00 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.7601734701531697 on epoch=74
06/17/2022 16:48:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=75
06/17/2022 16:48:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=75
06/17/2022 16:48:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=75
06/17/2022 16:48:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=76
06/17/2022 16:48:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=76
06/17/2022 16:48:20 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.7382835431337735 on epoch=76
06/17/2022 16:48:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.28 on epoch=76
06/17/2022 16:48:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=77
06/17/2022 16:48:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
06/17/2022 16:48:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=77
06/17/2022 16:48:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=78
06/17/2022 16:48:39 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.7402635335628542 on epoch=78
06/17/2022 16:48:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=78
06/17/2022 16:48:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=78
06/17/2022 16:48:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=79
06/17/2022 16:48:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=79
06/17/2022 16:48:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=79
06/17/2022 16:48:59 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.7612697730038502 on epoch=79
06/17/2022 16:49:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.24 on epoch=79
06/17/2022 16:49:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=80
06/17/2022 16:49:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=80
06/17/2022 16:49:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=80
06/17/2022 16:49:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=81
06/17/2022 16:49:18 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.7800447394432357 on epoch=81
06/17/2022 16:49:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=81
06/17/2022 16:49:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=81
06/17/2022 16:49:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=82
06/17/2022 16:49:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=82
06/17/2022 16:49:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.27 on epoch=82
06/17/2022 16:49:38 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.7387013393771371 on epoch=82
06/17/2022 16:49:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=83
06/17/2022 16:49:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=83
06/17/2022 16:49:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=83
06/17/2022 16:49:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=84
06/17/2022 16:49:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.26 on epoch=84
06/17/2022 16:49:58 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.7793758105145088 on epoch=84
06/17/2022 16:50:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=84
06/17/2022 16:50:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=84
06/17/2022 16:50:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=85
06/17/2022 16:50:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=85
06/17/2022 16:50:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=85
06/17/2022 16:50:17 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.7697071207101739 on epoch=85
06/17/2022 16:50:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=86
06/17/2022 16:50:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=86
06/17/2022 16:50:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=86
06/17/2022 16:50:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=87
06/17/2022 16:50:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=87
06/17/2022 16:50:36 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.7672205123032965 on epoch=87
06/17/2022 16:50:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=87
06/17/2022 16:50:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=88
06/17/2022 16:50:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=88
06/17/2022 16:50:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=88
06/17/2022 16:50:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=89
06/17/2022 16:50:56 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.7411810577977567 on epoch=89
06/17/2022 16:50:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
06/17/2022 16:51:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=89
06/17/2022 16:51:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=89
06/17/2022 16:51:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=90
06/17/2022 16:51:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.30 on epoch=90
06/17/2022 16:51:15 - INFO - __main__ - Global step 2900 Train loss 0.26 Classification-F1 0.7626802336936913 on epoch=90
06/17/2022 16:51:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.31 on epoch=90
06/17/2022 16:51:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=91
06/17/2022 16:51:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
06/17/2022 16:51:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.22 on epoch=91
06/17/2022 16:51:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.26 on epoch=92
06/17/2022 16:51:35 - INFO - __main__ - Global step 2950 Train loss 0.24 Classification-F1 0.7170449980433375 on epoch=92
06/17/2022 16:51:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=92
06/17/2022 16:51:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=92
06/17/2022 16:51:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=93
06/17/2022 16:51:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=93
06/17/2022 16:51:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=93
06/17/2022 16:51:49 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:51:49 - INFO - __main__ - Printing 3 examples
06/17/2022 16:51:49 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:51:49 - INFO - __main__ - ['sad']
06/17/2022 16:51:49 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:51:49 - INFO - __main__ - ['sad']
06/17/2022 16:51:49 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:51:49 - INFO - __main__ - ['sad']
06/17/2022 16:51:49 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:51:49 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:51:50 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:51:50 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:51:50 - INFO - __main__ - Printing 3 examples
06/17/2022 16:51:50 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:51:50 - INFO - __main__ - ['sad']
06/17/2022 16:51:50 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:51:50 - INFO - __main__ - ['sad']
06/17/2022 16:51:50 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:51:50 - INFO - __main__ - ['sad']
06/17/2022 16:51:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:51:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:51:51 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:51:55 - INFO - __main__ - Global step 3000 Train loss 0.24 Classification-F1 0.7037230492875551 on epoch=93
06/17/2022 16:51:55 - INFO - __main__ - save last model!
06/17/2022 16:51:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 16:51:55 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 16:51:55 - INFO - __main__ - Printing 3 examples
06/17/2022 16:51:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 16:51:55 - INFO - __main__ - ['others']
06/17/2022 16:51:55 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 16:51:55 - INFO - __main__ - ['others']
06/17/2022 16:51:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 16:51:55 - INFO - __main__ - ['others']
06/17/2022 16:51:55 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:51:57 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:52:03 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 16:52:08 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:52:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:52:09 - INFO - __main__ - Starting training!
06/17/2022 16:53:15 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_21_0.3_8_predictions.txt
06/17/2022 16:53:15 - INFO - __main__ - Classification-F1 on test data: 0.3760
06/17/2022 16:53:16 - INFO - __main__ - prefix=emo_128_21, lr=0.3, bsz=8, dev_performance=0.7921742111445776, test_performance=0.376033715745439
06/17/2022 16:53:16 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.2, bsz=8 ...
06/17/2022 16:53:17 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:53:17 - INFO - __main__ - Printing 3 examples
06/17/2022 16:53:17 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:53:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:53:17 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 16:53:17 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 16:53:17 - INFO - __main__ - Printing 3 examples
06/17/2022 16:53:17 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
06/17/2022 16:53:17 - INFO - __main__ - ['sad']
06/17/2022 16:53:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 16:53:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 16:53:18 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 16:53:37 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 16:53:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 16:53:38 - INFO - __main__ - Starting training!
06/17/2022 16:53:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.25 on epoch=0
06/17/2022 16:53:43 - INFO - __main__ - Step 20 Global step 20 Train loss 1.90 on epoch=0
06/17/2022 16:53:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=0
06/17/2022 16:53:48 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=1
06/17/2022 16:53:51 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=1
06/17/2022 16:53:58 - INFO - __main__ - Global step 50 Train loss 1.73 Classification-F1 0.1 on epoch=1
06/17/2022 16:53:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 16:54:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=1
06/17/2022 16:54:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
06/17/2022 16:54:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=2
06/17/2022 16:54:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=2
06/17/2022 16:54:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=3
06/17/2022 16:54:17 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.25472454062405775 on epoch=3
06/17/2022 16:54:17 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.25472454062405775 on epoch=3, global_step=100
06/17/2022 16:54:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=3
06/17/2022 16:54:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=3
06/17/2022 16:54:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=4
06/17/2022 16:54:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=4
06/17/2022 16:54:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=4
06/17/2022 16:54:37 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.2916002014767042 on epoch=4
06/17/2022 16:54:37 - INFO - __main__ - Saving model with best Classification-F1: 0.25472454062405775 -> 0.2916002014767042 on epoch=4, global_step=150
06/17/2022 16:54:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=4
06/17/2022 16:54:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=5
06/17/2022 16:54:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=5
06/17/2022 16:54:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=5
06/17/2022 16:54:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=6
06/17/2022 16:54:56 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.4480660853665193 on epoch=6
06/17/2022 16:54:56 - INFO - __main__ - Saving model with best Classification-F1: 0.2916002014767042 -> 0.4480660853665193 on epoch=6, global_step=200
06/17/2022 16:54:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=6
06/17/2022 16:55:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=6
06/17/2022 16:55:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=7
06/17/2022 16:55:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=7
06/17/2022 16:55:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=7
06/17/2022 16:55:16 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.5537435076848424 on epoch=7
06/17/2022 16:55:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4480660853665193 -> 0.5537435076848424 on epoch=7, global_step=250
06/17/2022 16:55:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.73 on epoch=8
06/17/2022 16:55:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=8
06/17/2022 16:55:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=8
06/17/2022 16:55:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=9
06/17/2022 16:55:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=9
06/17/2022 16:55:35 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.45907885247936175 on epoch=9
06/17/2022 16:55:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=9
06/17/2022 16:55:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=9
06/17/2022 16:55:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=10
06/17/2022 16:55:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=10
06/17/2022 16:55:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=10
06/17/2022 16:55:54 - INFO - __main__ - Global step 350 Train loss 0.78 Classification-F1 0.5098934362930299 on epoch=10
06/17/2022 16:55:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=11
06/17/2022 16:55:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=11
06/17/2022 16:56:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=11
06/17/2022 16:56:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=12
06/17/2022 16:56:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=12
06/17/2022 16:56:14 - INFO - __main__ - Global step 400 Train loss 0.69 Classification-F1 0.45099379818252294 on epoch=12
06/17/2022 16:56:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=12
06/17/2022 16:56:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=13
06/17/2022 16:56:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=13
06/17/2022 16:56:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.60 on epoch=13
06/17/2022 16:56:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=14
06/17/2022 16:56:33 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.6396702811516767 on epoch=14
06/17/2022 16:56:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5537435076848424 -> 0.6396702811516767 on epoch=14, global_step=450
06/17/2022 16:56:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=14
06/17/2022 16:56:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=14
06/17/2022 16:56:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.62 on epoch=14
06/17/2022 16:56:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.65 on epoch=15
06/17/2022 16:56:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.66 on epoch=15
06/17/2022 16:56:53 - INFO - __main__ - Global step 500 Train loss 0.63 Classification-F1 0.6095387909983597 on epoch=15
06/17/2022 16:56:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=15
06/17/2022 16:56:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=16
06/17/2022 16:57:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=16
06/17/2022 16:57:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=16
06/17/2022 16:57:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=17
06/17/2022 16:57:12 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6050238160145844 on epoch=17
06/17/2022 16:57:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=17
06/17/2022 16:57:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=17
06/17/2022 16:57:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=18
06/17/2022 16:57:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=18
06/17/2022 16:57:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=18
06/17/2022 16:57:32 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.5086079469346588 on epoch=18
06/17/2022 16:57:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.65 on epoch=19
06/17/2022 16:57:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=19
06/17/2022 16:57:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=19
06/17/2022 16:57:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=19
06/17/2022 16:57:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=20
06/17/2022 16:57:51 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.6959511999793331 on epoch=20
06/17/2022 16:57:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6396702811516767 -> 0.6959511999793331 on epoch=20, global_step=650
06/17/2022 16:57:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=20
06/17/2022 16:57:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=20
06/17/2022 16:57:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=21
06/17/2022 16:58:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=21
06/17/2022 16:58:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.74 on epoch=21
06/17/2022 16:58:11 - INFO - __main__ - Global step 700 Train loss 0.59 Classification-F1 0.6465670532354575 on epoch=21
06/17/2022 16:58:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=22
06/17/2022 16:58:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=22
06/17/2022 16:58:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=22
06/17/2022 16:58:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=23
06/17/2022 16:58:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=23
06/17/2022 16:58:30 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.6615405218860422 on epoch=23
06/17/2022 16:58:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=23
06/17/2022 16:58:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=24
06/17/2022 16:58:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=24
06/17/2022 16:58:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=24
06/17/2022 16:58:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=24
06/17/2022 16:58:50 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.6507743982912473 on epoch=24
06/17/2022 16:58:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=25
06/17/2022 16:58:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=25
06/17/2022 16:58:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=25
06/17/2022 16:59:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=26
06/17/2022 16:59:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=26
06/17/2022 16:59:09 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.6876800652387662 on epoch=26
06/17/2022 16:59:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.52 on epoch=26
06/17/2022 16:59:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=27
06/17/2022 16:59:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=27
06/17/2022 16:59:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=27
06/17/2022 16:59:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=28
06/17/2022 16:59:29 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.7154506857814031 on epoch=28
06/17/2022 16:59:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6959511999793331 -> 0.7154506857814031 on epoch=28, global_step=900
06/17/2022 16:59:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=28
06/17/2022 16:59:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=28
06/17/2022 16:59:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=29
06/17/2022 16:59:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=29
06/17/2022 16:59:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=29
06/17/2022 16:59:48 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.693443723217702 on epoch=29
06/17/2022 16:59:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=29
06/17/2022 16:59:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=30
06/17/2022 16:59:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=30
06/17/2022 16:59:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=30
06/17/2022 17:00:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=31
06/17/2022 17:00:08 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.6376816908581613 on epoch=31
06/17/2022 17:00:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=31
06/17/2022 17:00:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=31
06/17/2022 17:00:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.52 on epoch=32
06/17/2022 17:00:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=32
06/17/2022 17:00:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=32
06/17/2022 17:00:27 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.7082371752760342 on epoch=32
06/17/2022 17:00:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=33
06/17/2022 17:00:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.56 on epoch=33
06/17/2022 17:00:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=33
06/17/2022 17:00:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=34
06/17/2022 17:00:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=34
06/17/2022 17:00:47 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.6907626842149067 on epoch=34
06/17/2022 17:00:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=34
06/17/2022 17:00:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=34
06/17/2022 17:00:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.50 on epoch=35
06/17/2022 17:00:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=35
06/17/2022 17:00:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=35
06/17/2022 17:01:06 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.6774905599651482 on epoch=35
06/17/2022 17:01:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=36
06/17/2022 17:01:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=36
06/17/2022 17:01:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=36
06/17/2022 17:01:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=37
06/17/2022 17:01:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=37
06/17/2022 17:01:26 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.7021732451248144 on epoch=37
06/17/2022 17:01:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=37
06/17/2022 17:01:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=38
06/17/2022 17:01:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=38
06/17/2022 17:01:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=38
06/17/2022 17:01:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=39
06/17/2022 17:01:46 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.7240951622112425 on epoch=39
06/17/2022 17:01:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7154506857814031 -> 0.7240951622112425 on epoch=39, global_step=1250
06/17/2022 17:01:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=39
06/17/2022 17:01:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=39
06/17/2022 17:01:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=39
06/17/2022 17:01:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=40
06/17/2022 17:01:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=40
06/17/2022 17:02:05 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.7030738117185729 on epoch=40
06/17/2022 17:02:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=40
06/17/2022 17:02:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=41
06/17/2022 17:02:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=41
06/17/2022 17:02:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=41
06/17/2022 17:02:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=42
06/17/2022 17:02:24 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.7382225595109911 on epoch=42
06/17/2022 17:02:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7240951622112425 -> 0.7382225595109911 on epoch=42, global_step=1350
06/17/2022 17:02:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=42
06/17/2022 17:02:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
06/17/2022 17:02:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=43
06/17/2022 17:02:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=43
06/17/2022 17:02:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=43
06/17/2022 17:02:44 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.7160117966472898 on epoch=43
06/17/2022 17:02:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.59 on epoch=44
06/17/2022 17:02:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=44
06/17/2022 17:02:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=44
06/17/2022 17:02:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=44
06/17/2022 17:02:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=45
06/17/2022 17:03:04 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.7398258964748408 on epoch=45
06/17/2022 17:03:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7382225595109911 -> 0.7398258964748408 on epoch=45, global_step=1450
06/17/2022 17:03:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=45
06/17/2022 17:03:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=45
06/17/2022 17:03:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=46
06/17/2022 17:03:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=46
06/17/2022 17:03:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=46
06/17/2022 17:03:23 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.7366050191334139 on epoch=46
06/17/2022 17:03:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=47
06/17/2022 17:03:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=47
06/17/2022 17:03:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=47
06/17/2022 17:03:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=48
06/17/2022 17:03:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=48
06/17/2022 17:03:43 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.7553264856602022 on epoch=48
06/17/2022 17:03:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7398258964748408 -> 0.7553264856602022 on epoch=48, global_step=1550
06/17/2022 17:03:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=48
06/17/2022 17:03:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=49
06/17/2022 17:03:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=49
06/17/2022 17:03:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=49
06/17/2022 17:03:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.54 on epoch=49
06/17/2022 17:04:02 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.745186403508772 on epoch=49
06/17/2022 17:04:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=50
06/17/2022 17:04:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=50
06/17/2022 17:04:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=50
06/17/2022 17:04:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=51
06/17/2022 17:04:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=51
06/17/2022 17:04:22 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.7542380857124173 on epoch=51
06/17/2022 17:04:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=51
06/17/2022 17:04:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=52
06/17/2022 17:04:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=52
06/17/2022 17:04:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=52
06/17/2022 17:04:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=53
06/17/2022 17:04:41 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.7666636974497746 on epoch=53
06/17/2022 17:04:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7553264856602022 -> 0.7666636974497746 on epoch=53, global_step=1700
06/17/2022 17:04:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=53
06/17/2022 17:04:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=53
06/17/2022 17:04:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=54
06/17/2022 17:04:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=54
06/17/2022 17:04:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=54
06/17/2022 17:05:01 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.7308791295482686 on epoch=54
06/17/2022 17:05:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=54
06/17/2022 17:05:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=55
06/17/2022 17:05:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=55
06/17/2022 17:05:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=55
06/17/2022 17:05:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=56
06/17/2022 17:05:21 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.7545445588920354 on epoch=56
06/17/2022 17:05:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=56
06/17/2022 17:05:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=56
06/17/2022 17:05:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=57
06/17/2022 17:05:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=57
06/17/2022 17:05:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=57
06/17/2022 17:05:40 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.7700226190398269 on epoch=57
06/17/2022 17:05:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7666636974497746 -> 0.7700226190398269 on epoch=57, global_step=1850
06/17/2022 17:05:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=58
06/17/2022 17:05:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=58
06/17/2022 17:05:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=58
06/17/2022 17:05:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=59
06/17/2022 17:05:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=59
06/17/2022 17:06:00 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.7269475697569757 on epoch=59
06/17/2022 17:06:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=59
06/17/2022 17:06:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=59
06/17/2022 17:06:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=60
06/17/2022 17:06:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=60
06/17/2022 17:06:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=60
06/17/2022 17:06:19 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.7701120660560692 on epoch=60
06/17/2022 17:06:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7700226190398269 -> 0.7701120660560692 on epoch=60, global_step=1950
06/17/2022 17:06:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=61
06/17/2022 17:06:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=61
06/17/2022 17:06:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=61
06/17/2022 17:06:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=62
06/17/2022 17:06:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=62
06/17/2022 17:06:39 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.736639067203323 on epoch=62
06/17/2022 17:06:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=62
06/17/2022 17:06:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=63
06/17/2022 17:06:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=63
06/17/2022 17:06:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.39 on epoch=63
06/17/2022 17:06:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=64
06/17/2022 17:06:58 - INFO - __main__ - Global step 2050 Train loss 0.34 Classification-F1 0.7472548377630674 on epoch=64
06/17/2022 17:07:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
06/17/2022 17:07:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=64
06/17/2022 17:07:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=64
06/17/2022 17:07:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=65
06/17/2022 17:07:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=65
06/17/2022 17:07:18 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.7256474774631075 on epoch=65
06/17/2022 17:07:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.34 on epoch=65
06/17/2022 17:07:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=66
06/17/2022 17:07:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=66
06/17/2022 17:07:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.35 on epoch=66
06/17/2022 17:07:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=67
06/17/2022 17:07:37 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.7586037336359519 on epoch=67
06/17/2022 17:07:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=67
06/17/2022 17:07:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=67
06/17/2022 17:07:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.34 on epoch=68
06/17/2022 17:07:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=68
06/17/2022 17:07:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.32 on epoch=68
06/17/2022 17:07:57 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.7563841982796181 on epoch=68
06/17/2022 17:07:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=69
06/17/2022 17:08:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.31 on epoch=69
06/17/2022 17:08:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
06/17/2022 17:08:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.36 on epoch=69
06/17/2022 17:08:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=70
06/17/2022 17:08:16 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.7425501657104059 on epoch=70
06/17/2022 17:08:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=70
06/17/2022 17:08:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=70
06/17/2022 17:08:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=71
06/17/2022 17:08:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=71
06/17/2022 17:08:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.32 on epoch=71
06/17/2022 17:08:36 - INFO - __main__ - Global step 2300 Train loss 0.34 Classification-F1 0.7570677667562807 on epoch=71
06/17/2022 17:08:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=72
06/17/2022 17:08:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=72
06/17/2022 17:08:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=72
06/17/2022 17:08:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=73
06/17/2022 17:08:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=73
06/17/2022 17:08:55 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.7736900420155232 on epoch=73
06/17/2022 17:08:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7701120660560692 -> 0.7736900420155232 on epoch=73, global_step=2350
06/17/2022 17:08:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=73
06/17/2022 17:09:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=74
06/17/2022 17:09:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=74
06/17/2022 17:09:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=74
06/17/2022 17:09:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=74
06/17/2022 17:09:15 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.7046301802514041 on epoch=74
06/17/2022 17:09:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=75
06/17/2022 17:09:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=75
06/17/2022 17:09:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=75
06/17/2022 17:09:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.28 on epoch=76
06/17/2022 17:09:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=76
06/17/2022 17:09:35 - INFO - __main__ - Global step 2450 Train loss 0.29 Classification-F1 0.7409343562310017 on epoch=76
06/17/2022 17:09:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.33 on epoch=76
06/17/2022 17:09:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.36 on epoch=77
06/17/2022 17:09:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=77
06/17/2022 17:09:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.33 on epoch=77
06/17/2022 17:09:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.32 on epoch=78
06/17/2022 17:09:54 - INFO - __main__ - Global step 2500 Train loss 0.31 Classification-F1 0.8039302799929365 on epoch=78
06/17/2022 17:09:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7736900420155232 -> 0.8039302799929365 on epoch=78, global_step=2500
06/17/2022 17:09:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
06/17/2022 17:09:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=78
06/17/2022 17:10:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=79
06/17/2022 17:10:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=79
06/17/2022 17:10:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.27 on epoch=79
06/17/2022 17:10:14 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.790092064162227 on epoch=79
06/17/2022 17:10:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=79
06/17/2022 17:10:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=80
06/17/2022 17:10:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=80
06/17/2022 17:10:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=80
06/17/2022 17:10:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.26 on epoch=81
06/17/2022 17:10:33 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.7767527539234618 on epoch=81
06/17/2022 17:10:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=81
06/17/2022 17:10:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.30 on epoch=81
06/17/2022 17:10:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=82
06/17/2022 17:10:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.31 on epoch=82
06/17/2022 17:10:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=82
06/17/2022 17:10:53 - INFO - __main__ - Global step 2650 Train loss 0.33 Classification-F1 0.7621527777777778 on epoch=82
06/17/2022 17:10:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=83
06/17/2022 17:10:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.24 on epoch=83
06/17/2022 17:11:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=83
06/17/2022 17:11:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=84
06/17/2022 17:11:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.22 on epoch=84
06/17/2022 17:11:12 - INFO - __main__ - Global step 2700 Train loss 0.27 Classification-F1 0.7920215475005795 on epoch=84
06/17/2022 17:11:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=84
06/17/2022 17:11:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=84
06/17/2022 17:11:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=85
06/17/2022 17:11:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=85
06/17/2022 17:11:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=85
06/17/2022 17:11:32 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.7830251461703616 on epoch=85
06/17/2022 17:11:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=86
06/17/2022 17:11:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=86
06/17/2022 17:11:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
06/17/2022 17:11:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=87
06/17/2022 17:11:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=87
06/17/2022 17:11:51 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.7383846791457124 on epoch=87
06/17/2022 17:11:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.33 on epoch=87
06/17/2022 17:11:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.30 on epoch=88
06/17/2022 17:11:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=88
06/17/2022 17:12:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.24 on epoch=88
06/17/2022 17:12:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.24 on epoch=89
06/17/2022 17:12:11 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.7668032680441024 on epoch=89
06/17/2022 17:12:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=89
06/17/2022 17:12:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=89
06/17/2022 17:12:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.31 on epoch=89
06/17/2022 17:12:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=90
06/17/2022 17:12:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=90
06/17/2022 17:12:30 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.7667708464820377 on epoch=90
06/17/2022 17:12:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=90
06/17/2022 17:12:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=91
06/17/2022 17:12:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.22 on epoch=91
06/17/2022 17:12:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.26 on epoch=91
06/17/2022 17:12:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.27 on epoch=92
06/17/2022 17:12:50 - INFO - __main__ - Global step 2950 Train loss 0.25 Classification-F1 0.7575643509470258 on epoch=92
06/17/2022 17:12:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=92
06/17/2022 17:12:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=92
06/17/2022 17:12:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=93
06/17/2022 17:13:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
06/17/2022 17:13:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.29 on epoch=93
06/17/2022 17:13:04 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:13:04 - INFO - __main__ - Printing 3 examples
06/17/2022 17:13:04 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:13:04 - INFO - __main__ - ['happy']
06/17/2022 17:13:04 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:13:04 - INFO - __main__ - ['happy']
06/17/2022 17:13:04 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:13:04 - INFO - __main__ - ['happy']
06/17/2022 17:13:04 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:13:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:13:05 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:13:05 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:13:05 - INFO - __main__ - Printing 3 examples
06/17/2022 17:13:05 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:13:05 - INFO - __main__ - ['happy']
06/17/2022 17:13:05 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:13:05 - INFO - __main__ - ['happy']
06/17/2022 17:13:05 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:13:05 - INFO - __main__ - ['happy']
06/17/2022 17:13:05 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:13:05 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:13:05 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:13:10 - INFO - __main__ - Global step 3000 Train loss 0.22 Classification-F1 0.7761247028377117 on epoch=93
06/17/2022 17:13:10 - INFO - __main__ - save last model!
06/17/2022 17:13:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:13:10 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 17:13:10 - INFO - __main__ - Printing 3 examples
06/17/2022 17:13:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 17:13:10 - INFO - __main__ - ['others']
06/17/2022 17:13:10 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 17:13:10 - INFO - __main__ - ['others']
06/17/2022 17:13:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 17:13:10 - INFO - __main__ - ['others']
06/17/2022 17:13:10 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:13:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:13:17 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 17:13:22 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:13:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:13:22 - INFO - __main__ - Starting training!
06/17/2022 17:14:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_21_0.2_8_predictions.txt
06/17/2022 17:14:30 - INFO - __main__ - Classification-F1 on test data: 0.4505
06/17/2022 17:14:30 - INFO - __main__ - prefix=emo_128_21, lr=0.2, bsz=8, dev_performance=0.8039302799929365, test_performance=0.4504585720322259
06/17/2022 17:14:30 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.5, bsz=8 ...
06/17/2022 17:14:31 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:14:31 - INFO - __main__ - Printing 3 examples
06/17/2022 17:14:31 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:14:31 - INFO - __main__ - ['happy']
06/17/2022 17:14:31 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:14:31 - INFO - __main__ - ['happy']
06/17/2022 17:14:31 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:14:31 - INFO - __main__ - ['happy']
06/17/2022 17:14:31 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:14:31 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:14:32 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:14:32 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:14:32 - INFO - __main__ - Printing 3 examples
06/17/2022 17:14:32 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:14:32 - INFO - __main__ - ['happy']
06/17/2022 17:14:32 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:14:32 - INFO - __main__ - ['happy']
06/17/2022 17:14:32 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:14:32 - INFO - __main__ - ['happy']
06/17/2022 17:14:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:14:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:14:33 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:14:51 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:14:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:14:52 - INFO - __main__ - Starting training!
06/17/2022 17:14:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.36 on epoch=0
06/17/2022 17:14:58 - INFO - __main__ - Step 20 Global step 20 Train loss 1.29 on epoch=0
06/17/2022 17:15:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=0
06/17/2022 17:15:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=1
06/17/2022 17:15:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=1
06/17/2022 17:15:12 - INFO - __main__ - Global step 50 Train loss 1.34 Classification-F1 0.11578044596912522 on epoch=1
06/17/2022 17:15:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11578044596912522 on epoch=1, global_step=50
06/17/2022 17:15:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.93 on epoch=1
06/17/2022 17:15:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=2
06/17/2022 17:15:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=2
06/17/2022 17:15:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=2
06/17/2022 17:15:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=3
06/17/2022 17:15:32 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.2743715012722646 on epoch=3
06/17/2022 17:15:32 - INFO - __main__ - Saving model with best Classification-F1: 0.11578044596912522 -> 0.2743715012722646 on epoch=3, global_step=100
06/17/2022 17:15:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=3
06/17/2022 17:15:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=3
06/17/2022 17:15:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=4
06/17/2022 17:15:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=4
06/17/2022 17:15:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=4
06/17/2022 17:15:51 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.39312198790220765 on epoch=4
06/17/2022 17:15:51 - INFO - __main__ - Saving model with best Classification-F1: 0.2743715012722646 -> 0.39312198790220765 on epoch=4, global_step=150
06/17/2022 17:15:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=4
06/17/2022 17:15:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=5
06/17/2022 17:15:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=5
06/17/2022 17:16:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=5
06/17/2022 17:16:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
06/17/2022 17:16:11 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.46831704904183896 on epoch=6
06/17/2022 17:16:11 - INFO - __main__ - Saving model with best Classification-F1: 0.39312198790220765 -> 0.46831704904183896 on epoch=6, global_step=200
06/17/2022 17:16:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=6
06/17/2022 17:16:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=6
06/17/2022 17:16:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=7
06/17/2022 17:16:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=7
06/17/2022 17:16:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=7
06/17/2022 17:16:31 - INFO - __main__ - Global step 250 Train loss 0.74 Classification-F1 0.5157209871279633 on epoch=7
06/17/2022 17:16:31 - INFO - __main__ - Saving model with best Classification-F1: 0.46831704904183896 -> 0.5157209871279633 on epoch=7, global_step=250
06/17/2022 17:16:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.74 on epoch=8
06/17/2022 17:16:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=8
06/17/2022 17:16:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.66 on epoch=8
06/17/2022 17:16:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=9
06/17/2022 17:16:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=9
06/17/2022 17:16:50 - INFO - __main__ - Global step 300 Train loss 0.67 Classification-F1 0.6396733477501536 on epoch=9
06/17/2022 17:16:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5157209871279633 -> 0.6396733477501536 on epoch=9, global_step=300
06/17/2022 17:16:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=9
06/17/2022 17:16:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.61 on epoch=9
06/17/2022 17:16:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.77 on epoch=10
06/17/2022 17:17:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
06/17/2022 17:17:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=10
06/17/2022 17:17:10 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.7730881458463797 on epoch=10
06/17/2022 17:17:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6396733477501536 -> 0.7730881458463797 on epoch=10, global_step=350
06/17/2022 17:17:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=11
06/17/2022 17:17:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=11
06/17/2022 17:17:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=11
06/17/2022 17:17:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=12
06/17/2022 17:17:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.57 on epoch=12
06/17/2022 17:17:29 - INFO - __main__ - Global step 400 Train loss 0.59 Classification-F1 0.7336325806662201 on epoch=12
06/17/2022 17:17:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=12
06/17/2022 17:17:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=13
06/17/2022 17:17:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=13
06/17/2022 17:17:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=13
06/17/2022 17:17:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=14
06/17/2022 17:17:49 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.7889717947995061 on epoch=14
06/17/2022 17:17:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7730881458463797 -> 0.7889717947995061 on epoch=14, global_step=450
06/17/2022 17:17:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=14
06/17/2022 17:17:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=14
06/17/2022 17:17:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=14
06/17/2022 17:17:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=15
06/17/2022 17:18:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=15
06/17/2022 17:18:08 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.7267249691137192 on epoch=15
06/17/2022 17:18:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=15
06/17/2022 17:18:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=16
06/17/2022 17:18:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=16
06/17/2022 17:18:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=16
06/17/2022 17:18:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=17
06/17/2022 17:18:28 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.7172171568474557 on epoch=17
06/17/2022 17:18:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=17
06/17/2022 17:18:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=17
06/17/2022 17:18:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=18
06/17/2022 17:18:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=18
06/17/2022 17:18:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=18
06/17/2022 17:18:48 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.7276437847866418 on epoch=18
06/17/2022 17:18:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=19
06/17/2022 17:18:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=19
06/17/2022 17:18:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
06/17/2022 17:18:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=19
06/17/2022 17:19:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=20
06/17/2022 17:19:07 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.6914705218215539 on epoch=20
06/17/2022 17:19:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=20
06/17/2022 17:19:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=20
06/17/2022 17:19:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=21
06/17/2022 17:19:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=21
06/17/2022 17:19:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=21
06/17/2022 17:19:27 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.7957946302720833 on epoch=21
06/17/2022 17:19:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7889717947995061 -> 0.7957946302720833 on epoch=21, global_step=700
06/17/2022 17:19:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=22
06/17/2022 17:19:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=22
06/17/2022 17:19:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=22
06/17/2022 17:19:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=23
06/17/2022 17:19:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=23
06/17/2022 17:19:47 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.823734720459054 on epoch=23
06/17/2022 17:19:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7957946302720833 -> 0.823734720459054 on epoch=23, global_step=750
06/17/2022 17:19:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=23
06/17/2022 17:19:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=24
06/17/2022 17:19:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=24
06/17/2022 17:19:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=24
06/17/2022 17:19:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=24
06/17/2022 17:20:06 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.7938015584689248 on epoch=24
06/17/2022 17:20:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=25
06/17/2022 17:20:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=25
06/17/2022 17:20:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
06/17/2022 17:20:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
06/17/2022 17:20:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=26
06/17/2022 17:20:26 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.7830556292512815 on epoch=26
06/17/2022 17:20:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=26
06/17/2022 17:20:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=27
06/17/2022 17:20:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=27
06/17/2022 17:20:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=27
06/17/2022 17:20:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=28
06/17/2022 17:20:45 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.7965944673686755 on epoch=28
06/17/2022 17:20:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=28
06/17/2022 17:20:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=28
06/17/2022 17:20:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=29
06/17/2022 17:20:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=29
06/17/2022 17:20:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
06/17/2022 17:21:04 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.8215811327289333 on epoch=29
06/17/2022 17:21:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=29
06/17/2022 17:21:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=30
06/17/2022 17:21:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=30
06/17/2022 17:21:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=30
06/17/2022 17:21:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=31
06/17/2022 17:21:23 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.8255349389522851 on epoch=31
06/17/2022 17:21:23 - INFO - __main__ - Saving model with best Classification-F1: 0.823734720459054 -> 0.8255349389522851 on epoch=31, global_step=1000
06/17/2022 17:21:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=31
06/17/2022 17:21:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=31
06/17/2022 17:21:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=32
06/17/2022 17:21:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=32
06/17/2022 17:21:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=32
06/17/2022 17:21:42 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.7322021968764896 on epoch=32
06/17/2022 17:21:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=33
06/17/2022 17:21:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=33
06/17/2022 17:21:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
06/17/2022 17:21:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=34
06/17/2022 17:21:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=34
06/17/2022 17:22:01 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.8358276759561974 on epoch=34
06/17/2022 17:22:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8255349389522851 -> 0.8358276759561974 on epoch=34, global_step=1100
06/17/2022 17:22:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=34
06/17/2022 17:22:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=34
06/17/2022 17:22:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=35
06/17/2022 17:22:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=35
06/17/2022 17:22:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=35
06/17/2022 17:22:20 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.7917460608445263 on epoch=35
06/17/2022 17:22:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
06/17/2022 17:22:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=36
06/17/2022 17:22:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=36
06/17/2022 17:22:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=37
06/17/2022 17:22:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=37
06/17/2022 17:22:39 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.8373085384195107 on epoch=37
06/17/2022 17:22:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8358276759561974 -> 0.8373085384195107 on epoch=37, global_step=1200
06/17/2022 17:22:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=37
06/17/2022 17:22:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=38
06/17/2022 17:22:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=38
06/17/2022 17:22:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
06/17/2022 17:22:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=39
06/17/2022 17:22:58 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.8414407422710005 on epoch=39
06/17/2022 17:22:58 - INFO - __main__ - Saving model with best Classification-F1: 0.8373085384195107 -> 0.8414407422710005 on epoch=39, global_step=1250
06/17/2022 17:23:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=39
06/17/2022 17:23:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=39
06/17/2022 17:23:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=39
06/17/2022 17:23:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=40
06/17/2022 17:23:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=40
06/17/2022 17:23:17 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.8391173549197548 on epoch=40
06/17/2022 17:23:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=40
06/17/2022 17:23:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
06/17/2022 17:23:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=41
06/17/2022 17:23:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=41
06/17/2022 17:23:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=42
06/17/2022 17:23:36 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.82020392525054 on epoch=42
06/17/2022 17:23:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=42
06/17/2022 17:23:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=42
06/17/2022 17:23:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=43
06/17/2022 17:23:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=43
06/17/2022 17:23:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=43
06/17/2022 17:23:55 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.7153892808582153 on epoch=43
06/17/2022 17:23:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=44
06/17/2022 17:24:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=44
06/17/2022 17:24:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=44
06/17/2022 17:24:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=44
06/17/2022 17:24:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=45
06/17/2022 17:24:15 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.8177157412377798 on epoch=45
06/17/2022 17:24:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=45
06/17/2022 17:24:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=45
06/17/2022 17:24:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=46
06/17/2022 17:24:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=46
06/17/2022 17:24:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=46
06/17/2022 17:24:34 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.8263620589936379 on epoch=46
06/17/2022 17:24:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=47
06/17/2022 17:24:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=47
06/17/2022 17:24:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=47
06/17/2022 17:24:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=48
06/17/2022 17:24:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=48
06/17/2022 17:24:53 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.841902320622515 on epoch=48
06/17/2022 17:24:53 - INFO - __main__ - Saving model with best Classification-F1: 0.8414407422710005 -> 0.841902320622515 on epoch=48, global_step=1550
06/17/2022 17:24:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=48
06/17/2022 17:24:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=49
06/17/2022 17:25:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=49
06/17/2022 17:25:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=49
06/17/2022 17:25:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=49
06/17/2022 17:25:12 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.820300619175052 on epoch=49
06/17/2022 17:25:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=50
06/17/2022 17:25:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=50
06/17/2022 17:25:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=50
06/17/2022 17:25:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=51
06/17/2022 17:25:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=51
06/17/2022 17:25:31 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.776462753484464 on epoch=51
06/17/2022 17:25:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=51
06/17/2022 17:25:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=52
06/17/2022 17:25:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=52
06/17/2022 17:25:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=52
06/17/2022 17:25:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
06/17/2022 17:25:51 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.784392227152801 on epoch=53
06/17/2022 17:25:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=53
06/17/2022 17:25:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=53
06/17/2022 17:25:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=54
06/17/2022 17:26:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=54
06/17/2022 17:26:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=54
06/17/2022 17:26:10 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.8022156230464386 on epoch=54
06/17/2022 17:26:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=54
06/17/2022 17:26:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=55
06/17/2022 17:26:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=55
06/17/2022 17:26:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=55
06/17/2022 17:26:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=56
06/17/2022 17:26:29 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.8095218816271449 on epoch=56
06/17/2022 17:26:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=56
06/17/2022 17:26:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
06/17/2022 17:26:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=57
06/17/2022 17:26:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=57
06/17/2022 17:26:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=57
06/17/2022 17:26:49 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7870920436318711 on epoch=57
06/17/2022 17:26:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=58
06/17/2022 17:26:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=58
06/17/2022 17:26:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=58
06/17/2022 17:26:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=59
06/17/2022 17:27:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=59
06/17/2022 17:27:08 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.8115593205910655 on epoch=59
06/17/2022 17:27:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=59
06/17/2022 17:27:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=59
06/17/2022 17:27:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=60
06/17/2022 17:27:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
06/17/2022 17:27:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=60
06/17/2022 17:27:27 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.8340295297800588 on epoch=60
06/17/2022 17:27:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=61
06/17/2022 17:27:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
06/17/2022 17:27:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=61
06/17/2022 17:27:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=62
06/17/2022 17:27:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
06/17/2022 17:27:47 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.8171061150920697 on epoch=62
06/17/2022 17:27:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=62
06/17/2022 17:27:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=63
06/17/2022 17:27:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=63
06/17/2022 17:27:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=63
06/17/2022 17:27:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=64
06/17/2022 17:28:06 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.8462083220384928 on epoch=64
06/17/2022 17:28:06 - INFO - __main__ - Saving model with best Classification-F1: 0.841902320622515 -> 0.8462083220384928 on epoch=64, global_step=2050
06/17/2022 17:28:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=64
06/17/2022 17:28:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.32 on epoch=64
06/17/2022 17:28:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=64
06/17/2022 17:28:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=65
06/17/2022 17:28:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=65
06/17/2022 17:28:25 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.8388134478904452 on epoch=65
06/17/2022 17:28:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=65
06/17/2022 17:28:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=66
06/17/2022 17:28:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=66
06/17/2022 17:28:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
06/17/2022 17:28:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=67
06/17/2022 17:28:45 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.8352750932836268 on epoch=67
06/17/2022 17:28:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=67
06/17/2022 17:28:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=67
06/17/2022 17:28:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=68
06/17/2022 17:28:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=68
06/17/2022 17:28:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=68
06/17/2022 17:29:04 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.8447744301278958 on epoch=68
06/17/2022 17:29:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.24 on epoch=69
06/17/2022 17:29:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=69
06/17/2022 17:29:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=69
06/17/2022 17:29:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=69
06/17/2022 17:29:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=70
06/17/2022 17:29:23 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.800912360013543 on epoch=70
06/17/2022 17:29:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
06/17/2022 17:29:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=70
06/17/2022 17:29:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=71
06/17/2022 17:29:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=71
06/17/2022 17:29:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=71
06/17/2022 17:29:43 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7620337770031623 on epoch=71
06/17/2022 17:29:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
06/17/2022 17:29:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=72
06/17/2022 17:29:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=72
06/17/2022 17:29:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=73
06/17/2022 17:29:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=73
06/17/2022 17:30:02 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.8246114373742633 on epoch=73
06/17/2022 17:30:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=73
06/17/2022 17:30:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.25 on epoch=74
06/17/2022 17:30:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
06/17/2022 17:30:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=74
06/17/2022 17:30:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.25 on epoch=74
06/17/2022 17:30:21 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.8465481203748169 on epoch=74
06/17/2022 17:30:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8462083220384928 -> 0.8465481203748169 on epoch=74, global_step=2400
06/17/2022 17:30:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=75
06/17/2022 17:30:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=75
06/17/2022 17:30:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=75
06/17/2022 17:30:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=76
06/17/2022 17:30:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=76
06/17/2022 17:30:41 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.8311697762742698 on epoch=76
06/17/2022 17:30:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=76
06/17/2022 17:30:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=77
06/17/2022 17:30:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=77
06/17/2022 17:30:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=77
06/17/2022 17:30:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
06/17/2022 17:31:00 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.85261122238967 on epoch=78
06/17/2022 17:31:00 - INFO - __main__ - Saving model with best Classification-F1: 0.8465481203748169 -> 0.85261122238967 on epoch=78, global_step=2500
06/17/2022 17:31:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=78
06/17/2022 17:31:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=78
06/17/2022 17:31:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=79
06/17/2022 17:31:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=79
06/17/2022 17:31:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=79
06/17/2022 17:31:20 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.8489293047506044 on epoch=79
06/17/2022 17:31:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=79
06/17/2022 17:31:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=80
06/17/2022 17:31:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=80
06/17/2022 17:31:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=80
06/17/2022 17:31:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=81
06/17/2022 17:31:39 - INFO - __main__ - Global step 2600 Train loss 0.21 Classification-F1 0.8523121212574434 on epoch=81
06/17/2022 17:31:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
06/17/2022 17:31:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 17:31:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=82
06/17/2022 17:31:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=82
06/17/2022 17:31:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=82
06/17/2022 17:31:58 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.7489806288475251 on epoch=82
06/17/2022 17:32:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=83
06/17/2022 17:32:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=83
06/17/2022 17:32:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=83
06/17/2022 17:32:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=84
06/17/2022 17:32:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=84
06/17/2022 17:32:18 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.8436539619266044 on epoch=84
06/17/2022 17:32:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=84
06/17/2022 17:32:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 17:32:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=85
06/17/2022 17:32:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=85
06/17/2022 17:32:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=85
06/17/2022 17:32:37 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.7956980686420045 on epoch=85
06/17/2022 17:32:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=86
06/17/2022 17:32:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=86
06/17/2022 17:32:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.22 on epoch=86
06/17/2022 17:32:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=87
06/17/2022 17:32:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=87
06/17/2022 17:32:56 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.8607297524628235 on epoch=87
06/17/2022 17:32:56 - INFO - __main__ - Saving model with best Classification-F1: 0.85261122238967 -> 0.8607297524628235 on epoch=87, global_step=2800
06/17/2022 17:32:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=87
06/17/2022 17:33:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
06/17/2022 17:33:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.22 on epoch=88
06/17/2022 17:33:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=88
06/17/2022 17:33:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.25 on epoch=89
06/17/2022 17:33:16 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.865382834969789 on epoch=89
06/17/2022 17:33:16 - INFO - __main__ - Saving model with best Classification-F1: 0.8607297524628235 -> 0.865382834969789 on epoch=89, global_step=2850
06/17/2022 17:33:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=89
06/17/2022 17:33:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=89
06/17/2022 17:33:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=89
06/17/2022 17:33:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.25 on epoch=90
06/17/2022 17:33:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
06/17/2022 17:33:35 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.8319436906108807 on epoch=90
06/17/2022 17:33:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.29 on epoch=90
06/17/2022 17:33:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
06/17/2022 17:33:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
06/17/2022 17:33:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=91
06/17/2022 17:33:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=92
06/17/2022 17:33:54 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.869245395127748 on epoch=92
06/17/2022 17:33:54 - INFO - __main__ - Saving model with best Classification-F1: 0.865382834969789 -> 0.869245395127748 on epoch=92, global_step=2950
06/17/2022 17:33:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=92
06/17/2022 17:33:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=92
06/17/2022 17:34:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
06/17/2022 17:34:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=93
06/17/2022 17:34:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=93
06/17/2022 17:34:08 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:34:08 - INFO - __main__ - Printing 3 examples
06/17/2022 17:34:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:34:08 - INFO - __main__ - ['happy']
06/17/2022 17:34:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:34:08 - INFO - __main__ - ['happy']
06/17/2022 17:34:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:34:08 - INFO - __main__ - ['happy']
06/17/2022 17:34:08 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:34:08 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:34:09 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:34:09 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:34:09 - INFO - __main__ - Printing 3 examples
06/17/2022 17:34:09 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:34:09 - INFO - __main__ - ['happy']
06/17/2022 17:34:09 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:34:09 - INFO - __main__ - ['happy']
06/17/2022 17:34:09 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:34:09 - INFO - __main__ - ['happy']
06/17/2022 17:34:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:34:09 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:34:09 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:34:14 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.8119187907603618 on epoch=93
06/17/2022 17:34:14 - INFO - __main__ - save last model!
06/17/2022 17:34:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:34:14 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 17:34:14 - INFO - __main__ - Printing 3 examples
06/17/2022 17:34:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 17:34:14 - INFO - __main__ - ['others']
06/17/2022 17:34:14 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 17:34:14 - INFO - __main__ - ['others']
06/17/2022 17:34:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 17:34:14 - INFO - __main__ - ['others']
06/17/2022 17:34:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:34:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:34:21 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 17:34:26 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:34:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:34:27 - INFO - __main__ - Starting training!
06/17/2022 17:35:34 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_42_0.5_8_predictions.txt
06/17/2022 17:35:34 - INFO - __main__ - Classification-F1 on test data: 0.2047
06/17/2022 17:35:34 - INFO - __main__ - prefix=emo_128_42, lr=0.5, bsz=8, dev_performance=0.869245395127748, test_performance=0.20469575389575118
06/17/2022 17:35:34 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.4, bsz=8 ...
06/17/2022 17:35:35 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:35:35 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:35 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:35:35 - INFO - __main__ - ['happy']
06/17/2022 17:35:35 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:35:35 - INFO - __main__ - ['happy']
06/17/2022 17:35:35 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:35:35 - INFO - __main__ - ['happy']
06/17/2022 17:35:35 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:35:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:36 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:35:36 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:35:36 - INFO - __main__ - Printing 3 examples
06/17/2022 17:35:36 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:35:36 - INFO - __main__ - ['happy']
06/17/2022 17:35:36 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:35:36 - INFO - __main__ - ['happy']
06/17/2022 17:35:36 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:35:36 - INFO - __main__ - ['happy']
06/17/2022 17:35:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:35:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:35:37 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:35:55 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:35:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:35:56 - INFO - __main__ - Starting training!
06/17/2022 17:35:59 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=0
06/17/2022 17:36:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.44 on epoch=0
06/17/2022 17:36:04 - INFO - __main__ - Step 30 Global step 30 Train loss 1.20 on epoch=0
06/17/2022 17:36:07 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=1
06/17/2022 17:36:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.24 on epoch=1
06/17/2022 17:36:16 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.23523622047244092 on epoch=1
06/17/2022 17:36:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23523622047244092 on epoch=1, global_step=50
06/17/2022 17:36:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.38 on epoch=1
06/17/2022 17:36:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=2
06/17/2022 17:36:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=2
06/17/2022 17:36:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=2
06/17/2022 17:36:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=3
06/17/2022 17:36:36 - INFO - __main__ - Global step 100 Train loss 1.05 Classification-F1 0.11582499555976557 on epoch=3
06/17/2022 17:36:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=3
06/17/2022 17:36:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=3
06/17/2022 17:36:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=4
06/17/2022 17:36:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=4
06/17/2022 17:36:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=4
06/17/2022 17:36:55 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.2687013684328073 on epoch=4
06/17/2022 17:36:55 - INFO - __main__ - Saving model with best Classification-F1: 0.23523622047244092 -> 0.2687013684328073 on epoch=4, global_step=150
06/17/2022 17:36:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=4
06/17/2022 17:37:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=5
06/17/2022 17:37:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.93 on epoch=5
06/17/2022 17:37:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=5
06/17/2022 17:37:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=6
06/17/2022 17:37:14 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.17883883401589978 on epoch=6
06/17/2022 17:37:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=6
06/17/2022 17:37:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=6
06/17/2022 17:37:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=7
06/17/2022 17:37:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=7
06/17/2022 17:37:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=7
06/17/2022 17:37:34 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.18095414591583453 on epoch=7
06/17/2022 17:37:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.91 on epoch=8
06/17/2022 17:37:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=8
06/17/2022 17:37:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=8
06/17/2022 17:37:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.98 on epoch=9
06/17/2022 17:37:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=9
06/17/2022 17:37:53 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.20609330421303407 on epoch=9
06/17/2022 17:37:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=9
06/17/2022 17:37:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=9
06/17/2022 17:38:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=10
06/17/2022 17:38:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=10
06/17/2022 17:38:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=10
06/17/2022 17:38:13 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.43541107186967215 on epoch=10
06/17/2022 17:38:13 - INFO - __main__ - Saving model with best Classification-F1: 0.2687013684328073 -> 0.43541107186967215 on epoch=10, global_step=350
06/17/2022 17:38:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=11
06/17/2022 17:38:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.84 on epoch=11
06/17/2022 17:38:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.79 on epoch=11
06/17/2022 17:38:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.85 on epoch=12
06/17/2022 17:38:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=12
06/17/2022 17:38:32 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3591811815576439 on epoch=12
06/17/2022 17:38:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=12
06/17/2022 17:38:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.85 on epoch=13
06/17/2022 17:38:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=13
06/17/2022 17:38:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=13
06/17/2022 17:38:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=14
06/17/2022 17:38:52 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.3435169258083447 on epoch=14
06/17/2022 17:38:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.85 on epoch=14
06/17/2022 17:38:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=14
06/17/2022 17:38:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=14
06/17/2022 17:39:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=15
06/17/2022 17:39:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=15
06/17/2022 17:39:11 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.41755273997383874 on epoch=15
06/17/2022 17:39:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=15
06/17/2022 17:39:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=16
06/17/2022 17:39:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.78 on epoch=16
06/17/2022 17:39:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=16
06/17/2022 17:39:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=17
06/17/2022 17:39:31 - INFO - __main__ - Global step 550 Train loss 0.76 Classification-F1 0.417470457447705 on epoch=17
06/17/2022 17:39:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=17
06/17/2022 17:39:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=17
06/17/2022 17:39:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=18
06/17/2022 17:39:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=18
06/17/2022 17:39:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.76 on epoch=18
06/17/2022 17:39:50 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.48684107913922 on epoch=18
06/17/2022 17:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.43541107186967215 -> 0.48684107913922 on epoch=18, global_step=600
06/17/2022 17:39:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.77 on epoch=19
06/17/2022 17:39:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=19
06/17/2022 17:39:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=19
06/17/2022 17:40:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=19
06/17/2022 17:40:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=20
06/17/2022 17:40:10 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.4727748944392831 on epoch=20
06/17/2022 17:40:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=20
06/17/2022 17:40:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=20
06/17/2022 17:40:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.59 on epoch=21
06/17/2022 17:40:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=21
06/17/2022 17:40:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.75 on epoch=21
06/17/2022 17:40:29 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.6693984215627324 on epoch=21
06/17/2022 17:40:29 - INFO - __main__ - Saving model with best Classification-F1: 0.48684107913922 -> 0.6693984215627324 on epoch=21, global_step=700
06/17/2022 17:40:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=22
06/17/2022 17:40:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=22
06/17/2022 17:40:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=22
06/17/2022 17:40:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.64 on epoch=23
06/17/2022 17:40:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=23
06/17/2022 17:40:49 - INFO - __main__ - Global step 750 Train loss 0.63 Classification-F1 0.6801293293944686 on epoch=23
06/17/2022 17:40:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6693984215627324 -> 0.6801293293944686 on epoch=23, global_step=750
06/17/2022 17:40:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=23
06/17/2022 17:40:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=24
06/17/2022 17:40:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=24
06/17/2022 17:40:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=24
06/17/2022 17:41:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=24
06/17/2022 17:41:08 - INFO - __main__ - Global step 800 Train loss 0.61 Classification-F1 0.7435866454648025 on epoch=24
06/17/2022 17:41:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6801293293944686 -> 0.7435866454648025 on epoch=24, global_step=800
06/17/2022 17:41:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.63 on epoch=25
06/17/2022 17:41:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=25
06/17/2022 17:41:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.60 on epoch=25
06/17/2022 17:41:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=26
06/17/2022 17:41:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=26
06/17/2022 17:41:28 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.5981770796671377 on epoch=26
06/17/2022 17:41:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.57 on epoch=26
06/17/2022 17:41:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.62 on epoch=27
06/17/2022 17:41:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.65 on epoch=27
06/17/2022 17:41:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.64 on epoch=27
06/17/2022 17:41:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=28
06/17/2022 17:41:47 - INFO - __main__ - Global step 900 Train loss 0.61 Classification-F1 0.7868606877596276 on epoch=28
06/17/2022 17:41:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7435866454648025 -> 0.7868606877596276 on epoch=28, global_step=900
06/17/2022 17:41:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=28
06/17/2022 17:41:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=28
06/17/2022 17:41:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=29
06/17/2022 17:41:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.61 on epoch=29
06/17/2022 17:42:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=29
06/17/2022 17:42:07 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.6880189932698852 on epoch=29
06/17/2022 17:42:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=29
06/17/2022 17:42:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=30
06/17/2022 17:42:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=30
06/17/2022 17:42:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=30
06/17/2022 17:42:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=31
06/17/2022 17:42:26 - INFO - __main__ - Global step 1000 Train loss 0.54 Classification-F1 0.7339243938559277 on epoch=31
06/17/2022 17:42:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=31
06/17/2022 17:42:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.57 on epoch=31
06/17/2022 17:42:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=32
06/17/2022 17:42:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=32
06/17/2022 17:42:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=32
06/17/2022 17:42:46 - INFO - __main__ - Global step 1050 Train loss 0.52 Classification-F1 0.6927159645909646 on epoch=32
06/17/2022 17:42:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=33
06/17/2022 17:42:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=33
06/17/2022 17:42:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=33
06/17/2022 17:42:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=34
06/17/2022 17:42:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=34
06/17/2022 17:43:05 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.7527910264899619 on epoch=34
06/17/2022 17:43:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=34
06/17/2022 17:43:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=34
06/17/2022 17:43:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=35
06/17/2022 17:43:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=35
06/17/2022 17:43:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=35
06/17/2022 17:43:25 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.8084816722881618 on epoch=35
06/17/2022 17:43:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7868606877596276 -> 0.8084816722881618 on epoch=35, global_step=1150
06/17/2022 17:43:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=36
06/17/2022 17:43:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=36
06/17/2022 17:43:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=36
06/17/2022 17:43:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=37
06/17/2022 17:43:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.58 on epoch=37
06/17/2022 17:43:44 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.7864273409909137 on epoch=37
06/17/2022 17:43:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=37
06/17/2022 17:43:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=38
06/17/2022 17:43:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=38
06/17/2022 17:43:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=38
06/17/2022 17:43:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=39
06/17/2022 17:44:04 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.8135612772240679 on epoch=39
06/17/2022 17:44:04 - INFO - __main__ - Saving model with best Classification-F1: 0.8084816722881618 -> 0.8135612772240679 on epoch=39, global_step=1250
06/17/2022 17:44:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.51 on epoch=39
06/17/2022 17:44:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=39
06/17/2022 17:44:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=39
06/17/2022 17:44:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=40
06/17/2022 17:44:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=40
06/17/2022 17:44:23 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.7934872590640304 on epoch=40
06/17/2022 17:44:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=40
06/17/2022 17:44:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=41
06/17/2022 17:44:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=41
06/17/2022 17:44:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.51 on epoch=41
06/17/2022 17:44:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=42
06/17/2022 17:44:43 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.7661137544942473 on epoch=42
06/17/2022 17:44:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=42
06/17/2022 17:44:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
06/17/2022 17:44:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=43
06/17/2022 17:44:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=43
06/17/2022 17:44:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=43
06/17/2022 17:45:03 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.7445586986077657 on epoch=43
06/17/2022 17:45:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=44
06/17/2022 17:45:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=44
06/17/2022 17:45:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=44
06/17/2022 17:45:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=44
06/17/2022 17:45:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=45
06/17/2022 17:45:22 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.7667741561816929 on epoch=45
06/17/2022 17:45:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=45
06/17/2022 17:45:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=45
06/17/2022 17:45:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
06/17/2022 17:45:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=46
06/17/2022 17:45:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=46
06/17/2022 17:45:42 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.8103247499017743 on epoch=46
06/17/2022 17:45:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=47
06/17/2022 17:45:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=47
06/17/2022 17:45:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=47
06/17/2022 17:45:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=48
06/17/2022 17:45:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=48
06/17/2022 17:46:01 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.824024701379058 on epoch=48
06/17/2022 17:46:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8135612772240679 -> 0.824024701379058 on epoch=48, global_step=1550
06/17/2022 17:46:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=48
06/17/2022 17:46:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=49
06/17/2022 17:46:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=49
06/17/2022 17:46:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=49
06/17/2022 17:46:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=49
06/17/2022 17:46:21 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.825649562446176 on epoch=49
06/17/2022 17:46:21 - INFO - __main__ - Saving model with best Classification-F1: 0.824024701379058 -> 0.825649562446176 on epoch=49, global_step=1600
06/17/2022 17:46:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=50
06/17/2022 17:46:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=50
06/17/2022 17:46:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=50
06/17/2022 17:46:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=51
06/17/2022 17:46:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=51
06/17/2022 17:46:40 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.7534349582025729 on epoch=51
06/17/2022 17:46:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=51
06/17/2022 17:46:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=52
06/17/2022 17:46:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=52
06/17/2022 17:46:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=52
06/17/2022 17:46:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.49 on epoch=53
06/17/2022 17:47:00 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.8271767772556191 on epoch=53
06/17/2022 17:47:00 - INFO - __main__ - Saving model with best Classification-F1: 0.825649562446176 -> 0.8271767772556191 on epoch=53, global_step=1700
06/17/2022 17:47:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=53
06/17/2022 17:47:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
06/17/2022 17:47:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=54
06/17/2022 17:47:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=54
06/17/2022 17:47:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=54
06/17/2022 17:47:20 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.7984859674206762 on epoch=54
06/17/2022 17:47:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=54
06/17/2022 17:47:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=55
06/17/2022 17:47:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=55
06/17/2022 17:47:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=55
06/17/2022 17:47:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=56
06/17/2022 17:47:39 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.8323617967407839 on epoch=56
06/17/2022 17:47:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8271767772556191 -> 0.8323617967407839 on epoch=56, global_step=1800
06/17/2022 17:47:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=56
06/17/2022 17:47:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=56
06/17/2022 17:47:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=57
06/17/2022 17:47:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=57
06/17/2022 17:47:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=57
06/17/2022 17:47:59 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.7629767596460507 on epoch=57
06/17/2022 17:48:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=58
06/17/2022 17:48:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
06/17/2022 17:48:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=58
06/17/2022 17:48:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=59
06/17/2022 17:48:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=59
06/17/2022 17:48:18 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.820182284352291 on epoch=59
06/17/2022 17:48:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=59
06/17/2022 17:48:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=59
06/17/2022 17:48:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=60
06/17/2022 17:48:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=60
06/17/2022 17:48:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=60
06/17/2022 17:48:38 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.7758368910672134 on epoch=60
06/17/2022 17:48:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=61
06/17/2022 17:48:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.47 on epoch=61
06/17/2022 17:48:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=61
06/17/2022 17:48:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=62
06/17/2022 17:48:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=62
06/17/2022 17:48:57 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.820668738300942 on epoch=62
06/17/2022 17:49:00 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=62
06/17/2022 17:49:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=63
06/17/2022 17:49:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=63
06/17/2022 17:49:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=63
06/17/2022 17:49:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=64
06/17/2022 17:49:17 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.8350053034943306 on epoch=64
06/17/2022 17:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.8323617967407839 -> 0.8350053034943306 on epoch=64, global_step=2050
06/17/2022 17:49:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=64
06/17/2022 17:49:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.31 on epoch=64
06/17/2022 17:49:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=64
06/17/2022 17:49:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=65
06/17/2022 17:49:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.30 on epoch=65
06/17/2022 17:49:37 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.822185061315496 on epoch=65
06/17/2022 17:49:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.34 on epoch=65
06/17/2022 17:49:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=66
06/17/2022 17:49:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=66
06/17/2022 17:49:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=66
06/17/2022 17:49:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=67
06/17/2022 17:49:56 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.810527515873066 on epoch=67
06/17/2022 17:49:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=67
06/17/2022 17:50:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=67
06/17/2022 17:50:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=68
06/17/2022 17:50:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.32 on epoch=68
06/17/2022 17:50:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=68
06/17/2022 17:50:16 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.7964211276108437 on epoch=68
06/17/2022 17:50:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=69
06/17/2022 17:50:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.32 on epoch=69
06/17/2022 17:50:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=69
06/17/2022 17:50:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=69
06/17/2022 17:50:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=70
06/17/2022 17:50:36 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.8116723134997988 on epoch=70
06/17/2022 17:50:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=70
06/17/2022 17:50:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=70
06/17/2022 17:50:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.34 on epoch=71
06/17/2022 17:50:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=71
06/17/2022 17:50:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=71
06/17/2022 17:50:55 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.8264059601113172 on epoch=71
06/17/2022 17:50:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=72
06/17/2022 17:51:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=72
06/17/2022 17:51:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=72
06/17/2022 17:51:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=73
06/17/2022 17:51:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=73
06/17/2022 17:51:15 - INFO - __main__ - Global step 2350 Train loss 0.29 Classification-F1 0.8202495939712868 on epoch=73
06/17/2022 17:51:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
06/17/2022 17:51:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.27 on epoch=74
06/17/2022 17:51:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=74
06/17/2022 17:51:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=74
06/17/2022 17:51:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=74
06/17/2022 17:51:35 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.8017478993159934 on epoch=74
06/17/2022 17:51:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=75
06/17/2022 17:51:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.30 on epoch=75
06/17/2022 17:51:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.33 on epoch=75
06/17/2022 17:51:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=76
06/17/2022 17:51:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=76
06/17/2022 17:51:54 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.8047699323943173 on epoch=76
06/17/2022 17:51:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=76
06/17/2022 17:51:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=77
06/17/2022 17:52:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=77
06/17/2022 17:52:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.29 on epoch=77
06/17/2022 17:52:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=78
06/17/2022 17:52:14 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.8460081585081585 on epoch=78
06/17/2022 17:52:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8350053034943306 -> 0.8460081585081585 on epoch=78, global_step=2500
06/17/2022 17:52:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.28 on epoch=78
06/17/2022 17:52:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=78
06/17/2022 17:52:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=79
06/17/2022 17:52:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=79
06/17/2022 17:52:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=79
06/17/2022 17:52:33 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.8307489393756998 on epoch=79
06/17/2022 17:52:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.24 on epoch=79
06/17/2022 17:52:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=80
06/17/2022 17:52:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=80
06/17/2022 17:52:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
06/17/2022 17:52:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=81
06/17/2022 17:52:53 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.8353724215646179 on epoch=81
06/17/2022 17:52:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=81
06/17/2022 17:52:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=81
06/17/2022 17:53:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=82
06/17/2022 17:53:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.26 on epoch=82
06/17/2022 17:53:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=82
06/17/2022 17:53:13 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.797408903932683 on epoch=82
06/17/2022 17:53:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=83
06/17/2022 17:53:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=83
06/17/2022 17:53:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
06/17/2022 17:53:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.22 on epoch=84
06/17/2022 17:53:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.28 on epoch=84
06/17/2022 17:53:32 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.8397583834512775 on epoch=84
06/17/2022 17:53:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=84
06/17/2022 17:53:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=84
06/17/2022 17:53:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=85
06/17/2022 17:53:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=85
06/17/2022 17:53:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=85
06/17/2022 17:53:52 - INFO - __main__ - Global step 2750 Train loss 0.23 Classification-F1 0.8175651438528382 on epoch=85
06/17/2022 17:53:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=86
06/17/2022 17:53:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=86
06/17/2022 17:53:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.25 on epoch=86
06/17/2022 17:54:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=87
06/17/2022 17:54:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.27 on epoch=87
06/17/2022 17:54:11 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.849441597105657 on epoch=87
06/17/2022 17:54:11 - INFO - __main__ - Saving model with best Classification-F1: 0.8460081585081585 -> 0.849441597105657 on epoch=87, global_step=2800
06/17/2022 17:54:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=87
06/17/2022 17:54:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=88
06/17/2022 17:54:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=88
06/17/2022 17:54:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=88
06/17/2022 17:54:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=89
06/17/2022 17:54:31 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.8665432827623168 on epoch=89
06/17/2022 17:54:31 - INFO - __main__ - Saving model with best Classification-F1: 0.849441597105657 -> 0.8665432827623168 on epoch=89, global_step=2850
06/17/2022 17:54:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.23 on epoch=89
06/17/2022 17:54:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=89
06/17/2022 17:54:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=89
06/17/2022 17:54:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=90
06/17/2022 17:54:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=90
06/17/2022 17:54:50 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.8426479520438945 on epoch=90
06/17/2022 17:54:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=90
06/17/2022 17:54:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=91
06/17/2022 17:54:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.24 on epoch=91
06/17/2022 17:55:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=91
06/17/2022 17:55:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=92
06/17/2022 17:55:10 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.8219026688584334 on epoch=92
06/17/2022 17:55:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=92
06/17/2022 17:55:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=92
06/17/2022 17:55:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
06/17/2022 17:55:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=93
06/17/2022 17:55:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=93
06/17/2022 17:55:24 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:55:24 - INFO - __main__ - Printing 3 examples
06/17/2022 17:55:24 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:55:24 - INFO - __main__ - ['happy']
06/17/2022 17:55:24 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:55:24 - INFO - __main__ - ['happy']
06/17/2022 17:55:24 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:55:24 - INFO - __main__ - ['happy']
06/17/2022 17:55:24 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:55:24 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:55:25 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:55:25 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:55:25 - INFO - __main__ - Printing 3 examples
06/17/2022 17:55:25 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:55:25 - INFO - __main__ - ['happy']
06/17/2022 17:55:25 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:55:25 - INFO - __main__ - ['happy']
06/17/2022 17:55:25 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:55:25 - INFO - __main__ - ['happy']
06/17/2022 17:55:25 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:55:25 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:55:26 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:55:30 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7855307794930436 on epoch=93
06/17/2022 17:55:30 - INFO - __main__ - save last model!
06/17/2022 17:55:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 17:55:30 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 17:55:30 - INFO - __main__ - Printing 3 examples
06/17/2022 17:55:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 17:55:30 - INFO - __main__ - ['others']
06/17/2022 17:55:30 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 17:55:30 - INFO - __main__ - ['others']
06/17/2022 17:55:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 17:55:30 - INFO - __main__ - ['others']
06/17/2022 17:55:30 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:55:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:55:37 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 17:55:43 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:55:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:55:44 - INFO - __main__ - Starting training!
06/17/2022 17:56:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_42_0.4_8_predictions.txt
06/17/2022 17:56:50 - INFO - __main__ - Classification-F1 on test data: 0.3226
06/17/2022 17:56:50 - INFO - __main__ - prefix=emo_128_42, lr=0.4, bsz=8, dev_performance=0.8665432827623168, test_performance=0.3226074392346415
06/17/2022 17:56:50 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.3, bsz=8 ...
06/17/2022 17:56:51 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:56:51 - INFO - __main__ - Printing 3 examples
06/17/2022 17:56:51 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 17:56:51 - INFO - __main__ - ['happy']
06/17/2022 17:56:51 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 17:56:51 - INFO - __main__ - ['happy']
06/17/2022 17:56:51 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 17:56:51 - INFO - __main__ - ['happy']
06/17/2022 17:56:51 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:56:51 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:56:52 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 17:56:52 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 17:56:52 - INFO - __main__ - Printing 3 examples
06/17/2022 17:56:52 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 17:56:52 - INFO - __main__ - ['happy']
06/17/2022 17:56:52 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 17:56:52 - INFO - __main__ - ['happy']
06/17/2022 17:56:52 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 17:56:52 - INFO - __main__ - ['happy']
06/17/2022 17:56:52 - INFO - __main__ - Tokenizing Input ...
06/17/2022 17:56:52 - INFO - __main__ - Tokenizing Output ...
06/17/2022 17:56:52 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 17:57:08 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 17:57:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 17:57:09 - INFO - __main__ - Starting training!
06/17/2022 17:57:12 - INFO - __main__ - Step 10 Global step 10 Train loss 2.82 on epoch=0
06/17/2022 17:57:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.56 on epoch=0
06/17/2022 17:57:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.19 on epoch=0
06/17/2022 17:57:20 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=1
06/17/2022 17:57:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=1
06/17/2022 17:57:29 - INFO - __main__ - Global step 50 Train loss 1.52 Classification-F1 0.2329613298474265 on epoch=1
06/17/2022 17:57:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2329613298474265 on epoch=1, global_step=50
06/17/2022 17:57:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=1
06/17/2022 17:57:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=2
06/17/2022 17:57:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=2
06/17/2022 17:57:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=2
06/17/2022 17:57:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=3
06/17/2022 17:57:49 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.26490267639902676 on epoch=3
06/17/2022 17:57:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2329613298474265 -> 0.26490267639902676 on epoch=3, global_step=100
06/17/2022 17:57:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=3
06/17/2022 17:57:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=3
06/17/2022 17:57:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=4
06/17/2022 17:57:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.74 on epoch=4
06/17/2022 17:58:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=4
06/17/2022 17:58:08 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.15742887653917403 on epoch=4
06/17/2022 17:58:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=4
06/17/2022 17:58:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=5
06/17/2022 17:58:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=5
06/17/2022 17:58:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=5
06/17/2022 17:58:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=6
06/17/2022 17:58:27 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.41168010752688167 on epoch=6
06/17/2022 17:58:27 - INFO - __main__ - Saving model with best Classification-F1: 0.26490267639902676 -> 0.41168010752688167 on epoch=6, global_step=200
06/17/2022 17:58:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=6
06/17/2022 17:58:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=6
06/17/2022 17:58:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=7
06/17/2022 17:58:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=7
06/17/2022 17:58:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.76 on epoch=7
06/17/2022 17:58:47 - INFO - __main__ - Global step 250 Train loss 0.79 Classification-F1 0.31838753858983493 on epoch=7
06/17/2022 17:58:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=8
06/17/2022 17:58:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=8
06/17/2022 17:58:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=8
06/17/2022 17:58:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=9
06/17/2022 17:58:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=9
06/17/2022 17:59:06 - INFO - __main__ - Global step 300 Train loss 0.79 Classification-F1 0.6486399823087129 on epoch=9
06/17/2022 17:59:06 - INFO - __main__ - Saving model with best Classification-F1: 0.41168010752688167 -> 0.6486399823087129 on epoch=9, global_step=300
06/17/2022 17:59:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=9
06/17/2022 17:59:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=9
06/17/2022 17:59:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=10
06/17/2022 17:59:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=10
06/17/2022 17:59:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.73 on epoch=10
06/17/2022 17:59:26 - INFO - __main__ - Global step 350 Train loss 0.72 Classification-F1 0.6429588824500638 on epoch=10
06/17/2022 17:59:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.67 on epoch=11
06/17/2022 17:59:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.70 on epoch=11
06/17/2022 17:59:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.70 on epoch=11
06/17/2022 17:59:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=12
06/17/2022 17:59:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=12
06/17/2022 17:59:45 - INFO - __main__ - Global step 400 Train loss 0.68 Classification-F1 0.7052089734890461 on epoch=12
06/17/2022 17:59:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6486399823087129 -> 0.7052089734890461 on epoch=12, global_step=400
06/17/2022 17:59:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=12
06/17/2022 17:59:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=13
06/17/2022 17:59:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=13
06/17/2022 17:59:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=13
06/17/2022 17:59:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.60 on epoch=14
06/17/2022 18:00:04 - INFO - __main__ - Global step 450 Train loss 0.65 Classification-F1 0.7641084062381134 on epoch=14
06/17/2022 18:00:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7052089734890461 -> 0.7641084062381134 on epoch=14, global_step=450
06/17/2022 18:00:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=14
06/17/2022 18:00:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=14
06/17/2022 18:00:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=14
06/17/2022 18:00:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=15
06/17/2022 18:00:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=15
06/17/2022 18:00:24 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.7296898934398934 on epoch=15
06/17/2022 18:00:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=15
06/17/2022 18:00:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=16
06/17/2022 18:00:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=16
06/17/2022 18:00:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=16
06/17/2022 18:00:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=17
06/17/2022 18:00:43 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.5704037877023151 on epoch=17
06/17/2022 18:00:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=17
06/17/2022 18:00:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=17
06/17/2022 18:00:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=18
06/17/2022 18:00:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=18
06/17/2022 18:00:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=18
06/17/2022 18:01:03 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.6630580884592526 on epoch=18
06/17/2022 18:01:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=19
06/17/2022 18:01:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.59 on epoch=19
06/17/2022 18:01:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=19
06/17/2022 18:01:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=19
06/17/2022 18:01:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=20
06/17/2022 18:01:23 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.6752573505956874 on epoch=20
06/17/2022 18:01:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=20
06/17/2022 18:01:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=20
06/17/2022 18:01:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=21
06/17/2022 18:01:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.56 on epoch=21
06/17/2022 18:01:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=21
06/17/2022 18:01:43 - INFO - __main__ - Global step 700 Train loss 0.56 Classification-F1 0.7918701704116937 on epoch=21
06/17/2022 18:01:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7641084062381134 -> 0.7918701704116937 on epoch=21, global_step=700
06/17/2022 18:01:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=22
06/17/2022 18:01:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=22
06/17/2022 18:01:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=22
06/17/2022 18:01:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=23
06/17/2022 18:01:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.53 on epoch=23
06/17/2022 18:02:03 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.7448513290021758 on epoch=23
06/17/2022 18:02:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=23
06/17/2022 18:02:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=24
06/17/2022 18:02:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=24
06/17/2022 18:02:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=24
06/17/2022 18:02:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=24
06/17/2022 18:02:23 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.7585611901121008 on epoch=24
06/17/2022 18:02:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=25
06/17/2022 18:02:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=25
06/17/2022 18:02:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=25
06/17/2022 18:02:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=26
06/17/2022 18:02:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=26
06/17/2022 18:02:43 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.6708988768324413 on epoch=26
06/17/2022 18:02:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=26
06/17/2022 18:02:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=27
06/17/2022 18:02:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=27
06/17/2022 18:02:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=27
06/17/2022 18:02:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=28
06/17/2022 18:03:03 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.7580582771639683 on epoch=28
06/17/2022 18:03:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=28
06/17/2022 18:03:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=28
06/17/2022 18:03:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=29
06/17/2022 18:03:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=29
06/17/2022 18:03:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=29
06/17/2022 18:03:23 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.7136022525468625 on epoch=29
06/17/2022 18:03:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.55 on epoch=29
06/17/2022 18:03:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=30
06/17/2022 18:03:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=30
06/17/2022 18:03:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=30
06/17/2022 18:03:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=31
06/17/2022 18:03:43 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.7808025942730155 on epoch=31
06/17/2022 18:03:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=31
06/17/2022 18:03:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=31
06/17/2022 18:03:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=32
06/17/2022 18:03:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=32
06/17/2022 18:03:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=32
06/17/2022 18:04:03 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.6801563329921916 on epoch=32
06/17/2022 18:04:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=33
06/17/2022 18:04:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=33
06/17/2022 18:04:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=33
06/17/2022 18:04:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=34
06/17/2022 18:04:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=34
06/17/2022 18:04:23 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.7949126367378094 on epoch=34
06/17/2022 18:04:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7918701704116937 -> 0.7949126367378094 on epoch=34, global_step=1100
06/17/2022 18:04:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=34
06/17/2022 18:04:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=34
06/17/2022 18:04:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=35
06/17/2022 18:04:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=35
06/17/2022 18:04:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=35
06/17/2022 18:04:43 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.8177324725926929 on epoch=35
06/17/2022 18:04:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7949126367378094 -> 0.8177324725926929 on epoch=35, global_step=1150
06/17/2022 18:04:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=36
06/17/2022 18:04:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=36
06/17/2022 18:04:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=36
06/17/2022 18:04:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=37
06/17/2022 18:04:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
06/17/2022 18:05:04 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.7495981783694661 on epoch=37
06/17/2022 18:05:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=37
06/17/2022 18:05:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=38
06/17/2022 18:05:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=38
06/17/2022 18:05:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=38
06/17/2022 18:05:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=39
06/17/2022 18:05:24 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.8274625870386447 on epoch=39
06/17/2022 18:05:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8177324725926929 -> 0.8274625870386447 on epoch=39, global_step=1250
06/17/2022 18:05:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=39
06/17/2022 18:05:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
06/17/2022 18:05:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=39
06/17/2022 18:05:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=40
06/17/2022 18:05:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=40
06/17/2022 18:05:44 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.7983229232887767 on epoch=40
06/17/2022 18:05:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=40
06/17/2022 18:05:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=41
06/17/2022 18:05:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=41
06/17/2022 18:05:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=41
06/17/2022 18:05:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=42
06/17/2022 18:06:04 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.7609868853371405 on epoch=42
06/17/2022 18:06:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=42
06/17/2022 18:06:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=42
06/17/2022 18:06:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=43
06/17/2022 18:06:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=43
06/17/2022 18:06:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=43
06/17/2022 18:06:24 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.7491093068206137 on epoch=43
06/17/2022 18:06:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=44
06/17/2022 18:06:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=44
06/17/2022 18:06:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=44
06/17/2022 18:06:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=44
06/17/2022 18:06:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=45
06/17/2022 18:06:44 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.7650928390205253 on epoch=45
06/17/2022 18:06:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=45
06/17/2022 18:06:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=45
06/17/2022 18:06:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=46
06/17/2022 18:06:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=46
06/17/2022 18:06:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=46
06/17/2022 18:07:03 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.8218017616578935 on epoch=46
06/17/2022 18:07:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=47
06/17/2022 18:07:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=47
06/17/2022 18:07:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=47
06/17/2022 18:07:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=48
06/17/2022 18:07:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=48
06/17/2022 18:07:23 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.8113160125836035 on epoch=48
06/17/2022 18:07:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=48
06/17/2022 18:07:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=49
06/17/2022 18:07:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=49
06/17/2022 18:07:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=49
06/17/2022 18:07:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=49
06/17/2022 18:07:42 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.7917989804714837 on epoch=49
06/17/2022 18:07:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=50
06/17/2022 18:07:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=50
06/17/2022 18:07:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=50
06/17/2022 18:07:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=51
06/17/2022 18:07:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=51
06/17/2022 18:08:02 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.8034222743059067 on epoch=51
06/17/2022 18:08:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=51
06/17/2022 18:08:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
06/17/2022 18:08:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=52
06/17/2022 18:08:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=52
06/17/2022 18:08:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=53
06/17/2022 18:08:21 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.8378142759617938 on epoch=53
06/17/2022 18:08:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8274625870386447 -> 0.8378142759617938 on epoch=53, global_step=1700
06/17/2022 18:08:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=53
06/17/2022 18:08:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=53
06/17/2022 18:08:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=54
06/17/2022 18:08:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=54
06/17/2022 18:08:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=54
06/17/2022 18:08:41 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.7956579786036422 on epoch=54
06/17/2022 18:08:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=54
06/17/2022 18:08:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=55
06/17/2022 18:08:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=55
06/17/2022 18:08:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=55
06/17/2022 18:08:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=56
06/17/2022 18:09:00 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.8014910696165994 on epoch=56
06/17/2022 18:09:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=56
06/17/2022 18:09:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=56
06/17/2022 18:09:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=57
06/17/2022 18:09:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=57
06/17/2022 18:09:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=57
06/17/2022 18:09:20 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.7268383230970357 on epoch=57
06/17/2022 18:09:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=58
06/17/2022 18:09:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=58
06/17/2022 18:09:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=58
06/17/2022 18:09:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=59
06/17/2022 18:09:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=59
06/17/2022 18:09:39 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.8292050980163839 on epoch=59
06/17/2022 18:09:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=59
06/17/2022 18:09:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
06/17/2022 18:09:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=60
06/17/2022 18:09:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=60
06/17/2022 18:09:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=60
06/17/2022 18:09:58 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.826851964980532 on epoch=60
06/17/2022 18:10:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=61
06/17/2022 18:10:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
06/17/2022 18:10:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=61
06/17/2022 18:10:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=62
06/17/2022 18:10:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=62
06/17/2022 18:10:18 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.8381113061612314 on epoch=62
06/17/2022 18:10:18 - INFO - __main__ - Saving model with best Classification-F1: 0.8378142759617938 -> 0.8381113061612314 on epoch=62, global_step=2000
06/17/2022 18:10:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=62
06/17/2022 18:10:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=63
06/17/2022 18:10:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=63
06/17/2022 18:10:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=63
06/17/2022 18:10:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=64
06/17/2022 18:10:37 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.8100881747158274 on epoch=64
06/17/2022 18:10:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=64
06/17/2022 18:10:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=64
06/17/2022 18:10:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=64
06/17/2022 18:10:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=65
06/17/2022 18:10:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=65
06/17/2022 18:10:57 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7745403783355531 on epoch=65
06/17/2022 18:11:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=65
06/17/2022 18:11:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=66
06/17/2022 18:11:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=66
06/17/2022 18:11:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=66
06/17/2022 18:11:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=67
06/17/2022 18:11:16 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.8226204702076688 on epoch=67
06/17/2022 18:11:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=67
06/17/2022 18:11:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=67
06/17/2022 18:11:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=68
06/17/2022 18:11:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
06/17/2022 18:11:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
06/17/2022 18:11:36 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.744710749669363 on epoch=68
06/17/2022 18:11:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.30 on epoch=69
06/17/2022 18:11:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=69
06/17/2022 18:11:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
06/17/2022 18:11:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=69
06/17/2022 18:11:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=70
06/17/2022 18:11:56 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7978822080826058 on epoch=70
06/17/2022 18:11:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=70
06/17/2022 18:12:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=70
06/17/2022 18:12:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=71
06/17/2022 18:12:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=71
06/17/2022 18:12:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=71
06/17/2022 18:12:15 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.8337745463851631 on epoch=71
06/17/2022 18:12:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=72
06/17/2022 18:12:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=72
06/17/2022 18:12:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=72
06/17/2022 18:12:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=73
06/17/2022 18:12:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=73
06/17/2022 18:12:35 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.8279350087760657 on epoch=73
06/17/2022 18:12:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
06/17/2022 18:12:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=74
06/17/2022 18:12:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=74
06/17/2022 18:12:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
06/17/2022 18:12:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.30 on epoch=74
06/17/2022 18:12:54 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.8099224771049554 on epoch=74
06/17/2022 18:12:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=75
06/17/2022 18:12:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.28 on epoch=75
06/17/2022 18:13:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=75
06/17/2022 18:13:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=76
06/17/2022 18:13:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=76
06/17/2022 18:13:14 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.8164803583635121 on epoch=76
06/17/2022 18:13:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
06/17/2022 18:13:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
06/17/2022 18:13:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=77
06/17/2022 18:13:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=77
06/17/2022 18:13:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=78
06/17/2022 18:13:33 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.8490448975716566 on epoch=78
06/17/2022 18:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.8381113061612314 -> 0.8490448975716566 on epoch=78, global_step=2500
06/17/2022 18:13:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=78
06/17/2022 18:13:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=78
06/17/2022 18:13:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=79
06/17/2022 18:13:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=79
06/17/2022 18:13:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=79
06/17/2022 18:13:52 - INFO - __main__ - Global step 2550 Train loss 0.25 Classification-F1 0.7918254442152498 on epoch=79
06/17/2022 18:13:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=79
06/17/2022 18:13:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=80
06/17/2022 18:14:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=80
06/17/2022 18:14:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.27 on epoch=80
06/17/2022 18:14:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=81
06/17/2022 18:14:12 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.8132425429018314 on epoch=81
06/17/2022 18:14:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=81
06/17/2022 18:14:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=81
06/17/2022 18:14:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=82
06/17/2022 18:14:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=82
06/17/2022 18:14:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=82
06/17/2022 18:14:32 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.7624255090737881 on epoch=82
06/17/2022 18:14:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=83
06/17/2022 18:14:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=83
06/17/2022 18:14:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=83
06/17/2022 18:14:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.25 on epoch=84
06/17/2022 18:14:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.26 on epoch=84
06/17/2022 18:14:51 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.7973811542996712 on epoch=84
06/17/2022 18:14:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=84
06/17/2022 18:14:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=84
06/17/2022 18:14:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
06/17/2022 18:15:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=85
06/17/2022 18:15:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=85
06/17/2022 18:15:11 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.7992862384657579 on epoch=85
06/17/2022 18:15:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
06/17/2022 18:15:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=86
06/17/2022 18:15:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=86
06/17/2022 18:15:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
06/17/2022 18:15:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.23 on epoch=87
06/17/2022 18:15:30 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.8325385721090056 on epoch=87
06/17/2022 18:15:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=87
06/17/2022 18:15:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=88
06/17/2022 18:15:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=88
06/17/2022 18:15:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=88
06/17/2022 18:15:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.29 on epoch=89
06/17/2022 18:15:50 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.8512067404988483 on epoch=89
06/17/2022 18:15:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8490448975716566 -> 0.8512067404988483 on epoch=89, global_step=2850
06/17/2022 18:15:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=89
06/17/2022 18:15:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=89
06/17/2022 18:15:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=89
06/17/2022 18:16:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=90
06/17/2022 18:16:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
06/17/2022 18:16:09 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.8228599231370809 on epoch=90
06/17/2022 18:16:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=90
06/17/2022 18:16:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=91
06/17/2022 18:16:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=91
06/17/2022 18:16:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=91
06/17/2022 18:16:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=92
06/17/2022 18:16:29 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.8460083991878932 on epoch=92
06/17/2022 18:16:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=92
06/17/2022 18:16:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
06/17/2022 18:16:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
06/17/2022 18:16:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=93
06/17/2022 18:16:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
06/17/2022 18:16:43 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:16:43 - INFO - __main__ - Printing 3 examples
06/17/2022 18:16:43 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 18:16:43 - INFO - __main__ - ['happy']
06/17/2022 18:16:43 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 18:16:43 - INFO - __main__ - ['happy']
06/17/2022 18:16:43 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 18:16:43 - INFO - __main__ - ['happy']
06/17/2022 18:16:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:16:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:16:44 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 18:16:44 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:16:44 - INFO - __main__ - Printing 3 examples
06/17/2022 18:16:44 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 18:16:44 - INFO - __main__ - ['happy']
06/17/2022 18:16:44 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 18:16:44 - INFO - __main__ - ['happy']
06/17/2022 18:16:44 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 18:16:44 - INFO - __main__ - ['happy']
06/17/2022 18:16:44 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:16:44 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:16:45 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 18:16:49 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.8022490675276959 on epoch=93
06/17/2022 18:16:49 - INFO - __main__ - save last model!
06/17/2022 18:16:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:16:49 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 18:16:49 - INFO - __main__ - Printing 3 examples
06/17/2022 18:16:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 18:16:49 - INFO - __main__ - ['others']
06/17/2022 18:16:49 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 18:16:49 - INFO - __main__ - ['others']
06/17/2022 18:16:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 18:16:49 - INFO - __main__ - ['others']
06/17/2022 18:16:49 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:16:51 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:16:56 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 18:17:02 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:17:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:17:03 - INFO - __main__ - Starting training!
06/17/2022 18:18:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_42_0.3_8_predictions.txt
06/17/2022 18:18:09 - INFO - __main__ - Classification-F1 on test data: 0.4040
06/17/2022 18:18:10 - INFO - __main__ - prefix=emo_128_42, lr=0.3, bsz=8, dev_performance=0.8512067404988483, test_performance=0.4040057263169907
06/17/2022 18:18:10 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.2, bsz=8 ...
06/17/2022 18:18:11 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:18:11 - INFO - __main__ - Printing 3 examples
06/17/2022 18:18:11 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:18:11 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:18:11 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 18:18:11 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:18:11 - INFO - __main__ - Printing 3 examples
06/17/2022 18:18:11 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
06/17/2022 18:18:11 - INFO - __main__ - ['happy']
06/17/2022 18:18:11 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:18:12 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:18:12 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 18:18:27 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:18:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:18:28 - INFO - __main__ - Starting training!
06/17/2022 18:18:31 - INFO - __main__ - Step 10 Global step 10 Train loss 3.16 on epoch=0
06/17/2022 18:18:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.91 on epoch=0
06/17/2022 18:18:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.53 on epoch=0
06/17/2022 18:18:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.30 on epoch=1
06/17/2022 18:18:41 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=1
06/17/2022 18:18:48 - INFO - __main__ - Global step 50 Train loss 1.80 Classification-F1 0.17427270192397065 on epoch=1
06/17/2022 18:18:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17427270192397065 on epoch=1, global_step=50
06/17/2022 18:18:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.15 on epoch=1
06/17/2022 18:18:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=2
06/17/2022 18:18:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=2
06/17/2022 18:18:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=2
06/17/2022 18:19:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=3
06/17/2022 18:19:08 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.34594793634227095 on epoch=3
06/17/2022 18:19:08 - INFO - __main__ - Saving model with best Classification-F1: 0.17427270192397065 -> 0.34594793634227095 on epoch=3, global_step=100
06/17/2022 18:19:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=3
06/17/2022 18:19:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=3
06/17/2022 18:19:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=4
06/17/2022 18:19:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=4
06/17/2022 18:19:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
06/17/2022 18:19:27 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.2792134885790608 on epoch=4
06/17/2022 18:19:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=4
06/17/2022 18:19:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=5
06/17/2022 18:19:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=5
06/17/2022 18:19:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=5
06/17/2022 18:19:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=6
06/17/2022 18:19:46 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.22774425590883646 on epoch=6
06/17/2022 18:19:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=6
06/17/2022 18:19:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=6
06/17/2022 18:19:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=7
06/17/2022 18:19:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=7
06/17/2022 18:19:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=7
06/17/2022 18:20:05 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.2933261476345665 on epoch=7
06/17/2022 18:20:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=8
06/17/2022 18:20:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=8
06/17/2022 18:20:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.75 on epoch=8
06/17/2022 18:20:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=9
06/17/2022 18:20:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=9
06/17/2022 18:20:24 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.519328483686761 on epoch=9
06/17/2022 18:20:24 - INFO - __main__ - Saving model with best Classification-F1: 0.34594793634227095 -> 0.519328483686761 on epoch=9, global_step=300
06/17/2022 18:20:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=9
06/17/2022 18:20:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=9
06/17/2022 18:20:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=10
06/17/2022 18:20:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
06/17/2022 18:20:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.83 on epoch=10
06/17/2022 18:20:43 - INFO - __main__ - Global step 350 Train loss 0.76 Classification-F1 0.5262403782937116 on epoch=10
06/17/2022 18:20:43 - INFO - __main__ - Saving model with best Classification-F1: 0.519328483686761 -> 0.5262403782937116 on epoch=10, global_step=350
06/17/2022 18:20:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.74 on epoch=11
06/17/2022 18:20:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=11
06/17/2022 18:20:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=11
06/17/2022 18:20:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=12
06/17/2022 18:20:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=12
06/17/2022 18:21:02 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.6086817608841635 on epoch=12
06/17/2022 18:21:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5262403782937116 -> 0.6086817608841635 on epoch=12, global_step=400
06/17/2022 18:21:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=12
06/17/2022 18:21:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=13
06/17/2022 18:21:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=13
06/17/2022 18:21:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=13
06/17/2022 18:21:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=14
06/17/2022 18:21:21 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.686267529124672 on epoch=14
06/17/2022 18:21:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6086817608841635 -> 0.686267529124672 on epoch=14, global_step=450
06/17/2022 18:21:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.70 on epoch=14
06/17/2022 18:21:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=14
06/17/2022 18:21:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=14
06/17/2022 18:21:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.70 on epoch=15
06/17/2022 18:21:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=15
06/17/2022 18:21:40 - INFO - __main__ - Global step 500 Train loss 0.67 Classification-F1 0.6329685344104061 on epoch=15
06/17/2022 18:21:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=15
06/17/2022 18:21:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=16
06/17/2022 18:21:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=16
06/17/2022 18:21:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=16
06/17/2022 18:21:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=17
06/17/2022 18:22:00 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6926556582839266 on epoch=17
06/17/2022 18:22:00 - INFO - __main__ - Saving model with best Classification-F1: 0.686267529124672 -> 0.6926556582839266 on epoch=17, global_step=550
06/17/2022 18:22:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=17
06/17/2022 18:22:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=17
06/17/2022 18:22:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.65 on epoch=18
06/17/2022 18:22:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=18
06/17/2022 18:22:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.59 on epoch=18
06/17/2022 18:22:20 - INFO - __main__ - Global step 600 Train loss 0.62 Classification-F1 0.7335570992391227 on epoch=18
06/17/2022 18:22:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6926556582839266 -> 0.7335570992391227 on epoch=18, global_step=600
06/17/2022 18:22:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=19
06/17/2022 18:22:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=19
06/17/2022 18:22:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=19
06/17/2022 18:22:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.59 on epoch=19
06/17/2022 18:22:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=20
06/17/2022 18:22:40 - INFO - __main__ - Global step 650 Train loss 0.61 Classification-F1 0.6744056528402648 on epoch=20
06/17/2022 18:22:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=20
06/17/2022 18:22:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.69 on epoch=20
06/17/2022 18:22:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=21
06/17/2022 18:22:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.63 on epoch=21
06/17/2022 18:22:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=21
06/17/2022 18:23:00 - INFO - __main__ - Global step 700 Train loss 0.59 Classification-F1 0.7533441558441558 on epoch=21
06/17/2022 18:23:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7335570992391227 -> 0.7533441558441558 on epoch=21, global_step=700
06/17/2022 18:23:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=22
06/17/2022 18:23:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=22
06/17/2022 18:23:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.63 on epoch=22
06/17/2022 18:23:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=23
06/17/2022 18:23:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=23
06/17/2022 18:23:20 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.7475243150645188 on epoch=23
06/17/2022 18:23:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=23
06/17/2022 18:23:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=24
06/17/2022 18:23:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=24
06/17/2022 18:23:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=24
06/17/2022 18:23:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=24
06/17/2022 18:23:40 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.789920585064988 on epoch=24
06/17/2022 18:23:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7533441558441558 -> 0.789920585064988 on epoch=24, global_step=800
06/17/2022 18:23:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=25
06/17/2022 18:23:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=25
06/17/2022 18:23:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=25
06/17/2022 18:23:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=26
06/17/2022 18:23:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=26
06/17/2022 18:24:00 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.678639413097418 on epoch=26
06/17/2022 18:24:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.58 on epoch=26
06/17/2022 18:24:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.51 on epoch=27
06/17/2022 18:24:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=27
06/17/2022 18:24:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.61 on epoch=27
06/17/2022 18:24:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=28
06/17/2022 18:24:20 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.7296801793994312 on epoch=28
06/17/2022 18:24:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=28
06/17/2022 18:24:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=28
06/17/2022 18:24:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=29
06/17/2022 18:24:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=29
06/17/2022 18:24:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=29
06/17/2022 18:24:40 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.7450801846339372 on epoch=29
06/17/2022 18:24:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=29
06/17/2022 18:24:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=30
06/17/2022 18:24:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=30
06/17/2022 18:24:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=30
06/17/2022 18:24:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=31
06/17/2022 18:25:00 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.6528102524198043 on epoch=31
06/17/2022 18:25:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=31
06/17/2022 18:25:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=31
06/17/2022 18:25:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=32
06/17/2022 18:25:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=32
06/17/2022 18:25:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.54 on epoch=32
06/17/2022 18:25:20 - INFO - __main__ - Global step 1050 Train loss 0.48 Classification-F1 0.7269103603189407 on epoch=32
06/17/2022 18:25:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=33
06/17/2022 18:25:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=33
06/17/2022 18:25:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=33
06/17/2022 18:25:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=34
06/17/2022 18:25:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=34
06/17/2022 18:25:40 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.7950018032147339 on epoch=34
06/17/2022 18:25:40 - INFO - __main__ - Saving model with best Classification-F1: 0.789920585064988 -> 0.7950018032147339 on epoch=34, global_step=1100
06/17/2022 18:25:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=34
06/17/2022 18:25:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=34
06/17/2022 18:25:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=35
06/17/2022 18:25:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=35
06/17/2022 18:25:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=35
06/17/2022 18:25:59 - INFO - __main__ - Global step 1150 Train loss 0.51 Classification-F1 0.792991373237549 on epoch=35
06/17/2022 18:26:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=36
06/17/2022 18:26:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=36
06/17/2022 18:26:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=36
06/17/2022 18:26:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=37
06/17/2022 18:26:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
06/17/2022 18:26:18 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.7809436060261288 on epoch=37
06/17/2022 18:26:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=37
06/17/2022 18:26:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=38
06/17/2022 18:26:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=38
06/17/2022 18:26:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=38
06/17/2022 18:26:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=39
06/17/2022 18:26:38 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.8191057671668365 on epoch=39
06/17/2022 18:26:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7950018032147339 -> 0.8191057671668365 on epoch=39, global_step=1250
06/17/2022 18:26:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=39
06/17/2022 18:26:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=39
06/17/2022 18:26:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=39
06/17/2022 18:26:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=40
06/17/2022 18:26:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=40
06/17/2022 18:26:57 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.7769237791636103 on epoch=40
06/17/2022 18:26:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=40
06/17/2022 18:27:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=41
06/17/2022 18:27:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=41
06/17/2022 18:27:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=41
06/17/2022 18:27:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=42
06/17/2022 18:27:16 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.7937378904130823 on epoch=42
06/17/2022 18:27:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=42
06/17/2022 18:27:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=42
06/17/2022 18:27:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=43
06/17/2022 18:27:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=43
06/17/2022 18:27:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=43
06/17/2022 18:27:35 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.7027824903819156 on epoch=43
06/17/2022 18:27:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=44
06/17/2022 18:27:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=44
06/17/2022 18:27:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=44
06/17/2022 18:27:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=44
06/17/2022 18:27:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=45
06/17/2022 18:27:54 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.7994997037414477 on epoch=45
06/17/2022 18:27:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
06/17/2022 18:27:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=45
06/17/2022 18:28:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=46
06/17/2022 18:28:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=46
06/17/2022 18:28:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=46
06/17/2022 18:28:13 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.7939441154733577 on epoch=46
06/17/2022 18:28:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=47
06/17/2022 18:28:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=47
06/17/2022 18:28:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=47
06/17/2022 18:28:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=48
06/17/2022 18:28:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=48
06/17/2022 18:28:32 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.7875156751320219 on epoch=48
06/17/2022 18:28:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=48
06/17/2022 18:28:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=49
06/17/2022 18:28:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=49
06/17/2022 18:28:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=49
06/17/2022 18:28:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=49
06/17/2022 18:28:51 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.7991166031630044 on epoch=49
06/17/2022 18:28:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=50
06/17/2022 18:28:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=50
06/17/2022 18:28:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=50
06/17/2022 18:29:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=51
06/17/2022 18:29:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=51
06/17/2022 18:29:10 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.7854355753126463 on epoch=51
06/17/2022 18:29:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=51
06/17/2022 18:29:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=52
06/17/2022 18:29:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=52
06/17/2022 18:29:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=52
06/17/2022 18:29:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=53
06/17/2022 18:29:29 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.8297992328341746 on epoch=53
06/17/2022 18:29:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8191057671668365 -> 0.8297992328341746 on epoch=53, global_step=1700
06/17/2022 18:29:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=53
06/17/2022 18:29:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=53
06/17/2022 18:29:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=54
06/17/2022 18:29:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=54
06/17/2022 18:29:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=54
06/17/2022 18:29:48 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.7821709557890687 on epoch=54
06/17/2022 18:29:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.49 on epoch=54
06/17/2022 18:29:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=55
06/17/2022 18:29:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=55
06/17/2022 18:29:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=55
06/17/2022 18:30:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=56
06/17/2022 18:30:06 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.7667396403938549 on epoch=56
06/17/2022 18:30:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=56
06/17/2022 18:30:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=56
06/17/2022 18:30:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=57
06/17/2022 18:30:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=57
06/17/2022 18:30:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=57
06/17/2022 18:30:25 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.7522212999112632 on epoch=57
06/17/2022 18:30:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=58
06/17/2022 18:30:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=58
06/17/2022 18:30:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=58
06/17/2022 18:30:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=59
06/17/2022 18:30:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=59
06/17/2022 18:30:44 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.8092523685288103 on epoch=59
06/17/2022 18:30:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=59
06/17/2022 18:30:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=59
06/17/2022 18:30:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=60
06/17/2022 18:30:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=60
06/17/2022 18:30:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=60
06/17/2022 18:31:03 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.7968830362460071 on epoch=60
06/17/2022 18:31:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=61
06/17/2022 18:31:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=61
06/17/2022 18:31:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=61
06/17/2022 18:31:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=62
06/17/2022 18:31:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=62
06/17/2022 18:31:22 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.8238706285265085 on epoch=62
06/17/2022 18:31:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=62
06/17/2022 18:31:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.40 on epoch=63
06/17/2022 18:31:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=63
06/17/2022 18:31:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=63
06/17/2022 18:31:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=64
06/17/2022 18:31:41 - INFO - __main__ - Global step 2050 Train loss 0.36 Classification-F1 0.8210955828410362 on epoch=64
06/17/2022 18:31:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=64
06/17/2022 18:31:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=64
06/17/2022 18:31:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.32 on epoch=64
06/17/2022 18:31:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=65
06/17/2022 18:31:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=65
06/17/2022 18:32:00 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.8267585667039059 on epoch=65
06/17/2022 18:32:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=65
06/17/2022 18:32:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=66
06/17/2022 18:32:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=66
06/17/2022 18:32:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=66
06/17/2022 18:32:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=67
06/17/2022 18:32:19 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.7913029289424656 on epoch=67
06/17/2022 18:32:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.32 on epoch=67
06/17/2022 18:32:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.28 on epoch=67
06/17/2022 18:32:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=68
06/17/2022 18:32:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=68
06/17/2022 18:32:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=68
06/17/2022 18:32:38 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.7745928469366927 on epoch=68
06/17/2022 18:32:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=69
06/17/2022 18:32:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=69
06/17/2022 18:32:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=69
06/17/2022 18:32:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=69
06/17/2022 18:32:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=70
06/17/2022 18:32:57 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.8114496003954358 on epoch=70
06/17/2022 18:32:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=70
06/17/2022 18:33:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.28 on epoch=70
06/17/2022 18:33:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.29 on epoch=71
06/17/2022 18:33:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=71
06/17/2022 18:33:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=71
06/17/2022 18:33:16 - INFO - __main__ - Global step 2300 Train loss 0.31 Classification-F1 0.8092294922913708 on epoch=71
06/17/2022 18:33:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=72
06/17/2022 18:33:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=72
06/17/2022 18:33:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=72
06/17/2022 18:33:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=73
06/17/2022 18:33:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=73
06/17/2022 18:33:35 - INFO - __main__ - Global step 2350 Train loss 0.34 Classification-F1 0.816669634979148 on epoch=73
06/17/2022 18:33:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=73
06/17/2022 18:33:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=74
06/17/2022 18:33:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=74
06/17/2022 18:33:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
06/17/2022 18:33:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=74
06/17/2022 18:33:54 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.8349343003659391 on epoch=74
06/17/2022 18:33:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8297992328341746 -> 0.8349343003659391 on epoch=74, global_step=2400
06/17/2022 18:33:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.32 on epoch=75
06/17/2022 18:33:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=75
06/17/2022 18:34:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=75
06/17/2022 18:34:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=76
06/17/2022 18:34:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.26 on epoch=76
06/17/2022 18:34:13 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.823949160595502 on epoch=76
06/17/2022 18:34:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=76
06/17/2022 18:34:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=77
06/17/2022 18:34:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=77
06/17/2022 18:34:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.24 on epoch=77
06/17/2022 18:34:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=78
06/17/2022 18:34:32 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.8555123242969129 on epoch=78
06/17/2022 18:34:32 - INFO - __main__ - Saving model with best Classification-F1: 0.8349343003659391 -> 0.8555123242969129 on epoch=78, global_step=2500
06/17/2022 18:34:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=78
06/17/2022 18:34:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=78
06/17/2022 18:34:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=79
06/17/2022 18:34:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.32 on epoch=79
06/17/2022 18:34:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
06/17/2022 18:34:50 - INFO - __main__ - Global step 2550 Train loss 0.31 Classification-F1 0.8058110529404345 on epoch=79
06/17/2022 18:34:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.30 on epoch=79
06/17/2022 18:34:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=80
06/17/2022 18:34:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=80
06/17/2022 18:35:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=80
06/17/2022 18:35:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=81
06/17/2022 18:35:09 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.8313621097878262 on epoch=81
06/17/2022 18:35:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=81
06/17/2022 18:35:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=81
06/17/2022 18:35:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=82
06/17/2022 18:35:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=82
06/17/2022 18:35:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=82
06/17/2022 18:35:28 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.8078015754364615 on epoch=82
06/17/2022 18:35:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=83
06/17/2022 18:35:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.29 on epoch=83
06/17/2022 18:35:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.36 on epoch=83
06/17/2022 18:35:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=84
06/17/2022 18:35:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.30 on epoch=84
06/17/2022 18:35:47 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.8265969008442338 on epoch=84
06/17/2022 18:35:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=84
06/17/2022 18:35:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.30 on epoch=84
06/17/2022 18:35:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.26 on epoch=85
06/17/2022 18:35:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=85
06/17/2022 18:35:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=85
06/17/2022 18:36:06 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.8275870427536934 on epoch=85
06/17/2022 18:36:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.24 on epoch=86
06/17/2022 18:36:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=86
06/17/2022 18:36:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
06/17/2022 18:36:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.29 on epoch=87
06/17/2022 18:36:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=87
06/17/2022 18:36:25 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.8379219388998634 on epoch=87
06/17/2022 18:36:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=87
06/17/2022 18:36:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.24 on epoch=88
06/17/2022 18:36:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=88
06/17/2022 18:36:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.27 on epoch=88
06/17/2022 18:36:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.25 on epoch=89
06/17/2022 18:36:44 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.849738995868987 on epoch=89
06/17/2022 18:36:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.32 on epoch=89
06/17/2022 18:36:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=89
06/17/2022 18:36:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=89
06/17/2022 18:36:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=90
06/17/2022 18:36:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=90
06/17/2022 18:37:03 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.8306698789316419 on epoch=90
06/17/2022 18:37:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=90
06/17/2022 18:37:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=91
06/17/2022 18:37:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=91
06/17/2022 18:37:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.33 on epoch=91
06/17/2022 18:37:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=92
06/17/2022 18:37:22 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.8533979805917433 on epoch=92
06/17/2022 18:37:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=92
06/17/2022 18:37:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
06/17/2022 18:37:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.29 on epoch=93
06/17/2022 18:37:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=93
06/17/2022 18:37:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.21 on epoch=93
06/17/2022 18:37:36 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:37:36 - INFO - __main__ - Printing 3 examples
06/17/2022 18:37:36 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:37:36 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:37:36 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 18:37:36 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:37:36 - INFO - __main__ - Printing 3 examples
06/17/2022 18:37:36 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 18:37:36 - INFO - __main__ - ['others']
06/17/2022 18:37:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:37:37 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:37:37 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 18:37:41 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.8228233957753408 on epoch=93
06/17/2022 18:37:41 - INFO - __main__ - save last model!
06/17/2022 18:37:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:37:41 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 18:37:41 - INFO - __main__ - Printing 3 examples
06/17/2022 18:37:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 18:37:41 - INFO - __main__ - ['others']
06/17/2022 18:37:41 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 18:37:41 - INFO - __main__ - ['others']
06/17/2022 18:37:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 18:37:41 - INFO - __main__ - ['others']
06/17/2022 18:37:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:37:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:37:49 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 18:37:55 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:37:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:37:56 - INFO - __main__ - Starting training!
06/17/2022 18:39:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_42_0.2_8_predictions.txt
06/17/2022 18:39:01 - INFO - __main__ - Classification-F1 on test data: 0.4500
06/17/2022 18:39:02 - INFO - __main__ - prefix=emo_128_42, lr=0.2, bsz=8, dev_performance=0.8555123242969129, test_performance=0.4500260621854815
06/17/2022 18:39:07 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.5, bsz=8 ...
06/17/2022 18:39:08 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:39:08 - INFO - __main__ - Printing 3 examples
06/17/2022 18:39:08 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 18:39:08 - INFO - __main__ - ['others']
06/17/2022 18:39:08 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 18:39:08 - INFO - __main__ - ['others']
06/17/2022 18:39:08 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 18:39:08 - INFO - __main__ - ['others']
06/17/2022 18:39:08 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:39:08 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:39:09 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 18:39:09 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:39:09 - INFO - __main__ - Printing 3 examples
06/17/2022 18:39:09 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 18:39:09 - INFO - __main__ - ['others']
06/17/2022 18:39:09 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 18:39:09 - INFO - __main__ - ['others']
06/17/2022 18:39:09 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 18:39:09 - INFO - __main__ - ['others']
06/17/2022 18:39:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:39:09 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:39:09 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 18:39:28 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:39:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:39:29 - INFO - __main__ - Starting training!
06/17/2022 18:39:32 - INFO - __main__ - Step 10 Global step 10 Train loss 2.70 on epoch=0
06/17/2022 18:39:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.33 on epoch=0
06/17/2022 18:39:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=0
06/17/2022 18:39:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=1
06/17/2022 18:39:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=1
06/17/2022 18:39:49 - INFO - __main__ - Global step 50 Train loss 1.43 Classification-F1 0.1 on epoch=1
06/17/2022 18:39:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 18:39:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=1
06/17/2022 18:39:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=2
06/17/2022 18:39:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=2
06/17/2022 18:39:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=2
06/17/2022 18:40:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=3
06/17/2022 18:40:08 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.19389467871292534 on epoch=3
06/17/2022 18:40:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.19389467871292534 on epoch=3, global_step=100
06/17/2022 18:40:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=3
06/17/2022 18:40:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=3
06/17/2022 18:40:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=4
06/17/2022 18:40:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=4
06/17/2022 18:40:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=4
06/17/2022 18:40:28 - INFO - __main__ - Global step 150 Train loss 0.83 Classification-F1 0.4080680734777393 on epoch=4
06/17/2022 18:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.19389467871292534 -> 0.4080680734777393 on epoch=4, global_step=150
06/17/2022 18:40:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=4
06/17/2022 18:40:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=5
06/17/2022 18:40:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=5
06/17/2022 18:40:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=5
06/17/2022 18:40:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=6
06/17/2022 18:40:48 - INFO - __main__ - Global step 200 Train loss 0.75 Classification-F1 0.4628966326525606 on epoch=6
06/17/2022 18:40:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4080680734777393 -> 0.4628966326525606 on epoch=6, global_step=200
06/17/2022 18:40:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=6
06/17/2022 18:40:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.70 on epoch=6
06/17/2022 18:40:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.65 on epoch=7
06/17/2022 18:40:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=7
06/17/2022 18:41:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=7
06/17/2022 18:41:07 - INFO - __main__ - Global step 250 Train loss 0.68 Classification-F1 0.5877714973069608 on epoch=7
06/17/2022 18:41:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4628966326525606 -> 0.5877714973069608 on epoch=7, global_step=250
06/17/2022 18:41:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=8
06/17/2022 18:41:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=8
06/17/2022 18:41:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=8
06/17/2022 18:41:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=9
06/17/2022 18:41:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=9
06/17/2022 18:41:27 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.6616430466121076 on epoch=9
06/17/2022 18:41:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5877714973069608 -> 0.6616430466121076 on epoch=9, global_step=300
06/17/2022 18:41:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=9
06/17/2022 18:41:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=9
06/17/2022 18:41:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=10
06/17/2022 18:41:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=10
06/17/2022 18:41:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=10
06/17/2022 18:41:46 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.7220478212010392 on epoch=10
06/17/2022 18:41:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6616430466121076 -> 0.7220478212010392 on epoch=10, global_step=350
06/17/2022 18:41:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=11
06/17/2022 18:41:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=11
06/17/2022 18:41:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=11
06/17/2022 18:41:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=12
06/17/2022 18:41:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=12
06/17/2022 18:42:06 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.6768859827489999 on epoch=12
06/17/2022 18:42:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=12
06/17/2022 18:42:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=13
06/17/2022 18:42:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=13
06/17/2022 18:42:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=13
06/17/2022 18:42:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=14
06/17/2022 18:42:25 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.5618315846112456 on epoch=14
06/17/2022 18:42:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=14
06/17/2022 18:42:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=14
06/17/2022 18:42:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=14
06/17/2022 18:42:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=15
06/17/2022 18:42:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=15
06/17/2022 18:42:45 - INFO - __main__ - Global step 500 Train loss 0.56 Classification-F1 0.5457991685930123 on epoch=15
06/17/2022 18:42:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=15
06/17/2022 18:42:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=16
06/17/2022 18:42:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=16
06/17/2022 18:42:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=16
06/17/2022 18:42:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=17
06/17/2022 18:43:04 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.605828142961243 on epoch=17
06/17/2022 18:43:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=17
06/17/2022 18:43:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=17
06/17/2022 18:43:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=18
06/17/2022 18:43:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=18
06/17/2022 18:43:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=18
06/17/2022 18:43:23 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.7540229166852477 on epoch=18
06/17/2022 18:43:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7220478212010392 -> 0.7540229166852477 on epoch=18, global_step=600
06/17/2022 18:43:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=19
06/17/2022 18:43:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=19
06/17/2022 18:43:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=19
06/17/2022 18:43:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=19
06/17/2022 18:43:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=20
06/17/2022 18:43:43 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.7075861505174764 on epoch=20
06/17/2022 18:43:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=20
06/17/2022 18:43:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=20
06/17/2022 18:43:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=21
06/17/2022 18:43:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=21
06/17/2022 18:43:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=21
06/17/2022 18:44:02 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.7952740103689843 on epoch=21
06/17/2022 18:44:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7540229166852477 -> 0.7952740103689843 on epoch=21, global_step=700
06/17/2022 18:44:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=22
06/17/2022 18:44:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=22
06/17/2022 18:44:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=22
06/17/2022 18:44:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=23
06/17/2022 18:44:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=23
06/17/2022 18:44:22 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.7874935915249466 on epoch=23
06/17/2022 18:44:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=23
06/17/2022 18:44:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=24
06/17/2022 18:44:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.59 on epoch=24
06/17/2022 18:44:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=24
06/17/2022 18:44:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=24
06/17/2022 18:44:41 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.7150010407476155 on epoch=24
06/17/2022 18:44:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=25
06/17/2022 18:44:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=25
06/17/2022 18:44:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=25
06/17/2022 18:44:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=26
06/17/2022 18:44:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=26
06/17/2022 18:45:01 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.689960910632308 on epoch=26
06/17/2022 18:45:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=26
06/17/2022 18:45:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=27
06/17/2022 18:45:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=27
06/17/2022 18:45:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=27
06/17/2022 18:45:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=28
06/17/2022 18:45:20 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.6436608466719101 on epoch=28
06/17/2022 18:45:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=28
06/17/2022 18:45:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=28
06/17/2022 18:45:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=29
06/17/2022 18:45:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=29
06/17/2022 18:45:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
06/17/2022 18:45:40 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.7610870505048795 on epoch=29
06/17/2022 18:45:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=29
06/17/2022 18:45:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=30
06/17/2022 18:45:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=30
06/17/2022 18:45:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=30
06/17/2022 18:45:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=31
06/17/2022 18:45:59 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.7726221314710616 on epoch=31
06/17/2022 18:46:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=31
06/17/2022 18:46:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=31
06/17/2022 18:46:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=32
06/17/2022 18:46:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=32
06/17/2022 18:46:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=32
06/17/2022 18:46:19 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.6955743585121017 on epoch=32
06/17/2022 18:46:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=33
06/17/2022 18:46:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=33
06/17/2022 18:46:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=33
06/17/2022 18:46:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=34
06/17/2022 18:46:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=34
06/17/2022 18:46:39 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.7885878356093834 on epoch=34
06/17/2022 18:46:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=34
06/17/2022 18:46:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
06/17/2022 18:46:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=35
06/17/2022 18:46:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
06/17/2022 18:46:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=35
06/17/2022 18:46:58 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.7969326432942258 on epoch=35
06/17/2022 18:46:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7952740103689843 -> 0.7969326432942258 on epoch=35, global_step=1150
06/17/2022 18:47:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
06/17/2022 18:47:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=36
06/17/2022 18:47:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=36
06/17/2022 18:47:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=37
06/17/2022 18:47:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=37
06/17/2022 18:47:18 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.7916059120521322 on epoch=37
06/17/2022 18:47:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=37
06/17/2022 18:47:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=38
06/17/2022 18:47:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=38
06/17/2022 18:47:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=38
06/17/2022 18:47:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=39
06/17/2022 18:47:37 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.7354850995795696 on epoch=39
06/17/2022 18:47:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=39
06/17/2022 18:47:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=39
06/17/2022 18:47:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=39
06/17/2022 18:47:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=40
06/17/2022 18:47:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=40
06/17/2022 18:47:57 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.7503819364714888 on epoch=40
06/17/2022 18:47:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=40
06/17/2022 18:48:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=41
06/17/2022 18:48:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=41
06/17/2022 18:48:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=41
06/17/2022 18:48:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=42
06/17/2022 18:48:16 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.7800953808294708 on epoch=42
06/17/2022 18:48:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=42
06/17/2022 18:48:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=42
06/17/2022 18:48:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=43
06/17/2022 18:48:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=43
06/17/2022 18:48:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=43
06/17/2022 18:48:36 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.7878730702587843 on epoch=43
06/17/2022 18:48:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
06/17/2022 18:48:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=44
06/17/2022 18:48:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=44
06/17/2022 18:48:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=44
06/17/2022 18:48:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=45
06/17/2022 18:48:55 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.7523348609164144 on epoch=45
06/17/2022 18:48:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
06/17/2022 18:49:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=45
06/17/2022 18:49:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=46
06/17/2022 18:49:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=46
06/17/2022 18:49:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=46
06/17/2022 18:49:15 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.8026706472236967 on epoch=46
06/17/2022 18:49:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7969326432942258 -> 0.8026706472236967 on epoch=46, global_step=1500
06/17/2022 18:49:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=47
06/17/2022 18:49:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=47
06/17/2022 18:49:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=47
06/17/2022 18:49:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
06/17/2022 18:49:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=48
06/17/2022 18:49:34 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7927075464062725 on epoch=48
06/17/2022 18:49:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=48
06/17/2022 18:49:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=49
06/17/2022 18:49:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=49
06/17/2022 18:49:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=49
06/17/2022 18:49:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=49
06/17/2022 18:49:54 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.7955160228947294 on epoch=49
06/17/2022 18:49:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=50
06/17/2022 18:49:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
06/17/2022 18:50:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=50
06/17/2022 18:50:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=51
06/17/2022 18:50:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=51
06/17/2022 18:50:13 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.7710442885555133 on epoch=51
06/17/2022 18:50:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=51
06/17/2022 18:50:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=52
06/17/2022 18:50:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=52
06/17/2022 18:50:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=52
06/17/2022 18:50:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=53
06/17/2022 18:50:33 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.7092153032741636 on epoch=53
06/17/2022 18:50:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=53
06/17/2022 18:50:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=53
06/17/2022 18:50:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=54
06/17/2022 18:50:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=54
06/17/2022 18:50:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=54
06/17/2022 18:50:52 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.7968187766139577 on epoch=54
06/17/2022 18:50:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=54
06/17/2022 18:50:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=55
06/17/2022 18:51:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=55
06/17/2022 18:51:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=55
06/17/2022 18:51:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=56
06/17/2022 18:51:12 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.8171642908371713 on epoch=56
06/17/2022 18:51:12 - INFO - __main__ - Saving model with best Classification-F1: 0.8026706472236967 -> 0.8171642908371713 on epoch=56, global_step=1800
06/17/2022 18:51:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=56
06/17/2022 18:51:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
06/17/2022 18:51:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=57
06/17/2022 18:51:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=57
06/17/2022 18:51:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=57
06/17/2022 18:51:31 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.7781685509188037 on epoch=57
06/17/2022 18:51:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=58
06/17/2022 18:51:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=58
06/17/2022 18:51:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=58
06/17/2022 18:51:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=59
06/17/2022 18:51:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=59
06/17/2022 18:51:51 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.7986722820955314 on epoch=59
06/17/2022 18:51:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=59
06/17/2022 18:51:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=59
06/17/2022 18:51:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=60
06/17/2022 18:52:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=60
06/17/2022 18:52:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=60
06/17/2022 18:52:10 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7714765430023315 on epoch=60
06/17/2022 18:52:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=61
06/17/2022 18:52:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=61
06/17/2022 18:52:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=61
06/17/2022 18:52:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=62
06/17/2022 18:52:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=62
06/17/2022 18:52:30 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.8107926276682083 on epoch=62
06/17/2022 18:52:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=62
06/17/2022 18:52:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=63
06/17/2022 18:52:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=63
06/17/2022 18:52:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=63
06/17/2022 18:52:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=64
06/17/2022 18:52:49 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.8140036506757314 on epoch=64
06/17/2022 18:52:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
06/17/2022 18:52:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=64
06/17/2022 18:52:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=64
06/17/2022 18:52:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=65
06/17/2022 18:53:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=65
06/17/2022 18:53:09 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7910798256728488 on epoch=65
06/17/2022 18:53:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=65
06/17/2022 18:53:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=66
06/17/2022 18:53:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.28 on epoch=66
06/17/2022 18:53:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
06/17/2022 18:53:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=67
06/17/2022 18:53:28 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.7915312324461472 on epoch=67
06/17/2022 18:53:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=67
06/17/2022 18:53:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=67
06/17/2022 18:53:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=68
06/17/2022 18:53:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
06/17/2022 18:53:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=68
06/17/2022 18:53:48 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.7979903663314878 on epoch=68
06/17/2022 18:53:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=69
06/17/2022 18:53:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.24 on epoch=69
06/17/2022 18:53:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=69
06/17/2022 18:53:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=69
06/17/2022 18:54:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.31 on epoch=70
06/17/2022 18:54:07 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.7925685953306764 on epoch=70
06/17/2022 18:54:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=70
06/17/2022 18:54:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=70
06/17/2022 18:54:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=71
06/17/2022 18:54:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=71
06/17/2022 18:54:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=71
06/17/2022 18:54:27 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.7844817349725913 on epoch=71
06/17/2022 18:54:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=72
06/17/2022 18:54:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=72
06/17/2022 18:54:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=72
06/17/2022 18:54:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=73
06/17/2022 18:54:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=73
06/17/2022 18:54:46 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.8094267234452835 on epoch=73
06/17/2022 18:54:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=73
06/17/2022 18:54:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=74
06/17/2022 18:54:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
06/17/2022 18:54:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=74
06/17/2022 18:54:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=74
06/17/2022 18:55:06 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.7937569993412384 on epoch=74
06/17/2022 18:55:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=75
06/17/2022 18:55:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=75
06/17/2022 18:55:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=75
06/17/2022 18:55:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=76
06/17/2022 18:55:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=76
06/17/2022 18:55:26 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.7679225458852311 on epoch=76
06/17/2022 18:55:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=76
06/17/2022 18:55:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
06/17/2022 18:55:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=77
06/17/2022 18:55:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=77
06/17/2022 18:55:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=78
06/17/2022 18:55:45 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7712075081823483 on epoch=78
06/17/2022 18:55:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=78
06/17/2022 18:55:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.24 on epoch=78
06/17/2022 18:55:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=79
06/17/2022 18:55:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=79
06/17/2022 18:55:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=79
06/17/2022 18:56:05 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7752437232676694 on epoch=79
06/17/2022 18:56:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=79
06/17/2022 18:56:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=80
06/17/2022 18:56:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=80
06/17/2022 18:56:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
06/17/2022 18:56:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=81
06/17/2022 18:56:24 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.8285379524259149 on epoch=81
06/17/2022 18:56:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8171642908371713 -> 0.8285379524259149 on epoch=81, global_step=2600
06/17/2022 18:56:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=81
06/17/2022 18:56:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=81
06/17/2022 18:56:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=82
06/17/2022 18:56:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=82
06/17/2022 18:56:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=82
06/17/2022 18:56:44 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.7907223686172729 on epoch=82
06/17/2022 18:56:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=83
06/17/2022 18:56:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=83
06/17/2022 18:56:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=83
06/17/2022 18:56:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
06/17/2022 18:56:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=84
06/17/2022 18:57:04 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.8228145216725805 on epoch=84
06/17/2022 18:57:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=84
06/17/2022 18:57:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=84
06/17/2022 18:57:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=85
06/17/2022 18:57:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=85
06/17/2022 18:57:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=85
06/17/2022 18:57:23 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.7754300463311798 on epoch=85
06/17/2022 18:57:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=86
06/17/2022 18:57:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=86
06/17/2022 18:57:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=86
06/17/2022 18:57:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=87
06/17/2022 18:57:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=87
06/17/2022 18:57:43 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.8181232740993697 on epoch=87
06/17/2022 18:57:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=87
06/17/2022 18:57:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=88
06/17/2022 18:57:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=88
06/17/2022 18:57:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=88
06/17/2022 18:57:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=89
06/17/2022 18:58:02 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.8042293509490795 on epoch=89
06/17/2022 18:58:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=89
06/17/2022 18:58:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=89
06/17/2022 18:58:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=89
06/17/2022 18:58:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
06/17/2022 18:58:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=90
06/17/2022 18:58:22 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.7711797566119738 on epoch=90
06/17/2022 18:58:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=90
06/17/2022 18:58:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=91
06/17/2022 18:58:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=91
06/17/2022 18:58:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=91
06/17/2022 18:58:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=92
06/17/2022 18:58:42 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.7780450444010537 on epoch=92
06/17/2022 18:58:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=92
06/17/2022 18:58:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
06/17/2022 18:58:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=93
06/17/2022 18:58:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
06/17/2022 18:58:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=93
06/17/2022 18:58:56 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:58:56 - INFO - __main__ - Printing 3 examples
06/17/2022 18:58:56 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 18:58:56 - INFO - __main__ - ['others']
06/17/2022 18:58:56 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 18:58:56 - INFO - __main__ - ['others']
06/17/2022 18:58:56 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 18:58:56 - INFO - __main__ - ['others']
06/17/2022 18:58:56 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:58:56 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:58:57 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 18:58:57 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 18:58:57 - INFO - __main__ - Printing 3 examples
06/17/2022 18:58:57 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 18:58:57 - INFO - __main__ - ['others']
06/17/2022 18:58:57 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 18:58:57 - INFO - __main__ - ['others']
06/17/2022 18:58:57 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 18:58:57 - INFO - __main__ - ['others']
06/17/2022 18:58:57 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:58:57 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:58:58 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 18:59:01 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.794926904651901 on epoch=93
06/17/2022 18:59:01 - INFO - __main__ - save last model!
06/17/2022 18:59:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 18:59:01 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 18:59:01 - INFO - __main__ - Printing 3 examples
06/17/2022 18:59:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 18:59:01 - INFO - __main__ - ['others']
06/17/2022 18:59:01 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 18:59:01 - INFO - __main__ - ['others']
06/17/2022 18:59:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 18:59:01 - INFO - __main__ - ['others']
06/17/2022 18:59:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 18:59:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 18:59:09 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 18:59:15 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 18:59:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 18:59:16 - INFO - __main__ - Starting training!
06/17/2022 19:00:22 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_87_0.5_8_predictions.txt
06/17/2022 19:00:22 - INFO - __main__ - Classification-F1 on test data: 0.4507
06/17/2022 19:00:22 - INFO - __main__ - prefix=emo_128_87, lr=0.5, bsz=8, dev_performance=0.8285379524259149, test_performance=0.45065167160229463
06/17/2022 19:00:22 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.4, bsz=8 ...
06/17/2022 19:00:23 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:00:23 - INFO - __main__ - Printing 3 examples
06/17/2022 19:00:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 19:00:23 - INFO - __main__ - ['others']
06/17/2022 19:00:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 19:00:23 - INFO - __main__ - ['others']
06/17/2022 19:00:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 19:00:23 - INFO - __main__ - ['others']
06/17/2022 19:00:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:00:24 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:00:24 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 19:00:24 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:00:24 - INFO - __main__ - Printing 3 examples
06/17/2022 19:00:24 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 19:00:24 - INFO - __main__ - ['others']
06/17/2022 19:00:24 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 19:00:24 - INFO - __main__ - ['others']
06/17/2022 19:00:24 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 19:00:24 - INFO - __main__ - ['others']
06/17/2022 19:00:24 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:00:24 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:00:25 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 19:00:43 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:00:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:00:44 - INFO - __main__ - Starting training!
06/17/2022 19:00:47 - INFO - __main__ - Step 10 Global step 10 Train loss 2.63 on epoch=0
06/17/2022 19:00:50 - INFO - __main__ - Step 20 Global step 20 Train loss 1.54 on epoch=0
06/17/2022 19:00:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=0
06/17/2022 19:00:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=1
06/17/2022 19:00:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=1
06/17/2022 19:01:05 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.10403246351493978 on epoch=1
06/17/2022 19:01:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10403246351493978 on epoch=1, global_step=50
06/17/2022 19:01:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=1
06/17/2022 19:01:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=2
06/17/2022 19:01:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=2
06/17/2022 19:01:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=2
06/17/2022 19:01:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=3
06/17/2022 19:01:24 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.3037517849057918 on epoch=3
06/17/2022 19:01:24 - INFO - __main__ - Saving model with best Classification-F1: 0.10403246351493978 -> 0.3037517849057918 on epoch=3, global_step=100
06/17/2022 19:01:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=3
06/17/2022 19:01:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=3
06/17/2022 19:01:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=4
06/17/2022 19:01:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=4
06/17/2022 19:01:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
06/17/2022 19:01:44 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.5298665385171621 on epoch=4
06/17/2022 19:01:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3037517849057918 -> 0.5298665385171621 on epoch=4, global_step=150
06/17/2022 19:01:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=4
06/17/2022 19:01:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=5
06/17/2022 19:01:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=5
06/17/2022 19:01:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=5
06/17/2022 19:01:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=6
06/17/2022 19:02:03 - INFO - __main__ - Global step 200 Train loss 0.77 Classification-F1 0.5452205882352941 on epoch=6
06/17/2022 19:02:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5298665385171621 -> 0.5452205882352941 on epoch=6, global_step=200
06/17/2022 19:02:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.72 on epoch=6
06/17/2022 19:02:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=6
06/17/2022 19:02:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=7
06/17/2022 19:02:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=7
06/17/2022 19:02:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=7
06/17/2022 19:02:23 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.5861818803594464 on epoch=7
06/17/2022 19:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5452205882352941 -> 0.5861818803594464 on epoch=7, global_step=250
06/17/2022 19:02:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=8
06/17/2022 19:02:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.70 on epoch=8
06/17/2022 19:02:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.66 on epoch=8
06/17/2022 19:02:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=9
06/17/2022 19:02:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.69 on epoch=9
06/17/2022 19:02:42 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.6527130946282788 on epoch=9
06/17/2022 19:02:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5861818803594464 -> 0.6527130946282788 on epoch=9, global_step=300
06/17/2022 19:02:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=9
06/17/2022 19:02:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=9
06/17/2022 19:02:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=10
06/17/2022 19:02:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=10
06/17/2022 19:02:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.63 on epoch=10
06/17/2022 19:03:02 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5556761281734308 on epoch=10
06/17/2022 19:03:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.61 on epoch=11
06/17/2022 19:03:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=11
06/17/2022 19:03:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=11
06/17/2022 19:03:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=12
06/17/2022 19:03:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=12
06/17/2022 19:03:22 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.6739896832950181 on epoch=12
06/17/2022 19:03:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6527130946282788 -> 0.6739896832950181 on epoch=12, global_step=400
06/17/2022 19:03:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=12
06/17/2022 19:03:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=13
06/17/2022 19:03:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=13
06/17/2022 19:03:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=13
06/17/2022 19:03:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=14
06/17/2022 19:03:42 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.5737634325087997 on epoch=14
06/17/2022 19:03:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.64 on epoch=14
06/17/2022 19:03:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.66 on epoch=14
06/17/2022 19:03:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=14
06/17/2022 19:03:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=15
06/17/2022 19:03:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=15
06/17/2022 19:04:01 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.4692477131308537 on epoch=15
06/17/2022 19:04:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=15
06/17/2022 19:04:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=16
06/17/2022 19:04:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=16
06/17/2022 19:04:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=16
06/17/2022 19:04:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=17
06/17/2022 19:04:21 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.5785083791492109 on epoch=17
06/17/2022 19:04:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=17
06/17/2022 19:04:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=17
06/17/2022 19:04:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=18
06/17/2022 19:04:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=18
06/17/2022 19:04:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=18
06/17/2022 19:04:40 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.7325901950122216 on epoch=18
06/17/2022 19:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6739896832950181 -> 0.7325901950122216 on epoch=18, global_step=600
06/17/2022 19:04:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=19
06/17/2022 19:04:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=19
06/17/2022 19:04:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=19
06/17/2022 19:04:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=19
06/17/2022 19:04:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=20
06/17/2022 19:05:00 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.714032500578302 on epoch=20
06/17/2022 19:05:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=20
06/17/2022 19:05:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=20
06/17/2022 19:05:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=21
06/17/2022 19:05:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=21
06/17/2022 19:05:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=21
06/17/2022 19:05:20 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.695182526609035 on epoch=21
06/17/2022 19:05:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=22
06/17/2022 19:05:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=22
06/17/2022 19:05:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=22
06/17/2022 19:05:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=23
06/17/2022 19:05:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=23
06/17/2022 19:05:39 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.7815067345970094 on epoch=23
06/17/2022 19:05:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7325901950122216 -> 0.7815067345970094 on epoch=23, global_step=750
06/17/2022 19:05:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=23
06/17/2022 19:05:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=24
06/17/2022 19:05:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=24
06/17/2022 19:05:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=24
06/17/2022 19:05:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=24
06/17/2022 19:05:59 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.7867986377100231 on epoch=24
06/17/2022 19:05:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7815067345970094 -> 0.7867986377100231 on epoch=24, global_step=800
06/17/2022 19:06:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=25
06/17/2022 19:06:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=25
06/17/2022 19:06:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=25
06/17/2022 19:06:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=26
06/17/2022 19:06:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=26
06/17/2022 19:06:18 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.6624398494375612 on epoch=26
06/17/2022 19:06:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=26
06/17/2022 19:06:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=27
06/17/2022 19:06:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=27
06/17/2022 19:06:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=27
06/17/2022 19:06:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=28
06/17/2022 19:06:38 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.7343249314707048 on epoch=28
06/17/2022 19:06:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=28
06/17/2022 19:06:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=28
06/17/2022 19:06:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=29
06/17/2022 19:06:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=29
06/17/2022 19:06:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=29
06/17/2022 19:06:57 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.7543681505561668 on epoch=29
06/17/2022 19:07:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=29
06/17/2022 19:07:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=30
06/17/2022 19:07:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=30
06/17/2022 19:07:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=30
06/17/2022 19:07:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=31
06/17/2022 19:07:17 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.8082458466790445 on epoch=31
06/17/2022 19:07:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7867986377100231 -> 0.8082458466790445 on epoch=31, global_step=1000
06/17/2022 19:07:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=31
06/17/2022 19:07:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=31
06/17/2022 19:07:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=32
06/17/2022 19:07:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=32
06/17/2022 19:07:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=32
06/17/2022 19:07:36 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7835208293943403 on epoch=32
06/17/2022 19:07:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=33
06/17/2022 19:07:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=33
06/17/2022 19:07:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
06/17/2022 19:07:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=34
06/17/2022 19:07:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=34
06/17/2022 19:07:56 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.8114425153350668 on epoch=34
06/17/2022 19:07:56 - INFO - __main__ - Saving model with best Classification-F1: 0.8082458466790445 -> 0.8114425153350668 on epoch=34, global_step=1100
06/17/2022 19:07:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=34
06/17/2022 19:08:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=34
06/17/2022 19:08:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=35
06/17/2022 19:08:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
06/17/2022 19:08:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=35
06/17/2022 19:08:15 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.8145193731013582 on epoch=35
06/17/2022 19:08:15 - INFO - __main__ - Saving model with best Classification-F1: 0.8114425153350668 -> 0.8145193731013582 on epoch=35, global_step=1150
06/17/2022 19:08:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=36
06/17/2022 19:08:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=36
06/17/2022 19:08:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=36
06/17/2022 19:08:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=37
06/17/2022 19:08:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=37
06/17/2022 19:08:35 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.8029003267973855 on epoch=37
06/17/2022 19:08:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=37
06/17/2022 19:08:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=38
06/17/2022 19:08:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=38
06/17/2022 19:08:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
06/17/2022 19:08:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=39
06/17/2022 19:08:54 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7673292342587948 on epoch=39
06/17/2022 19:08:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=39
06/17/2022 19:08:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=39
06/17/2022 19:09:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=39
06/17/2022 19:09:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=40
06/17/2022 19:09:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=40
06/17/2022 19:09:14 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.750378036912845 on epoch=40
06/17/2022 19:09:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=40
06/17/2022 19:09:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
06/17/2022 19:09:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=41
06/17/2022 19:09:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=41
06/17/2022 19:09:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=42
06/17/2022 19:09:34 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.7920759637421894 on epoch=42
06/17/2022 19:09:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=42
06/17/2022 19:09:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=42
06/17/2022 19:09:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=43
06/17/2022 19:09:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=43
06/17/2022 19:09:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=43
06/17/2022 19:09:53 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.8078461122496893 on epoch=43
06/17/2022 19:09:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=44
06/17/2022 19:09:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=44
06/17/2022 19:10:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=44
06/17/2022 19:10:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=44
06/17/2022 19:10:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=45
06/17/2022 19:10:13 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.8204911241143257 on epoch=45
06/17/2022 19:10:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8145193731013582 -> 0.8204911241143257 on epoch=45, global_step=1450
06/17/2022 19:10:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=45
06/17/2022 19:10:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=45
06/17/2022 19:10:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=46
06/17/2022 19:10:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=46
06/17/2022 19:10:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=46
06/17/2022 19:10:32 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.814469605572829 on epoch=46
06/17/2022 19:10:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=47
06/17/2022 19:10:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=47
06/17/2022 19:10:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=47
06/17/2022 19:10:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=48
06/17/2022 19:10:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=48
06/17/2022 19:10:52 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.8047749946342183 on epoch=48
06/17/2022 19:10:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=48
06/17/2022 19:10:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=49
06/17/2022 19:10:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=49
06/17/2022 19:11:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=49
06/17/2022 19:11:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=49
06/17/2022 19:11:11 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.8160636767549045 on epoch=49
06/17/2022 19:11:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=50
06/17/2022 19:11:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=50
06/17/2022 19:11:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=50
06/17/2022 19:11:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
06/17/2022 19:11:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=51
06/17/2022 19:11:31 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.7260597180621529 on epoch=51
06/17/2022 19:11:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=51
06/17/2022 19:11:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=52
06/17/2022 19:11:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=52
06/17/2022 19:11:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=52
06/17/2022 19:11:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=53
06/17/2022 19:11:51 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.7395052853310567 on epoch=53
06/17/2022 19:11:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=53
06/17/2022 19:11:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=53
06/17/2022 19:11:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=54
06/17/2022 19:12:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=54
06/17/2022 19:12:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=54
06/17/2022 19:12:10 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.7788608917371574 on epoch=54
06/17/2022 19:12:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=54
06/17/2022 19:12:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
06/17/2022 19:12:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=55
06/17/2022 19:12:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=55
06/17/2022 19:12:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=56
06/17/2022 19:12:30 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.7839346637705589 on epoch=56
06/17/2022 19:12:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=56
06/17/2022 19:12:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=56
06/17/2022 19:12:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=57
06/17/2022 19:12:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=57
06/17/2022 19:12:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=57
06/17/2022 19:12:49 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.7935205440723554 on epoch=57
06/17/2022 19:12:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=58
06/17/2022 19:12:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=58
06/17/2022 19:12:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=58
06/17/2022 19:12:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=59
06/17/2022 19:13:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=59
06/17/2022 19:13:09 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.8027669170050216 on epoch=59
06/17/2022 19:13:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=59
06/17/2022 19:13:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
06/17/2022 19:13:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=60
06/17/2022 19:13:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=60
06/17/2022 19:13:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=60
06/17/2022 19:13:28 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.7654222297512896 on epoch=60
06/17/2022 19:13:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
06/17/2022 19:13:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=61
06/17/2022 19:13:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=61
06/17/2022 19:13:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
06/17/2022 19:13:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=62
06/17/2022 19:13:48 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.7837342316908857 on epoch=62
06/17/2022 19:13:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=62
06/17/2022 19:13:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=63
06/17/2022 19:13:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=63
06/17/2022 19:13:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=63
06/17/2022 19:14:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=64
06/17/2022 19:14:07 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.7975596064411721 on epoch=64
06/17/2022 19:14:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=64
06/17/2022 19:14:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=64
06/17/2022 19:14:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
06/17/2022 19:14:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=65
06/17/2022 19:14:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=65
06/17/2022 19:14:27 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7863928357349382 on epoch=65
06/17/2022 19:14:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=65
06/17/2022 19:14:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=66
06/17/2022 19:14:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=66
06/17/2022 19:14:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=66
06/17/2022 19:14:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=67
06/17/2022 19:14:46 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.7731614942204044 on epoch=67
06/17/2022 19:14:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=67
06/17/2022 19:14:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=67
06/17/2022 19:14:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=68
06/17/2022 19:14:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=68
06/17/2022 19:14:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=68
06/17/2022 19:15:06 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.8220372908993642 on epoch=68
06/17/2022 19:15:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8204911241143257 -> 0.8220372908993642 on epoch=68, global_step=2200
06/17/2022 19:15:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=69
06/17/2022 19:15:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=69
06/17/2022 19:15:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=69
06/17/2022 19:15:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
06/17/2022 19:15:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=70
06/17/2022 19:15:25 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.8044277688386207 on epoch=70
06/17/2022 19:15:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=70
06/17/2022 19:15:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=70
06/17/2022 19:15:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=71
06/17/2022 19:15:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=71
06/17/2022 19:15:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=71
06/17/2022 19:15:45 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.7744478642204253 on epoch=71
06/17/2022 19:15:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=72
06/17/2022 19:15:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=72
06/17/2022 19:15:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=72
06/17/2022 19:15:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=73
06/17/2022 19:15:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
06/17/2022 19:16:05 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7996665544548711 on epoch=73
06/17/2022 19:16:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=73
06/17/2022 19:16:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=74
06/17/2022 19:16:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=74
06/17/2022 19:16:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=74
06/17/2022 19:16:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=74
06/17/2022 19:16:24 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.8036830713146502 on epoch=74
06/17/2022 19:16:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=75
06/17/2022 19:16:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=75
06/17/2022 19:16:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=75
06/17/2022 19:16:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=76
06/17/2022 19:16:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=76
06/17/2022 19:16:44 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.79436094118622 on epoch=76
06/17/2022 19:16:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=76
06/17/2022 19:16:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=77
06/17/2022 19:16:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=77
06/17/2022 19:16:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=77
06/17/2022 19:16:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
06/17/2022 19:17:04 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7977841278656024 on epoch=78
06/17/2022 19:17:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=78
06/17/2022 19:17:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=78
06/17/2022 19:17:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=79
06/17/2022 19:17:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=79
06/17/2022 19:17:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=79
06/17/2022 19:17:23 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.7958255773796985 on epoch=79
06/17/2022 19:17:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=79
06/17/2022 19:17:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=80
06/17/2022 19:17:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.38 on epoch=80
06/17/2022 19:17:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=80
06/17/2022 19:17:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=81
06/17/2022 19:17:43 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.8164545097479112 on epoch=81
06/17/2022 19:17:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=81
06/17/2022 19:17:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=81
06/17/2022 19:17:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=82
06/17/2022 19:17:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.22 on epoch=82
06/17/2022 19:17:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=82
06/17/2022 19:18:02 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.7872097921970045 on epoch=82
06/17/2022 19:18:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=83
06/17/2022 19:18:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=83
06/17/2022 19:18:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=83
06/17/2022 19:18:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
06/17/2022 19:18:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=84
06/17/2022 19:18:22 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.834101382805179 on epoch=84
06/17/2022 19:18:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8220372908993642 -> 0.834101382805179 on epoch=84, global_step=2700
06/17/2022 19:18:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=84
06/17/2022 19:18:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
06/17/2022 19:18:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=85
06/17/2022 19:18:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=85
06/17/2022 19:18:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=85
06/17/2022 19:18:42 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.7908986878017645 on epoch=85
06/17/2022 19:18:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=86
06/17/2022 19:18:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=86
06/17/2022 19:18:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=86
06/17/2022 19:18:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.25 on epoch=87
06/17/2022 19:18:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=87
06/17/2022 19:19:01 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.8049034938197881 on epoch=87
06/17/2022 19:19:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=87
06/17/2022 19:19:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
06/17/2022 19:19:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=88
06/17/2022 19:19:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=88
06/17/2022 19:19:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=89
06/17/2022 19:19:21 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.7946104345068608 on epoch=89
06/17/2022 19:19:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
06/17/2022 19:19:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=89
06/17/2022 19:19:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=89
06/17/2022 19:19:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=90
06/17/2022 19:19:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=90
06/17/2022 19:19:41 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.8014292059956716 on epoch=90
06/17/2022 19:19:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=90
06/17/2022 19:19:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
06/17/2022 19:19:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=91
06/17/2022 19:19:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=91
06/17/2022 19:19:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=92
06/17/2022 19:20:00 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.7946064907271804 on epoch=92
06/17/2022 19:20:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=92
06/17/2022 19:20:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=92
06/17/2022 19:20:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
06/17/2022 19:20:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=93
06/17/2022 19:20:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=93
06/17/2022 19:20:14 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:20:14 - INFO - __main__ - Printing 3 examples
06/17/2022 19:20:14 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 19:20:14 - INFO - __main__ - ['others']
06/17/2022 19:20:14 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 19:20:14 - INFO - __main__ - ['others']
06/17/2022 19:20:14 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 19:20:14 - INFO - __main__ - ['others']
06/17/2022 19:20:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:20:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:20:15 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 19:20:15 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:20:15 - INFO - __main__ - Printing 3 examples
06/17/2022 19:20:15 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 19:20:15 - INFO - __main__ - ['others']
06/17/2022 19:20:15 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 19:20:15 - INFO - __main__ - ['others']
06/17/2022 19:20:15 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 19:20:15 - INFO - __main__ - ['others']
06/17/2022 19:20:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:20:15 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:20:16 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 19:20:20 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.8163569583447879 on epoch=93
06/17/2022 19:20:20 - INFO - __main__ - save last model!
06/17/2022 19:20:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:20:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 19:20:20 - INFO - __main__ - Printing 3 examples
06/17/2022 19:20:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 19:20:20 - INFO - __main__ - ['others']
06/17/2022 19:20:20 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 19:20:20 - INFO - __main__ - ['others']
06/17/2022 19:20:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 19:20:20 - INFO - __main__ - ['others']
06/17/2022 19:20:20 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:20:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:20:27 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 19:20:33 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:20:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:20:34 - INFO - __main__ - Starting training!
06/17/2022 19:21:40 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_87_0.4_8_predictions.txt
06/17/2022 19:21:40 - INFO - __main__ - Classification-F1 on test data: 0.4103
06/17/2022 19:21:41 - INFO - __main__ - prefix=emo_128_87, lr=0.4, bsz=8, dev_performance=0.834101382805179, test_performance=0.4103351019448457
06/17/2022 19:21:41 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.3, bsz=8 ...
06/17/2022 19:21:41 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:21:41 - INFO - __main__ - Printing 3 examples
06/17/2022 19:21:41 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 19:21:41 - INFO - __main__ - ['others']
06/17/2022 19:21:41 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 19:21:41 - INFO - __main__ - ['others']
06/17/2022 19:21:41 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 19:21:41 - INFO - __main__ - ['others']
06/17/2022 19:21:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:21:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:21:42 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 19:21:42 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:21:42 - INFO - __main__ - Printing 3 examples
06/17/2022 19:21:42 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 19:21:42 - INFO - __main__ - ['others']
06/17/2022 19:21:42 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 19:21:42 - INFO - __main__ - ['others']
06/17/2022 19:21:42 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 19:21:42 - INFO - __main__ - ['others']
06/17/2022 19:21:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:21:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:21:43 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 19:21:58 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:21:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:21:59 - INFO - __main__ - Starting training!
06/17/2022 19:22:02 - INFO - __main__ - Step 10 Global step 10 Train loss 2.74 on epoch=0
06/17/2022 19:22:05 - INFO - __main__ - Step 20 Global step 20 Train loss 1.60 on epoch=0
06/17/2022 19:22:08 - INFO - __main__ - Step 30 Global step 30 Train loss 1.50 on epoch=0
06/17/2022 19:22:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=1
06/17/2022 19:22:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=1
06/17/2022 19:22:19 - INFO - __main__ - Global step 50 Train loss 1.58 Classification-F1 0.11192133929320407 on epoch=1
06/17/2022 19:22:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11192133929320407 on epoch=1, global_step=50
06/17/2022 19:22:22 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=1
06/17/2022 19:22:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
06/17/2022 19:22:27 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=2
06/17/2022 19:22:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=2
06/17/2022 19:22:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.99 on epoch=3
06/17/2022 19:22:39 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.31581645298115274 on epoch=3
06/17/2022 19:22:39 - INFO - __main__ - Saving model with best Classification-F1: 0.11192133929320407 -> 0.31581645298115274 on epoch=3, global_step=100
06/17/2022 19:22:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=3
06/17/2022 19:22:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=3
06/17/2022 19:22:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=4
06/17/2022 19:22:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=4
06/17/2022 19:22:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=4
06/17/2022 19:22:58 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.23626217824937584 on epoch=4
06/17/2022 19:23:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=4
06/17/2022 19:23:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=5
06/17/2022 19:23:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=5
06/17/2022 19:23:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=5
06/17/2022 19:23:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=6
06/17/2022 19:23:17 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.4114590465943383 on epoch=6
06/17/2022 19:23:17 - INFO - __main__ - Saving model with best Classification-F1: 0.31581645298115274 -> 0.4114590465943383 on epoch=6, global_step=200
06/17/2022 19:23:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=6
06/17/2022 19:23:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=6
06/17/2022 19:23:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=7
06/17/2022 19:23:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=7
06/17/2022 19:23:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=7
06/17/2022 19:23:37 - INFO - __main__ - Global step 250 Train loss 0.76 Classification-F1 0.5989577046826893 on epoch=7
06/17/2022 19:23:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4114590465943383 -> 0.5989577046826893 on epoch=7, global_step=250
06/17/2022 19:23:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=8
06/17/2022 19:23:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=8
06/17/2022 19:23:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=8
06/17/2022 19:23:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=9
06/17/2022 19:23:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=9
06/17/2022 19:23:56 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.4447056117862488 on epoch=9
06/17/2022 19:23:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.69 on epoch=9
06/17/2022 19:24:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.69 on epoch=9
06/17/2022 19:24:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.63 on epoch=10
06/17/2022 19:24:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
06/17/2022 19:24:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.66 on epoch=10
06/17/2022 19:24:15 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.6145256161226618 on epoch=10
06/17/2022 19:24:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5989577046826893 -> 0.6145256161226618 on epoch=10, global_step=350
06/17/2022 19:24:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=11
06/17/2022 19:24:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=11
06/17/2022 19:24:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=11
06/17/2022 19:24:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=12
06/17/2022 19:24:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.66 on epoch=12
06/17/2022 19:24:34 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.704100304812384 on epoch=12
06/17/2022 19:24:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6145256161226618 -> 0.704100304812384 on epoch=12, global_step=400
06/17/2022 19:24:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=12
06/17/2022 19:24:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=13
06/17/2022 19:24:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.64 on epoch=13
06/17/2022 19:24:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=13
06/17/2022 19:24:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=14
06/17/2022 19:24:54 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.39450843180882444 on epoch=14
06/17/2022 19:24:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.76 on epoch=14
06/17/2022 19:24:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=14
06/17/2022 19:25:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=14
06/17/2022 19:25:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=15
06/17/2022 19:25:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=15
06/17/2022 19:25:13 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.6061489012507919 on epoch=15
06/17/2022 19:25:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=15
06/17/2022 19:25:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=16
06/17/2022 19:25:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=16
06/17/2022 19:25:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=16
06/17/2022 19:25:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=17
06/17/2022 19:25:33 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.6022216070482971 on epoch=17
06/17/2022 19:25:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=17
06/17/2022 19:25:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=17
06/17/2022 19:25:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=18
06/17/2022 19:25:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=18
06/17/2022 19:25:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=18
06/17/2022 19:25:52 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.7344721305159527 on epoch=18
06/17/2022 19:25:52 - INFO - __main__ - Saving model with best Classification-F1: 0.704100304812384 -> 0.7344721305159527 on epoch=18, global_step=600
06/17/2022 19:25:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=19
06/17/2022 19:25:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.68 on epoch=19
06/17/2022 19:25:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.57 on epoch=19
06/17/2022 19:26:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=19
06/17/2022 19:26:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=20
06/17/2022 19:26:11 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.7105430746830608 on epoch=20
06/17/2022 19:26:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=20
06/17/2022 19:26:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=20
06/17/2022 19:26:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=21
06/17/2022 19:26:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=21
06/17/2022 19:26:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=21
06/17/2022 19:26:31 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.7238203964387545 on epoch=21
06/17/2022 19:26:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=22
06/17/2022 19:26:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=22
06/17/2022 19:26:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=22
06/17/2022 19:26:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=23
06/17/2022 19:26:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=23
06/17/2022 19:26:50 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.7592449584256258 on epoch=23
06/17/2022 19:26:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7344721305159527 -> 0.7592449584256258 on epoch=23, global_step=750
06/17/2022 19:26:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.54 on epoch=23
06/17/2022 19:26:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=24
06/17/2022 19:26:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=24
06/17/2022 19:27:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=24
06/17/2022 19:27:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=24
06/17/2022 19:27:09 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.6238727178508082 on epoch=24
06/17/2022 19:27:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=25
06/17/2022 19:27:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=25
06/17/2022 19:27:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=25
06/17/2022 19:27:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=26
06/17/2022 19:27:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=26
06/17/2022 19:27:29 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.5727550531941447 on epoch=26
06/17/2022 19:27:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=26
06/17/2022 19:27:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=27
06/17/2022 19:27:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=27
06/17/2022 19:27:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=27
06/17/2022 19:27:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=28
06/17/2022 19:27:48 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.6141296747588847 on epoch=28
06/17/2022 19:27:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=28
06/17/2022 19:27:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=28
06/17/2022 19:27:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=29
06/17/2022 19:27:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=29
06/17/2022 19:28:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=29
06/17/2022 19:28:07 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.7042057165997844 on epoch=29
06/17/2022 19:28:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=29
06/17/2022 19:28:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=30
06/17/2022 19:28:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=30
06/17/2022 19:28:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=30
06/17/2022 19:28:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=31
06/17/2022 19:28:27 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.7409213277003626 on epoch=31
06/17/2022 19:28:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=31
06/17/2022 19:28:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=31
06/17/2022 19:28:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=32
06/17/2022 19:28:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=32
06/17/2022 19:28:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.51 on epoch=32
06/17/2022 19:28:46 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.7082807958171737 on epoch=32
06/17/2022 19:28:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=33
06/17/2022 19:28:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=33
06/17/2022 19:28:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=33
06/17/2022 19:28:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=34
06/17/2022 19:28:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=34
06/17/2022 19:29:05 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.7746155033550193 on epoch=34
06/17/2022 19:29:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7592449584256258 -> 0.7746155033550193 on epoch=34, global_step=1100
06/17/2022 19:29:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=34
06/17/2022 19:29:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=34
06/17/2022 19:29:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=35
06/17/2022 19:29:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=35
06/17/2022 19:29:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=35
06/17/2022 19:29:25 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.7574540470117935 on epoch=35
06/17/2022 19:29:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=36
06/17/2022 19:29:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=36
06/17/2022 19:29:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=36
06/17/2022 19:29:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=37
06/17/2022 19:29:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=37
06/17/2022 19:29:44 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.7605358336676026 on epoch=37
06/17/2022 19:29:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=37
06/17/2022 19:29:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=38
06/17/2022 19:29:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=38
06/17/2022 19:29:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=38
06/17/2022 19:29:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=39
06/17/2022 19:30:04 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.7098943102581672 on epoch=39
06/17/2022 19:30:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
06/17/2022 19:30:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
06/17/2022 19:30:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=39
06/17/2022 19:30:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=40
06/17/2022 19:30:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=40
06/17/2022 19:30:24 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.7398868843396427 on epoch=40
06/17/2022 19:30:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=40
06/17/2022 19:30:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
06/17/2022 19:30:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=41
06/17/2022 19:30:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=41
06/17/2022 19:30:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=42
06/17/2022 19:30:43 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7952097615899435 on epoch=42
06/17/2022 19:30:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7746155033550193 -> 0.7952097615899435 on epoch=42, global_step=1350
06/17/2022 19:30:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=42
06/17/2022 19:30:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=42
06/17/2022 19:30:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=43
06/17/2022 19:30:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=43
06/17/2022 19:30:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=43
06/17/2022 19:31:02 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.7937325525079373 on epoch=43
06/17/2022 19:31:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=44
06/17/2022 19:31:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=44
06/17/2022 19:31:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=44
06/17/2022 19:31:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=44
06/17/2022 19:31:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=45
06/17/2022 19:31:22 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.7984357558382549 on epoch=45
06/17/2022 19:31:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7952097615899435 -> 0.7984357558382549 on epoch=45, global_step=1450
06/17/2022 19:31:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=45
06/17/2022 19:31:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=45
06/17/2022 19:31:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=46
06/17/2022 19:31:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=46
06/17/2022 19:31:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=46
06/17/2022 19:31:42 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.7881902882562769 on epoch=46
06/17/2022 19:31:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=47
06/17/2022 19:31:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=47
06/17/2022 19:31:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=47
06/17/2022 19:31:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=48
06/17/2022 19:31:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=48
06/17/2022 19:32:01 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.7794910552470941 on epoch=48
06/17/2022 19:32:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=48
06/17/2022 19:32:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=49
06/17/2022 19:32:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=49
06/17/2022 19:32:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=49
06/17/2022 19:32:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=49
06/17/2022 19:32:20 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.7536420903202911 on epoch=49
06/17/2022 19:32:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=50
06/17/2022 19:32:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=50
06/17/2022 19:32:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=50
06/17/2022 19:32:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=51
06/17/2022 19:32:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=51
06/17/2022 19:32:40 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.6829676071055382 on epoch=51
06/17/2022 19:32:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=51
06/17/2022 19:32:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=52
06/17/2022 19:32:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=52
06/17/2022 19:32:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=52
06/17/2022 19:32:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=53
06/17/2022 19:32:59 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7756863180675163 on epoch=53
06/17/2022 19:33:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=53
06/17/2022 19:33:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=53
06/17/2022 19:33:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=54
06/17/2022 19:33:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=54
06/17/2022 19:33:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=54
06/17/2022 19:33:19 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.8062968198735948 on epoch=54
06/17/2022 19:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7984357558382549 -> 0.8062968198735948 on epoch=54, global_step=1750
06/17/2022 19:33:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=54
06/17/2022 19:33:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=55
06/17/2022 19:33:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=55
06/17/2022 19:33:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=55
06/17/2022 19:33:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=56
06/17/2022 19:33:38 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.789935190514562 on epoch=56
06/17/2022 19:33:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=56
06/17/2022 19:33:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=56
06/17/2022 19:33:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=57
06/17/2022 19:33:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=57
06/17/2022 19:33:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=57
06/17/2022 19:33:58 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.758857385537185 on epoch=57
06/17/2022 19:34:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=58
06/17/2022 19:34:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=58
06/17/2022 19:34:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=58
06/17/2022 19:34:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=59
06/17/2022 19:34:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=59
06/17/2022 19:34:17 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.7894947540898728 on epoch=59
06/17/2022 19:34:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=59
06/17/2022 19:34:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=59
06/17/2022 19:34:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=60
06/17/2022 19:34:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=60
06/17/2022 19:34:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=60
06/17/2022 19:34:36 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.7733068116581214 on epoch=60
06/17/2022 19:34:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=61
06/17/2022 19:34:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=61
06/17/2022 19:34:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=61
06/17/2022 19:34:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=62
06/17/2022 19:34:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
06/17/2022 19:34:56 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.8061974144162669 on epoch=62
06/17/2022 19:34:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=62
06/17/2022 19:35:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=63
06/17/2022 19:35:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=63
06/17/2022 19:35:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
06/17/2022 19:35:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=64
06/17/2022 19:35:15 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.7481028066352104 on epoch=64
06/17/2022 19:35:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=64
06/17/2022 19:35:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=64
06/17/2022 19:35:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=64
06/17/2022 19:35:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=65
06/17/2022 19:35:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=65
06/17/2022 19:35:34 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7824665025751982 on epoch=65
06/17/2022 19:35:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=65
06/17/2022 19:35:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=66
06/17/2022 19:35:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=66
06/17/2022 19:35:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
06/17/2022 19:35:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=67
06/17/2022 19:35:54 - INFO - __main__ - Global step 2150 Train loss 0.26 Classification-F1 0.7926484242887482 on epoch=67
06/17/2022 19:35:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=67
06/17/2022 19:35:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=67
06/17/2022 19:36:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=68
06/17/2022 19:36:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=68
06/17/2022 19:36:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=68
06/17/2022 19:36:13 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.7875059058869001 on epoch=68
06/17/2022 19:36:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.24 on epoch=69
06/17/2022 19:36:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=69
06/17/2022 19:36:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
06/17/2022 19:36:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=69
06/17/2022 19:36:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=70
06/17/2022 19:36:33 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.7818307182948305 on epoch=70
06/17/2022 19:36:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=70
06/17/2022 19:36:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=70
06/17/2022 19:36:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.31 on epoch=71
06/17/2022 19:36:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=71
06/17/2022 19:36:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=71
06/17/2022 19:36:52 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.7802106901804771 on epoch=71
06/17/2022 19:36:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=72
06/17/2022 19:36:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=72
06/17/2022 19:37:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=72
06/17/2022 19:37:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=73
06/17/2022 19:37:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=73
06/17/2022 19:37:12 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.7588103366053358 on epoch=73
06/17/2022 19:37:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=73
06/17/2022 19:37:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=74
06/17/2022 19:37:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=74
06/17/2022 19:37:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=74
06/17/2022 19:37:25 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
06/17/2022 19:37:31 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.7737188058153752 on epoch=74
06/17/2022 19:37:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=75
06/17/2022 19:37:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=75
06/17/2022 19:37:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=75
06/17/2022 19:37:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
06/17/2022 19:37:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=76
06/17/2022 19:37:51 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.8031217750476549 on epoch=76
06/17/2022 19:37:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=76
06/17/2022 19:37:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.29 on epoch=77
06/17/2022 19:37:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
06/17/2022 19:38:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=77
06/17/2022 19:38:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=78
06/17/2022 19:38:10 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.7868785190351788 on epoch=78
06/17/2022 19:38:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.26 on epoch=78
06/17/2022 19:38:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.25 on epoch=78
06/17/2022 19:38:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=79
06/17/2022 19:38:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=79
06/17/2022 19:38:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=79
06/17/2022 19:38:30 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.7904949165298877 on epoch=79
06/17/2022 19:38:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=79
06/17/2022 19:38:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=80
06/17/2022 19:38:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
06/17/2022 19:38:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=80
06/17/2022 19:38:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=81
06/17/2022 19:38:49 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.7714657054599106 on epoch=81
06/17/2022 19:38:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
06/17/2022 19:38:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
06/17/2022 19:38:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.23 on epoch=82
06/17/2022 19:38:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=82
06/17/2022 19:39:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=82
06/17/2022 19:39:09 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.7983751349901793 on epoch=82
06/17/2022 19:39:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=83
06/17/2022 19:39:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
06/17/2022 19:39:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=83
06/17/2022 19:39:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.24 on epoch=84
06/17/2022 19:39:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=84
06/17/2022 19:39:28 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.8087921136834181 on epoch=84
06/17/2022 19:39:28 - INFO - __main__ - Saving model with best Classification-F1: 0.8062968198735948 -> 0.8087921136834181 on epoch=84, global_step=2700
06/17/2022 19:39:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=84
06/17/2022 19:39:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=84
06/17/2022 19:39:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=85
06/17/2022 19:39:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=85
06/17/2022 19:39:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=85
06/17/2022 19:39:48 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.8141587610824548 on epoch=85
06/17/2022 19:39:48 - INFO - __main__ - Saving model with best Classification-F1: 0.8087921136834181 -> 0.8141587610824548 on epoch=85, global_step=2750
06/17/2022 19:39:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=86
06/17/2022 19:39:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=86
06/17/2022 19:39:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=86
06/17/2022 19:39:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.28 on epoch=87
06/17/2022 19:40:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=87
06/17/2022 19:40:08 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.7915041694996046 on epoch=87
06/17/2022 19:40:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=87
06/17/2022 19:40:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=88
06/17/2022 19:40:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.32 on epoch=88
06/17/2022 19:40:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=88
06/17/2022 19:40:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
06/17/2022 19:40:27 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.7991063275128342 on epoch=89
06/17/2022 19:40:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=89
06/17/2022 19:40:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=89
06/17/2022 19:40:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
06/17/2022 19:40:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=90
06/17/2022 19:40:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=90
06/17/2022 19:40:47 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.7904941794803002 on epoch=90
06/17/2022 19:40:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=90
06/17/2022 19:40:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.25 on epoch=91
06/17/2022 19:40:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.24 on epoch=91
06/17/2022 19:40:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=91
06/17/2022 19:40:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=92
06/17/2022 19:41:06 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.8148205997001552 on epoch=92
06/17/2022 19:41:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8141587610824548 -> 0.8148205997001552 on epoch=92, global_step=2950
06/17/2022 19:41:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=92
06/17/2022 19:41:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=92
06/17/2022 19:41:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
06/17/2022 19:41:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=93
06/17/2022 19:41:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=93
06/17/2022 19:41:20 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:41:20 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:20 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 19:41:20 - INFO - __main__ - ['others']
06/17/2022 19:41:20 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 19:41:20 - INFO - __main__ - ['others']
06/17/2022 19:41:20 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 19:41:20 - INFO - __main__ - ['others']
06/17/2022 19:41:20 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:20 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:21 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 19:41:21 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:41:21 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:21 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 19:41:21 - INFO - __main__ - ['others']
06/17/2022 19:41:21 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 19:41:21 - INFO - __main__ - ['others']
06/17/2022 19:41:21 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 19:41:21 - INFO - __main__ - ['others']
06/17/2022 19:41:21 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:21 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:22 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 19:41:26 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7996370968575691 on epoch=93
06/17/2022 19:41:26 - INFO - __main__ - save last model!
06/17/2022 19:41:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 19:41:26 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 19:41:26 - INFO - __main__ - Printing 3 examples
06/17/2022 19:41:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 19:41:26 - INFO - __main__ - ['others']
06/17/2022 19:41:26 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 19:41:26 - INFO - __main__ - ['others']
06/17/2022 19:41:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 19:41:26 - INFO - __main__ - ['others']
06/17/2022 19:41:26 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:41:28 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:41:33 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 19:41:39 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:41:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:41:39 - INFO - __main__ - Starting training!
06/17/2022 19:42:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_87_0.3_8_predictions.txt
06/17/2022 19:42:46 - INFO - __main__ - Classification-F1 on test data: 0.2417
06/17/2022 19:42:47 - INFO - __main__ - prefix=emo_128_87, lr=0.3, bsz=8, dev_performance=0.8148205997001552, test_performance=0.24165014161432305
06/17/2022 19:42:47 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.2, bsz=8 ...
06/17/2022 19:42:47 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:42:47 - INFO - __main__ - Printing 3 examples
06/17/2022 19:42:47 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/17/2022 19:42:47 - INFO - __main__ - ['others']
06/17/2022 19:42:47 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/17/2022 19:42:47 - INFO - __main__ - ['others']
06/17/2022 19:42:47 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/17/2022 19:42:47 - INFO - __main__ - ['others']
06/17/2022 19:42:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:42:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:42:48 - INFO - __main__ - Loaded 512 examples from train data
06/17/2022 19:42:48 - INFO - __main__ - Start tokenizing ... 512 instances
06/17/2022 19:42:48 - INFO - __main__ - Printing 3 examples
06/17/2022 19:42:48 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
06/17/2022 19:42:48 - INFO - __main__ - ['others']
06/17/2022 19:42:48 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
06/17/2022 19:42:48 - INFO - __main__ - ['others']
06/17/2022 19:42:48 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
06/17/2022 19:42:48 - INFO - __main__ - ['others']
06/17/2022 19:42:48 - INFO - __main__ - Tokenizing Input ...
06/17/2022 19:42:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 19:42:49 - INFO - __main__ - Loaded 512 examples from dev data
06/17/2022 19:43:04 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 19:43:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 19:43:05 - INFO - __main__ - Starting training!
06/17/2022 19:43:08 - INFO - __main__ - Step 10 Global step 10 Train loss 3.06 on epoch=0
06/17/2022 19:43:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.96 on epoch=0
06/17/2022 19:43:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=0
06/17/2022 19:43:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.21 on epoch=1
06/17/2022 19:43:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=1
06/17/2022 19:43:25 - INFO - __main__ - Global step 50 Train loss 1.79 Classification-F1 0.1 on epoch=1
06/17/2022 19:43:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=1, global_step=50
06/17/2022 19:43:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=1
06/17/2022 19:43:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=2
06/17/2022 19:43:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=2
06/17/2022 19:43:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=2
06/17/2022 19:43:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.99 on epoch=3
06/17/2022 19:43:44 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.11192133929320407 on epoch=3
06/17/2022 19:43:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.11192133929320407 on epoch=3, global_step=100
06/17/2022 19:43:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=3
06/17/2022 19:43:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=3
06/17/2022 19:43:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=4
06/17/2022 19:43:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=4
06/17/2022 19:43:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=4
06/17/2022 19:44:03 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.2748153080544003 on epoch=4
06/17/2022 19:44:03 - INFO - __main__ - Saving model with best Classification-F1: 0.11192133929320407 -> 0.2748153080544003 on epoch=4, global_step=150
06/17/2022 19:44:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=4
06/17/2022 19:44:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=5
06/17/2022 19:44:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=5
06/17/2022 19:44:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=5
06/17/2022 19:44:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=6
06/17/2022 19:44:22 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.4213010348724446 on epoch=6
06/17/2022 19:44:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2748153080544003 -> 0.4213010348724446 on epoch=6, global_step=200
06/17/2022 19:44:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=6
06/17/2022 19:44:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=6
06/17/2022 19:44:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=7
06/17/2022 19:44:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=7
06/17/2022 19:44:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=7
06/17/2022 19:44:41 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.4636284522968177 on epoch=7
06/17/2022 19:44:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4213010348724446 -> 0.4636284522968177 on epoch=7, global_step=250
06/17/2022 19:44:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=8
06/17/2022 19:44:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.68 on epoch=8
06/17/2022 19:44:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=8
06/17/2022 19:44:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=9
06/17/2022 19:44:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=9
06/17/2022 19:45:00 - INFO - __main__ - Global step 300 Train loss 0.76 Classification-F1 0.5546896470341148 on epoch=9
06/17/2022 19:45:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4636284522968177 -> 0.5546896470341148 on epoch=9, global_step=300
06/17/2022 19:45:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=9
06/17/2022 19:45:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=9
06/17/2022 19:45:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.67 on epoch=10
06/17/2022 19:45:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=10
06/17/2022 19:45:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.77 on epoch=10
06/17/2022 19:45:19 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.5237262148798164 on epoch=10
06/17/2022 19:45:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.75 on epoch=11
06/17/2022 19:45:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=11
06/17/2022 19:45:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=11
06/17/2022 19:45:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=12
06/17/2022 19:45:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=12
06/17/2022 19:45:38 - INFO - __main__ - Global step 400 Train loss 0.71 Classification-F1 0.6423281840245061 on epoch=12
06/17/2022 19:45:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5546896470341148 -> 0.6423281840245061 on epoch=12, global_step=400
06/17/2022 19:45:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.62 on epoch=12
06/17/2022 19:45:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=13
06/17/2022 19:45:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.68 on epoch=13
06/17/2022 19:45:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.64 on epoch=13
06/17/2022 19:45:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=14
06/17/2022 19:45:57 - INFO - __main__ - Global step 450 Train loss 0.65 Classification-F1 0.5815330167400736 on epoch=14
06/17/2022 19:46:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=14
06/17/2022 19:46:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=14
06/17/2022 19:46:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.67 on epoch=14
06/17/2022 19:46:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=15
06/17/2022 19:46:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=15
06/17/2022 19:46:16 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.5963229543329558 on epoch=15
06/17/2022 19:46:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=15
06/17/2022 19:46:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=16
06/17/2022 19:46:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.63 on epoch=16
06/17/2022 19:46:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=16
06/17/2022 19:46:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.65 on epoch=17
06/17/2022 19:46:35 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.5311738517642752 on epoch=17
06/17/2022 19:46:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=17
06/17/2022 19:46:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=17
06/17/2022 19:46:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=18
06/17/2022 19:46:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=18
06/17/2022 19:46:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.69 on epoch=18
06/17/2022 19:46:54 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.6625597655461248 on epoch=18
06/17/2022 19:46:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6423281840245061 -> 0.6625597655461248 on epoch=18, global_step=600
06/17/2022 19:46:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=19
06/17/2022 19:46:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=19
06/17/2022 19:47:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=19
06/17/2022 19:47:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=19
06/17/2022 19:47:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=20
06/17/2022 19:47:13 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.6979184466255454 on epoch=20
06/17/2022 19:47:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6625597655461248 -> 0.6979184466255454 on epoch=20, global_step=650
06/17/2022 19:47:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=20
06/17/2022 19:47:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=20
06/17/2022 19:47:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=21
06/17/2022 19:47:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=21
06/17/2022 19:47:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=21
06/17/2022 19:47:33 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.6986261858509862 on epoch=21
06/17/2022 19:47:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6979184466255454 -> 0.6986261858509862 on epoch=21, global_step=700
06/17/2022 19:47:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=22
06/17/2022 19:47:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.56 on epoch=22
06/17/2022 19:47:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.56 on epoch=22
06/17/2022 19:47:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=23
06/17/2022 19:47:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.61 on epoch=23
06/17/2022 19:47:52 - INFO - __main__ - Global step 750 Train loss 0.57 Classification-F1 0.7158020321540973 on epoch=23
06/17/2022 19:47:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6986261858509862 -> 0.7158020321540973 on epoch=23, global_step=750
06/17/2022 19:47:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=23
06/17/2022 19:47:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=24
06/17/2022 19:47:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.57 on epoch=24
06/17/2022 19:48:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=24
06/17/2022 19:48:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=24
06/17/2022 19:48:11 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.6712441970155311 on epoch=24
06/17/2022 19:48:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=25
06/17/2022 19:48:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=25
06/17/2022 19:48:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=25
06/17/2022 19:48:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=26
06/17/2022 19:48:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.53 on epoch=26
06/17/2022 19:48:30 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.6779759482974183 on epoch=26
06/17/2022 19:48:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=26
06/17/2022 19:48:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=27
06/17/2022 19:48:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=27
06/17/2022 19:48:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=27
06/17/2022 19:48:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=28
06/17/2022 19:48:48 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.5975981584398908 on epoch=28
06/17/2022 19:48:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.61 on epoch=28
06/17/2022 19:48:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=28
06/17/2022 19:48:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=29
06/17/2022 19:48:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.60 on epoch=29
06/17/2022 19:49:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=29
06/17/2022 19:49:07 - INFO - __main__ - Global step 950 Train loss 0.54 Classification-F1 0.6713381578302688 on epoch=29
06/17/2022 19:49:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=29
06/17/2022 19:49:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=30
06/17/2022 19:49:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.57 on epoch=30
06/17/2022 19:49:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=30
06/17/2022 19:49:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=31
06/17/2022 19:49:26 - INFO - __main__ - Global step 1000 Train loss 0.52 Classification-F1 0.7312763587389449 on epoch=31
06/17/2022 19:49:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7158020321540973 -> 0.7312763587389449 on epoch=31, global_step=1000
06/17/2022 19:49:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=31
06/17/2022 19:49:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=31
06/17/2022 19:49:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=32
06/17/2022 19:49:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.57 on epoch=32
06/17/2022 19:49:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=32
06/17/2022 19:49:45 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.6957644109594502 on epoch=32
06/17/2022 19:49:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.51 on epoch=33
06/17/2022 19:49:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=33
06/17/2022 19:49:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.53 on epoch=33
06/17/2022 19:49:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=34
06/17/2022 19:49:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=34
06/17/2022 19:50:04 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.7838302359028954 on epoch=34
06/17/2022 19:50:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7312763587389449 -> 0.7838302359028954 on epoch=34, global_step=1100
06/17/2022 19:50:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=34
06/17/2022 19:50:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=34
06/17/2022 19:50:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.54 on epoch=35
06/17/2022 19:50:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=35
06/17/2022 19:50:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=35
06/17/2022 19:50:22 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.7486022956762193 on epoch=35
06/17/2022 19:50:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
06/17/2022 19:50:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=36
06/17/2022 19:50:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=36
06/17/2022 19:50:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=37
06/17/2022 19:50:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=37
06/17/2022 19:50:41 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.8047000336585286 on epoch=37
06/17/2022 19:50:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7838302359028954 -> 0.8047000336585286 on epoch=37, global_step=1200
06/17/2022 19:50:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=37
06/17/2022 19:50:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=38
06/17/2022 19:50:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=38
06/17/2022 19:50:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.53 on epoch=38
06/17/2022 19:50:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=39
06/17/2022 19:51:00 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.7676299555924362 on epoch=39
06/17/2022 19:51:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=39
06/17/2022 19:51:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=39
06/17/2022 19:51:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=39
06/17/2022 19:51:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=40
06/17/2022 19:51:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=40
06/17/2022 19:51:19 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.7174766437845287 on epoch=40
06/17/2022 19:51:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=40
06/17/2022 19:51:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=41
06/17/2022 19:51:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=41
06/17/2022 19:51:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=41
06/17/2022 19:51:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=42
06/17/2022 19:51:38 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.7142044559065835 on epoch=42
06/17/2022 19:51:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=42
06/17/2022 19:51:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=42
06/17/2022 19:51:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=43
06/17/2022 19:51:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=43
06/17/2022 19:51:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=43
06/17/2022 19:51:57 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.7970058724738729 on epoch=43
06/17/2022 19:51:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=44
06/17/2022 19:52:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=44
06/17/2022 19:52:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=44
06/17/2022 19:52:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=44
06/17/2022 19:52:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=45
06/17/2022 19:52:16 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.7450386132064815 on epoch=45
06/17/2022 19:52:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=45
06/17/2022 19:52:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=45
06/17/2022 19:52:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=46
06/17/2022 19:52:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=46
06/17/2022 19:52:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=46
06/17/2022 19:52:35 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.7929010213853602 on epoch=46
06/17/2022 19:52:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=47
06/17/2022 19:52:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=47
06/17/2022 19:52:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=47
06/17/2022 19:52:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=48
06/17/2022 19:52:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=48
06/17/2022 19:52:54 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.7931076223813536 on epoch=48
06/17/2022 19:52:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=48
06/17/2022 19:52:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=49
06/17/2022 19:53:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=49
06/17/2022 19:53:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=49
06/17/2022 19:53:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=49
06/17/2022 19:53:13 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.7493841480699279 on epoch=49
06/17/2022 19:53:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=50
06/17/2022 19:53:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=50
06/17/2022 19:53:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=50
06/17/2022 19:53:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=51
06/17/2022 19:53:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=51
06/17/2022 19:53:33 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.7401614057377107 on epoch=51
06/17/2022 19:53:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=51
06/17/2022 19:53:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=52
06/17/2022 19:53:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=52
06/17/2022 19:53:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=52
06/17/2022 19:53:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=53
06/17/2022 19:53:52 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.7109440219130517 on epoch=53
06/17/2022 19:53:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=53
06/17/2022 19:53:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=53
06/17/2022 19:53:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=54
06/17/2022 19:54:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=54
06/17/2022 19:54:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=54
06/17/2022 19:54:11 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.742521090279066 on epoch=54
06/17/2022 19:54:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=54
06/17/2022 19:54:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=55
06/17/2022 19:54:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=55
06/17/2022 19:54:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=55
06/17/2022 19:54:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=56
06/17/2022 19:54:30 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.7801562657653113 on epoch=56
06/17/2022 19:54:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=56
06/17/2022 19:54:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=56
06/17/2022 19:54:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=57
06/17/2022 19:54:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=57
06/17/2022 19:54:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=57
06/17/2022 19:54:50 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.7836993540293493 on epoch=57
06/17/2022 19:54:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=58
06/17/2022 19:54:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=58
06/17/2022 19:54:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=58
06/17/2022 19:54:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=59
06/17/2022 19:55:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=59
06/17/2022 19:55:09 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.7669211041731306 on epoch=59
06/17/2022 19:55:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=59
06/17/2022 19:55:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=59
06/17/2022 19:55:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=60
06/17/2022 19:55:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=60
06/17/2022 19:55:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=60
06/17/2022 19:55:28 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.7866472499811066 on epoch=60
06/17/2022 19:55:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=61
06/17/2022 19:55:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=61
06/17/2022 19:55:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=61
06/17/2022 19:55:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
06/17/2022 19:55:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=62
06/17/2022 19:55:47 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.784375383099143 on epoch=62
06/17/2022 19:55:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=62
06/17/2022 19:55:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=63
06/17/2022 19:55:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=63
06/17/2022 19:55:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.39 on epoch=63
06/17/2022 19:55:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=64
06/17/2022 19:56:06 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.7951445266973628 on epoch=64
06/17/2022 19:56:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=64
06/17/2022 19:56:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=64
06/17/2022 19:56:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
06/17/2022 19:56:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=65
06/17/2022 19:56:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=65
06/17/2022 19:56:25 - INFO - __main__ - Global step 2100 Train loss 0.31 Classification-F1 0.7706199724078666 on epoch=65
06/17/2022 19:56:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=65
06/17/2022 19:56:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=66
06/17/2022 19:56:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=66
06/17/2022 19:56:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=66
06/17/2022 19:56:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=67
06/17/2022 19:56:45 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.773573141290721 on epoch=67
06/17/2022 19:56:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=67
06/17/2022 19:56:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=67
06/17/2022 19:56:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=68
06/17/2022 19:56:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=68
06/17/2022 19:56:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=68
06/17/2022 19:57:04 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.7804871540411338 on epoch=68
06/17/2022 19:57:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=69
06/17/2022 19:57:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.31 on epoch=69
06/17/2022 19:57:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
06/17/2022 19:57:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=69
06/17/2022 19:57:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=70
06/17/2022 19:57:23 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.779973784385549 on epoch=70
06/17/2022 19:57:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.31 on epoch=70
06/17/2022 19:57:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.32 on epoch=70
06/17/2022 19:57:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=71
06/17/2022 19:57:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.32 on epoch=71
06/17/2022 19:57:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=71
06/17/2022 19:57:43 - INFO - __main__ - Global step 2300 Train loss 0.30 Classification-F1 0.7842051511765737 on epoch=71
06/17/2022 19:57:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=72
06/17/2022 19:57:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.30 on epoch=72
06/17/2022 19:57:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=72
06/17/2022 19:57:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.34 on epoch=73
06/17/2022 19:57:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=73
06/17/2022 19:58:02 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.7982680954145929 on epoch=73
06/17/2022 19:58:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=73
06/17/2022 19:58:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=74
06/17/2022 19:58:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.31 on epoch=74
06/17/2022 19:58:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=74
06/17/2022 19:58:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.30 on epoch=74
06/17/2022 19:58:21 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.7998431992525532 on epoch=74
06/17/2022 19:58:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.33 on epoch=75
06/17/2022 19:58:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=75
06/17/2022 19:58:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=75
06/17/2022 19:58:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=76
06/17/2022 19:58:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=76
06/17/2022 19:58:41 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.7760516471903955 on epoch=76
06/17/2022 19:58:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.31 on epoch=76
06/17/2022 19:58:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.31 on epoch=77
06/17/2022 19:58:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.30 on epoch=77
06/17/2022 19:58:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=77
06/17/2022 19:58:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
06/17/2022 19:59:01 - INFO - __main__ - Global step 2500 Train loss 0.29 Classification-F1 0.7574813241702818 on epoch=78
06/17/2022 19:59:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=78
06/17/2022 19:59:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=78
06/17/2022 19:59:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=79
06/17/2022 19:59:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.31 on epoch=79
06/17/2022 19:59:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.32 on epoch=79
06/17/2022 19:59:21 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.7910048904892948 on epoch=79
06/17/2022 19:59:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.30 on epoch=79
06/17/2022 19:59:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=80
06/17/2022 19:59:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.26 on epoch=80
06/17/2022 19:59:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=80
06/17/2022 19:59:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.29 on epoch=81
06/17/2022 19:59:40 - INFO - __main__ - Global step 2600 Train loss 0.26 Classification-F1 0.8011270830858186 on epoch=81
06/17/2022 19:59:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=81
06/17/2022 19:59:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=81
06/17/2022 19:59:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=82
06/17/2022 19:59:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=82
06/17/2022 19:59:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=82
06/17/2022 20:00:00 - INFO - __main__ - Global step 2650 Train loss 0.24 Classification-F1 0.7978080466503091 on epoch=82
06/17/2022 20:00:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.27 on epoch=83
06/17/2022 20:00:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=83
06/17/2022 20:00:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=83
06/17/2022 20:00:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=84
06/17/2022 20:00:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=84
06/17/2022 20:00:19 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.8005773972935788 on epoch=84
06/17/2022 20:00:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=84
06/17/2022 20:00:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=84
06/17/2022 20:00:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=85
06/17/2022 20:00:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=85
06/17/2022 20:00:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=85
06/17/2022 20:00:39 - INFO - __main__ - Global step 2750 Train loss 0.25 Classification-F1 0.7917851282151434 on epoch=85
06/17/2022 20:00:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=86
06/17/2022 20:00:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=86
06/17/2022 20:00:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=86
06/17/2022 20:00:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.24 on epoch=87
06/17/2022 20:00:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=87
06/17/2022 20:00:58 - INFO - __main__ - Global step 2800 Train loss 0.22 Classification-F1 0.787705173509961 on epoch=87
06/17/2022 20:01:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=87
06/17/2022 20:01:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=88
06/17/2022 20:01:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=88
06/17/2022 20:01:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=88
06/17/2022 20:01:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=89
06/17/2022 20:01:18 - INFO - __main__ - Global step 2850 Train loss 0.25 Classification-F1 0.7841618698309314 on epoch=89
06/17/2022 20:01:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
06/17/2022 20:01:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=89
06/17/2022 20:01:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=89
06/17/2022 20:01:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.32 on epoch=90
06/17/2022 20:01:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.25 on epoch=90
06/17/2022 20:01:37 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.7670422001094448 on epoch=90
06/17/2022 20:01:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=90
06/17/2022 20:01:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.27 on epoch=91
06/17/2022 20:01:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=91
06/17/2022 20:01:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=91
06/17/2022 20:01:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.25 on epoch=92
06/17/2022 20:01:57 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.7717243878408697 on epoch=92
06/17/2022 20:02:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=92
06/17/2022 20:02:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=92
06/17/2022 20:02:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=93
06/17/2022 20:02:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.25 on epoch=93
06/17/2022 20:02:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
06/17/2022 20:02:17 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.7794340766885132 on epoch=93
06/17/2022 20:02:17 - INFO - __main__ - save last model!
06/17/2022 20:02:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 20:02:17 - INFO - __main__ - Start tokenizing ... 5509 instances
06/17/2022 20:02:17 - INFO - __main__ - Printing 3 examples
06/17/2022 20:02:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/17/2022 20:02:17 - INFO - __main__ - ['others']
06/17/2022 20:02:17 - INFO - __main__ -  [emo] what you like very little things ok
06/17/2022 20:02:17 - INFO - __main__ - ['others']
06/17/2022 20:02:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/17/2022 20:02:17 - INFO - __main__ - ['others']
06/17/2022 20:02:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 20:02:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 20:02:24 - INFO - __main__ - Loaded 5509 examples from test data
06/17/2022 20:03:37 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-emo/emo_128_87_0.2_8_predictions.txt
06/17/2022 20:03:37 - INFO - __main__ - Classification-F1 on test data: 0.4974
06/17/2022 20:03:37 - INFO - __main__ - prefix=emo_128_87, lr=0.2, bsz=8, dev_performance=0.8047000336585286, test_performance=0.49739814395888926
