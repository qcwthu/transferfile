nohup: ignoring input
Task: superglue-cb, Checkpoint: models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10
03/17/2022 15:08:42 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/17/2022 15:08:42 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb
Output directory () already exists and is not empty.
03/17/2022 15:08:42 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/17/2022 15:08:42 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb
03/17/2022 15:08:44 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/17/2022 15:08:44 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/17/2022 15:08:44 - INFO - __main__ - args.device: cuda:0
03/17/2022 15:08:44 - INFO - __main__ - Using 2 gpus
03/17/2022 15:08:44 - INFO - __main__ - args.device: cuda:1
03/17/2022 15:08:44 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/17/2022 15:08:44 - INFO - __main__ - Using 2 gpus
03/17/2022 15:08:44 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/17/2022 15:08:52 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.5, bsz=8 ...
03/17/2022 15:08:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:08:53 - INFO - __main__ - Printing 3 examples
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 15:08:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:08:53 - INFO - __main__ - Printing 3 examples
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:08:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:08:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:08:53 - INFO - __main__ - Printing 3 examples
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:08:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:08:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:08:53 - INFO - __main__ - Printing 3 examples
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:08:53 - INFO - __main__ - ['contradiction']
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:08:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:08:53 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:08:53 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:09:10 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:09:10 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:09:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:09:11 - INFO - __main__ - Starting training!
03/17/2022 15:09:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:09:16 - INFO - __main__ - Starting training!
03/17/2022 15:09:23 - INFO - __main__ - Step 10 Global step 10 Train loss 5.33 on epoch=3
03/17/2022 15:09:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.49 on epoch=6
03/17/2022 15:09:29 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=9
03/17/2022 15:09:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=13
03/17/2022 15:09:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=16
03/17/2022 15:09:36 - INFO - __main__ - Global step 50 Train loss 2.08 ACC 0.0 on epoch=16
03/17/2022 15:09:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 15:09:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=19
03/17/2022 15:09:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
03/17/2022 15:09:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
03/17/2022 15:09:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=29
03/17/2022 15:09:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
03/17/2022 15:09:51 - INFO - __main__ - Global step 100 Train loss 0.54 ACC 0.5 on epoch=33
03/17/2022 15:09:51 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.5 on epoch=33, global_step=100
03/17/2022 15:09:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
03/17/2022 15:09:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
03/17/2022 15:10:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
03/17/2022 15:10:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=46
03/17/2022 15:10:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
03/17/2022 15:10:06 - INFO - __main__ - Global step 150 Train loss 0.49 ACC 0.0 on epoch=49
03/17/2022 15:10:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
03/17/2022 15:10:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=56
03/17/2022 15:10:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
03/17/2022 15:10:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/17/2022 15:10:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
03/17/2022 15:10:21 - INFO - __main__ - Global step 200 Train loss 0.49 ACC 0.1875 on epoch=66
03/17/2022 15:10:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
03/17/2022 15:10:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
03/17/2022 15:10:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/17/2022 15:10:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
03/17/2022 15:10:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=83
03/17/2022 15:10:37 - INFO - __main__ - Global step 250 Train loss 0.48 ACC 0.125 on epoch=83
03/17/2022 15:10:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
03/17/2022 15:10:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
03/17/2022 15:10:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
03/17/2022 15:10:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
03/17/2022 15:10:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
03/17/2022 15:10:52 - INFO - __main__ - Global step 300 Train loss 0.44 ACC 0.28125 on epoch=99
03/17/2022 15:10:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
03/17/2022 15:10:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
03/17/2022 15:11:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
03/17/2022 15:11:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=113
03/17/2022 15:11:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
03/17/2022 15:11:07 - INFO - __main__ - Global step 350 Train loss 0.43 ACC 0.5 on epoch=116
03/17/2022 15:11:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
03/17/2022 15:11:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
03/17/2022 15:11:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
03/17/2022 15:11:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/17/2022 15:11:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
03/17/2022 15:11:22 - INFO - __main__ - Global step 400 Train loss 0.42 ACC 0.65625 on epoch=133
03/17/2022 15:11:22 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=133, global_step=400
03/17/2022 15:11:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=136
03/17/2022 15:11:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
03/17/2022 15:11:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=143
03/17/2022 15:11:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=146
03/17/2022 15:11:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
03/17/2022 15:11:37 - INFO - __main__ - Global step 450 Train loss 0.41 ACC 0.65625 on epoch=149
03/17/2022 15:11:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=153
03/17/2022 15:11:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
03/17/2022 15:11:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=159
03/17/2022 15:11:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=163
03/17/2022 15:11:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=166
03/17/2022 15:11:52 - INFO - __main__ - Global step 500 Train loss 0.42 ACC 0.4375 on epoch=166
03/17/2022 15:11:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=169
03/17/2022 15:11:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
03/17/2022 15:12:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
03/17/2022 15:12:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=179
03/17/2022 15:12:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=183
03/17/2022 15:12:07 - INFO - __main__ - Global step 550 Train loss 0.35 ACC 0.5 on epoch=183
03/17/2022 15:12:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=186
03/17/2022 15:12:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=189
03/17/2022 15:12:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=193
03/17/2022 15:12:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=196
03/17/2022 15:12:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=199
03/17/2022 15:12:22 - INFO - __main__ - Global step 600 Train loss 0.33 ACC 0.53125 on epoch=199
03/17/2022 15:12:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
03/17/2022 15:12:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=206
03/17/2022 15:12:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
03/17/2022 15:12:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=213
03/17/2022 15:12:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=216
03/17/2022 15:12:37 - INFO - __main__ - Global step 650 Train loss 0.34 ACC 0.71875 on epoch=216
03/17/2022 15:12:37 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.71875 on epoch=216, global_step=650
03/17/2022 15:12:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=219
03/17/2022 15:12:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
03/17/2022 15:12:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=226
03/17/2022 15:12:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
03/17/2022 15:12:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=233
03/17/2022 15:12:52 - INFO - __main__ - Global step 700 Train loss 0.28 ACC 0.6875 on epoch=233
03/17/2022 15:12:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=236
03/17/2022 15:12:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=239
03/17/2022 15:13:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=243
03/17/2022 15:13:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=246
03/17/2022 15:13:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=249
03/17/2022 15:13:08 - INFO - __main__ - Global step 750 Train loss 0.26 ACC 0.46875 on epoch=249
03/17/2022 15:13:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=253
03/17/2022 15:13:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=256
03/17/2022 15:13:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=259
03/17/2022 15:13:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=263
03/17/2022 15:13:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=266
03/17/2022 15:13:23 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.625 on epoch=266
03/17/2022 15:13:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=269
03/17/2022 15:13:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=273
03/17/2022 15:13:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=276
03/17/2022 15:13:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
03/17/2022 15:13:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
03/17/2022 15:13:38 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.59375 on epoch=283
03/17/2022 15:13:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=286
03/17/2022 15:13:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=289
03/17/2022 15:13:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=293
03/17/2022 15:13:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=296
03/17/2022 15:13:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=299
03/17/2022 15:13:53 - INFO - __main__ - Global step 900 Train loss 0.20 ACC 0.46875 on epoch=299
03/17/2022 15:13:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=303
03/17/2022 15:13:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
03/17/2022 15:14:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=309
03/17/2022 15:14:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=313
03/17/2022 15:14:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=316
03/17/2022 15:14:08 - INFO - __main__ - Global step 950 Train loss 0.17 ACC 0.34375 on epoch=316
03/17/2022 15:14:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=319
03/17/2022 15:14:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=323
03/17/2022 15:14:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=326
03/17/2022 15:14:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
03/17/2022 15:14:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=333
03/17/2022 15:14:23 - INFO - __main__ - Global step 1000 Train loss 0.18 ACC 0.5 on epoch=333
03/17/2022 15:14:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=336
03/17/2022 15:14:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
03/17/2022 15:14:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=343
03/17/2022 15:14:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=346
03/17/2022 15:14:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=349
03/17/2022 15:14:38 - INFO - __main__ - Global step 1050 Train loss 0.19 ACC 0.59375 on epoch=349
03/17/2022 15:14:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=353
03/17/2022 15:14:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=356
03/17/2022 15:14:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=359
03/17/2022 15:14:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
03/17/2022 15:14:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=366
03/17/2022 15:14:53 - INFO - __main__ - Global step 1100 Train loss 0.16 ACC 0.625 on epoch=366
03/17/2022 15:14:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=369
03/17/2022 15:14:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
03/17/2022 15:15:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=376
03/17/2022 15:15:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=379
03/17/2022 15:15:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
03/17/2022 15:15:08 - INFO - __main__ - Global step 1150 Train loss 0.15 ACC 0.46875 on epoch=383
03/17/2022 15:15:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=386
03/17/2022 15:15:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=389
03/17/2022 15:15:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=393
03/17/2022 15:15:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=396
03/17/2022 15:15:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
03/17/2022 15:15:23 - INFO - __main__ - Global step 1200 Train loss 0.15 ACC 0.46875 on epoch=399
03/17/2022 15:15:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
03/17/2022 15:15:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
03/17/2022 15:15:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
03/17/2022 15:15:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=413
03/17/2022 15:15:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
03/17/2022 15:15:39 - INFO - __main__ - Global step 1250 Train loss 0.10 ACC 0.59375 on epoch=416
03/17/2022 15:15:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
03/17/2022 15:15:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
03/17/2022 15:15:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
03/17/2022 15:15:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
03/17/2022 15:15:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=433
03/17/2022 15:15:54 - INFO - __main__ - Global step 1300 Train loss 0.07 ACC 0.65625 on epoch=433
03/17/2022 15:15:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=436
03/17/2022 15:16:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
03/17/2022 15:16:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=443
03/17/2022 15:16:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=446
03/17/2022 15:16:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
03/17/2022 15:16:10 - INFO - __main__ - Global step 1350 Train loss 0.09 ACC 0.625 on epoch=449
03/17/2022 15:16:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
03/17/2022 15:16:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
03/17/2022 15:16:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
03/17/2022 15:16:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=463
03/17/2022 15:16:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=466
03/17/2022 15:16:25 - INFO - __main__ - Global step 1400 Train loss 0.06 ACC 0.625 on epoch=466
03/17/2022 15:16:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
03/17/2022 15:16:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
03/17/2022 15:16:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
03/17/2022 15:16:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=479
03/17/2022 15:16:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=483
03/17/2022 15:16:40 - INFO - __main__ - Global step 1450 Train loss 0.05 ACC 0.625 on epoch=483
03/17/2022 15:16:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
03/17/2022 15:16:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=489
03/17/2022 15:16:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
03/17/2022 15:16:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
03/17/2022 15:16:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
03/17/2022 15:16:56 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.59375 on epoch=499
03/17/2022 15:16:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
03/17/2022 15:17:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
03/17/2022 15:17:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
03/17/2022 15:17:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/17/2022 15:17:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=516
03/17/2022 15:17:11 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.65625 on epoch=516
03/17/2022 15:17:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
03/17/2022 15:17:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
03/17/2022 15:17:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
03/17/2022 15:17:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
03/17/2022 15:17:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
03/17/2022 15:17:26 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 0.65625 on epoch=533
03/17/2022 15:17:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
03/17/2022 15:17:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/17/2022 15:17:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=543
03/17/2022 15:17:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
03/17/2022 15:17:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
03/17/2022 15:17:42 - INFO - __main__ - Global step 1650 Train loss 0.04 ACC 0.59375 on epoch=549
03/17/2022 15:17:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
03/17/2022 15:17:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
03/17/2022 15:17:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
03/17/2022 15:17:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/17/2022 15:17:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
03/17/2022 15:17:57 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.65625 on epoch=566
03/17/2022 15:18:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/17/2022 15:18:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
03/17/2022 15:18:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
03/17/2022 15:18:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
03/17/2022 15:18:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/17/2022 15:18:12 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.5625 on epoch=583
03/17/2022 15:18:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
03/17/2022 15:18:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
03/17/2022 15:18:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
03/17/2022 15:18:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
03/17/2022 15:18:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/17/2022 15:18:27 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.65625 on epoch=599
03/17/2022 15:18:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
03/17/2022 15:18:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
03/17/2022 15:18:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
03/17/2022 15:18:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
03/17/2022 15:18:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
03/17/2022 15:18:43 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=616
03/17/2022 15:18:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
03/17/2022 15:18:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
03/17/2022 15:18:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
03/17/2022 15:18:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/17/2022 15:18:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=633
03/17/2022 15:18:58 - INFO - __main__ - Global step 1900 Train loss 0.03 ACC 0.65625 on epoch=633
03/17/2022 15:19:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/17/2022 15:19:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
03/17/2022 15:19:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
03/17/2022 15:19:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
03/17/2022 15:19:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/17/2022 15:19:13 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.59375 on epoch=649
03/17/2022 15:19:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
03/17/2022 15:19:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/17/2022 15:19:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
03/17/2022 15:19:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
03/17/2022 15:19:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
03/17/2022 15:19:28 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.46875 on epoch=666
03/17/2022 15:19:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
03/17/2022 15:19:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=673
03/17/2022 15:19:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
03/17/2022 15:19:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/17/2022 15:19:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
03/17/2022 15:19:44 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.59375 on epoch=683
03/17/2022 15:19:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/17/2022 15:19:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/17/2022 15:19:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
03/17/2022 15:19:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/17/2022 15:19:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/17/2022 15:19:59 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.59375 on epoch=699
03/17/2022 15:20:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/17/2022 15:20:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=706
03/17/2022 15:20:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/17/2022 15:20:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
03/17/2022 15:20:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
03/17/2022 15:20:14 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.53125 on epoch=716
03/17/2022 15:20:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/17/2022 15:20:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/17/2022 15:20:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
03/17/2022 15:20:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/17/2022 15:20:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
03/17/2022 15:20:29 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.46875 on epoch=733
03/17/2022 15:20:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
03/17/2022 15:20:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/17/2022 15:20:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/17/2022 15:20:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/17/2022 15:20:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/17/2022 15:20:44 - INFO - __main__ - Global step 2250 Train loss 0.00 ACC 0.65625 on epoch=749
03/17/2022 15:20:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/17/2022 15:20:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/17/2022 15:20:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
03/17/2022 15:20:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
03/17/2022 15:20:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
03/17/2022 15:20:59 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.5625 on epoch=766
03/17/2022 15:21:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/17/2022 15:21:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
03/17/2022 15:21:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/17/2022 15:21:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/17/2022 15:21:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/17/2022 15:21:14 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.625 on epoch=783
03/17/2022 15:21:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
03/17/2022 15:21:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/17/2022 15:21:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/17/2022 15:21:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=796
03/17/2022 15:21:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/17/2022 15:21:29 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.5625 on epoch=799
03/17/2022 15:21:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
03/17/2022 15:21:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
03/17/2022 15:21:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/17/2022 15:21:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/17/2022 15:21:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
03/17/2022 15:21:44 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.6875 on epoch=816
03/17/2022 15:21:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
03/17/2022 15:21:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/17/2022 15:21:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/17/2022 15:21:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
03/17/2022 15:21:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
03/17/2022 15:22:00 - INFO - __main__ - Global step 2500 Train loss 0.00 ACC 0.71875 on epoch=833
03/17/2022 15:22:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
03/17/2022 15:22:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/17/2022 15:22:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=843
03/17/2022 15:22:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/17/2022 15:22:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/17/2022 15:22:15 - INFO - __main__ - Global step 2550 Train loss 0.02 ACC 0.625 on epoch=849
03/17/2022 15:22:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/17/2022 15:22:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/17/2022 15:22:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/17/2022 15:22:26 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/17/2022 15:22:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/17/2022 15:22:30 - INFO - __main__ - Global step 2600 Train loss 0.00 ACC 0.59375 on epoch=866
03/17/2022 15:22:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/17/2022 15:22:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/17/2022 15:22:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
03/17/2022 15:22:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/17/2022 15:22:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/17/2022 15:22:45 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.625 on epoch=883
03/17/2022 15:22:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/17/2022 15:22:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/17/2022 15:22:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
03/17/2022 15:22:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
03/17/2022 15:22:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
03/17/2022 15:23:00 - INFO - __main__ - Global step 2700 Train loss 0.00 ACC 0.65625 on epoch=899
03/17/2022 15:23:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/17/2022 15:23:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/17/2022 15:23:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/17/2022 15:23:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
03/17/2022 15:23:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/17/2022 15:23:15 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.5 on epoch=916
03/17/2022 15:23:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/17/2022 15:23:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/17/2022 15:23:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/17/2022 15:23:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/17/2022 15:23:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/17/2022 15:23:30 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.6875 on epoch=933
03/17/2022 15:23:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/17/2022 15:23:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/17/2022 15:23:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/17/2022 15:23:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/17/2022 15:23:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/17/2022 15:23:45 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.59375 on epoch=949
03/17/2022 15:23:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/17/2022 15:23:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/17/2022 15:23:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/17/2022 15:23:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/17/2022 15:24:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/17/2022 15:24:01 - INFO - __main__ - Global step 2900 Train loss 0.00 ACC 0.5 on epoch=966
03/17/2022 15:24:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/17/2022 15:24:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/17/2022 15:24:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/17/2022 15:24:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
03/17/2022 15:24:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/17/2022 15:24:16 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.46875 on epoch=983
03/17/2022 15:24:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/17/2022 15:24:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/17/2022 15:24:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
03/17/2022 15:24:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/17/2022 15:24:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/17/2022 15:24:32 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.65625 on epoch=999
03/17/2022 15:24:32 - INFO - __main__ - save last model!
03/17/2022 15:24:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 15:24:32 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 15:24:32 - INFO - __main__ - Printing 3 examples
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 15:24:32 - INFO - __main__ - ['neutral']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 15:24:32 - INFO - __main__ - ['entailment']
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:24:32 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 15:24:32 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:24:32 - INFO - __main__ - Printing 3 examples
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:24:32 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:24:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:24:32 - INFO - __main__ - Printing 3 examples
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:24:32 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:24:32 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:24:32 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:24:34 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_100_0.5_8_predictions.txt
03/17/2022 15:24:34 - INFO - __main__ - ACC on test data: 0.6964
03/17/2022 15:24:34 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.5, bsz=8, dev_performance=0.71875, test_performance=0.6964285714285714
03/17/2022 15:24:34 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.4, bsz=8 ...
03/17/2022 15:24:35 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:24:35 - INFO - __main__ - Printing 3 examples
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 15:24:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:24:35 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:24:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:24:35 - INFO - __main__ - Printing 3 examples
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:24:35 - INFO - __main__ - ['contradiction']
03/17/2022 15:24:35 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:24:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:24:35 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:24:47 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:24:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:24:48 - INFO - __main__ - Starting training!
03/17/2022 15:24:50 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:24:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:24:51 - INFO - __main__ - Starting training!
03/17/2022 15:24:55 - INFO - __main__ - Step 10 Global step 10 Train loss 5.63 on epoch=3
03/17/2022 15:24:57 - INFO - __main__ - Step 20 Global step 20 Train loss 4.09 on epoch=6
03/17/2022 15:25:00 - INFO - __main__ - Step 30 Global step 30 Train loss 2.78 on epoch=9
03/17/2022 15:25:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.16 on epoch=13
03/17/2022 15:25:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=16
03/17/2022 15:25:07 - INFO - __main__ - Global step 50 Train loss 3.28 ACC 0.5 on epoch=16
03/17/2022 15:25:07 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 15:25:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.41 on epoch=19
03/17/2022 15:25:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=23
03/17/2022 15:25:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=26
03/17/2022 15:25:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.78 on epoch=29
03/17/2022 15:25:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=33
03/17/2022 15:25:23 - INFO - __main__ - Global step 100 Train loss 1.00 ACC 0.3125 on epoch=33
03/17/2022 15:25:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=36
03/17/2022 15:25:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=39
03/17/2022 15:25:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
03/17/2022 15:25:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=46
03/17/2022 15:25:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.63 on epoch=49
03/17/2022 15:25:38 - INFO - __main__ - Global step 150 Train loss 0.61 ACC 0.46875 on epoch=49
03/17/2022 15:25:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=53
03/17/2022 15:25:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=56
03/17/2022 15:25:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=59
03/17/2022 15:25:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
03/17/2022 15:25:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=66
03/17/2022 15:25:53 - INFO - __main__ - Global step 200 Train loss 0.56 ACC 0.5 on epoch=66
03/17/2022 15:25:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=69
03/17/2022 15:25:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
03/17/2022 15:26:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
03/17/2022 15:26:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.63 on epoch=79
03/17/2022 15:26:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=83
03/17/2022 15:26:08 - INFO - __main__ - Global step 250 Train loss 0.53 ACC 0.5 on epoch=83
03/17/2022 15:26:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
03/17/2022 15:26:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=89
03/17/2022 15:26:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
03/17/2022 15:26:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
03/17/2022 15:26:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
03/17/2022 15:26:23 - INFO - __main__ - Global step 300 Train loss 0.45 ACC 0.40625 on epoch=99
03/17/2022 15:26:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=103
03/17/2022 15:26:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=106
03/17/2022 15:26:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/17/2022 15:26:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
03/17/2022 15:26:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
03/17/2022 15:26:39 - INFO - __main__ - Global step 350 Train loss 0.46 ACC 0.5625 on epoch=116
03/17/2022 15:26:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=116, global_step=350
03/17/2022 15:26:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
03/17/2022 15:26:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=123
03/17/2022 15:26:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
03/17/2022 15:26:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
03/17/2022 15:26:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
03/17/2022 15:26:54 - INFO - __main__ - Global step 400 Train loss 0.44 ACC 0.53125 on epoch=133
03/17/2022 15:26:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
03/17/2022 15:26:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=139
03/17/2022 15:27:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
03/17/2022 15:27:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=146
03/17/2022 15:27:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=149
03/17/2022 15:27:09 - INFO - __main__ - Global step 450 Train loss 0.43 ACC 0.03125 on epoch=149
03/17/2022 15:27:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
03/17/2022 15:27:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=156
03/17/2022 15:27:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=159
03/17/2022 15:27:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
03/17/2022 15:27:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=166
03/17/2022 15:27:24 - INFO - __main__ - Global step 500 Train loss 0.41 ACC 0.5625 on epoch=166
03/17/2022 15:27:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=169
03/17/2022 15:27:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
03/17/2022 15:27:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=176
03/17/2022 15:27:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=179
03/17/2022 15:27:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=183
03/17/2022 15:27:39 - INFO - __main__ - Global step 550 Train loss 0.42 ACC 0.625 on epoch=183
03/17/2022 15:27:39 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=183, global_step=550
03/17/2022 15:27:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=186
03/17/2022 15:27:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=189
03/17/2022 15:27:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=193
03/17/2022 15:27:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=196
03/17/2022 15:27:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=199
03/17/2022 15:27:54 - INFO - __main__ - Global step 600 Train loss 0.39 ACC 0.53125 on epoch=199
03/17/2022 15:27:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
03/17/2022 15:28:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=206
03/17/2022 15:28:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=209
03/17/2022 15:28:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=213
03/17/2022 15:28:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=216
03/17/2022 15:28:09 - INFO - __main__ - Global step 650 Train loss 0.38 ACC 0.625 on epoch=216
03/17/2022 15:28:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=219
03/17/2022 15:28:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=223
03/17/2022 15:28:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=226
03/17/2022 15:28:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=229
03/17/2022 15:28:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=233
03/17/2022 15:28:25 - INFO - __main__ - Global step 700 Train loss 0.37 ACC 0.75 on epoch=233
03/17/2022 15:28:25 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.75 on epoch=233, global_step=700
03/17/2022 15:28:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=236
03/17/2022 15:28:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=239
03/17/2022 15:28:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=243
03/17/2022 15:28:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=246
03/17/2022 15:28:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=249
03/17/2022 15:28:40 - INFO - __main__ - Global step 750 Train loss 0.35 ACC 0.59375 on epoch=249
03/17/2022 15:28:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=253
03/17/2022 15:28:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=256
03/17/2022 15:28:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=259
03/17/2022 15:28:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=263
03/17/2022 15:28:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=266
03/17/2022 15:28:55 - INFO - __main__ - Global step 800 Train loss 0.29 ACC 0.53125 on epoch=266
03/17/2022 15:28:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
03/17/2022 15:29:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=273
03/17/2022 15:29:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=276
03/17/2022 15:29:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=279
03/17/2022 15:29:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=283
03/17/2022 15:29:10 - INFO - __main__ - Global step 850 Train loss 0.26 ACC 0.46875 on epoch=283
03/17/2022 15:29:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
03/17/2022 15:29:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=289
03/17/2022 15:29:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=293
03/17/2022 15:29:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=296
03/17/2022 15:29:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=299
03/17/2022 15:29:26 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.53125 on epoch=299
03/17/2022 15:29:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=303
03/17/2022 15:29:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=306
03/17/2022 15:29:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=309
03/17/2022 15:29:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=313
03/17/2022 15:29:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=316
03/17/2022 15:29:41 - INFO - __main__ - Global step 950 Train loss 0.26 ACC 0.53125 on epoch=316
03/17/2022 15:29:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=319
03/17/2022 15:29:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=323
03/17/2022 15:29:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=326
03/17/2022 15:29:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
03/17/2022 15:29:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=333
03/17/2022 15:29:56 - INFO - __main__ - Global step 1000 Train loss 0.21 ACC 0.53125 on epoch=333
03/17/2022 15:29:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=336
03/17/2022 15:30:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
03/17/2022 15:30:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=343
03/17/2022 15:30:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=346
03/17/2022 15:30:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=349
03/17/2022 15:30:11 - INFO - __main__ - Global step 1050 Train loss 0.20 ACC 0.53125 on epoch=349
03/17/2022 15:30:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=353
03/17/2022 15:30:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=356
03/17/2022 15:30:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=359
03/17/2022 15:30:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=363
03/17/2022 15:30:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=366
03/17/2022 15:30:26 - INFO - __main__ - Global step 1100 Train loss 0.16 ACC 0.40625 on epoch=366
03/17/2022 15:30:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=369
03/17/2022 15:30:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=373
03/17/2022 15:30:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=376
03/17/2022 15:30:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=379
03/17/2022 15:30:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=383
03/17/2022 15:30:41 - INFO - __main__ - Global step 1150 Train loss 0.14 ACC 0.53125 on epoch=383
03/17/2022 15:30:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=386
03/17/2022 15:30:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=389
03/17/2022 15:30:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=393
03/17/2022 15:30:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=396
03/17/2022 15:30:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=399
03/17/2022 15:30:57 - INFO - __main__ - Global step 1200 Train loss 0.17 ACC 0.40625 on epoch=399
03/17/2022 15:30:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
03/17/2022 15:31:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=406
03/17/2022 15:31:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=409
03/17/2022 15:31:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=413
03/17/2022 15:31:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=416
03/17/2022 15:31:12 - INFO - __main__ - Global step 1250 Train loss 0.15 ACC 0.03125 on epoch=416
03/17/2022 15:31:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=419
03/17/2022 15:31:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=423
03/17/2022 15:31:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=426
03/17/2022 15:31:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=429
03/17/2022 15:31:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=433
03/17/2022 15:31:27 - INFO - __main__ - Global step 1300 Train loss 0.12 ACC 0.46875 on epoch=433
03/17/2022 15:31:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=436
03/17/2022 15:31:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
03/17/2022 15:31:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
03/17/2022 15:31:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=446
03/17/2022 15:31:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=449
03/17/2022 15:31:42 - INFO - __main__ - Global step 1350 Train loss 0.11 ACC 0.46875 on epoch=449
03/17/2022 15:31:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
03/17/2022 15:31:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=456
03/17/2022 15:31:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=459
03/17/2022 15:31:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=463
03/17/2022 15:31:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
03/17/2022 15:31:57 - INFO - __main__ - Global step 1400 Train loss 0.10 ACC 0.34375 on epoch=466
03/17/2022 15:32:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=469
03/17/2022 15:32:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=473
03/17/2022 15:32:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=476
03/17/2022 15:32:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=479
03/17/2022 15:32:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=483
03/17/2022 15:32:12 - INFO - __main__ - Global step 1450 Train loss 0.10 ACC 0.21875 on epoch=483
03/17/2022 15:32:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=486
03/17/2022 15:32:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=489
03/17/2022 15:32:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=493
03/17/2022 15:32:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
03/17/2022 15:32:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
03/17/2022 15:32:28 - INFO - __main__ - Global step 1500 Train loss 0.09 ACC 0.40625 on epoch=499
03/17/2022 15:32:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
03/17/2022 15:32:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=506
03/17/2022 15:32:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
03/17/2022 15:32:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
03/17/2022 15:32:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
03/17/2022 15:32:43 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.34375 on epoch=516
03/17/2022 15:32:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
03/17/2022 15:32:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
03/17/2022 15:32:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
03/17/2022 15:32:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=529
03/17/2022 15:32:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
03/17/2022 15:32:59 - INFO - __main__ - Global step 1600 Train loss 0.06 ACC 0.46875 on epoch=533
03/17/2022 15:33:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
03/17/2022 15:33:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=539
03/17/2022 15:33:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
03/17/2022 15:33:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=546
03/17/2022 15:33:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
03/17/2022 15:33:14 - INFO - __main__ - Global step 1650 Train loss 0.05 ACC 0.46875 on epoch=549
03/17/2022 15:33:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
03/17/2022 15:33:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=556
03/17/2022 15:33:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
03/17/2022 15:33:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
03/17/2022 15:33:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
03/17/2022 15:33:29 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.4375 on epoch=566
03/17/2022 15:33:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=569
03/17/2022 15:33:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/17/2022 15:33:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
03/17/2022 15:33:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
03/17/2022 15:33:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
03/17/2022 15:33:44 - INFO - __main__ - Global step 1750 Train loss 0.04 ACC 0.5 on epoch=583
03/17/2022 15:33:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=586
03/17/2022 15:33:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/17/2022 15:33:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=593
03/17/2022 15:33:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/17/2022 15:33:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
03/17/2022 15:34:00 - INFO - __main__ - Global step 1800 Train loss 0.03 ACC 0.5 on epoch=599
03/17/2022 15:34:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/17/2022 15:34:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
03/17/2022 15:34:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
03/17/2022 15:34:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
03/17/2022 15:34:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
03/17/2022 15:34:15 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.53125 on epoch=616
03/17/2022 15:34:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=619
03/17/2022 15:34:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
03/17/2022 15:34:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=626
03/17/2022 15:34:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=629
03/17/2022 15:34:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
03/17/2022 15:34:30 - INFO - __main__ - Global step 1900 Train loss 0.05 ACC 0.5 on epoch=633
03/17/2022 15:34:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
03/17/2022 15:34:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=639
03/17/2022 15:34:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
03/17/2022 15:34:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/17/2022 15:34:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
03/17/2022 15:34:45 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.53125 on epoch=649
03/17/2022 15:34:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/17/2022 15:34:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=656
03/17/2022 15:34:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
03/17/2022 15:34:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
03/17/2022 15:35:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/17/2022 15:35:01 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.375 on epoch=666
03/17/2022 15:35:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/17/2022 15:35:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=673
03/17/2022 15:35:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
03/17/2022 15:35:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=679
03/17/2022 15:35:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/17/2022 15:35:16 - INFO - __main__ - Global step 2050 Train loss 0.05 ACC 0.53125 on epoch=683
03/17/2022 15:35:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
03/17/2022 15:35:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
03/17/2022 15:35:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
03/17/2022 15:35:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
03/17/2022 15:35:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
03/17/2022 15:35:31 - INFO - __main__ - Global step 2100 Train loss 0.02 ACC 0.53125 on epoch=699
03/17/2022 15:35:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/17/2022 15:35:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=706
03/17/2022 15:35:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/17/2022 15:35:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
03/17/2022 15:35:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/17/2022 15:35:46 - INFO - __main__ - Global step 2150 Train loss 0.02 ACC 0.5 on epoch=716
03/17/2022 15:35:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/17/2022 15:35:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/17/2022 15:35:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
03/17/2022 15:35:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/17/2022 15:36:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
03/17/2022 15:36:01 - INFO - __main__ - Global step 2200 Train loss 0.01 ACC 0.625 on epoch=733
03/17/2022 15:36:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
03/17/2022 15:36:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/17/2022 15:36:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=743
03/17/2022 15:36:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
03/17/2022 15:36:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
03/17/2022 15:36:16 - INFO - __main__ - Global step 2250 Train loss 0.02 ACC 0.5 on epoch=749
03/17/2022 15:36:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/17/2022 15:36:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=756
03/17/2022 15:36:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
03/17/2022 15:36:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
03/17/2022 15:36:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/17/2022 15:36:31 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.46875 on epoch=766
03/17/2022 15:36:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
03/17/2022 15:36:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=773
03/17/2022 15:36:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
03/17/2022 15:36:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
03/17/2022 15:36:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
03/17/2022 15:36:47 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.46875 on epoch=783
03/17/2022 15:36:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
03/17/2022 15:36:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
03/17/2022 15:36:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/17/2022 15:36:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/17/2022 15:37:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
03/17/2022 15:37:02 - INFO - __main__ - Global step 2400 Train loss 0.02 ACC 0.40625 on epoch=799
03/17/2022 15:37:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
03/17/2022 15:37:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/17/2022 15:37:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/17/2022 15:37:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=813
03/17/2022 15:37:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/17/2022 15:37:17 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.40625 on epoch=816
03/17/2022 15:37:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/17/2022 15:37:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
03/17/2022 15:37:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/17/2022 15:37:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
03/17/2022 15:37:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
03/17/2022 15:37:32 - INFO - __main__ - Global step 2500 Train loss 0.02 ACC 0.53125 on epoch=833
03/17/2022 15:37:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/17/2022 15:37:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/17/2022 15:37:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
03/17/2022 15:37:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/17/2022 15:37:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
03/17/2022 15:37:47 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.5 on epoch=849
03/17/2022 15:37:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/17/2022 15:37:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
03/17/2022 15:37:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/17/2022 15:37:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
03/17/2022 15:38:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
03/17/2022 15:38:02 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.53125 on epoch=866
03/17/2022 15:38:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
03/17/2022 15:38:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/17/2022 15:38:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
03/17/2022 15:38:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/17/2022 15:38:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
03/17/2022 15:38:17 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.5625 on epoch=883
03/17/2022 15:38:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
03/17/2022 15:38:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/17/2022 15:38:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
03/17/2022 15:38:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
03/17/2022 15:38:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/17/2022 15:38:32 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.53125 on epoch=899
03/17/2022 15:38:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/17/2022 15:38:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
03/17/2022 15:38:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
03/17/2022 15:38:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/17/2022 15:38:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/17/2022 15:38:48 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.625 on epoch=916
03/17/2022 15:38:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/17/2022 15:38:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/17/2022 15:38:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/17/2022 15:38:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/17/2022 15:39:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/17/2022 15:39:03 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.375 on epoch=933
03/17/2022 15:39:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/17/2022 15:39:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
03/17/2022 15:39:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/17/2022 15:39:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/17/2022 15:39:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
03/17/2022 15:39:18 - INFO - __main__ - Global step 2850 Train loss 0.00 ACC 0.59375 on epoch=949
03/17/2022 15:39:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
03/17/2022 15:39:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/17/2022 15:39:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/17/2022 15:39:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/17/2022 15:39:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
03/17/2022 15:39:33 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.40625 on epoch=966
03/17/2022 15:39:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/17/2022 15:39:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/17/2022 15:39:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/17/2022 15:39:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/17/2022 15:39:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
03/17/2022 15:39:48 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.5625 on epoch=983
03/17/2022 15:39:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
03/17/2022 15:39:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/17/2022 15:39:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/17/2022 15:39:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/17/2022 15:40:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/17/2022 15:40:03 - INFO - __main__ - Global step 3000 Train loss 0.01 ACC 0.46875 on epoch=999
03/17/2022 15:40:03 - INFO - __main__ - save last model!
03/17/2022 15:40:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 15:40:03 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 15:40:03 - INFO - __main__ - Printing 3 examples
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 15:40:03 - INFO - __main__ - ['neutral']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 15:40:03 - INFO - __main__ - ['entailment']
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:40:03 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 15:40:03 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:40:03 - INFO - __main__ - Printing 3 examples
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:40:03 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:40:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:40:03 - INFO - __main__ - Printing 3 examples
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:40:03 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:40:03 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:40:03 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:40:05 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_100_0.4_8_predictions.txt
03/17/2022 15:40:05 - INFO - __main__ - ACC on test data: 0.5714
03/17/2022 15:40:05 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.4, bsz=8, dev_performance=0.75, test_performance=0.5714285714285714
03/17/2022 15:40:05 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.3, bsz=8 ...
03/17/2022 15:40:06 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:40:06 - INFO - __main__ - Printing 3 examples
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 15:40:06 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:40:06 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:40:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:40:06 - INFO - __main__ - Printing 3 examples
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:40:06 - INFO - __main__ - ['contradiction']
03/17/2022 15:40:06 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:40:06 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:40:06 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:40:21 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:40:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:40:22 - INFO - __main__ - Starting training!
03/17/2022 15:40:22 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:40:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:40:23 - INFO - __main__ - Starting training!
03/17/2022 15:40:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.88 on epoch=3
03/17/2022 15:40:29 - INFO - __main__ - Step 20 Global step 20 Train loss 4.19 on epoch=6
03/17/2022 15:40:32 - INFO - __main__ - Step 30 Global step 30 Train loss 5.44 on epoch=9
03/17/2022 15:40:35 - INFO - __main__ - Step 40 Global step 40 Train loss 4.09 on epoch=13
03/17/2022 15:40:38 - INFO - __main__ - Step 50 Global step 50 Train loss 3.83 on epoch=16
03/17/2022 15:40:43 - INFO - __main__ - Global step 50 Train loss 4.69 ACC 0.375 on epoch=16
03/17/2022 15:40:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.375 on epoch=16, global_step=50
03/17/2022 15:40:46 - INFO - __main__ - Step 60 Global step 60 Train loss 3.84 on epoch=19
03/17/2022 15:40:49 - INFO - __main__ - Step 70 Global step 70 Train loss 3.57 on epoch=23
03/17/2022 15:40:52 - INFO - __main__ - Step 80 Global step 80 Train loss 3.52 on epoch=26
03/17/2022 15:40:55 - INFO - __main__ - Step 90 Global step 90 Train loss 3.35 on epoch=29
03/17/2022 15:40:57 - INFO - __main__ - Step 100 Global step 100 Train loss 3.12 on epoch=33
03/17/2022 15:40:59 - INFO - __main__ - Global step 100 Train loss 3.48 ACC 0.4375 on epoch=33
03/17/2022 15:40:59 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.4375 on epoch=33, global_step=100
03/17/2022 15:41:02 - INFO - __main__ - Step 110 Global step 110 Train loss 3.03 on epoch=36
03/17/2022 15:41:05 - INFO - __main__ - Step 120 Global step 120 Train loss 2.86 on epoch=39
03/17/2022 15:41:08 - INFO - __main__ - Step 130 Global step 130 Train loss 2.75 on epoch=43
03/17/2022 15:41:11 - INFO - __main__ - Step 140 Global step 140 Train loss 2.61 on epoch=46
03/17/2022 15:41:14 - INFO - __main__ - Step 150 Global step 150 Train loss 2.45 on epoch=49
03/17/2022 15:41:15 - INFO - __main__ - Global step 150 Train loss 2.74 ACC 0.0 on epoch=49
03/17/2022 15:41:18 - INFO - __main__ - Step 160 Global step 160 Train loss 2.26 on epoch=53
03/17/2022 15:41:21 - INFO - __main__ - Step 170 Global step 170 Train loss 2.09 on epoch=56
03/17/2022 15:41:24 - INFO - __main__ - Step 180 Global step 180 Train loss 2.11 on epoch=59
03/17/2022 15:41:26 - INFO - __main__ - Step 190 Global step 190 Train loss 1.88 on epoch=63
03/17/2022 15:41:29 - INFO - __main__ - Step 200 Global step 200 Train loss 1.72 on epoch=66
03/17/2022 15:41:30 - INFO - __main__ - Global step 200 Train loss 2.01 ACC 0.4375 on epoch=66
03/17/2022 15:41:33 - INFO - __main__ - Step 210 Global step 210 Train loss 1.70 on epoch=69
03/17/2022 15:41:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.57 on epoch=73
03/17/2022 15:41:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.51 on epoch=76
03/17/2022 15:41:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.37 on epoch=79
03/17/2022 15:41:45 - INFO - __main__ - Step 250 Global step 250 Train loss 1.39 on epoch=83
03/17/2022 15:41:45 - INFO - __main__ - Global step 250 Train loss 1.51 ACC 0.5 on epoch=83
03/17/2022 15:41:45 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=83, global_step=250
03/17/2022 15:41:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.27 on epoch=86
03/17/2022 15:41:51 - INFO - __main__ - Step 270 Global step 270 Train loss 1.18 on epoch=89
03/17/2022 15:41:54 - INFO - __main__ - Step 280 Global step 280 Train loss 1.09 on epoch=93
03/17/2022 15:41:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.99 on epoch=96
03/17/2022 15:42:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=99
03/17/2022 15:42:01 - INFO - __main__ - Global step 300 Train loss 1.09 ACC 0.5 on epoch=99
03/17/2022 15:42:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=103
03/17/2022 15:42:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=106
03/17/2022 15:42:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.75 on epoch=109
03/17/2022 15:42:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=113
03/17/2022 15:42:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=116
03/17/2022 15:42:16 - INFO - __main__ - Global step 350 Train loss 0.81 ACC 0.5 on epoch=116
03/17/2022 15:42:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=119
03/17/2022 15:42:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.75 on epoch=123
03/17/2022 15:42:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=126
03/17/2022 15:42:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=129
03/17/2022 15:42:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=133
03/17/2022 15:42:32 - INFO - __main__ - Global step 400 Train loss 0.68 ACC 0.5 on epoch=133
03/17/2022 15:42:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=136
03/17/2022 15:42:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=139
03/17/2022 15:42:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=143
03/17/2022 15:42:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=146
03/17/2022 15:42:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.60 on epoch=149
03/17/2022 15:42:47 - INFO - __main__ - Global step 450 Train loss 0.64 ACC 0.5 on epoch=149
03/17/2022 15:42:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=153
03/17/2022 15:42:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=156
03/17/2022 15:42:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=159
03/17/2022 15:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=163
03/17/2022 15:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=166
03/17/2022 15:43:02 - INFO - __main__ - Global step 500 Train loss 0.58 ACC 0.5 on epoch=166
03/17/2022 15:43:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.64 on epoch=169
03/17/2022 15:43:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=173
03/17/2022 15:43:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=176
03/17/2022 15:43:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=179
03/17/2022 15:43:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=183
03/17/2022 15:43:18 - INFO - __main__ - Global step 550 Train loss 0.57 ACC 0.5 on epoch=183
03/17/2022 15:43:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=186
03/17/2022 15:43:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.56 on epoch=189
03/17/2022 15:43:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=193
03/17/2022 15:43:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=196
03/17/2022 15:43:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=199
03/17/2022 15:43:33 - INFO - __main__ - Global step 600 Train loss 0.52 ACC 0.5 on epoch=199
03/17/2022 15:43:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=203
03/17/2022 15:43:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.58 on epoch=206
03/17/2022 15:43:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=209
03/17/2022 15:43:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=213
03/17/2022 15:43:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=216
03/17/2022 15:43:49 - INFO - __main__ - Global step 650 Train loss 0.56 ACC 0.5 on epoch=216
03/17/2022 15:43:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=219
03/17/2022 15:43:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=223
03/17/2022 15:43:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=226
03/17/2022 15:44:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=229
03/17/2022 15:44:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=233
03/17/2022 15:44:04 - INFO - __main__ - Global step 700 Train loss 0.53 ACC 0.5 on epoch=233
03/17/2022 15:44:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=236
03/17/2022 15:44:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=239
03/17/2022 15:44:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=243
03/17/2022 15:44:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=246
03/17/2022 15:44:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=249
03/17/2022 15:44:20 - INFO - __main__ - Global step 750 Train loss 0.48 ACC 0.46875 on epoch=249
03/17/2022 15:44:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=253
03/17/2022 15:44:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=256
03/17/2022 15:44:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=259
03/17/2022 15:44:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=263
03/17/2022 15:44:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=266
03/17/2022 15:44:35 - INFO - __main__ - Global step 800 Train loss 0.49 ACC 0.5 on epoch=266
03/17/2022 15:44:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=269
03/17/2022 15:44:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=273
03/17/2022 15:44:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=276
03/17/2022 15:44:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=279
03/17/2022 15:44:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=283
03/17/2022 15:44:50 - INFO - __main__ - Global step 850 Train loss 0.47 ACC 0.5 on epoch=283
03/17/2022 15:44:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=286
03/17/2022 15:44:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=289
03/17/2022 15:44:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=293
03/17/2022 15:45:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=296
03/17/2022 15:45:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=299
03/17/2022 15:45:06 - INFO - __main__ - Global step 900 Train loss 0.47 ACC 0.5 on epoch=299
03/17/2022 15:45:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=303
03/17/2022 15:45:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=306
03/17/2022 15:45:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=309
03/17/2022 15:45:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=313
03/17/2022 15:45:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=316
03/17/2022 15:45:21 - INFO - __main__ - Global step 950 Train loss 0.45 ACC 0.5 on epoch=316
03/17/2022 15:45:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=319
03/17/2022 15:45:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=323
03/17/2022 15:45:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=326
03/17/2022 15:45:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=329
03/17/2022 15:45:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=333
03/17/2022 15:45:37 - INFO - __main__ - Global step 1000 Train loss 0.42 ACC 0.5 on epoch=333
03/17/2022 15:45:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=336
03/17/2022 15:45:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=339
03/17/2022 15:45:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=343
03/17/2022 15:45:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=346
03/17/2022 15:45:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=349
03/17/2022 15:45:52 - INFO - __main__ - Global step 1050 Train loss 0.46 ACC 0.3125 on epoch=349
03/17/2022 15:45:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=353
03/17/2022 15:45:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=356
03/17/2022 15:46:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=359
03/17/2022 15:46:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=363
03/17/2022 15:46:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=366
03/17/2022 15:46:08 - INFO - __main__ - Global step 1100 Train loss 0.44 ACC 0.5 on epoch=366
03/17/2022 15:46:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=369
03/17/2022 15:46:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=373
03/17/2022 15:46:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=376
03/17/2022 15:46:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=379
03/17/2022 15:46:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=383
03/17/2022 15:46:23 - INFO - __main__ - Global step 1150 Train loss 0.42 ACC 0.5 on epoch=383
03/17/2022 15:46:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=386
03/17/2022 15:46:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=389
03/17/2022 15:46:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=393
03/17/2022 15:46:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=396
03/17/2022 15:46:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=399
03/17/2022 15:46:39 - INFO - __main__ - Global step 1200 Train loss 0.41 ACC 0.5 on epoch=399
03/17/2022 15:46:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=403
03/17/2022 15:46:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=406
03/17/2022 15:46:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=409
03/17/2022 15:46:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=413
03/17/2022 15:46:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=416
03/17/2022 15:46:54 - INFO - __main__ - Global step 1250 Train loss 0.42 ACC 0.5 on epoch=416
03/17/2022 15:46:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=419
03/17/2022 15:47:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=423
03/17/2022 15:47:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=426
03/17/2022 15:47:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=429
03/17/2022 15:47:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=433
03/17/2022 15:47:09 - INFO - __main__ - Global step 1300 Train loss 0.39 ACC 0.25 on epoch=433
03/17/2022 15:47:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=436
03/17/2022 15:47:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=439
03/17/2022 15:47:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=443
03/17/2022 15:47:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=446
03/17/2022 15:47:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=449
03/17/2022 15:47:25 - INFO - __main__ - Global step 1350 Train loss 0.35 ACC 0.59375 on epoch=449
03/17/2022 15:47:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=449, global_step=1350
03/17/2022 15:47:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=453
03/17/2022 15:47:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=456
03/17/2022 15:47:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=459
03/17/2022 15:47:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=463
03/17/2022 15:47:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=466
03/17/2022 15:47:40 - INFO - __main__ - Global step 1400 Train loss 0.36 ACC 0.5 on epoch=466
03/17/2022 15:47:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=469
03/17/2022 15:47:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=473
03/17/2022 15:47:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=476
03/17/2022 15:47:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=479
03/17/2022 15:47:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=483
03/17/2022 15:47:56 - INFO - __main__ - Global step 1450 Train loss 0.36 ACC 0.46875 on epoch=483
03/17/2022 15:47:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=486
03/17/2022 15:48:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=489
03/17/2022 15:48:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=493
03/17/2022 15:48:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=496
03/17/2022 15:48:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=499
03/17/2022 15:48:11 - INFO - __main__ - Global step 1500 Train loss 0.31 ACC 0.5 on epoch=499
03/17/2022 15:48:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=503
03/17/2022 15:48:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=506
03/17/2022 15:48:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=509
03/17/2022 15:48:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=513
03/17/2022 15:48:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=516
03/17/2022 15:48:27 - INFO - __main__ - Global step 1550 Train loss 0.30 ACC 0.59375 on epoch=516
03/17/2022 15:48:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=519
03/17/2022 15:48:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=523
03/17/2022 15:48:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=526
03/17/2022 15:48:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=529
03/17/2022 15:48:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=533
03/17/2022 15:48:42 - INFO - __main__ - Global step 1600 Train loss 0.31 ACC 0.4375 on epoch=533
03/17/2022 15:48:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=536
03/17/2022 15:48:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=539
03/17/2022 15:48:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=543
03/17/2022 15:48:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=546
03/17/2022 15:48:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=549
03/17/2022 15:48:58 - INFO - __main__ - Global step 1650 Train loss 0.27 ACC 0.53125 on epoch=549
03/17/2022 15:49:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=553
03/17/2022 15:49:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=556
03/17/2022 15:49:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=559
03/17/2022 15:49:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=563
03/17/2022 15:49:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=566
03/17/2022 15:49:13 - INFO - __main__ - Global step 1700 Train loss 0.26 ACC 0.625 on epoch=566
03/17/2022 15:49:13 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=566, global_step=1700
03/17/2022 15:49:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=569
03/17/2022 15:49:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=573
03/17/2022 15:49:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=576
03/17/2022 15:49:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=579
03/17/2022 15:49:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=583
03/17/2022 15:49:29 - INFO - __main__ - Global step 1750 Train loss 0.26 ACC 0.5 on epoch=583
03/17/2022 15:49:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=586
03/17/2022 15:49:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=589
03/17/2022 15:49:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=593
03/17/2022 15:49:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=596
03/17/2022 15:49:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=599
03/17/2022 15:49:44 - INFO - __main__ - Global step 1800 Train loss 0.24 ACC 0.53125 on epoch=599
03/17/2022 15:49:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=603
03/17/2022 15:49:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=606
03/17/2022 15:49:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=609
03/17/2022 15:49:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=613
03/17/2022 15:49:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=616
03/17/2022 15:49:59 - INFO - __main__ - Global step 1850 Train loss 0.24 ACC 0.46875 on epoch=616
03/17/2022 15:50:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=619
03/17/2022 15:50:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=623
03/17/2022 15:50:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=626
03/17/2022 15:50:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=629
03/17/2022 15:50:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=633
03/17/2022 15:50:15 - INFO - __main__ - Global step 1900 Train loss 0.21 ACC 0.5625 on epoch=633
03/17/2022 15:50:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=636
03/17/2022 15:50:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=639
03/17/2022 15:50:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=643
03/17/2022 15:50:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=646
03/17/2022 15:50:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=649
03/17/2022 15:50:31 - INFO - __main__ - Global step 1950 Train loss 0.23 ACC 0.46875 on epoch=649
03/17/2022 15:50:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=653
03/17/2022 15:50:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=656
03/17/2022 15:50:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=659
03/17/2022 15:50:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=663
03/17/2022 15:50:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=666
03/17/2022 15:50:46 - INFO - __main__ - Global step 2000 Train loss 0.20 ACC 0.5625 on epoch=666
03/17/2022 15:50:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=669
03/17/2022 15:50:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=673
03/17/2022 15:50:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=676
03/17/2022 15:50:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=679
03/17/2022 15:51:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=683
03/17/2022 15:51:02 - INFO - __main__ - Global step 2050 Train loss 0.22 ACC 0.40625 on epoch=683
03/17/2022 15:51:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=686
03/17/2022 15:51:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=689
03/17/2022 15:51:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=693
03/17/2022 15:51:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=696
03/17/2022 15:51:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=699
03/17/2022 15:51:17 - INFO - __main__ - Global step 2100 Train loss 0.18 ACC 0.5 on epoch=699
03/17/2022 15:51:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=703
03/17/2022 15:51:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=706
03/17/2022 15:51:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=709
03/17/2022 15:51:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=713
03/17/2022 15:51:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=716
03/17/2022 15:51:33 - INFO - __main__ - Global step 2150 Train loss 0.20 ACC 0.4375 on epoch=716
03/17/2022 15:51:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=719
03/17/2022 15:51:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=723
03/17/2022 15:51:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=726
03/17/2022 15:51:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=729
03/17/2022 15:51:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=733
03/17/2022 15:51:49 - INFO - __main__ - Global step 2200 Train loss 0.18 ACC 0.5625 on epoch=733
03/17/2022 15:51:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=736
03/17/2022 15:51:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=739
03/17/2022 15:51:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=743
03/17/2022 15:52:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=746
03/17/2022 15:52:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=749
03/17/2022 15:52:05 - INFO - __main__ - Global step 2250 Train loss 0.17 ACC 0.53125 on epoch=749
03/17/2022 15:52:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=753
03/17/2022 15:52:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=756
03/17/2022 15:52:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=759
03/17/2022 15:52:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=763
03/17/2022 15:52:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=766
03/17/2022 15:52:21 - INFO - __main__ - Global step 2300 Train loss 0.18 ACC 0.65625 on epoch=766
03/17/2022 15:52:21 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.65625 on epoch=766, global_step=2300
03/17/2022 15:52:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=769
03/17/2022 15:52:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=773
03/17/2022 15:52:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=776
03/17/2022 15:52:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=779
03/17/2022 15:52:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=783
03/17/2022 15:52:37 - INFO - __main__ - Global step 2350 Train loss 0.18 ACC 0.375 on epoch=783
03/17/2022 15:52:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=786
03/17/2022 15:52:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=789
03/17/2022 15:52:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=793
03/17/2022 15:52:49 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=796
03/17/2022 15:52:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.16 on epoch=799
03/17/2022 15:52:54 - INFO - __main__ - Global step 2400 Train loss 0.15 ACC 0.5 on epoch=799
03/17/2022 15:52:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=803
03/17/2022 15:52:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=806
03/17/2022 15:53:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=809
03/17/2022 15:53:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=813
03/17/2022 15:53:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=816
03/17/2022 15:53:10 - INFO - __main__ - Global step 2450 Train loss 0.14 ACC 0.46875 on epoch=816
03/17/2022 15:53:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=819
03/17/2022 15:53:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=823
03/17/2022 15:53:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=826
03/17/2022 15:53:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=829
03/17/2022 15:53:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=833
03/17/2022 15:53:26 - INFO - __main__ - Global step 2500 Train loss 0.14 ACC 0.40625 on epoch=833
03/17/2022 15:53:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=836
03/17/2022 15:53:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=839
03/17/2022 15:53:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=843
03/17/2022 15:53:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=846
03/17/2022 15:53:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=849
03/17/2022 15:53:42 - INFO - __main__ - Global step 2550 Train loss 0.13 ACC 0.53125 on epoch=849
03/17/2022 15:53:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=853
03/17/2022 15:53:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=856
03/17/2022 15:53:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=859
03/17/2022 15:53:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=863
03/17/2022 15:53:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=866
03/17/2022 15:53:58 - INFO - __main__ - Global step 2600 Train loss 0.11 ACC 0.53125 on epoch=866
03/17/2022 15:54:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=869
03/17/2022 15:54:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=873
03/17/2022 15:54:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=876
03/17/2022 15:54:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=879
03/17/2022 15:54:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=883
03/17/2022 15:54:14 - INFO - __main__ - Global step 2650 Train loss 0.12 ACC 0.5 on epoch=883
03/17/2022 15:54:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=886
03/17/2022 15:54:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=889
03/17/2022 15:54:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=893
03/17/2022 15:54:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=896
03/17/2022 15:54:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=899
03/17/2022 15:54:30 - INFO - __main__ - Global step 2700 Train loss 0.10 ACC 0.34375 on epoch=899
03/17/2022 15:54:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=903
03/17/2022 15:54:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=906
03/17/2022 15:54:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=909
03/17/2022 15:54:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=913
03/17/2022 15:54:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=916
03/17/2022 15:54:46 - INFO - __main__ - Global step 2750 Train loss 0.08 ACC 0.5 on epoch=916
03/17/2022 15:54:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=919
03/17/2022 15:54:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=923
03/17/2022 15:54:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=926
03/17/2022 15:54:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=929
03/17/2022 15:55:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=933
03/17/2022 15:55:03 - INFO - __main__ - Global step 2800 Train loss 0.07 ACC 0.53125 on epoch=933
03/17/2022 15:55:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=936
03/17/2022 15:55:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
03/17/2022 15:55:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=943
03/17/2022 15:55:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
03/17/2022 15:55:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=949
03/17/2022 15:55:19 - INFO - __main__ - Global step 2850 Train loss 0.07 ACC 0.46875 on epoch=949
03/17/2022 15:55:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=953
03/17/2022 15:55:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=956
03/17/2022 15:55:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.37 on epoch=959
03/17/2022 15:55:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.83 on epoch=963
03/17/2022 15:55:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=966
03/17/2022 15:55:35 - INFO - __main__ - Global step 2900 Train loss 0.35 ACC 0.40625 on epoch=966
03/17/2022 15:55:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=969
03/17/2022 15:55:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=973
03/17/2022 15:55:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=976
03/17/2022 15:55:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=979
03/17/2022 15:55:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=983
03/17/2022 15:55:51 - INFO - __main__ - Global step 2950 Train loss 0.10 ACC 0.5 on epoch=983
03/17/2022 15:55:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=986
03/17/2022 15:55:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.30 on epoch=989
03/17/2022 15:56:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=993
03/17/2022 15:56:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=996
03/17/2022 15:56:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.29 on epoch=999
03/17/2022 15:56:07 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:56:07 - INFO - __main__ - Printing 3 examples
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:56:07 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:56:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:56:07 - INFO - __main__ - Printing 3 examples
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:56:07 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:56:07 - INFO - __main__ - Global step 3000 Train loss 0.20 ACC 0.46875 on epoch=999
03/17/2022 15:56:07 - INFO - __main__ - save last model!
03/17/2022 15:56:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 15:56:07 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 15:56:07 - INFO - __main__ - Printing 3 examples
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 15:56:07 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 15:56:07 - INFO - __main__ - ['neutral']
03/17/2022 15:56:07 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 15:56:07 - INFO - __main__ - ['entailment']
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:56:07 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:56:07 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 15:56:09 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_100_0.3_8_predictions.txt
03/17/2022 15:56:09 - INFO - __main__ - ACC on test data: 0.6071
03/17/2022 15:56:10 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.3, bsz=8, dev_performance=0.65625, test_performance=0.6071428571428571
03/17/2022 15:56:10 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.2, bsz=8 ...
03/17/2022 15:56:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 15:56:10 - INFO - __main__ - Printing 3 examples
03/17/2022 15:56:10 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 15:56:10 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:10 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
03/17/2022 15:56:10 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:10 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
03/17/2022 15:56:10 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 15:56:10 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:56:11 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 15:56:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 15:56:11 - INFO - __main__ - Printing 3 examples
03/17/2022 15:56:11 - INFO - __main__ -  [superglue-cb] premise: A: I do too. I believe about ten years ago that we went through a terrible time, but I don't, I believe that they're better now, you know, wh-, B: I think so. I don't think they're shoddy [SEP] hypothesis: they're shoddy
03/17/2022 15:56:11 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:11 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 15:56:11 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:11 - INFO - __main__ -  [superglue-cb] premise: B: All right, well. A: Um, short term, I don't think anything's going to be done about it or probably should be done about it. [SEP] hypothesis: something's going to be done about it
03/17/2022 15:56:11 - INFO - __main__ - ['contradiction']
03/17/2022 15:56:11 - INFO - __main__ - Tokenizing Input ...
03/17/2022 15:56:11 - INFO - __main__ - Tokenizing Output ...
03/17/2022 15:56:11 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 15:56:22 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:56:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:56:23 - INFO - __main__ - Starting training!
03/17/2022 15:56:25 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 15:56:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 15:56:26 - INFO - __main__ - Starting training!
03/17/2022 15:56:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.38 on epoch=3
03/17/2022 15:56:33 - INFO - __main__ - Step 20 Global step 20 Train loss 4.12 on epoch=6
03/17/2022 15:56:36 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=9
03/17/2022 15:56:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.56 on epoch=13
03/17/2022 15:56:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.12 on epoch=16
03/17/2022 15:56:42 - INFO - __main__ - Global step 50 Train loss 3.13 ACC 0.5 on epoch=16
03/17/2022 15:56:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 15:56:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=19
03/17/2022 15:56:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.72 on epoch=23
03/17/2022 15:56:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.72 on epoch=26
03/17/2022 15:56:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=29
03/17/2022 15:56:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=33
03/17/2022 15:56:57 - INFO - __main__ - Global step 100 Train loss 0.72 ACC 0.5 on epoch=33
03/17/2022 15:57:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
03/17/2022 15:57:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=39
03/17/2022 15:57:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=43
03/17/2022 15:57:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=46
03/17/2022 15:57:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
03/17/2022 15:57:13 - INFO - __main__ - Global step 150 Train loss 0.57 ACC 0.5 on epoch=49
03/17/2022 15:57:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=53
03/17/2022 15:57:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=56
03/17/2022 15:57:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=59
03/17/2022 15:57:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
03/17/2022 15:57:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
03/17/2022 15:57:28 - INFO - __main__ - Global step 200 Train loss 0.53 ACC 0.46875 on epoch=66
03/17/2022 15:57:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=69
03/17/2022 15:57:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
03/17/2022 15:57:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
03/17/2022 15:57:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
03/17/2022 15:57:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=83
03/17/2022 15:57:43 - INFO - __main__ - Global step 250 Train loss 0.51 ACC 0.5 on epoch=83
03/17/2022 15:57:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
03/17/2022 15:57:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=89
03/17/2022 15:57:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
03/17/2022 15:57:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=96
03/17/2022 15:57:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
03/17/2022 15:57:58 - INFO - __main__ - Global step 300 Train loss 0.49 ACC 0.5 on epoch=99
03/17/2022 15:58:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
03/17/2022 15:58:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
03/17/2022 15:58:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=109
03/17/2022 15:58:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
03/17/2022 15:58:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
03/17/2022 15:58:14 - INFO - __main__ - Global step 350 Train loss 0.47 ACC 0.5 on epoch=116
03/17/2022 15:58:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
03/17/2022 15:58:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
03/17/2022 15:58:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
03/17/2022 15:58:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
03/17/2022 15:58:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
03/17/2022 15:58:29 - INFO - __main__ - Global step 400 Train loss 0.43 ACC 0.03125 on epoch=133
03/17/2022 15:58:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
03/17/2022 15:58:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=139
03/17/2022 15:58:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=143
03/17/2022 15:58:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=146
03/17/2022 15:58:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=149
03/17/2022 15:58:44 - INFO - __main__ - Global step 450 Train loss 0.46 ACC 0.5625 on epoch=149
03/17/2022 15:58:44 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=149, global_step=450
03/17/2022 15:58:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
03/17/2022 15:58:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
03/17/2022 15:58:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=159
03/17/2022 15:58:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=163
03/17/2022 15:58:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
03/17/2022 15:58:59 - INFO - __main__ - Global step 500 Train loss 0.43 ACC 0.53125 on epoch=166
03/17/2022 15:59:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
03/17/2022 15:59:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
03/17/2022 15:59:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
03/17/2022 15:59:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=179
03/17/2022 15:59:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=183
03/17/2022 15:59:14 - INFO - __main__ - Global step 550 Train loss 0.40 ACC 0.0 on epoch=183
03/17/2022 15:59:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=186
03/17/2022 15:59:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=189
03/17/2022 15:59:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=193
03/17/2022 15:59:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=196
03/17/2022 15:59:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=199
03/17/2022 15:59:29 - INFO - __main__ - Global step 600 Train loss 0.39 ACC 0.0625 on epoch=199
03/17/2022 15:59:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=203
03/17/2022 15:59:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=206
03/17/2022 15:59:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=209
03/17/2022 15:59:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=213
03/17/2022 15:59:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=216
03/17/2022 15:59:44 - INFO - __main__ - Global step 650 Train loss 0.35 ACC 0.59375 on epoch=216
03/17/2022 15:59:44 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=216, global_step=650
03/17/2022 15:59:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=219
03/17/2022 15:59:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
03/17/2022 15:59:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=226
03/17/2022 15:59:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=229
03/17/2022 15:59:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=233
03/17/2022 16:00:00 - INFO - __main__ - Global step 700 Train loss 0.33 ACC 0.4375 on epoch=233
03/17/2022 16:00:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=236
03/17/2022 16:00:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=239
03/17/2022 16:00:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=243
03/17/2022 16:00:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=246
03/17/2022 16:00:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=249
03/17/2022 16:00:15 - INFO - __main__ - Global step 750 Train loss 0.31 ACC 0.15625 on epoch=249
03/17/2022 16:00:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=253
03/17/2022 16:00:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=256
03/17/2022 16:00:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=259
03/17/2022 16:00:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=263
03/17/2022 16:00:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=266
03/17/2022 16:00:30 - INFO - __main__ - Global step 800 Train loss 0.28 ACC 0.4375 on epoch=266
03/17/2022 16:00:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=269
03/17/2022 16:00:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
03/17/2022 16:00:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=276
03/17/2022 16:00:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=279
03/17/2022 16:00:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=283
03/17/2022 16:00:45 - INFO - __main__ - Global step 850 Train loss 0.28 ACC 0.46875 on epoch=283
03/17/2022 16:00:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=286
03/17/2022 16:00:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=289
03/17/2022 16:00:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=293
03/17/2022 16:00:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=296
03/17/2022 16:01:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=299
03/17/2022 16:01:01 - INFO - __main__ - Global step 900 Train loss 0.26 ACC 0.5 on epoch=299
03/17/2022 16:01:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
03/17/2022 16:01:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=306
03/17/2022 16:01:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=309
03/17/2022 16:01:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=313
03/17/2022 16:01:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=316
03/17/2022 16:01:16 - INFO - __main__ - Global step 950 Train loss 0.23 ACC 0.46875 on epoch=316
03/17/2022 16:01:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=319
03/17/2022 16:01:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=323
03/17/2022 16:01:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=326
03/17/2022 16:01:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=329
03/17/2022 16:01:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=333
03/17/2022 16:01:32 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.15625 on epoch=333
03/17/2022 16:01:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=336
03/17/2022 16:01:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=339
03/17/2022 16:01:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=343
03/17/2022 16:01:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=346
03/17/2022 16:01:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=349
03/17/2022 16:01:47 - INFO - __main__ - Global step 1050 Train loss 0.24 ACC 0.28125 on epoch=349
03/17/2022 16:01:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=353
03/17/2022 16:01:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=356
03/17/2022 16:01:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=359
03/17/2022 16:01:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=363
03/17/2022 16:02:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=366
03/17/2022 16:02:03 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.375 on epoch=366
03/17/2022 16:02:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=369
03/17/2022 16:02:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=373
03/17/2022 16:02:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=376
03/17/2022 16:02:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=379
03/17/2022 16:02:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=383
03/17/2022 16:02:18 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.4375 on epoch=383
03/17/2022 16:02:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=386
03/17/2022 16:02:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=389
03/17/2022 16:02:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=393
03/17/2022 16:02:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=396
03/17/2022 16:02:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=399
03/17/2022 16:02:33 - INFO - __main__ - Global step 1200 Train loss 0.20 ACC 0.21875 on epoch=399
03/17/2022 16:02:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=403
03/17/2022 16:02:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=406
03/17/2022 16:02:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=409
03/17/2022 16:02:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=413
03/17/2022 16:02:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=416
03/17/2022 16:02:49 - INFO - __main__ - Global step 1250 Train loss 0.21 ACC 0.3125 on epoch=416
03/17/2022 16:02:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=419
03/17/2022 16:02:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=423
03/17/2022 16:02:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=426
03/17/2022 16:03:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=429
03/17/2022 16:03:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=433
03/17/2022 16:03:05 - INFO - __main__ - Global step 1300 Train loss 0.20 ACC 0.46875 on epoch=433
03/17/2022 16:03:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=436
03/17/2022 16:03:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=439
03/17/2022 16:03:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=443
03/17/2022 16:03:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=446
03/17/2022 16:03:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=449
03/17/2022 16:03:20 - INFO - __main__ - Global step 1350 Train loss 0.17 ACC 0.5625 on epoch=449
03/17/2022 16:03:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=453
03/17/2022 16:03:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=456
03/17/2022 16:03:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=459
03/17/2022 16:03:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=463
03/17/2022 16:03:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=466
03/17/2022 16:03:36 - INFO - __main__ - Global step 1400 Train loss 0.19 ACC 0.53125 on epoch=466
03/17/2022 16:03:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=469
03/17/2022 16:03:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=473
03/17/2022 16:03:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=476
03/17/2022 16:03:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=479
03/17/2022 16:03:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=483
03/17/2022 16:03:51 - INFO - __main__ - Global step 1450 Train loss 0.17 ACC 0.4375 on epoch=483
03/17/2022 16:03:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=486
03/17/2022 16:03:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=489
03/17/2022 16:04:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=493
03/17/2022 16:04:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=496
03/17/2022 16:04:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=499
03/17/2022 16:04:07 - INFO - __main__ - Global step 1500 Train loss 0.17 ACC 0.34375 on epoch=499
03/17/2022 16:04:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=503
03/17/2022 16:04:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=506
03/17/2022 16:04:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=509
03/17/2022 16:04:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=513
03/17/2022 16:04:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=516
03/17/2022 16:04:23 - INFO - __main__ - Global step 1550 Train loss 0.15 ACC 0.53125 on epoch=516
03/17/2022 16:04:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=519
03/17/2022 16:04:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=523
03/17/2022 16:04:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=526
03/17/2022 16:04:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=529
03/17/2022 16:04:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=533
03/17/2022 16:04:39 - INFO - __main__ - Global step 1600 Train loss 0.17 ACC 0.46875 on epoch=533
03/17/2022 16:04:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=536
03/17/2022 16:04:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=539
03/17/2022 16:04:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=543
03/17/2022 16:04:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=546
03/17/2022 16:04:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=549
03/17/2022 16:04:54 - INFO - __main__ - Global step 1650 Train loss 0.14 ACC 0.5 on epoch=549
03/17/2022 16:04:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=553
03/17/2022 16:05:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=556
03/17/2022 16:05:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=559
03/17/2022 16:05:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=563
03/17/2022 16:05:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=566
03/17/2022 16:05:10 - INFO - __main__ - Global step 1700 Train loss 0.11 ACC 0.46875 on epoch=566
03/17/2022 16:05:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=569
03/17/2022 16:05:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=573
03/17/2022 16:05:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=576
03/17/2022 16:05:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=579
03/17/2022 16:05:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=583
03/17/2022 16:05:25 - INFO - __main__ - Global step 1750 Train loss 0.13 ACC 0.5625 on epoch=583
03/17/2022 16:05:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=586
03/17/2022 16:05:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=589
03/17/2022 16:05:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=593
03/17/2022 16:05:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=596
03/17/2022 16:05:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=599
03/17/2022 16:05:40 - INFO - __main__ - Global step 1800 Train loss 0.15 ACC 0.53125 on epoch=599
03/17/2022 16:05:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=603
03/17/2022 16:05:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=606
03/17/2022 16:05:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=609
03/17/2022 16:05:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=613
03/17/2022 16:05:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=616
03/17/2022 16:05:56 - INFO - __main__ - Global step 1850 Train loss 0.13 ACC 0.53125 on epoch=616
03/17/2022 16:05:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=619
03/17/2022 16:06:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=623
03/17/2022 16:06:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=626
03/17/2022 16:06:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=629
03/17/2022 16:06:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=633
03/17/2022 16:06:12 - INFO - __main__ - Global step 1900 Train loss 0.10 ACC 0.53125 on epoch=633
03/17/2022 16:06:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=636
03/17/2022 16:06:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=639
03/17/2022 16:06:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=643
03/17/2022 16:06:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=646
03/17/2022 16:06:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=649
03/17/2022 16:06:27 - INFO - __main__ - Global step 1950 Train loss 0.10 ACC 0.46875 on epoch=649
03/17/2022 16:06:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=653
03/17/2022 16:06:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=656
03/17/2022 16:06:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=659
03/17/2022 16:06:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=663
03/17/2022 16:06:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=666
03/17/2022 16:06:43 - INFO - __main__ - Global step 2000 Train loss 0.09 ACC 0.46875 on epoch=666
03/17/2022 16:06:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=669
03/17/2022 16:06:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=673
03/17/2022 16:06:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=676
03/17/2022 16:06:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=679
03/17/2022 16:06:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=683
03/17/2022 16:06:58 - INFO - __main__ - Global step 2050 Train loss 0.08 ACC 0.4375 on epoch=683
03/17/2022 16:07:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=686
03/17/2022 16:07:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
03/17/2022 16:07:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=693
03/17/2022 16:07:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=696
03/17/2022 16:07:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=699
03/17/2022 16:07:14 - INFO - __main__ - Global step 2100 Train loss 0.08 ACC 0.53125 on epoch=699
03/17/2022 16:07:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=703
03/17/2022 16:07:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=706
03/17/2022 16:07:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
03/17/2022 16:07:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=713
03/17/2022 16:07:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=716
03/17/2022 16:07:30 - INFO - __main__ - Global step 2150 Train loss 0.07 ACC 0.40625 on epoch=716
03/17/2022 16:07:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
03/17/2022 16:07:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
03/17/2022 16:07:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=726
03/17/2022 16:07:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=729
03/17/2022 16:07:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
03/17/2022 16:07:46 - INFO - __main__ - Global step 2200 Train loss 0.05 ACC 0.375 on epoch=733
03/17/2022 16:07:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=736
03/17/2022 16:07:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=739
03/17/2022 16:07:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
03/17/2022 16:07:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=746
03/17/2022 16:08:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=749
03/17/2022 16:08:02 - INFO - __main__ - Global step 2250 Train loss 0.08 ACC 0.5 on epoch=749
03/17/2022 16:08:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=753
03/17/2022 16:08:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=756
03/17/2022 16:08:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=759
03/17/2022 16:08:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/17/2022 16:08:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
03/17/2022 16:08:17 - INFO - __main__ - Global step 2300 Train loss 0.05 ACC 0.5625 on epoch=766
03/17/2022 16:08:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=769
03/17/2022 16:08:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=773
03/17/2022 16:08:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=776
03/17/2022 16:08:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
03/17/2022 16:08:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=783
03/17/2022 16:08:33 - INFO - __main__ - Global step 2350 Train loss 0.05 ACC 0.46875 on epoch=783
03/17/2022 16:08:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=786
03/17/2022 16:08:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
03/17/2022 16:08:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=793
03/17/2022 16:08:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=796
03/17/2022 16:08:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=799
03/17/2022 16:08:49 - INFO - __main__ - Global step 2400 Train loss 0.06 ACC 0.5625 on epoch=799
03/17/2022 16:08:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=803
03/17/2022 16:08:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=806
03/17/2022 16:08:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=809
03/17/2022 16:09:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
03/17/2022 16:09:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
03/17/2022 16:09:05 - INFO - __main__ - Global step 2450 Train loss 0.05 ACC 0.46875 on epoch=816
03/17/2022 16:09:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
03/17/2022 16:09:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
03/17/2022 16:09:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=826
03/17/2022 16:09:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=829
03/17/2022 16:09:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=833
03/17/2022 16:09:20 - INFO - __main__ - Global step 2500 Train loss 0.05 ACC 0.59375 on epoch=833
03/17/2022 16:09:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
03/17/2022 16:09:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=839
03/17/2022 16:09:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
03/17/2022 16:09:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=846
03/17/2022 16:09:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=849
03/17/2022 16:09:36 - INFO - __main__ - Global step 2550 Train loss 0.05 ACC 0.4375 on epoch=849
03/17/2022 16:09:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
03/17/2022 16:09:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/17/2022 16:09:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
03/17/2022 16:09:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
03/17/2022 16:09:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=866
03/17/2022 16:09:51 - INFO - __main__ - Global step 2600 Train loss 0.03 ACC 0.46875 on epoch=866
03/17/2022 16:09:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=869
03/17/2022 16:09:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
03/17/2022 16:10:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=876
03/17/2022 16:10:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
03/17/2022 16:10:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
03/17/2022 16:10:07 - INFO - __main__ - Global step 2650 Train loss 0.04 ACC 0.5 on epoch=883
03/17/2022 16:10:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
03/17/2022 16:10:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
03/17/2022 16:10:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/17/2022 16:10:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
03/17/2022 16:10:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
03/17/2022 16:10:23 - INFO - __main__ - Global step 2700 Train loss 0.03 ACC 0.625 on epoch=899
03/17/2022 16:10:23 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=899, global_step=2700
03/17/2022 16:10:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/17/2022 16:10:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=906
03/17/2022 16:10:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
03/17/2022 16:10:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
03/17/2022 16:10:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/17/2022 16:10:38 - INFO - __main__ - Global step 2750 Train loss 0.02 ACC 0.46875 on epoch=916
03/17/2022 16:10:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/17/2022 16:10:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
03/17/2022 16:10:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
03/17/2022 16:10:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
03/17/2022 16:10:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
03/17/2022 16:10:54 - INFO - __main__ - Global step 2800 Train loss 0.01 ACC 0.5625 on epoch=933
03/17/2022 16:10:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=936
03/17/2022 16:11:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
03/17/2022 16:11:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/17/2022 16:11:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
03/17/2022 16:11:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
03/17/2022 16:11:11 - INFO - __main__ - Global step 2850 Train loss 0.03 ACC 0.6875 on epoch=949
03/17/2022 16:11:11 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=949, global_step=2850
03/17/2022 16:11:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=953
03/17/2022 16:11:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
03/17/2022 16:11:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=959
03/17/2022 16:11:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=963
03/17/2022 16:11:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=966
03/17/2022 16:11:27 - INFO - __main__ - Global step 2900 Train loss 0.05 ACC 0.625 on epoch=966
03/17/2022 16:11:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
03/17/2022 16:11:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
03/17/2022 16:11:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
03/17/2022 16:11:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
03/17/2022 16:11:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
03/17/2022 16:11:42 - INFO - __main__ - Global step 2950 Train loss 0.02 ACC 0.65625 on epoch=983
03/17/2022 16:11:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=986
03/17/2022 16:11:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=989
03/17/2022 16:11:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=993
03/17/2022 16:11:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/17/2022 16:11:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
03/17/2022 16:11:58 - INFO - __main__ - Global step 3000 Train loss 0.05 ACC 0.53125 on epoch=999
03/17/2022 16:11:58 - INFO - __main__ - save last model!
03/17/2022 16:11:58 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:11:58 - INFO - __main__ - Printing 3 examples
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 16:11:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 16:11:58 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 16:11:58 - INFO - __main__ - Printing 3 examples
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 16:11:58 - INFO - __main__ - ['neutral']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 16:11:58 - INFO - __main__ - ['entailment']
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:11:58 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:11:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:11:58 - INFO - __main__ - Printing 3 examples
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:11:58 - INFO - __main__ - ['contradiction']
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:11:58 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:11:58 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 16:11:58 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:12:00 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_100_0.2_8_predictions.txt
03/17/2022 16:12:00 - INFO - __main__ - ACC on test data: 0.6250
03/17/2022 16:12:01 - INFO - __main__ - prefix=superglue-cb_16_100, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.625
03/17/2022 16:12:01 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.5, bsz=8 ...
03/17/2022 16:12:02 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:12:02 - INFO - __main__ - Printing 3 examples
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 16:12:02 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:12:02 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:12:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:12:02 - INFO - __main__ - Printing 3 examples
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:12:02 - INFO - __main__ - ['contradiction']
03/17/2022 16:12:02 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:12:02 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:12:02 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:12:17 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:12:17 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:12:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:12:17 - INFO - __main__ - Starting training!
03/17/2022 16:12:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:12:18 - INFO - __main__ - Starting training!
03/17/2022 16:12:24 - INFO - __main__ - Step 10 Global step 10 Train loss 6.81 on epoch=3
03/17/2022 16:12:27 - INFO - __main__ - Step 20 Global step 20 Train loss 6.98 on epoch=6
03/17/2022 16:12:30 - INFO - __main__ - Step 30 Global step 30 Train loss 6.72 on epoch=9
03/17/2022 16:12:32 - INFO - __main__ - Step 40 Global step 40 Train loss 7.16 on epoch=13
03/17/2022 16:12:35 - INFO - __main__ - Step 50 Global step 50 Train loss 7.41 on epoch=16
03/17/2022 16:12:54 - INFO - __main__ - Global step 50 Train loss 7.01 ACC 0.0 on epoch=16
03/17/2022 16:12:54 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 16:12:57 - INFO - __main__ - Step 60 Global step 60 Train loss 7.30 on epoch=19
03/17/2022 16:13:00 - INFO - __main__ - Step 70 Global step 70 Train loss 7.61 on epoch=23
03/17/2022 16:13:03 - INFO - __main__ - Step 80 Global step 80 Train loss 7.65 on epoch=26
03/17/2022 16:13:05 - INFO - __main__ - Step 90 Global step 90 Train loss 7.67 on epoch=29
03/17/2022 16:13:08 - INFO - __main__ - Step 100 Global step 100 Train loss 7.69 on epoch=33
03/17/2022 16:13:21 - INFO - __main__ - Global step 100 Train loss 7.58 ACC 0.0 on epoch=33
03/17/2022 16:13:24 - INFO - __main__ - Step 110 Global step 110 Train loss 7.27 on epoch=36
03/17/2022 16:13:27 - INFO - __main__ - Step 120 Global step 120 Train loss 7.22 on epoch=39
03/17/2022 16:13:30 - INFO - __main__ - Step 130 Global step 130 Train loss 7.09 on epoch=43
03/17/2022 16:13:32 - INFO - __main__ - Step 140 Global step 140 Train loss 6.96 on epoch=46
03/17/2022 16:13:35 - INFO - __main__ - Step 150 Global step 150 Train loss 6.76 on epoch=49
03/17/2022 16:13:55 - INFO - __main__ - Global step 150 Train loss 7.06 ACC 0.0 on epoch=49
03/17/2022 16:13:57 - INFO - __main__ - Step 160 Global step 160 Train loss 6.39 on epoch=53
03/17/2022 16:14:00 - INFO - __main__ - Step 170 Global step 170 Train loss 6.23 on epoch=56
03/17/2022 16:14:03 - INFO - __main__ - Step 180 Global step 180 Train loss 6.22 on epoch=59
03/17/2022 16:14:05 - INFO - __main__ - Step 190 Global step 190 Train loss 6.36 on epoch=63
03/17/2022 16:14:08 - INFO - __main__ - Step 200 Global step 200 Train loss 6.25 on epoch=66
03/17/2022 16:14:23 - INFO - __main__ - Global step 200 Train loss 6.29 ACC 0.0 on epoch=66
03/17/2022 16:14:25 - INFO - __main__ - Step 210 Global step 210 Train loss 6.27 on epoch=69
03/17/2022 16:14:28 - INFO - __main__ - Step 220 Global step 220 Train loss 6.26 on epoch=73
03/17/2022 16:14:31 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=76
03/17/2022 16:14:34 - INFO - __main__ - Step 240 Global step 240 Train loss 6.17 on epoch=79
03/17/2022 16:14:36 - INFO - __main__ - Step 250 Global step 250 Train loss 6.24 on epoch=83
03/17/2022 16:14:54 - INFO - __main__ - Global step 250 Train loss 6.23 ACC 0.0 on epoch=83
03/17/2022 16:14:57 - INFO - __main__ - Step 260 Global step 260 Train loss 6.11 on epoch=86
03/17/2022 16:15:00 - INFO - __main__ - Step 270 Global step 270 Train loss 5.96 on epoch=89
03/17/2022 16:15:02 - INFO - __main__ - Step 280 Global step 280 Train loss 6.01 on epoch=93
03/17/2022 16:15:05 - INFO - __main__ - Step 290 Global step 290 Train loss 5.90 on epoch=96
03/17/2022 16:15:08 - INFO - __main__ - Step 300 Global step 300 Train loss 5.87 on epoch=99
03/17/2022 16:15:27 - INFO - __main__ - Global step 300 Train loss 5.97 ACC 0.0 on epoch=99
03/17/2022 16:15:29 - INFO - __main__ - Step 310 Global step 310 Train loss 5.92 on epoch=103
03/17/2022 16:15:32 - INFO - __main__ - Step 320 Global step 320 Train loss 5.75 on epoch=106
03/17/2022 16:15:35 - INFO - __main__ - Step 330 Global step 330 Train loss 5.75 on epoch=109
03/17/2022 16:15:38 - INFO - __main__ - Step 340 Global step 340 Train loss 5.71 on epoch=113
03/17/2022 16:15:40 - INFO - __main__ - Step 350 Global step 350 Train loss 5.59 on epoch=116
03/17/2022 16:15:59 - INFO - __main__ - Global step 350 Train loss 5.75 ACC 0.0 on epoch=116
03/17/2022 16:16:02 - INFO - __main__ - Step 360 Global step 360 Train loss 5.45 on epoch=119
03/17/2022 16:16:05 - INFO - __main__ - Step 370 Global step 370 Train loss 5.59 on epoch=123
03/17/2022 16:16:07 - INFO - __main__ - Step 380 Global step 380 Train loss 5.47 on epoch=126
03/17/2022 16:16:10 - INFO - __main__ - Step 390 Global step 390 Train loss 5.44 on epoch=129
03/17/2022 16:16:13 - INFO - __main__ - Step 400 Global step 400 Train loss 5.32 on epoch=133
03/17/2022 16:16:18 - INFO - __main__ - Global step 400 Train loss 5.45 ACC 0.0 on epoch=133
03/17/2022 16:16:21 - INFO - __main__ - Step 410 Global step 410 Train loss 5.18 on epoch=136
03/17/2022 16:16:24 - INFO - __main__ - Step 420 Global step 420 Train loss 5.05 on epoch=139
03/17/2022 16:16:26 - INFO - __main__ - Step 430 Global step 430 Train loss 5.03 on epoch=143
03/17/2022 16:16:29 - INFO - __main__ - Step 440 Global step 440 Train loss 5.00 on epoch=146
03/17/2022 16:16:32 - INFO - __main__ - Step 450 Global step 450 Train loss 4.92 on epoch=149
03/17/2022 16:16:44 - INFO - __main__ - Global step 450 Train loss 5.03 ACC 0.0 on epoch=149
03/17/2022 16:16:46 - INFO - __main__ - Step 460 Global step 460 Train loss 4.95 on epoch=153
03/17/2022 16:16:49 - INFO - __main__ - Step 470 Global step 470 Train loss 4.69 on epoch=156
03/17/2022 16:16:52 - INFO - __main__ - Step 480 Global step 480 Train loss 4.57 on epoch=159
03/17/2022 16:16:55 - INFO - __main__ - Step 490 Global step 490 Train loss 4.49 on epoch=163
03/17/2022 16:16:57 - INFO - __main__ - Step 500 Global step 500 Train loss 4.69 on epoch=166
03/17/2022 16:17:01 - INFO - __main__ - Global step 500 Train loss 4.68 ACC 0.0 on epoch=166
03/17/2022 16:17:04 - INFO - __main__ - Step 510 Global step 510 Train loss 4.67 on epoch=169
03/17/2022 16:17:07 - INFO - __main__ - Step 520 Global step 520 Train loss 4.40 on epoch=173
03/17/2022 16:17:09 - INFO - __main__ - Step 530 Global step 530 Train loss 4.15 on epoch=176
03/17/2022 16:17:12 - INFO - __main__ - Step 540 Global step 540 Train loss 4.10 on epoch=179
03/17/2022 16:17:15 - INFO - __main__ - Step 550 Global step 550 Train loss 4.03 on epoch=183
03/17/2022 16:17:17 - INFO - __main__ - Global step 550 Train loss 4.27 ACC 0.0625 on epoch=183
03/17/2022 16:17:17 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=183, global_step=550
03/17/2022 16:17:20 - INFO - __main__ - Step 560 Global step 560 Train loss 3.89 on epoch=186
03/17/2022 16:17:22 - INFO - __main__ - Step 570 Global step 570 Train loss 3.65 on epoch=189
03/17/2022 16:17:25 - INFO - __main__ - Step 580 Global step 580 Train loss 3.33 on epoch=193
03/17/2022 16:17:28 - INFO - __main__ - Step 590 Global step 590 Train loss 3.31 on epoch=196
03/17/2022 16:17:31 - INFO - __main__ - Step 600 Global step 600 Train loss 3.10 on epoch=199
03/17/2022 16:17:32 - INFO - __main__ - Global step 600 Train loss 3.46 ACC 0.0 on epoch=199
03/17/2022 16:17:35 - INFO - __main__ - Step 610 Global step 610 Train loss 2.93 on epoch=203
03/17/2022 16:17:37 - INFO - __main__ - Step 620 Global step 620 Train loss 2.76 on epoch=206
03/17/2022 16:17:40 - INFO - __main__ - Step 630 Global step 630 Train loss 2.68 on epoch=209
03/17/2022 16:17:43 - INFO - __main__ - Step 640 Global step 640 Train loss 2.72 on epoch=213
03/17/2022 16:17:46 - INFO - __main__ - Step 650 Global step 650 Train loss 2.36 on epoch=216
03/17/2022 16:17:47 - INFO - __main__ - Global step 650 Train loss 2.69 ACC 0.0 on epoch=216
03/17/2022 16:17:50 - INFO - __main__ - Step 660 Global step 660 Train loss 2.49 on epoch=219
03/17/2022 16:17:52 - INFO - __main__ - Step 670 Global step 670 Train loss 2.35 on epoch=223
03/17/2022 16:17:55 - INFO - __main__ - Step 680 Global step 680 Train loss 2.29 on epoch=226
03/17/2022 16:17:58 - INFO - __main__ - Step 690 Global step 690 Train loss 2.15 on epoch=229
03/17/2022 16:18:00 - INFO - __main__ - Step 700 Global step 700 Train loss 2.13 on epoch=233
03/17/2022 16:18:02 - INFO - __main__ - Global step 700 Train loss 2.28 ACC 0.0625 on epoch=233
03/17/2022 16:18:04 - INFO - __main__ - Step 710 Global step 710 Train loss 2.12 on epoch=236
03/17/2022 16:18:07 - INFO - __main__ - Step 720 Global step 720 Train loss 2.01 on epoch=239
03/17/2022 16:18:10 - INFO - __main__ - Step 730 Global step 730 Train loss 2.00 on epoch=243
03/17/2022 16:18:12 - INFO - __main__ - Step 740 Global step 740 Train loss 2.00 on epoch=246
03/17/2022 16:18:15 - INFO - __main__ - Step 750 Global step 750 Train loss 1.86 on epoch=249
03/17/2022 16:18:16 - INFO - __main__ - Global step 750 Train loss 2.00 ACC 0.5 on epoch=249
03/17/2022 16:18:16 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.5 on epoch=249, global_step=750
03/17/2022 16:18:19 - INFO - __main__ - Step 760 Global step 760 Train loss 2.00 on epoch=253
03/17/2022 16:18:22 - INFO - __main__ - Step 770 Global step 770 Train loss 1.93 on epoch=256
03/17/2022 16:18:24 - INFO - __main__ - Step 780 Global step 780 Train loss 1.71 on epoch=259
03/17/2022 16:18:27 - INFO - __main__ - Step 790 Global step 790 Train loss 1.80 on epoch=263
03/17/2022 16:18:30 - INFO - __main__ - Step 800 Global step 800 Train loss 1.73 on epoch=266
03/17/2022 16:18:31 - INFO - __main__ - Global step 800 Train loss 1.83 ACC 0.5 on epoch=266
03/17/2022 16:18:34 - INFO - __main__ - Step 810 Global step 810 Train loss 1.76 on epoch=269
03/17/2022 16:18:36 - INFO - __main__ - Step 820 Global step 820 Train loss 1.65 on epoch=273
03/17/2022 16:18:39 - INFO - __main__ - Step 830 Global step 830 Train loss 1.55 on epoch=276
03/17/2022 16:18:42 - INFO - __main__ - Step 840 Global step 840 Train loss 1.44 on epoch=279
03/17/2022 16:18:44 - INFO - __main__ - Step 850 Global step 850 Train loss 1.58 on epoch=283
03/17/2022 16:18:46 - INFO - __main__ - Global step 850 Train loss 1.60 ACC 0.5 on epoch=283
03/17/2022 16:18:48 - INFO - __main__ - Step 860 Global step 860 Train loss 1.48 on epoch=286
03/17/2022 16:18:51 - INFO - __main__ - Step 870 Global step 870 Train loss 1.47 on epoch=289
03/17/2022 16:18:54 - INFO - __main__ - Step 880 Global step 880 Train loss 1.31 on epoch=293
03/17/2022 16:18:56 - INFO - __main__ - Step 890 Global step 890 Train loss 1.42 on epoch=296
03/17/2022 16:18:59 - INFO - __main__ - Step 900 Global step 900 Train loss 1.43 on epoch=299
03/17/2022 16:19:00 - INFO - __main__ - Global step 900 Train loss 1.42 ACC 0.5 on epoch=299
03/17/2022 16:19:03 - INFO - __main__ - Step 910 Global step 910 Train loss 1.23 on epoch=303
03/17/2022 16:19:06 - INFO - __main__ - Step 920 Global step 920 Train loss 1.37 on epoch=306
03/17/2022 16:19:08 - INFO - __main__ - Step 930 Global step 930 Train loss 1.28 on epoch=309
03/17/2022 16:19:11 - INFO - __main__ - Step 940 Global step 940 Train loss 1.27 on epoch=313
03/17/2022 16:19:14 - INFO - __main__ - Step 950 Global step 950 Train loss 1.19 on epoch=316
03/17/2022 16:19:15 - INFO - __main__ - Global step 950 Train loss 1.27 ACC 0.5 on epoch=316
03/17/2022 16:19:17 - INFO - __main__ - Step 960 Global step 960 Train loss 1.24 on epoch=319
03/17/2022 16:19:20 - INFO - __main__ - Step 970 Global step 970 Train loss 1.20 on epoch=323
03/17/2022 16:19:23 - INFO - __main__ - Step 980 Global step 980 Train loss 1.20 on epoch=326
03/17/2022 16:19:26 - INFO - __main__ - Step 990 Global step 990 Train loss 1.13 on epoch=329
03/17/2022 16:19:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.10 on epoch=333
03/17/2022 16:19:29 - INFO - __main__ - Global step 1000 Train loss 1.17 ACC 0.28125 on epoch=333
03/17/2022 16:19:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.11 on epoch=336
03/17/2022 16:19:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.98 on epoch=339
03/17/2022 16:19:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.93 on epoch=343
03/17/2022 16:19:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.06 on epoch=346
03/17/2022 16:19:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.03 on epoch=349
03/17/2022 16:19:44 - INFO - __main__ - Global step 1050 Train loss 1.02 ACC 0.5 on epoch=349
03/17/2022 16:19:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.01 on epoch=353
03/17/2022 16:19:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.95 on epoch=356
03/17/2022 16:19:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.90 on epoch=359
03/17/2022 16:19:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.96 on epoch=363
03/17/2022 16:19:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.88 on epoch=366
03/17/2022 16:19:58 - INFO - __main__ - Global step 1100 Train loss 0.94 ACC 0.5 on epoch=366
03/17/2022 16:20:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.97 on epoch=369
03/17/2022 16:20:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.85 on epoch=373
03/17/2022 16:20:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.89 on epoch=376
03/17/2022 16:20:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.80 on epoch=379
03/17/2022 16:20:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.86 on epoch=383
03/17/2022 16:20:13 - INFO - __main__ - Global step 1150 Train loss 0.87 ACC 0.4375 on epoch=383
03/17/2022 16:20:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.87 on epoch=386
03/17/2022 16:20:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.80 on epoch=389
03/17/2022 16:20:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=393
03/17/2022 16:20:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.81 on epoch=396
03/17/2022 16:20:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=399
03/17/2022 16:20:27 - INFO - __main__ - Global step 1200 Train loss 0.81 ACC 0.5 on epoch=399
03/17/2022 16:20:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=403
03/17/2022 16:20:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.83 on epoch=406
03/17/2022 16:20:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.74 on epoch=409
03/17/2022 16:20:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.85 on epoch=413
03/17/2022 16:20:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=416
03/17/2022 16:20:41 - INFO - __main__ - Global step 1250 Train loss 0.79 ACC 0.1875 on epoch=416
03/17/2022 16:20:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.72 on epoch=419
03/17/2022 16:20:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.65 on epoch=423
03/17/2022 16:20:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.76 on epoch=426
03/17/2022 16:20:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.71 on epoch=429
03/17/2022 16:20:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.74 on epoch=433
03/17/2022 16:20:56 - INFO - __main__ - Global step 1300 Train loss 0.72 ACC 0.0 on epoch=433
03/17/2022 16:20:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.69 on epoch=436
03/17/2022 16:21:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.72 on epoch=439
03/17/2022 16:21:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.76 on epoch=443
03/17/2022 16:21:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=446
03/17/2022 16:21:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.66 on epoch=449
03/17/2022 16:21:10 - INFO - __main__ - Global step 1350 Train loss 0.71 ACC 0.5 on epoch=449
03/17/2022 16:21:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.72 on epoch=453
03/17/2022 16:21:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=456
03/17/2022 16:21:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.74 on epoch=459
03/17/2022 16:21:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.70 on epoch=463
03/17/2022 16:21:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=466
03/17/2022 16:21:24 - INFO - __main__ - Global step 1400 Train loss 0.72 ACC 0.5 on epoch=466
03/17/2022 16:21:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.61 on epoch=469
03/17/2022 16:21:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.71 on epoch=473
03/17/2022 16:21:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.67 on epoch=476
03/17/2022 16:21:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.67 on epoch=479
03/17/2022 16:21:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.63 on epoch=483
03/17/2022 16:21:39 - INFO - __main__ - Global step 1450 Train loss 0.66 ACC 0.0 on epoch=483
03/17/2022 16:21:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.64 on epoch=486
03/17/2022 16:21:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.69 on epoch=489
03/17/2022 16:21:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.73 on epoch=493
03/17/2022 16:21:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.64 on epoch=496
03/17/2022 16:21:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.62 on epoch=499
03/17/2022 16:21:53 - INFO - __main__ - Global step 1500 Train loss 0.67 ACC 0.5 on epoch=499
03/17/2022 16:21:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=503
03/17/2022 16:21:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.65 on epoch=506
03/17/2022 16:22:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.71 on epoch=509
03/17/2022 16:22:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.61 on epoch=513
03/17/2022 16:22:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.63 on epoch=516
03/17/2022 16:22:07 - INFO - __main__ - Global step 1550 Train loss 0.65 ACC 0.5 on epoch=516
03/17/2022 16:22:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.61 on epoch=519
03/17/2022 16:22:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.64 on epoch=523
03/17/2022 16:22:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.55 on epoch=526
03/17/2022 16:22:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.63 on epoch=529
03/17/2022 16:22:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.58 on epoch=533
03/17/2022 16:22:22 - INFO - __main__ - Global step 1600 Train loss 0.60 ACC 0.03125 on epoch=533
03/17/2022 16:22:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=536
03/17/2022 16:22:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.64 on epoch=539
03/17/2022 16:22:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=543
03/17/2022 16:22:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.59 on epoch=546
03/17/2022 16:22:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.53 on epoch=549
03/17/2022 16:22:36 - INFO - __main__ - Global step 1650 Train loss 0.78 ACC 0.25 on epoch=549
03/17/2022 16:22:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.10 on epoch=553
03/17/2022 16:22:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.64 on epoch=556
03/17/2022 16:22:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.68 on epoch=559
03/17/2022 16:22:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.77 on epoch=563
03/17/2022 16:22:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.73 on epoch=566
03/17/2022 16:22:51 - INFO - __main__ - Global step 1700 Train loss 0.78 ACC 0.0 on epoch=566
03/17/2022 16:22:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.73 on epoch=569
03/17/2022 16:22:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.68 on epoch=573
03/17/2022 16:22:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.62 on epoch=576
03/17/2022 16:23:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=579
03/17/2022 16:23:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.68 on epoch=583
03/17/2022 16:23:05 - INFO - __main__ - Global step 1750 Train loss 0.66 ACC 0.0625 on epoch=583
03/17/2022 16:23:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.71 on epoch=586
03/17/2022 16:23:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.72 on epoch=589
03/17/2022 16:23:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.65 on epoch=593
03/17/2022 16:23:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.66 on epoch=596
03/17/2022 16:23:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.66 on epoch=599
03/17/2022 16:23:19 - INFO - __main__ - Global step 1800 Train loss 0.68 ACC 0.5 on epoch=599
03/17/2022 16:23:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.64 on epoch=603
03/17/2022 16:23:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.58 on epoch=606
03/17/2022 16:23:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.61 on epoch=609
03/17/2022 16:23:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.62 on epoch=613
03/17/2022 16:23:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.77 on epoch=616
03/17/2022 16:23:34 - INFO - __main__ - Global step 1850 Train loss 0.64 ACC 0.5 on epoch=616
03/17/2022 16:23:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.65 on epoch=619
03/17/2022 16:23:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.06 on epoch=623
03/17/2022 16:23:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.83 on epoch=626
03/17/2022 16:23:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.00 on epoch=629
03/17/2022 16:23:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.94 on epoch=633
03/17/2022 16:23:48 - INFO - __main__ - Global step 1900 Train loss 0.90 ACC 0.5 on epoch=633
03/17/2022 16:23:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.75 on epoch=636
03/17/2022 16:23:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.59 on epoch=639
03/17/2022 16:23:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.81 on epoch=643
03/17/2022 16:23:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=646
03/17/2022 16:24:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.73 on epoch=649
03/17/2022 16:24:02 - INFO - __main__ - Global step 1950 Train loss 0.72 ACC 0.46875 on epoch=649
03/17/2022 16:24:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.81 on epoch=653
03/17/2022 16:24:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.65 on epoch=656
03/17/2022 16:24:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.65 on epoch=659
03/17/2022 16:24:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.59 on epoch=663
03/17/2022 16:24:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.62 on epoch=666
03/17/2022 16:24:16 - INFO - __main__ - Global step 2000 Train loss 0.67 ACC 0.5 on epoch=666
03/17/2022 16:24:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.64 on epoch=669
03/17/2022 16:24:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.73 on epoch=673
03/17/2022 16:24:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.58 on epoch=676
03/17/2022 16:24:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.61 on epoch=679
03/17/2022 16:24:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.61 on epoch=683
03/17/2022 16:24:31 - INFO - __main__ - Global step 2050 Train loss 0.63 ACC 0.375 on epoch=683
03/17/2022 16:24:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.56 on epoch=686
03/17/2022 16:24:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.55 on epoch=689
03/17/2022 16:24:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.66 on epoch=693
03/17/2022 16:24:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.66 on epoch=696
03/17/2022 16:24:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.59 on epoch=699
03/17/2022 16:24:45 - INFO - __main__ - Global step 2100 Train loss 0.60 ACC 0.375 on epoch=699
03/17/2022 16:24:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.61 on epoch=703
03/17/2022 16:24:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.57 on epoch=706
03/17/2022 16:24:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.57 on epoch=709
03/17/2022 16:24:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.62 on epoch=713
03/17/2022 16:24:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.63 on epoch=716
03/17/2022 16:24:59 - INFO - __main__ - Global step 2150 Train loss 0.60 ACC 0.5 on epoch=716
03/17/2022 16:25:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.58 on epoch=719
03/17/2022 16:25:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.60 on epoch=723
03/17/2022 16:25:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.49 on epoch=726
03/17/2022 16:25:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.55 on epoch=729
03/17/2022 16:25:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.64 on epoch=733
03/17/2022 16:25:14 - INFO - __main__ - Global step 2200 Train loss 0.57 ACC 0.5 on epoch=733
03/17/2022 16:25:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.58 on epoch=736
03/17/2022 16:25:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.54 on epoch=739
03/17/2022 16:25:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.55 on epoch=743
03/17/2022 16:25:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.58 on epoch=746
03/17/2022 16:25:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.54 on epoch=749
03/17/2022 16:25:28 - INFO - __main__ - Global step 2250 Train loss 0.56 ACC 0.5 on epoch=749
03/17/2022 16:25:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.57 on epoch=753
03/17/2022 16:25:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.53 on epoch=756
03/17/2022 16:25:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.56 on epoch=759
03/17/2022 16:25:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.56 on epoch=763
03/17/2022 16:25:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.55 on epoch=766
03/17/2022 16:25:42 - INFO - __main__ - Global step 2300 Train loss 0.55 ACC 0.3125 on epoch=766
03/17/2022 16:25:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.56 on epoch=769
03/17/2022 16:25:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.60 on epoch=773
03/17/2022 16:25:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.52 on epoch=776
03/17/2022 16:25:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.50 on epoch=779
03/17/2022 16:25:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.60 on epoch=783
03/17/2022 16:25:56 - INFO - __main__ - Global step 2350 Train loss 0.56 ACC 0.40625 on epoch=783
03/17/2022 16:25:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.54 on epoch=786
03/17/2022 16:26:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.51 on epoch=789
03/17/2022 16:26:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.59 on epoch=793
03/17/2022 16:26:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.56 on epoch=796
03/17/2022 16:26:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.57 on epoch=799
03/17/2022 16:26:11 - INFO - __main__ - Global step 2400 Train loss 0.55 ACC 0.5 on epoch=799
03/17/2022 16:26:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.54 on epoch=803
03/17/2022 16:26:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.59 on epoch=806
03/17/2022 16:26:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.59 on epoch=809
03/17/2022 16:26:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.50 on epoch=813
03/17/2022 16:26:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.58 on epoch=816
03/17/2022 16:26:25 - INFO - __main__ - Global step 2450 Train loss 0.56 ACC 0.34375 on epoch=816
03/17/2022 16:26:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.49 on epoch=819
03/17/2022 16:26:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.51 on epoch=823
03/17/2022 16:26:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=826
03/17/2022 16:26:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.51 on epoch=829
03/17/2022 16:26:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.56 on epoch=833
03/17/2022 16:26:39 - INFO - __main__ - Global step 2500 Train loss 0.51 ACC 0.3125 on epoch=833
03/17/2022 16:26:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.52 on epoch=836
03/17/2022 16:26:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.55 on epoch=839
03/17/2022 16:26:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.54 on epoch=843
03/17/2022 16:26:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.59 on epoch=846
03/17/2022 16:26:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=849
03/17/2022 16:26:53 - INFO - __main__ - Global step 2550 Train loss 0.53 ACC 0.25 on epoch=849
03/17/2022 16:26:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.54 on epoch=853
03/17/2022 16:26:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.52 on epoch=856
03/17/2022 16:27:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.55 on epoch=859
03/17/2022 16:27:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.52 on epoch=863
03/17/2022 16:27:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.56 on epoch=866
03/17/2022 16:27:08 - INFO - __main__ - Global step 2600 Train loss 0.54 ACC 0.09375 on epoch=866
03/17/2022 16:27:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.54 on epoch=869
03/17/2022 16:27:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.53 on epoch=873
03/17/2022 16:27:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.55 on epoch=876
03/17/2022 16:27:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.53 on epoch=879
03/17/2022 16:27:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.49 on epoch=883
03/17/2022 16:27:22 - INFO - __main__ - Global step 2650 Train loss 0.53 ACC 0.03125 on epoch=883
03/17/2022 16:27:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.51 on epoch=886
03/17/2022 16:27:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.50 on epoch=889
03/17/2022 16:27:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.56 on epoch=893
03/17/2022 16:27:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.48 on epoch=896
03/17/2022 16:27:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.52 on epoch=899
03/17/2022 16:27:37 - INFO - __main__ - Global step 2700 Train loss 0.51 ACC 0.0 on epoch=899
03/17/2022 16:27:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.56 on epoch=903
03/17/2022 16:27:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.52 on epoch=906
03/17/2022 16:27:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.51 on epoch=909
03/17/2022 16:27:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.52 on epoch=913
03/17/2022 16:27:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.53 on epoch=916
03/17/2022 16:27:51 - INFO - __main__ - Global step 2750 Train loss 0.53 ACC 0.0625 on epoch=916
03/17/2022 16:27:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.54 on epoch=919
03/17/2022 16:27:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.53 on epoch=923
03/17/2022 16:28:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.55 on epoch=926
03/17/2022 16:28:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=929
03/17/2022 16:28:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.52 on epoch=933
03/17/2022 16:28:06 - INFO - __main__ - Global step 2800 Train loss 0.52 ACC 0.15625 on epoch=933
03/17/2022 16:28:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.55 on epoch=936
03/17/2022 16:28:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.45 on epoch=939
03/17/2022 16:28:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.51 on epoch=943
03/17/2022 16:28:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.53 on epoch=946
03/17/2022 16:28:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.55 on epoch=949
03/17/2022 16:28:21 - INFO - __main__ - Global step 2850 Train loss 0.52 ACC 0.40625 on epoch=949
03/17/2022 16:28:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.55 on epoch=953
03/17/2022 16:28:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.57 on epoch=956
03/17/2022 16:28:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=959
03/17/2022 16:28:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.50 on epoch=963
03/17/2022 16:28:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.53 on epoch=966
03/17/2022 16:28:35 - INFO - __main__ - Global step 2900 Train loss 0.52 ACC 0.46875 on epoch=966
03/17/2022 16:28:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.51 on epoch=969
03/17/2022 16:28:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.47 on epoch=973
03/17/2022 16:28:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.50 on epoch=976
03/17/2022 16:28:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.50 on epoch=979
03/17/2022 16:28:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.56 on epoch=983
03/17/2022 16:28:50 - INFO - __main__ - Global step 2950 Train loss 0.51 ACC 0.03125 on epoch=983
03/17/2022 16:28:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.50 on epoch=986
03/17/2022 16:28:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.48 on epoch=989
03/17/2022 16:28:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=993
03/17/2022 16:29:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.52 on epoch=996
03/17/2022 16:29:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.50 on epoch=999
03/17/2022 16:29:05 - INFO - __main__ - Global step 3000 Train loss 0.49 ACC 0.46875 on epoch=999
03/17/2022 16:29:05 - INFO - __main__ - save last model!
03/17/2022 16:29:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 16:29:05 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 16:29:05 - INFO - __main__ - Printing 3 examples
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 16:29:05 - INFO - __main__ - ['neutral']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 16:29:05 - INFO - __main__ - ['entailment']
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:29:05 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 16:29:05 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:29:05 - INFO - __main__ - Printing 3 examples
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:29:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:29:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:29:05 - INFO - __main__ - Printing 3 examples
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:29:05 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:29:05 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:29:05 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:29:07 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_13_0.5_8_predictions.txt
03/17/2022 16:29:07 - INFO - __main__ - ACC on test data: 0.4821
03/17/2022 16:29:07 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.5, bsz=8, dev_performance=0.5, test_performance=0.48214285714285715
03/17/2022 16:29:07 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.4, bsz=8 ...
03/17/2022 16:29:08 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:29:08 - INFO - __main__ - Printing 3 examples
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 16:29:08 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:29:08 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:29:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:29:08 - INFO - __main__ - Printing 3 examples
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:29:08 - INFO - __main__ - ['contradiction']
03/17/2022 16:29:08 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:29:08 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:29:08 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:29:20 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:29:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:29:21 - INFO - __main__ - Starting training!
03/17/2022 16:29:23 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:29:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:29:24 - INFO - __main__ - Starting training!
03/17/2022 16:29:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.29 on epoch=3
03/17/2022 16:29:30 - INFO - __main__ - Step 20 Global step 20 Train loss 5.01 on epoch=6
03/17/2022 16:29:33 - INFO - __main__ - Step 30 Global step 30 Train loss 6.85 on epoch=9
03/17/2022 16:29:36 - INFO - __main__ - Step 40 Global step 40 Train loss 5.67 on epoch=13
03/17/2022 16:29:38 - INFO - __main__ - Step 50 Global step 50 Train loss 4.85 on epoch=16
03/17/2022 16:29:54 - INFO - __main__ - Global step 50 Train loss 5.73 ACC 0.03125 on epoch=16
03/17/2022 16:29:54 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.03125 on epoch=16, global_step=50
03/17/2022 16:29:57 - INFO - __main__ - Step 60 Global step 60 Train loss 4.81 on epoch=19
03/17/2022 16:29:59 - INFO - __main__ - Step 70 Global step 70 Train loss 4.80 on epoch=23
03/17/2022 16:30:02 - INFO - __main__ - Step 80 Global step 80 Train loss 4.64 on epoch=26
03/17/2022 16:30:05 - INFO - __main__ - Step 90 Global step 90 Train loss 4.46 on epoch=29
03/17/2022 16:30:08 - INFO - __main__ - Step 100 Global step 100 Train loss 4.49 on epoch=33
03/17/2022 16:30:20 - INFO - __main__ - Global step 100 Train loss 4.64 ACC 0.15625 on epoch=33
03/17/2022 16:30:20 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.15625 on epoch=33, global_step=100
03/17/2022 16:30:23 - INFO - __main__ - Step 110 Global step 110 Train loss 4.33 on epoch=36
03/17/2022 16:30:26 - INFO - __main__ - Step 120 Global step 120 Train loss 4.29 on epoch=39
03/17/2022 16:30:28 - INFO - __main__ - Step 130 Global step 130 Train loss 4.26 on epoch=43
03/17/2022 16:30:31 - INFO - __main__ - Step 140 Global step 140 Train loss 4.06 on epoch=46
03/17/2022 16:30:34 - INFO - __main__ - Step 150 Global step 150 Train loss 3.97 on epoch=49
03/17/2022 16:30:41 - INFO - __main__ - Global step 150 Train loss 4.18 ACC 0.34375 on epoch=49
03/17/2022 16:30:41 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.34375 on epoch=49, global_step=150
03/17/2022 16:30:44 - INFO - __main__ - Step 160 Global step 160 Train loss 3.74 on epoch=53
03/17/2022 16:30:47 - INFO - __main__ - Step 170 Global step 170 Train loss 3.53 on epoch=56
03/17/2022 16:30:49 - INFO - __main__ - Step 180 Global step 180 Train loss 3.43 on epoch=59
03/17/2022 16:30:52 - INFO - __main__ - Step 190 Global step 190 Train loss 3.25 on epoch=63
03/17/2022 16:30:55 - INFO - __main__ - Step 200 Global step 200 Train loss 3.06 on epoch=66
03/17/2022 16:30:56 - INFO - __main__ - Global step 200 Train loss 3.40 ACC 0.28125 on epoch=66
03/17/2022 16:30:59 - INFO - __main__ - Step 210 Global step 210 Train loss 2.92 on epoch=69
03/17/2022 16:31:02 - INFO - __main__ - Step 220 Global step 220 Train loss 2.76 on epoch=73
03/17/2022 16:31:04 - INFO - __main__ - Step 230 Global step 230 Train loss 2.63 on epoch=76
03/17/2022 16:31:07 - INFO - __main__ - Step 240 Global step 240 Train loss 2.36 on epoch=79
03/17/2022 16:31:10 - INFO - __main__ - Step 250 Global step 250 Train loss 2.26 on epoch=83
03/17/2022 16:31:11 - INFO - __main__ - Global step 250 Train loss 2.59 ACC 0.375 on epoch=83
03/17/2022 16:31:11 - INFO - __main__ - Saving model with best ACC: 0.34375 -> 0.375 on epoch=83, global_step=250
03/17/2022 16:31:14 - INFO - __main__ - Step 260 Global step 260 Train loss 2.17 on epoch=86
03/17/2022 16:31:16 - INFO - __main__ - Step 270 Global step 270 Train loss 2.02 on epoch=89
03/17/2022 16:31:19 - INFO - __main__ - Step 280 Global step 280 Train loss 1.96 on epoch=93
03/17/2022 16:31:22 - INFO - __main__ - Step 290 Global step 290 Train loss 1.85 on epoch=96
03/17/2022 16:31:25 - INFO - __main__ - Step 300 Global step 300 Train loss 1.86 on epoch=99
03/17/2022 16:31:25 - INFO - __main__ - Global step 300 Train loss 1.97 ACC 0.5 on epoch=99
03/17/2022 16:31:25 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.5 on epoch=99, global_step=300
03/17/2022 16:31:28 - INFO - __main__ - Step 310 Global step 310 Train loss 1.86 on epoch=103
03/17/2022 16:31:31 - INFO - __main__ - Step 320 Global step 320 Train loss 1.66 on epoch=106
03/17/2022 16:31:34 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=109
03/17/2022 16:31:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.59 on epoch=113
03/17/2022 16:31:39 - INFO - __main__ - Step 350 Global step 350 Train loss 1.64 on epoch=116
03/17/2022 16:31:40 - INFO - __main__ - Global step 350 Train loss 1.69 ACC 0.5 on epoch=116
03/17/2022 16:31:43 - INFO - __main__ - Step 360 Global step 360 Train loss 1.57 on epoch=119
03/17/2022 16:31:46 - INFO - __main__ - Step 370 Global step 370 Train loss 1.45 on epoch=123
03/17/2022 16:31:48 - INFO - __main__ - Step 380 Global step 380 Train loss 1.49 on epoch=126
03/17/2022 16:31:51 - INFO - __main__ - Step 390 Global step 390 Train loss 1.38 on epoch=129
03/17/2022 16:31:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.32 on epoch=133
03/17/2022 16:31:55 - INFO - __main__ - Global step 400 Train loss 1.44 ACC 0.5 on epoch=133
03/17/2022 16:31:58 - INFO - __main__ - Step 410 Global step 410 Train loss 1.28 on epoch=136
03/17/2022 16:32:01 - INFO - __main__ - Step 420 Global step 420 Train loss 1.17 on epoch=139
03/17/2022 16:32:03 - INFO - __main__ - Step 430 Global step 430 Train loss 1.19 on epoch=143
03/17/2022 16:32:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.23 on epoch=146
03/17/2022 16:32:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=149
03/17/2022 16:32:10 - INFO - __main__ - Global step 450 Train loss 1.18 ACC 0.5 on epoch=149
03/17/2022 16:32:13 - INFO - __main__ - Step 460 Global step 460 Train loss 1.08 on epoch=153
03/17/2022 16:32:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=156
03/17/2022 16:32:18 - INFO - __main__ - Step 480 Global step 480 Train loss 1.04 on epoch=159
03/17/2022 16:32:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=163
03/17/2022 16:32:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.95 on epoch=166
03/17/2022 16:32:25 - INFO - __main__ - Global step 500 Train loss 0.99 ACC 0.5 on epoch=166
03/17/2022 16:32:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=169
03/17/2022 16:32:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.85 on epoch=173
03/17/2022 16:32:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.75 on epoch=176
03/17/2022 16:32:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=179
03/17/2022 16:32:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=183
03/17/2022 16:32:40 - INFO - __main__ - Global step 550 Train loss 0.82 ACC 0.34375 on epoch=183
03/17/2022 16:32:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=186
03/17/2022 16:32:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=189
03/17/2022 16:32:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=193
03/17/2022 16:32:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.74 on epoch=196
03/17/2022 16:32:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=199
03/17/2022 16:32:54 - INFO - __main__ - Global step 600 Train loss 0.73 ACC 0.5 on epoch=199
03/17/2022 16:32:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=203
03/17/2022 16:33:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=206
03/17/2022 16:33:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=209
03/17/2022 16:33:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.63 on epoch=213
03/17/2022 16:33:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=216
03/17/2022 16:33:09 - INFO - __main__ - Global step 650 Train loss 0.63 ACC 0.125 on epoch=216
03/17/2022 16:33:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.71 on epoch=219
03/17/2022 16:33:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=223
03/17/2022 16:33:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.64 on epoch=226
03/17/2022 16:33:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=229
03/17/2022 16:33:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=233
03/17/2022 16:33:23 - INFO - __main__ - Global step 700 Train loss 0.61 ACC 0.5 on epoch=233
03/17/2022 16:33:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=236
03/17/2022 16:33:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=239
03/17/2022 16:33:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=243
03/17/2022 16:33:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=246
03/17/2022 16:33:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=249
03/17/2022 16:33:37 - INFO - __main__ - Global step 750 Train loss 0.60 ACC 0.5 on epoch=249
03/17/2022 16:33:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=253
03/17/2022 16:33:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=256
03/17/2022 16:33:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=259
03/17/2022 16:33:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.57 on epoch=263
03/17/2022 16:33:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=266
03/17/2022 16:33:52 - INFO - __main__ - Global step 800 Train loss 0.53 ACC 0.4375 on epoch=266
03/17/2022 16:33:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.55 on epoch=269
03/17/2022 16:33:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=273
03/17/2022 16:34:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=276
03/17/2022 16:34:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.62 on epoch=279
03/17/2022 16:34:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=283
03/17/2022 16:34:07 - INFO - __main__ - Global step 850 Train loss 0.54 ACC 0.25 on epoch=283
03/17/2022 16:34:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=286
03/17/2022 16:34:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=289
03/17/2022 16:34:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.64 on epoch=293
03/17/2022 16:34:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=296
03/17/2022 16:34:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=299
03/17/2022 16:34:21 - INFO - __main__ - Global step 900 Train loss 0.52 ACC 0.5 on epoch=299
03/17/2022 16:34:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=303
03/17/2022 16:34:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=306
03/17/2022 16:34:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=309
03/17/2022 16:34:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.58 on epoch=313
03/17/2022 16:34:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.55 on epoch=316
03/17/2022 16:34:36 - INFO - __main__ - Global step 950 Train loss 0.54 ACC 0.125 on epoch=316
03/17/2022 16:34:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=319
03/17/2022 16:34:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.56 on epoch=323
03/17/2022 16:34:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=326
03/17/2022 16:34:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=329
03/17/2022 16:34:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=333
03/17/2022 16:34:50 - INFO - __main__ - Global step 1000 Train loss 0.50 ACC 0.3125 on epoch=333
03/17/2022 16:34:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.52 on epoch=336
03/17/2022 16:34:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=339
03/17/2022 16:34:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.53 on epoch=343
03/17/2022 16:35:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=346
03/17/2022 16:35:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=349
03/17/2022 16:35:05 - INFO - __main__ - Global step 1050 Train loss 0.52 ACC 0.5 on epoch=349
03/17/2022 16:35:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=353
03/17/2022 16:35:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=356
03/17/2022 16:35:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=359
03/17/2022 16:35:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=363
03/17/2022 16:35:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=366
03/17/2022 16:35:19 - INFO - __main__ - Global step 1100 Train loss 0.48 ACC 0.3125 on epoch=366
03/17/2022 16:35:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=369
03/17/2022 16:35:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=373
03/17/2022 16:35:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=376
03/17/2022 16:35:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=379
03/17/2022 16:35:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=383
03/17/2022 16:35:34 - INFO - __main__ - Global step 1150 Train loss 0.46 ACC 0.09375 on epoch=383
03/17/2022 16:35:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=386
03/17/2022 16:35:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=389
03/17/2022 16:35:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=393
03/17/2022 16:35:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=396
03/17/2022 16:35:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=399
03/17/2022 16:35:49 - INFO - __main__ - Global step 1200 Train loss 0.47 ACC 0.5 on epoch=399
03/17/2022 16:35:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=403
03/17/2022 16:35:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=406
03/17/2022 16:35:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=409
03/17/2022 16:36:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=413
03/17/2022 16:36:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=416
03/17/2022 16:36:03 - INFO - __main__ - Global step 1250 Train loss 0.49 ACC 0.5 on epoch=416
03/17/2022 16:36:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=419
03/17/2022 16:36:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=423
03/17/2022 16:36:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=426
03/17/2022 16:36:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=429
03/17/2022 16:36:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=433
03/17/2022 16:36:18 - INFO - __main__ - Global step 1300 Train loss 0.45 ACC 0.0625 on epoch=433
03/17/2022 16:36:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=436
03/17/2022 16:36:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=439
03/17/2022 16:36:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=443
03/17/2022 16:36:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=446
03/17/2022 16:36:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=449
03/17/2022 16:36:33 - INFO - __main__ - Global step 1350 Train loss 0.46 ACC 0.5 on epoch=449
03/17/2022 16:36:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=453
03/17/2022 16:36:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=456
03/17/2022 16:36:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=459
03/17/2022 16:36:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=463
03/17/2022 16:36:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=466
03/17/2022 16:36:47 - INFO - __main__ - Global step 1400 Train loss 0.43 ACC 0.21875 on epoch=466
03/17/2022 16:36:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=469
03/17/2022 16:36:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=473
03/17/2022 16:36:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=476
03/17/2022 16:36:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=479
03/17/2022 16:37:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=483
03/17/2022 16:37:02 - INFO - __main__ - Global step 1450 Train loss 0.44 ACC 0.0625 on epoch=483
03/17/2022 16:37:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=486
03/17/2022 16:37:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=489
03/17/2022 16:37:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=493
03/17/2022 16:37:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=496
03/17/2022 16:37:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=499
03/17/2022 16:37:17 - INFO - __main__ - Global step 1500 Train loss 0.43 ACC 0.53125 on epoch=499
03/17/2022 16:37:17 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=499, global_step=1500
03/17/2022 16:37:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=503
03/17/2022 16:37:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=506
03/17/2022 16:37:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=509
03/17/2022 16:37:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=513
03/17/2022 16:37:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=516
03/17/2022 16:37:32 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.3125 on epoch=516
03/17/2022 16:37:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=519
03/17/2022 16:37:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=523
03/17/2022 16:37:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=526
03/17/2022 16:37:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=529
03/17/2022 16:37:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=533
03/17/2022 16:37:46 - INFO - __main__ - Global step 1600 Train loss 0.41 ACC 0.21875 on epoch=533
03/17/2022 16:37:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=536
03/17/2022 16:37:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=539
03/17/2022 16:37:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=543
03/17/2022 16:37:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=546
03/17/2022 16:38:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=549
03/17/2022 16:38:01 - INFO - __main__ - Global step 1650 Train loss 0.40 ACC 0.375 on epoch=549
03/17/2022 16:38:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=553
03/17/2022 16:38:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=556
03/17/2022 16:38:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=559
03/17/2022 16:38:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=563
03/17/2022 16:38:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=566
03/17/2022 16:38:16 - INFO - __main__ - Global step 1700 Train loss 0.39 ACC 0.125 on epoch=566
03/17/2022 16:38:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=569
03/17/2022 16:38:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=573
03/17/2022 16:38:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=576
03/17/2022 16:38:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=579
03/17/2022 16:38:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=583
03/17/2022 16:38:30 - INFO - __main__ - Global step 1750 Train loss 0.38 ACC 0.25 on epoch=583
03/17/2022 16:38:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=586
03/17/2022 16:38:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=589
03/17/2022 16:38:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=593
03/17/2022 16:38:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=596
03/17/2022 16:38:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=599
03/17/2022 16:38:45 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.3125 on epoch=599
03/17/2022 16:38:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=603
03/17/2022 16:38:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=606
03/17/2022 16:38:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=609
03/17/2022 16:38:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=613
03/17/2022 16:38:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=616
03/17/2022 16:39:00 - INFO - __main__ - Global step 1850 Train loss 0.34 ACC 0.34375 on epoch=616
03/17/2022 16:39:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=619
03/17/2022 16:39:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=623
03/17/2022 16:39:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=626
03/17/2022 16:39:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=629
03/17/2022 16:39:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=633
03/17/2022 16:39:15 - INFO - __main__ - Global step 1900 Train loss 0.33 ACC 0.3125 on epoch=633
03/17/2022 16:39:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=636
03/17/2022 16:39:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=639
03/17/2022 16:39:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=643
03/17/2022 16:39:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=646
03/17/2022 16:39:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=649
03/17/2022 16:39:29 - INFO - __main__ - Global step 1950 Train loss 0.32 ACC 0.5625 on epoch=649
03/17/2022 16:39:29 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=649, global_step=1950
03/17/2022 16:39:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=653
03/17/2022 16:39:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=656
03/17/2022 16:39:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=659
03/17/2022 16:39:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=663
03/17/2022 16:39:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=666
03/17/2022 16:39:44 - INFO - __main__ - Global step 2000 Train loss 0.33 ACC 0.375 on epoch=666
03/17/2022 16:39:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=669
03/17/2022 16:39:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=673
03/17/2022 16:39:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=676
03/17/2022 16:39:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=679
03/17/2022 16:39:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=683
03/17/2022 16:39:59 - INFO - __main__ - Global step 2050 Train loss 0.31 ACC 0.46875 on epoch=683
03/17/2022 16:40:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=686
03/17/2022 16:40:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=689
03/17/2022 16:40:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=693
03/17/2022 16:40:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=696
03/17/2022 16:40:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=699
03/17/2022 16:40:13 - INFO - __main__ - Global step 2100 Train loss 0.29 ACC 0.46875 on epoch=699
03/17/2022 16:40:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=703
03/17/2022 16:40:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=706
03/17/2022 16:40:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=709
03/17/2022 16:40:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.31 on epoch=713
03/17/2022 16:40:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=716
03/17/2022 16:40:28 - INFO - __main__ - Global step 2150 Train loss 0.28 ACC 0.5 on epoch=716
03/17/2022 16:40:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=719
03/17/2022 16:40:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.34 on epoch=723
03/17/2022 16:40:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=726
03/17/2022 16:40:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=729
03/17/2022 16:40:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=733
03/17/2022 16:40:43 - INFO - __main__ - Global step 2200 Train loss 0.29 ACC 0.3125 on epoch=733
03/17/2022 16:40:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=736
03/17/2022 16:40:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=739
03/17/2022 16:40:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.30 on epoch=743
03/17/2022 16:40:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=746
03/17/2022 16:40:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=749
03/17/2022 16:40:58 - INFO - __main__ - Global step 2250 Train loss 0.26 ACC 0.4375 on epoch=749
03/17/2022 16:41:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=753
03/17/2022 16:41:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.28 on epoch=756
03/17/2022 16:41:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=759
03/17/2022 16:41:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=763
03/17/2022 16:41:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=766
03/17/2022 16:41:12 - INFO - __main__ - Global step 2300 Train loss 0.25 ACC 0.28125 on epoch=766
03/17/2022 16:41:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=769
03/17/2022 16:41:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.33 on epoch=773
03/17/2022 16:41:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=776
03/17/2022 16:41:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=779
03/17/2022 16:41:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=783
03/17/2022 16:41:27 - INFO - __main__ - Global step 2350 Train loss 0.27 ACC 0.34375 on epoch=783
03/17/2022 16:41:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=786
03/17/2022 16:41:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.25 on epoch=789
03/17/2022 16:41:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=793
03/17/2022 16:41:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=796
03/17/2022 16:41:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=799
03/17/2022 16:41:42 - INFO - __main__ - Global step 2400 Train loss 0.23 ACC 0.46875 on epoch=799
03/17/2022 16:41:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=803
03/17/2022 16:41:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=806
03/17/2022 16:41:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=809
03/17/2022 16:41:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=813
03/17/2022 16:41:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=816
03/17/2022 16:41:57 - INFO - __main__ - Global step 2450 Train loss 0.20 ACC 0.3125 on epoch=816
03/17/2022 16:41:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=819
03/17/2022 16:42:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=823
03/17/2022 16:42:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=826
03/17/2022 16:42:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=829
03/17/2022 16:42:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=833
03/17/2022 16:42:11 - INFO - __main__ - Global step 2500 Train loss 0.21 ACC 0.5625 on epoch=833
03/17/2022 16:42:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=836
03/17/2022 16:42:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=839
03/17/2022 16:42:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=843
03/17/2022 16:42:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=846
03/17/2022 16:42:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=849
03/17/2022 16:42:26 - INFO - __main__ - Global step 2550 Train loss 0.17 ACC 0.46875 on epoch=849
03/17/2022 16:42:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=853
03/17/2022 16:42:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=856
03/17/2022 16:42:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=859
03/17/2022 16:42:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=863
03/17/2022 16:42:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=866
03/17/2022 16:42:41 - INFO - __main__ - Global step 2600 Train loss 0.21 ACC 0.46875 on epoch=866
03/17/2022 16:42:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=869
03/17/2022 16:42:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=873
03/17/2022 16:42:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=876
03/17/2022 16:42:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=879
03/17/2022 16:42:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=883
03/17/2022 16:42:56 - INFO - __main__ - Global step 2650 Train loss 0.15 ACC 0.5 on epoch=883
03/17/2022 16:42:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=886
03/17/2022 16:43:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=889
03/17/2022 16:43:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=893
03/17/2022 16:43:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=896
03/17/2022 16:43:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=899
03/17/2022 16:43:11 - INFO - __main__ - Global step 2700 Train loss 0.13 ACC 0.5 on epoch=899
03/17/2022 16:43:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=903
03/17/2022 16:43:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=906
03/17/2022 16:43:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=909
03/17/2022 16:43:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=913
03/17/2022 16:43:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=916
03/17/2022 16:43:26 - INFO - __main__ - Global step 2750 Train loss 0.13 ACC 0.625 on epoch=916
03/17/2022 16:43:26 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=916, global_step=2750
03/17/2022 16:43:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=919
03/17/2022 16:43:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=923
03/17/2022 16:43:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=926
03/17/2022 16:43:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=929
03/17/2022 16:43:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=933
03/17/2022 16:43:41 - INFO - __main__ - Global step 2800 Train loss 0.11 ACC 0.625 on epoch=933
03/17/2022 16:43:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=936
03/17/2022 16:43:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=939
03/17/2022 16:43:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=943
03/17/2022 16:43:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=946
03/17/2022 16:43:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=949
03/17/2022 16:43:56 - INFO - __main__ - Global step 2850 Train loss 0.09 ACC 0.5625 on epoch=949
03/17/2022 16:43:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=953
03/17/2022 16:44:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
03/17/2022 16:44:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=959
03/17/2022 16:44:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=963
03/17/2022 16:44:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=966
03/17/2022 16:44:11 - INFO - __main__ - Global step 2900 Train loss 0.08 ACC 0.40625 on epoch=966
03/17/2022 16:44:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=969
03/17/2022 16:44:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=973
03/17/2022 16:44:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=976
03/17/2022 16:44:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=979
03/17/2022 16:44:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=983
03/17/2022 16:44:25 - INFO - __main__ - Global step 2950 Train loss 0.07 ACC 0.375 on epoch=983
03/17/2022 16:44:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
03/17/2022 16:44:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=989
03/17/2022 16:44:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
03/17/2022 16:44:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=996
03/17/2022 16:44:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=999
03/17/2022 16:44:40 - INFO - __main__ - Global step 3000 Train loss 0.06 ACC 0.5625 on epoch=999
03/17/2022 16:44:40 - INFO - __main__ - save last model!
03/17/2022 16:44:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 16:44:41 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 16:44:41 - INFO - __main__ - Printing 3 examples
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 16:44:41 - INFO - __main__ - ['neutral']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 16:44:41 - INFO - __main__ - ['entailment']
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:44:41 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 16:44:41 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:44:41 - INFO - __main__ - Printing 3 examples
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:44:41 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:44:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:44:41 - INFO - __main__ - Printing 3 examples
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:44:41 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:44:41 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:44:41 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:44:43 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_13_0.4_8_predictions.txt
03/17/2022 16:44:43 - INFO - __main__ - ACC on test data: 0.6250
03/17/2022 16:44:43 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.4, bsz=8, dev_performance=0.625, test_performance=0.625
03/17/2022 16:44:43 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.3, bsz=8 ...
03/17/2022 16:44:44 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:44:44 - INFO - __main__ - Printing 3 examples
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 16:44:44 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:44:44 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:44:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:44:44 - INFO - __main__ - Printing 3 examples
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:44:44 - INFO - __main__ - ['contradiction']
03/17/2022 16:44:44 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:44:44 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:44:44 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:44:57 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:44:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:44:57 - INFO - __main__ - Starting training!
03/17/2022 16:45:03 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 16:45:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 16:45:04 - INFO - __main__ - Starting training!
03/17/2022 16:45:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.50 on epoch=3
03/17/2022 16:45:10 - INFO - __main__ - Step 20 Global step 20 Train loss 3.58 on epoch=6
03/17/2022 16:45:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.83 on epoch=9
03/17/2022 16:45:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=13
03/17/2022 16:45:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.93 on epoch=16
03/17/2022 16:45:19 - INFO - __main__ - Global step 50 Train loss 2.79 ACC 0.5 on epoch=16
03/17/2022 16:45:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 16:45:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=19
03/17/2022 16:45:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=23
03/17/2022 16:45:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
03/17/2022 16:45:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=29
03/17/2022 16:45:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=33
03/17/2022 16:45:33 - INFO - __main__ - Global step 100 Train loss 0.63 ACC 0.0 on epoch=33
03/17/2022 16:45:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=36
03/17/2022 16:45:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=39
03/17/2022 16:45:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
03/17/2022 16:45:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
03/17/2022 16:45:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/17/2022 16:45:48 - INFO - __main__ - Global step 150 Train loss 0.54 ACC 0.5 on epoch=49
03/17/2022 16:45:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=53
03/17/2022 16:45:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
03/17/2022 16:45:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=59
03/17/2022 16:45:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
03/17/2022 16:46:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
03/17/2022 16:46:03 - INFO - __main__ - Global step 200 Train loss 0.51 ACC 0.5 on epoch=66
03/17/2022 16:46:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
03/17/2022 16:46:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=73
03/17/2022 16:46:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=76
03/17/2022 16:46:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
03/17/2022 16:46:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=83
03/17/2022 16:46:17 - INFO - __main__ - Global step 250 Train loss 0.50 ACC 0.03125 on epoch=83
03/17/2022 16:46:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=86
03/17/2022 16:46:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=89
03/17/2022 16:46:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=93
03/17/2022 16:46:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
03/17/2022 16:46:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
03/17/2022 16:46:32 - INFO - __main__ - Global step 300 Train loss 0.46 ACC 0.5 on epoch=99
03/17/2022 16:46:35 - INFO - __main__ - Step 310 Global step 310 Train loss 2.24 on epoch=103
03/17/2022 16:46:38 - INFO - __main__ - Step 320 Global step 320 Train loss 1.22 on epoch=106
03/17/2022 16:46:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/17/2022 16:46:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=113
03/17/2022 16:46:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=116
03/17/2022 16:46:47 - INFO - __main__ - Global step 350 Train loss 0.97 ACC 0.3125 on epoch=116
03/17/2022 16:46:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=119
03/17/2022 16:46:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=123
03/17/2022 16:46:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=126
03/17/2022 16:46:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=129
03/17/2022 16:47:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=133
03/17/2022 16:47:02 - INFO - __main__ - Global step 400 Train loss 0.47 ACC 0.375 on epoch=133
03/17/2022 16:47:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=136
03/17/2022 16:47:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
03/17/2022 16:47:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
03/17/2022 16:47:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
03/17/2022 16:47:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=149
03/17/2022 16:47:16 - INFO - __main__ - Global step 450 Train loss 0.42 ACC 0.34375 on epoch=149
03/17/2022 16:47:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=153
03/17/2022 16:47:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=156
03/17/2022 16:47:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=159
03/17/2022 16:47:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
03/17/2022 16:47:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
03/17/2022 16:47:31 - INFO - __main__ - Global step 500 Train loss 0.42 ACC 0.1875 on epoch=166
03/17/2022 16:47:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=169
03/17/2022 16:47:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=173
03/17/2022 16:47:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
03/17/2022 16:47:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=179
03/17/2022 16:47:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=183
03/17/2022 16:47:46 - INFO - __main__ - Global step 550 Train loss 0.44 ACC 0.25 on epoch=183
03/17/2022 16:47:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=186
03/17/2022 16:47:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=189
03/17/2022 16:47:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=193
03/17/2022 16:47:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=196
03/17/2022 16:47:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=199
03/17/2022 16:48:00 - INFO - __main__ - Global step 600 Train loss 0.44 ACC 0.34375 on epoch=199
03/17/2022 16:48:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=203
03/17/2022 16:48:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=206
03/17/2022 16:48:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=209
03/17/2022 16:48:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=213
03/17/2022 16:48:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=216
03/17/2022 16:48:15 - INFO - __main__ - Global step 650 Train loss 0.44 ACC 0.125 on epoch=216
03/17/2022 16:48:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=219
03/17/2022 16:48:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=223
03/17/2022 16:48:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=226
03/17/2022 16:48:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=229
03/17/2022 16:48:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=233
03/17/2022 16:48:30 - INFO - __main__ - Global step 700 Train loss 0.44 ACC 0.03125 on epoch=233
03/17/2022 16:48:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=236
03/17/2022 16:48:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=239
03/17/2022 16:48:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=243
03/17/2022 16:48:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=246
03/17/2022 16:48:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=249
03/17/2022 16:48:45 - INFO - __main__ - Global step 750 Train loss 0.43 ACC 0.1875 on epoch=249
03/17/2022 16:48:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=253
03/17/2022 16:48:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=256
03/17/2022 16:48:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=259
03/17/2022 16:48:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=263
03/17/2022 16:48:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=266
03/17/2022 16:49:00 - INFO - __main__ - Global step 800 Train loss 0.40 ACC 0.375 on epoch=266
03/17/2022 16:49:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=269
03/17/2022 16:49:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=273
03/17/2022 16:49:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=276
03/17/2022 16:49:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=279
03/17/2022 16:49:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=283
03/17/2022 16:49:14 - INFO - __main__ - Global step 850 Train loss 0.43 ACC 0.09375 on epoch=283
03/17/2022 16:49:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=286
03/17/2022 16:49:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=289
03/17/2022 16:49:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=293
03/17/2022 16:49:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=296
03/17/2022 16:49:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=299
03/17/2022 16:49:29 - INFO - __main__ - Global step 900 Train loss 0.41 ACC 0.21875 on epoch=299
03/17/2022 16:49:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=303
03/17/2022 16:49:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=306
03/17/2022 16:49:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=309
03/17/2022 16:49:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=313
03/17/2022 16:49:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=316
03/17/2022 16:49:44 - INFO - __main__ - Global step 950 Train loss 0.40 ACC 0.09375 on epoch=316
03/17/2022 16:49:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=319
03/17/2022 16:49:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=323
03/17/2022 16:49:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=326
03/17/2022 16:49:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=329
03/17/2022 16:49:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=333
03/17/2022 16:49:58 - INFO - __main__ - Global step 1000 Train loss 0.43 ACC 0.15625 on epoch=333
03/17/2022 16:50:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=336
03/17/2022 16:50:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=339
03/17/2022 16:50:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=343
03/17/2022 16:50:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=346
03/17/2022 16:50:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=349
03/17/2022 16:50:13 - INFO - __main__ - Global step 1050 Train loss 0.39 ACC 0.375 on epoch=349
03/17/2022 16:50:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=353
03/17/2022 16:50:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=356
03/17/2022 16:50:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=359
03/17/2022 16:50:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=363
03/17/2022 16:50:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=366
03/17/2022 16:50:28 - INFO - __main__ - Global step 1100 Train loss 0.40 ACC 0.09375 on epoch=366
03/17/2022 16:50:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=369
03/17/2022 16:50:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=373
03/17/2022 16:50:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=376
03/17/2022 16:50:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=379
03/17/2022 16:50:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=383
03/17/2022 16:50:42 - INFO - __main__ - Global step 1150 Train loss 0.37 ACC 0.1875 on epoch=383
03/17/2022 16:50:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=386
03/17/2022 16:50:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=389
03/17/2022 16:50:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=393
03/17/2022 16:50:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=396
03/17/2022 16:50:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=399
03/17/2022 16:50:57 - INFO - __main__ - Global step 1200 Train loss 0.38 ACC 0.15625 on epoch=399
03/17/2022 16:51:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=403
03/17/2022 16:51:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=406
03/17/2022 16:51:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=409
03/17/2022 16:51:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=413
03/17/2022 16:51:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=416
03/17/2022 16:51:12 - INFO - __main__ - Global step 1250 Train loss 0.41 ACC 0.1875 on epoch=416
03/17/2022 16:51:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=419
03/17/2022 16:51:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=423
03/17/2022 16:51:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=426
03/17/2022 16:51:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=429
03/17/2022 16:51:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=433
03/17/2022 16:51:27 - INFO - __main__ - Global step 1300 Train loss 0.40 ACC 0.28125 on epoch=433
03/17/2022 16:51:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=436
03/17/2022 16:51:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=439
03/17/2022 16:51:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=443
03/17/2022 16:51:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=446
03/17/2022 16:51:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=449
03/17/2022 16:51:42 - INFO - __main__ - Global step 1350 Train loss 0.38 ACC 0.15625 on epoch=449
03/17/2022 16:51:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=453
03/17/2022 16:51:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=456
03/17/2022 16:51:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=459
03/17/2022 16:51:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=463
03/17/2022 16:51:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=466
03/17/2022 16:51:56 - INFO - __main__ - Global step 1400 Train loss 0.38 ACC 0.46875 on epoch=466
03/17/2022 16:51:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=469
03/17/2022 16:52:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=473
03/17/2022 16:52:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=476
03/17/2022 16:52:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=479
03/17/2022 16:52:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=483
03/17/2022 16:52:11 - INFO - __main__ - Global step 1450 Train loss 0.38 ACC 0.125 on epoch=483
03/17/2022 16:52:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=486
03/17/2022 16:52:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=489
03/17/2022 16:52:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=493
03/17/2022 16:52:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=496
03/17/2022 16:52:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=499
03/17/2022 16:52:26 - INFO - __main__ - Global step 1500 Train loss 0.34 ACC 0.46875 on epoch=499
03/17/2022 16:52:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=503
03/17/2022 16:52:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=506
03/17/2022 16:52:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=509
03/17/2022 16:52:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=513
03/17/2022 16:52:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=516
03/17/2022 16:52:41 - INFO - __main__ - Global step 1550 Train loss 0.36 ACC 0.28125 on epoch=516
03/17/2022 16:52:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=519
03/17/2022 16:52:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=523
03/17/2022 16:52:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=526
03/17/2022 16:52:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=529
03/17/2022 16:52:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=533
03/17/2022 16:52:56 - INFO - __main__ - Global step 1600 Train loss 0.37 ACC 0.34375 on epoch=533
03/17/2022 16:52:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=536
03/17/2022 16:53:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=539
03/17/2022 16:53:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=543
03/17/2022 16:53:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=546
03/17/2022 16:53:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=549
03/17/2022 16:53:10 - INFO - __main__ - Global step 1650 Train loss 0.33 ACC 0.53125 on epoch=549
03/17/2022 16:53:10 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=549, global_step=1650
03/17/2022 16:53:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=553
03/17/2022 16:53:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=556
03/17/2022 16:53:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=559
03/17/2022 16:53:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=563
03/17/2022 16:53:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=566
03/17/2022 16:53:25 - INFO - __main__ - Global step 1700 Train loss 0.35 ACC 0.5625 on epoch=566
03/17/2022 16:53:25 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=566, global_step=1700
03/17/2022 16:53:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=569
03/17/2022 16:53:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=573
03/17/2022 16:53:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=576
03/17/2022 16:53:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=579
03/17/2022 16:53:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=583
03/17/2022 16:53:40 - INFO - __main__ - Global step 1750 Train loss 0.32 ACC 0.34375 on epoch=583
03/17/2022 16:53:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=586
03/17/2022 16:53:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=589
03/17/2022 16:53:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=593
03/17/2022 16:53:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=596
03/17/2022 16:53:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=599
03/17/2022 16:53:54 - INFO - __main__ - Global step 1800 Train loss 0.29 ACC 0.5625 on epoch=599
03/17/2022 16:53:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=603
03/17/2022 16:54:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=606
03/17/2022 16:54:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=609
03/17/2022 16:54:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=613
03/17/2022 16:54:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=616
03/17/2022 16:54:09 - INFO - __main__ - Global step 1850 Train loss 0.29 ACC 0.46875 on epoch=616
03/17/2022 16:54:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=619
03/17/2022 16:54:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=623
03/17/2022 16:54:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=626
03/17/2022 16:54:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=629
03/17/2022 16:54:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=633
03/17/2022 16:54:24 - INFO - __main__ - Global step 1900 Train loss 0.27 ACC 0.375 on epoch=633
03/17/2022 16:54:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=636
03/17/2022 16:54:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=639
03/17/2022 16:54:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=643
03/17/2022 16:54:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=646
03/17/2022 16:54:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=649
03/17/2022 16:54:39 - INFO - __main__ - Global step 1950 Train loss 0.28 ACC 0.40625 on epoch=649
03/17/2022 16:54:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=653
03/17/2022 16:54:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=656
03/17/2022 16:54:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=659
03/17/2022 16:54:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=663
03/17/2022 16:54:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=666
03/17/2022 16:54:53 - INFO - __main__ - Global step 2000 Train loss 0.22 ACC 0.40625 on epoch=666
03/17/2022 16:54:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=669
03/17/2022 16:54:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=673
03/17/2022 16:55:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=676
03/17/2022 16:55:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=679
03/17/2022 16:55:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=683
03/17/2022 16:55:08 - INFO - __main__ - Global step 2050 Train loss 0.22 ACC 0.25 on epoch=683
03/17/2022 16:55:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=686
03/17/2022 16:55:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=689
03/17/2022 16:55:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=693
03/17/2022 16:55:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=696
03/17/2022 16:55:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=699
03/17/2022 16:55:23 - INFO - __main__ - Global step 2100 Train loss 0.19 ACC 0.28125 on epoch=699
03/17/2022 16:55:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=703
03/17/2022 16:55:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=706
03/17/2022 16:55:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=709
03/17/2022 16:55:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=713
03/17/2022 16:55:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=716
03/17/2022 16:55:38 - INFO - __main__ - Global step 2150 Train loss 0.14 ACC 0.46875 on epoch=716
03/17/2022 16:55:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=719
03/17/2022 16:55:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=723
03/17/2022 16:55:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=726
03/17/2022 16:55:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=729
03/17/2022 16:55:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=733
03/17/2022 16:55:53 - INFO - __main__ - Global step 2200 Train loss 0.24 ACC 0.375 on epoch=733
03/17/2022 16:55:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=736
03/17/2022 16:55:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=739
03/17/2022 16:56:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=743
03/17/2022 16:56:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=746
03/17/2022 16:56:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=749
03/17/2022 16:56:07 - INFO - __main__ - Global step 2250 Train loss 0.16 ACC 0.40625 on epoch=749
03/17/2022 16:56:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=753
03/17/2022 16:56:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=756
03/17/2022 16:56:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=759
03/17/2022 16:56:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=763
03/17/2022 16:56:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=766
03/17/2022 16:56:22 - INFO - __main__ - Global step 2300 Train loss 0.14 ACC 0.375 on epoch=766
03/17/2022 16:56:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=769
03/17/2022 16:56:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=773
03/17/2022 16:56:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=776
03/17/2022 16:56:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=779
03/17/2022 16:56:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=783
03/17/2022 16:56:37 - INFO - __main__ - Global step 2350 Train loss 0.14 ACC 0.40625 on epoch=783
03/17/2022 16:56:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=786
03/17/2022 16:56:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=789
03/17/2022 16:56:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=793
03/17/2022 16:56:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=796
03/17/2022 16:56:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=799
03/17/2022 16:56:52 - INFO - __main__ - Global step 2400 Train loss 0.10 ACC 0.34375 on epoch=799
03/17/2022 16:56:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=803
03/17/2022 16:56:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=806
03/17/2022 16:57:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=809
03/17/2022 16:57:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=813
03/17/2022 16:57:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=816
03/17/2022 16:57:06 - INFO - __main__ - Global step 2450 Train loss 0.10 ACC 0.34375 on epoch=816
03/17/2022 16:57:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
03/17/2022 16:57:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=823
03/17/2022 16:57:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=826
03/17/2022 16:57:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=829
03/17/2022 16:57:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=833
03/17/2022 16:57:21 - INFO - __main__ - Global step 2500 Train loss 0.07 ACC 0.5625 on epoch=833
03/17/2022 16:57:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
03/17/2022 16:57:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=839
03/17/2022 16:57:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=843
03/17/2022 16:57:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=846
03/17/2022 16:57:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=849
03/17/2022 16:57:36 - INFO - __main__ - Global step 2550 Train loss 0.09 ACC 0.46875 on epoch=849
03/17/2022 16:57:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=853
03/17/2022 16:57:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=856
03/17/2022 16:57:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=859
03/17/2022 16:57:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
03/17/2022 16:57:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=866
03/17/2022 16:57:50 - INFO - __main__ - Global step 2600 Train loss 0.06 ACC 0.28125 on epoch=866
03/17/2022 16:57:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
03/17/2022 16:57:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=873
03/17/2022 16:57:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=876
03/17/2022 16:58:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=879
03/17/2022 16:58:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
03/17/2022 16:58:05 - INFO - __main__ - Global step 2650 Train loss 0.05 ACC 0.4375 on epoch=883
03/17/2022 16:58:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=886
03/17/2022 16:58:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=889
03/17/2022 16:58:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=893
03/17/2022 16:58:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/17/2022 16:58:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
03/17/2022 16:58:20 - INFO - __main__ - Global step 2700 Train loss 0.05 ACC 0.4375 on epoch=899
03/17/2022 16:58:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=903
03/17/2022 16:58:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=906
03/17/2022 16:58:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
03/17/2022 16:58:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
03/17/2022 16:58:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=916
03/17/2022 16:58:35 - INFO - __main__ - Global step 2750 Train loss 0.05 ACC 0.5 on epoch=916
03/17/2022 16:58:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=919
03/17/2022 16:58:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=923
03/17/2022 16:58:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=926
03/17/2022 16:58:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
03/17/2022 16:58:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=933
03/17/2022 16:58:49 - INFO - __main__ - Global step 2800 Train loss 0.04 ACC 0.5625 on epoch=933
03/17/2022 16:58:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=936
03/17/2022 16:58:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
03/17/2022 16:58:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=943
03/17/2022 16:59:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
03/17/2022 16:59:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
03/17/2022 16:59:04 - INFO - __main__ - Global step 2850 Train loss 0.04 ACC 0.375 on epoch=949
03/17/2022 16:59:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
03/17/2022 16:59:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
03/17/2022 16:59:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=959
03/17/2022 16:59:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=963
03/17/2022 16:59:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/17/2022 16:59:19 - INFO - __main__ - Global step 2900 Train loss 0.04 ACC 0.5 on epoch=966
03/17/2022 16:59:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=969
03/17/2022 16:59:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=973
03/17/2022 16:59:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=976
03/17/2022 16:59:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=979
03/17/2022 16:59:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=983
03/17/2022 16:59:34 - INFO - __main__ - Global step 2950 Train loss 0.07 ACC 0.375 on epoch=983
03/17/2022 16:59:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
03/17/2022 16:59:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=989
03/17/2022 16:59:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
03/17/2022 16:59:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/17/2022 16:59:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
03/17/2022 16:59:49 - INFO - __main__ - Global step 3000 Train loss 0.04 ACC 0.40625 on epoch=999
03/17/2022 16:59:49 - INFO - __main__ - save last model!
03/17/2022 16:59:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 16:59:49 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 16:59:49 - INFO - __main__ - Printing 3 examples
03/17/2022 16:59:49 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 16:59:49 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:49 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 16:59:49 - INFO - __main__ - ['neutral']
03/17/2022 16:59:49 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 16:59:49 - INFO - __main__ - ['entailment']
03/17/2022 16:59:49 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:59:49 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:59:49 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 16:59:50 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:59:50 - INFO - __main__ - Printing 3 examples
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 16:59:50 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:59:50 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:59:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:59:50 - INFO - __main__ - Printing 3 examples
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:59:50 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:50 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:59:50 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:59:50 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 16:59:51 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_13_0.3_8_predictions.txt
03/17/2022 16:59:51 - INFO - __main__ - ACC on test data: 0.5179
03/17/2022 16:59:52 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.5178571428571429
03/17/2022 16:59:52 - INFO - __main__ - Running ... prefix=superglue-cb_16_13, lr=0.2, bsz=8 ...
03/17/2022 16:59:53 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 16:59:53 - INFO - __main__ - Printing 3 examples
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 16:59:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:59:53 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 16:59:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 16:59:53 - INFO - __main__ - Printing 3 examples
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: Why should this topic matter? You talked about everything else as you usually do. Why should I feel Maelmuire is important? [SEP] hypothesis: Maelmuire is important
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: She swallowed hard, unsure if she had the nerve to go ahead. The memory of the pain in Tara's eyes last night decided her. Did he really expect her to believe that Tara was only the housekeeper? [SEP] hypothesis: Tara was only the housekeeper
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
03/17/2022 16:59:53 - INFO - __main__ - ['contradiction']
03/17/2022 16:59:53 - INFO - __main__ - Tokenizing Input ...
03/17/2022 16:59:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 16:59:53 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:00:05 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:00:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:00:06 - INFO - __main__ - Starting training!
03/17/2022 17:00:08 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:00:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:00:09 - INFO - __main__ - Starting training!
03/17/2022 17:00:12 - INFO - __main__ - Step 10 Global step 10 Train loss 6.91 on epoch=3
03/17/2022 17:00:15 - INFO - __main__ - Step 20 Global step 20 Train loss 6.52 on epoch=6
03/17/2022 17:00:18 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=9
03/17/2022 17:00:21 - INFO - __main__ - Step 40 Global step 40 Train loss 7.36 on epoch=13
03/17/2022 17:00:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.39 on epoch=16
03/17/2022 17:00:42 - INFO - __main__ - Global step 50 Train loss 6.80 ACC 0.0 on epoch=16
03/17/2022 17:00:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 17:00:45 - INFO - __main__ - Step 60 Global step 60 Train loss 6.35 on epoch=19
03/17/2022 17:00:48 - INFO - __main__ - Step 70 Global step 70 Train loss 6.27 on epoch=23
03/17/2022 17:00:50 - INFO - __main__ - Step 80 Global step 80 Train loss 6.10 on epoch=26
03/17/2022 17:00:53 - INFO - __main__ - Step 90 Global step 90 Train loss 6.16 on epoch=29
03/17/2022 17:00:56 - INFO - __main__ - Step 100 Global step 100 Train loss 6.24 on epoch=33
03/17/2022 17:01:12 - INFO - __main__ - Global step 100 Train loss 6.22 ACC 0.0 on epoch=33
03/17/2022 17:01:15 - INFO - __main__ - Step 110 Global step 110 Train loss 6.06 on epoch=36
03/17/2022 17:01:18 - INFO - __main__ - Step 120 Global step 120 Train loss 6.02 on epoch=39
03/17/2022 17:01:20 - INFO - __main__ - Step 130 Global step 130 Train loss 5.89 on epoch=43
03/17/2022 17:01:23 - INFO - __main__ - Step 140 Global step 140 Train loss 5.85 on epoch=46
03/17/2022 17:01:26 - INFO - __main__ - Step 150 Global step 150 Train loss 5.81 on epoch=49
03/17/2022 17:01:42 - INFO - __main__ - Global step 150 Train loss 5.93 ACC 0.0 on epoch=49
03/17/2022 17:01:45 - INFO - __main__ - Step 160 Global step 160 Train loss 5.70 on epoch=53
03/17/2022 17:01:47 - INFO - __main__ - Step 170 Global step 170 Train loss 5.59 on epoch=56
03/17/2022 17:01:50 - INFO - __main__ - Step 180 Global step 180 Train loss 5.48 on epoch=59
03/17/2022 17:01:53 - INFO - __main__ - Step 190 Global step 190 Train loss 5.40 on epoch=63
03/17/2022 17:01:56 - INFO - __main__ - Step 200 Global step 200 Train loss 5.21 on epoch=66
03/17/2022 17:02:13 - INFO - __main__ - Global step 200 Train loss 5.48 ACC 0.0 on epoch=66
03/17/2022 17:02:16 - INFO - __main__ - Step 210 Global step 210 Train loss 5.13 on epoch=69
03/17/2022 17:02:18 - INFO - __main__ - Step 220 Global step 220 Train loss 5.09 on epoch=73
03/17/2022 17:02:21 - INFO - __main__ - Step 230 Global step 230 Train loss 4.90 on epoch=76
03/17/2022 17:02:24 - INFO - __main__ - Step 240 Global step 240 Train loss 4.78 on epoch=79
03/17/2022 17:02:27 - INFO - __main__ - Step 250 Global step 250 Train loss 4.64 on epoch=83
03/17/2022 17:02:42 - INFO - __main__ - Global step 250 Train loss 4.91 ACC 0.0 on epoch=83
03/17/2022 17:02:45 - INFO - __main__ - Step 260 Global step 260 Train loss 4.48 on epoch=86
03/17/2022 17:02:48 - INFO - __main__ - Step 270 Global step 270 Train loss 4.33 on epoch=89
03/17/2022 17:02:51 - INFO - __main__ - Step 280 Global step 280 Train loss 4.17 on epoch=93
03/17/2022 17:02:53 - INFO - __main__ - Step 290 Global step 290 Train loss 3.81 on epoch=96
03/17/2022 17:02:56 - INFO - __main__ - Step 300 Global step 300 Train loss 3.56 on epoch=99
03/17/2022 17:03:00 - INFO - __main__ - Global step 300 Train loss 4.07 ACC 0.28125 on epoch=99
03/17/2022 17:03:00 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.28125 on epoch=99, global_step=300
03/17/2022 17:03:03 - INFO - __main__ - Step 310 Global step 310 Train loss 3.31 on epoch=103
03/17/2022 17:03:05 - INFO - __main__ - Step 320 Global step 320 Train loss 3.01 on epoch=106
03/17/2022 17:03:08 - INFO - __main__ - Step 330 Global step 330 Train loss 2.74 on epoch=109
03/17/2022 17:03:11 - INFO - __main__ - Step 340 Global step 340 Train loss 2.56 on epoch=113
03/17/2022 17:03:14 - INFO - __main__ - Step 350 Global step 350 Train loss 2.34 on epoch=116
03/17/2022 17:03:15 - INFO - __main__ - Global step 350 Train loss 2.79 ACC 0.0 on epoch=116
03/17/2022 17:03:18 - INFO - __main__ - Step 360 Global step 360 Train loss 2.18 on epoch=119
03/17/2022 17:03:20 - INFO - __main__ - Step 370 Global step 370 Train loss 2.20 on epoch=123
03/17/2022 17:03:23 - INFO - __main__ - Step 380 Global step 380 Train loss 2.06 on epoch=126
03/17/2022 17:03:26 - INFO - __main__ - Step 390 Global step 390 Train loss 1.92 on epoch=129
03/17/2022 17:03:28 - INFO - __main__ - Step 400 Global step 400 Train loss 1.89 on epoch=133
03/17/2022 17:03:30 - INFO - __main__ - Global step 400 Train loss 2.05 ACC 0.40625 on epoch=133
03/17/2022 17:03:30 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.40625 on epoch=133, global_step=400
03/17/2022 17:03:32 - INFO - __main__ - Step 410 Global step 410 Train loss 1.91 on epoch=136
03/17/2022 17:03:35 - INFO - __main__ - Step 420 Global step 420 Train loss 1.70 on epoch=139
03/17/2022 17:03:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.64 on epoch=143
03/17/2022 17:03:41 - INFO - __main__ - Step 440 Global step 440 Train loss 1.54 on epoch=146
03/17/2022 17:03:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=149
03/17/2022 17:03:44 - INFO - __main__ - Global step 450 Train loss 1.66 ACC 0.5 on epoch=149
03/17/2022 17:03:44 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=149, global_step=450
03/17/2022 17:03:47 - INFO - __main__ - Step 460 Global step 460 Train loss 1.42 on epoch=153
03/17/2022 17:03:49 - INFO - __main__ - Step 470 Global step 470 Train loss 1.22 on epoch=156
03/17/2022 17:03:52 - INFO - __main__ - Step 480 Global step 480 Train loss 1.25 on epoch=159
03/17/2022 17:03:55 - INFO - __main__ - Step 490 Global step 490 Train loss 1.09 on epoch=163
03/17/2022 17:03:58 - INFO - __main__ - Step 500 Global step 500 Train loss 1.05 on epoch=166
03/17/2022 17:03:59 - INFO - __main__ - Global step 500 Train loss 1.20 ACC 0.5 on epoch=166
03/17/2022 17:04:01 - INFO - __main__ - Step 510 Global step 510 Train loss 1.09 on epoch=169
03/17/2022 17:04:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.94 on epoch=173
03/17/2022 17:04:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=176
03/17/2022 17:04:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=179
03/17/2022 17:04:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=183
03/17/2022 17:04:13 - INFO - __main__ - Global step 550 Train loss 0.91 ACC 0.5 on epoch=183
03/17/2022 17:04:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=186
03/17/2022 17:04:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=189
03/17/2022 17:04:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.72 on epoch=193
03/17/2022 17:04:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.74 on epoch=196
03/17/2022 17:04:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.76 on epoch=199
03/17/2022 17:04:28 - INFO - __main__ - Global step 600 Train loss 0.76 ACC 0.5 on epoch=199
03/17/2022 17:04:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.63 on epoch=203
03/17/2022 17:04:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=206
03/17/2022 17:04:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=209
03/17/2022 17:04:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.74 on epoch=213
03/17/2022 17:04:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.66 on epoch=216
03/17/2022 17:04:42 - INFO - __main__ - Global step 650 Train loss 0.70 ACC 0.5 on epoch=216
03/17/2022 17:04:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.56 on epoch=219
03/17/2022 17:04:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.61 on epoch=223
03/17/2022 17:04:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=226
03/17/2022 17:04:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.64 on epoch=229
03/17/2022 17:04:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=233
03/17/2022 17:04:57 - INFO - __main__ - Global step 700 Train loss 0.61 ACC 0.40625 on epoch=233
03/17/2022 17:05:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=236
03/17/2022 17:05:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.59 on epoch=239
03/17/2022 17:05:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.65 on epoch=243
03/17/2022 17:05:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=246
03/17/2022 17:05:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=249
03/17/2022 17:05:11 - INFO - __main__ - Global step 750 Train loss 0.58 ACC 0.40625 on epoch=249
03/17/2022 17:05:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=253
03/17/2022 17:05:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.59 on epoch=256
03/17/2022 17:05:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=259
03/17/2022 17:05:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=263
03/17/2022 17:05:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=266
03/17/2022 17:05:26 - INFO - __main__ - Global step 800 Train loss 0.57 ACC 0.5 on epoch=266
03/17/2022 17:05:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=269
03/17/2022 17:05:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=273
03/17/2022 17:05:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=276
03/17/2022 17:05:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=279
03/17/2022 17:05:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=283
03/17/2022 17:05:40 - INFO - __main__ - Global step 850 Train loss 0.56 ACC 0.21875 on epoch=283
03/17/2022 17:05:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=286
03/17/2022 17:05:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=289
03/17/2022 17:05:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=293
03/17/2022 17:05:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=296
03/17/2022 17:05:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.59 on epoch=299
03/17/2022 17:05:55 - INFO - __main__ - Global step 900 Train loss 0.55 ACC 0.5 on epoch=299
03/17/2022 17:05:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=303
03/17/2022 17:06:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=306
03/17/2022 17:06:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=309
03/17/2022 17:06:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=313
03/17/2022 17:06:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=316
03/17/2022 17:06:09 - INFO - __main__ - Global step 950 Train loss 0.49 ACC 0.5 on epoch=316
03/17/2022 17:06:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=319
03/17/2022 17:06:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.57 on epoch=323
03/17/2022 17:06:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=326
03/17/2022 17:06:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=329
03/17/2022 17:06:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=333
03/17/2022 17:06:23 - INFO - __main__ - Global step 1000 Train loss 0.52 ACC 0.3125 on epoch=333
03/17/2022 17:06:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=336
03/17/2022 17:06:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=339
03/17/2022 17:06:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=343
03/17/2022 17:06:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=346
03/17/2022 17:06:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=349
03/17/2022 17:06:38 - INFO - __main__ - Global step 1050 Train loss 0.49 ACC 0.5 on epoch=349
03/17/2022 17:06:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=353
03/17/2022 17:06:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.62 on epoch=356
03/17/2022 17:06:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=359
03/17/2022 17:06:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=363
03/17/2022 17:06:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=366
03/17/2022 17:06:53 - INFO - __main__ - Global step 1100 Train loss 0.55 ACC 0.5 on epoch=366
03/17/2022 17:06:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=369
03/17/2022 17:06:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=373
03/17/2022 17:07:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=376
03/17/2022 17:07:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=379
03/17/2022 17:07:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.53 on epoch=383
03/17/2022 17:07:07 - INFO - __main__ - Global step 1150 Train loss 0.49 ACC 0.21875 on epoch=383
03/17/2022 17:07:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.51 on epoch=386
03/17/2022 17:07:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=389
03/17/2022 17:07:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=393
03/17/2022 17:07:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=396
03/17/2022 17:07:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=399
03/17/2022 17:07:22 - INFO - __main__ - Global step 1200 Train loss 0.49 ACC 0.375 on epoch=399
03/17/2022 17:07:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.51 on epoch=403
03/17/2022 17:07:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=406
03/17/2022 17:07:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=409
03/17/2022 17:07:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=413
03/17/2022 17:07:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=416
03/17/2022 17:07:36 - INFO - __main__ - Global step 1250 Train loss 0.46 ACC 0.125 on epoch=416
03/17/2022 17:07:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=419
03/17/2022 17:07:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=423
03/17/2022 17:07:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=426
03/17/2022 17:07:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=429
03/17/2022 17:07:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.52 on epoch=433
03/17/2022 17:07:51 - INFO - __main__ - Global step 1300 Train loss 0.48 ACC 0.03125 on epoch=433
03/17/2022 17:07:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=436
03/17/2022 17:07:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=439
03/17/2022 17:07:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=443
03/17/2022 17:08:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=446
03/17/2022 17:08:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=449
03/17/2022 17:08:05 - INFO - __main__ - Global step 1350 Train loss 0.45 ACC 0.4375 on epoch=449
03/17/2022 17:08:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=453
03/17/2022 17:08:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=456
03/17/2022 17:08:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=459
03/17/2022 17:08:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=463
03/17/2022 17:08:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=466
03/17/2022 17:08:20 - INFO - __main__ - Global step 1400 Train loss 0.45 ACC 0.09375 on epoch=466
03/17/2022 17:08:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=469
03/17/2022 17:08:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=473
03/17/2022 17:08:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=476
03/17/2022 17:08:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=479
03/17/2022 17:08:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=483
03/17/2022 17:08:34 - INFO - __main__ - Global step 1450 Train loss 0.46 ACC 0.03125 on epoch=483
03/17/2022 17:08:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=486
03/17/2022 17:08:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=489
03/17/2022 17:08:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=493
03/17/2022 17:08:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=496
03/17/2022 17:08:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=499
03/17/2022 17:08:49 - INFO - __main__ - Global step 1500 Train loss 0.41 ACC 0.0625 on epoch=499
03/17/2022 17:08:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=503
03/17/2022 17:08:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=506
03/17/2022 17:08:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=509
03/17/2022 17:09:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=513
03/17/2022 17:09:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=516
03/17/2022 17:09:03 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.1875 on epoch=516
03/17/2022 17:09:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=519
03/17/2022 17:09:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=523
03/17/2022 17:09:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=526
03/17/2022 17:09:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=529
03/17/2022 17:09:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=533
03/17/2022 17:09:18 - INFO - __main__ - Global step 1600 Train loss 0.42 ACC 0.125 on epoch=533
03/17/2022 17:09:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=536
03/17/2022 17:09:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=539
03/17/2022 17:09:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=543
03/17/2022 17:09:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=546
03/17/2022 17:09:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=549
03/17/2022 17:09:32 - INFO - __main__ - Global step 1650 Train loss 0.42 ACC 0.3125 on epoch=549
03/17/2022 17:09:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.52 on epoch=553
03/17/2022 17:09:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=556
03/17/2022 17:09:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=559
03/17/2022 17:09:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=563
03/17/2022 17:09:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=566
03/17/2022 17:09:47 - INFO - __main__ - Global step 1700 Train loss 0.45 ACC 0.125 on epoch=566
03/17/2022 17:09:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=569
03/17/2022 17:09:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=573
03/17/2022 17:09:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=576
03/17/2022 17:09:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=579
03/17/2022 17:10:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=583
03/17/2022 17:10:01 - INFO - __main__ - Global step 1750 Train loss 0.42 ACC 0.03125 on epoch=583
03/17/2022 17:10:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=586
03/17/2022 17:10:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=589
03/17/2022 17:10:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=593
03/17/2022 17:10:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=596
03/17/2022 17:10:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=599
03/17/2022 17:10:16 - INFO - __main__ - Global step 1800 Train loss 0.41 ACC 0.3125 on epoch=599
03/17/2022 17:10:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=603
03/17/2022 17:10:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=606
03/17/2022 17:10:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=609
03/17/2022 17:10:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=613
03/17/2022 17:10:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=616
03/17/2022 17:10:30 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.125 on epoch=616
03/17/2022 17:10:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=619
03/17/2022 17:10:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=623
03/17/2022 17:10:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=626
03/17/2022 17:10:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=629
03/17/2022 17:10:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=633
03/17/2022 17:10:45 - INFO - __main__ - Global step 1900 Train loss 0.40 ACC 0.03125 on epoch=633
03/17/2022 17:10:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=636
03/17/2022 17:10:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=639
03/17/2022 17:10:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=643
03/17/2022 17:10:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=646
03/17/2022 17:10:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=649
03/17/2022 17:10:59 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.25 on epoch=649
03/17/2022 17:11:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=653
03/17/2022 17:11:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=656
03/17/2022 17:11:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=659
03/17/2022 17:11:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=663
03/17/2022 17:11:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=666
03/17/2022 17:11:14 - INFO - __main__ - Global step 2000 Train loss 0.38 ACC 0.21875 on epoch=666
03/17/2022 17:11:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=669
03/17/2022 17:11:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=673
03/17/2022 17:11:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.34 on epoch=676
03/17/2022 17:11:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=679
03/17/2022 17:11:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=683
03/17/2022 17:11:29 - INFO - __main__ - Global step 2050 Train loss 0.36 ACC 0.03125 on epoch=683
03/17/2022 17:11:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=686
03/17/2022 17:11:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=689
03/17/2022 17:11:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=693
03/17/2022 17:11:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=696
03/17/2022 17:11:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=699
03/17/2022 17:11:43 - INFO - __main__ - Global step 2100 Train loss 0.36 ACC 0.1875 on epoch=699
03/17/2022 17:11:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=703
03/17/2022 17:11:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.32 on epoch=706
03/17/2022 17:11:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=709
03/17/2022 17:11:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=713
03/17/2022 17:11:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=716
03/17/2022 17:11:58 - INFO - __main__ - Global step 2150 Train loss 0.33 ACC 0.0625 on epoch=716
03/17/2022 17:12:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=719
03/17/2022 17:12:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.33 on epoch=723
03/17/2022 17:12:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=726
03/17/2022 17:12:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.34 on epoch=729
03/17/2022 17:12:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.29 on epoch=733
03/17/2022 17:12:12 - INFO - __main__ - Global step 2200 Train loss 0.31 ACC 0.15625 on epoch=733
03/17/2022 17:12:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=736
03/17/2022 17:12:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=739
03/17/2022 17:12:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=743
03/17/2022 17:12:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=746
03/17/2022 17:12:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=749
03/17/2022 17:12:27 - INFO - __main__ - Global step 2250 Train loss 0.30 ACC 0.15625 on epoch=749
03/17/2022 17:12:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.28 on epoch=753
03/17/2022 17:12:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=756
03/17/2022 17:12:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=759
03/17/2022 17:12:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.32 on epoch=763
03/17/2022 17:12:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=766
03/17/2022 17:12:41 - INFO - __main__ - Global step 2300 Train loss 0.30 ACC 0.25 on epoch=766
03/17/2022 17:12:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=769
03/17/2022 17:12:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=773
03/17/2022 17:12:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=776
03/17/2022 17:12:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=779
03/17/2022 17:12:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.32 on epoch=783
03/17/2022 17:12:56 - INFO - __main__ - Global step 2350 Train loss 0.28 ACC 0.15625 on epoch=783
03/17/2022 17:12:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=786
03/17/2022 17:13:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=789
03/17/2022 17:13:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.25 on epoch=793
03/17/2022 17:13:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=796
03/17/2022 17:13:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=799
03/17/2022 17:13:11 - INFO - __main__ - Global step 2400 Train loss 0.24 ACC 0.3125 on epoch=799
03/17/2022 17:13:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=803
03/17/2022 17:13:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=806
03/17/2022 17:13:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=809
03/17/2022 17:13:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=813
03/17/2022 17:13:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=816
03/17/2022 17:13:25 - INFO - __main__ - Global step 2450 Train loss 0.20 ACC 0.28125 on epoch=816
03/17/2022 17:13:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=819
03/17/2022 17:13:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=823
03/17/2022 17:13:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=826
03/17/2022 17:13:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=829
03/17/2022 17:13:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=833
03/17/2022 17:13:40 - INFO - __main__ - Global step 2500 Train loss 0.20 ACC 0.1875 on epoch=833
03/17/2022 17:13:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=836
03/17/2022 17:13:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=839
03/17/2022 17:13:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=843
03/17/2022 17:13:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.20 on epoch=846
03/17/2022 17:13:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=849
03/17/2022 17:13:55 - INFO - __main__ - Global step 2550 Train loss 0.18 ACC 0.28125 on epoch=849
03/17/2022 17:13:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=853
03/17/2022 17:14:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=856
03/17/2022 17:14:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=859
03/17/2022 17:14:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=863
03/17/2022 17:14:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=866
03/17/2022 17:14:09 - INFO - __main__ - Global step 2600 Train loss 0.15 ACC 0.1875 on epoch=866
03/17/2022 17:14:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=869
03/17/2022 17:14:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=873
03/17/2022 17:14:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=876
03/17/2022 17:14:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=879
03/17/2022 17:14:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=883
03/17/2022 17:14:24 - INFO - __main__ - Global step 2650 Train loss 0.15 ACC 0.1875 on epoch=883
03/17/2022 17:14:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=886
03/17/2022 17:14:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=889
03/17/2022 17:14:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=893
03/17/2022 17:14:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=896
03/17/2022 17:14:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=899
03/17/2022 17:14:39 - INFO - __main__ - Global step 2700 Train loss 0.13 ACC 0.21875 on epoch=899
03/17/2022 17:14:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=903
03/17/2022 17:14:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=906
03/17/2022 17:14:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=909
03/17/2022 17:14:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=913
03/17/2022 17:14:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=916
03/17/2022 17:14:53 - INFO - __main__ - Global step 2750 Train loss 0.11 ACC 0.125 on epoch=916
03/17/2022 17:14:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=919
03/17/2022 17:14:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=923
03/17/2022 17:15:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=926
03/17/2022 17:15:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=929
03/17/2022 17:15:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=933
03/17/2022 17:15:08 - INFO - __main__ - Global step 2800 Train loss 0.08 ACC 0.1875 on epoch=933
03/17/2022 17:15:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=936
03/17/2022 17:15:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=939
03/17/2022 17:15:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=943
03/17/2022 17:15:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=946
03/17/2022 17:15:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=949
03/17/2022 17:15:22 - INFO - __main__ - Global step 2850 Train loss 0.08 ACC 0.25 on epoch=949
03/17/2022 17:15:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=953
03/17/2022 17:15:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
03/17/2022 17:15:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
03/17/2022 17:15:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=963
03/17/2022 17:15:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=966
03/17/2022 17:15:37 - INFO - __main__ - Global step 2900 Train loss 0.09 ACC 0.15625 on epoch=966
03/17/2022 17:15:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=969
03/17/2022 17:15:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=973
03/17/2022 17:15:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=976
03/17/2022 17:15:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
03/17/2022 17:15:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=983
03/17/2022 17:15:51 - INFO - __main__ - Global step 2950 Train loss 0.06 ACC 0.25 on epoch=983
03/17/2022 17:15:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
03/17/2022 17:15:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=989
03/17/2022 17:16:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=993
03/17/2022 17:16:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=996
03/17/2022 17:16:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=999
03/17/2022 17:16:06 - INFO - __main__ - Global step 3000 Train loss 0.06 ACC 0.1875 on epoch=999
03/17/2022 17:16:06 - INFO - __main__ - save last model!
03/17/2022 17:16:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 17:16:06 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 17:16:06 - INFO - __main__ - Printing 3 examples
03/17/2022 17:16:06 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 17:16:06 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:06 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 17:16:06 - INFO - __main__ - ['neutral']
03/17/2022 17:16:06 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 17:16:06 - INFO - __main__ - ['entailment']
03/17/2022 17:16:06 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:16:06 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:16:06 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 17:16:07 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:16:07 - INFO - __main__ - Printing 3 examples
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 17:16:07 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:16:07 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:16:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:16:07 - INFO - __main__ - Printing 3 examples
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:16:07 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:07 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:16:07 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:16:07 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:16:09 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_13_0.2_8_predictions.txt
03/17/2022 17:16:09 - INFO - __main__ - ACC on test data: 0.3750
03/17/2022 17:16:09 - INFO - __main__ - prefix=superglue-cb_16_13, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.375
03/17/2022 17:16:09 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.5, bsz=8 ...
03/17/2022 17:16:10 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:16:10 - INFO - __main__ - Printing 3 examples
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 17:16:10 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:16:10 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:16:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:16:10 - INFO - __main__ - Printing 3 examples
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:16:10 - INFO - __main__ - ['contradiction']
03/17/2022 17:16:10 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:16:10 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:16:10 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:16:25 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:16:26 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:16:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:16:26 - INFO - __main__ - Starting training!
03/17/2022 17:16:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:16:27 - INFO - __main__ - Starting training!
03/17/2022 17:16:31 - INFO - __main__ - Step 10 Global step 10 Train loss 5.77 on epoch=3
03/17/2022 17:16:34 - INFO - __main__ - Step 20 Global step 20 Train loss 4.12 on epoch=6
03/17/2022 17:16:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.68 on epoch=9
03/17/2022 17:16:40 - INFO - __main__ - Step 40 Global step 40 Train loss 2.93 on epoch=13
03/17/2022 17:16:43 - INFO - __main__ - Step 50 Global step 50 Train loss 2.73 on epoch=16
03/17/2022 17:16:45 - INFO - __main__ - Global step 50 Train loss 3.84 ACC 0.40625 on epoch=16
03/17/2022 17:16:45 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.40625 on epoch=16, global_step=50
03/17/2022 17:16:48 - INFO - __main__ - Step 60 Global step 60 Train loss 2.52 on epoch=19
03/17/2022 17:16:51 - INFO - __main__ - Step 70 Global step 70 Train loss 2.21 on epoch=23
03/17/2022 17:16:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.12 on epoch=26
03/17/2022 17:16:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.90 on epoch=29
03/17/2022 17:17:00 - INFO - __main__ - Step 100 Global step 100 Train loss 1.64 on epoch=33
03/17/2022 17:17:01 - INFO - __main__ - Global step 100 Train loss 2.08 ACC 0.21875 on epoch=33
03/17/2022 17:17:04 - INFO - __main__ - Step 110 Global step 110 Train loss 1.47 on epoch=36
03/17/2022 17:17:07 - INFO - __main__ - Step 120 Global step 120 Train loss 1.34 on epoch=39
03/17/2022 17:17:10 - INFO - __main__ - Step 130 Global step 130 Train loss 1.08 on epoch=43
03/17/2022 17:17:13 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=46
03/17/2022 17:17:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=49
03/17/2022 17:17:17 - INFO - __main__ - Global step 150 Train loss 1.19 ACC 0.5 on epoch=49
03/17/2022 17:17:17 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=49, global_step=150
03/17/2022 17:17:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=53
03/17/2022 17:17:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=56
03/17/2022 17:17:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=59
03/17/2022 17:17:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=63
03/17/2022 17:17:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.60 on epoch=66
03/17/2022 17:17:33 - INFO - __main__ - Global step 200 Train loss 0.71 ACC 0.21875 on epoch=66
03/17/2022 17:17:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=69
03/17/2022 17:17:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=73
03/17/2022 17:17:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.60 on epoch=76
03/17/2022 17:17:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=79
03/17/2022 17:17:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
03/17/2022 17:17:50 - INFO - __main__ - Global step 250 Train loss 0.58 ACC 0.03125 on epoch=83
03/17/2022 17:17:53 - INFO - __main__ - Step 260 Global step 260 Train loss 1.26 on epoch=86
03/17/2022 17:17:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=89
03/17/2022 17:17:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=93
03/17/2022 17:18:02 - INFO - __main__ - Step 290 Global step 290 Train loss 3.40 on epoch=96
03/17/2022 17:18:05 - INFO - __main__ - Step 300 Global step 300 Train loss 2.57 on epoch=99
03/17/2022 17:18:06 - INFO - __main__ - Global step 300 Train loss 1.66 ACC 0.3125 on epoch=99
03/17/2022 17:18:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.77 on epoch=103
03/17/2022 17:18:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.28 on epoch=106
03/17/2022 17:18:15 - INFO - __main__ - Step 330 Global step 330 Train loss 1.24 on epoch=109
03/17/2022 17:18:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=113
03/17/2022 17:18:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.07 on epoch=116
03/17/2022 17:18:23 - INFO - __main__ - Global step 350 Train loss 1.26 ACC 0.28125 on epoch=116
03/17/2022 17:18:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.93 on epoch=119
03/17/2022 17:18:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.95 on epoch=123
03/17/2022 17:18:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.97 on epoch=126
03/17/2022 17:18:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=129
03/17/2022 17:18:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.79 on epoch=133
03/17/2022 17:18:39 - INFO - __main__ - Global step 400 Train loss 0.91 ACC 0.25 on epoch=133
03/17/2022 17:18:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=136
03/17/2022 17:18:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=139
03/17/2022 17:18:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=143
03/17/2022 17:18:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=146
03/17/2022 17:18:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=149
03/17/2022 17:18:55 - INFO - __main__ - Global step 450 Train loss 0.72 ACC 0.40625 on epoch=149
03/17/2022 17:18:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=153
03/17/2022 17:19:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.73 on epoch=156
03/17/2022 17:19:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=159
03/17/2022 17:19:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=163
03/17/2022 17:19:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.66 on epoch=166
03/17/2022 17:19:11 - INFO - __main__ - Global step 500 Train loss 0.71 ACC 0.34375 on epoch=166
03/17/2022 17:19:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.68 on epoch=169
03/17/2022 17:19:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.72 on epoch=173
03/17/2022 17:19:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=176
03/17/2022 17:19:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=179
03/17/2022 17:19:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=183
03/17/2022 17:19:27 - INFO - __main__ - Global step 550 Train loss 0.67 ACC 0.28125 on epoch=183
03/17/2022 17:19:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=186
03/17/2022 17:19:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=189
03/17/2022 17:19:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=193
03/17/2022 17:19:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=196
03/17/2022 17:19:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=199
03/17/2022 17:19:43 - INFO - __main__ - Global step 600 Train loss 0.59 ACC 0.15625 on epoch=199
03/17/2022 17:19:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=203
03/17/2022 17:19:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=206
03/17/2022 17:19:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=209
03/17/2022 17:19:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=213
03/17/2022 17:19:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=216
03/17/2022 17:19:59 - INFO - __main__ - Global step 650 Train loss 0.56 ACC 0.4375 on epoch=216
03/17/2022 17:20:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.61 on epoch=219
03/17/2022 17:20:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=223
03/17/2022 17:20:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.63 on epoch=226
03/17/2022 17:20:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.59 on epoch=229
03/17/2022 17:20:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=233
03/17/2022 17:20:15 - INFO - __main__ - Global step 700 Train loss 0.60 ACC 0.0 on epoch=233
03/17/2022 17:20:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=236
03/17/2022 17:20:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.67 on epoch=239
03/17/2022 17:20:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.62 on epoch=243
03/17/2022 17:20:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=246
03/17/2022 17:20:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.66 on epoch=249
03/17/2022 17:20:31 - INFO - __main__ - Global step 750 Train loss 0.61 ACC 0.5 on epoch=249
03/17/2022 17:20:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=253
03/17/2022 17:20:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.55 on epoch=256
03/17/2022 17:20:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=259
03/17/2022 17:20:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=263
03/17/2022 17:20:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=266
03/17/2022 17:20:47 - INFO - __main__ - Global step 800 Train loss 0.54 ACC 0.25 on epoch=266
03/17/2022 17:20:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=269
03/17/2022 17:20:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=273
03/17/2022 17:20:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=276
03/17/2022 17:20:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.58 on epoch=279
03/17/2022 17:21:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=283
03/17/2022 17:21:03 - INFO - __main__ - Global step 850 Train loss 0.53 ACC 0.09375 on epoch=283
03/17/2022 17:21:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.56 on epoch=286
03/17/2022 17:21:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=289
03/17/2022 17:21:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=293
03/17/2022 17:21:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=296
03/17/2022 17:21:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=299
03/17/2022 17:21:19 - INFO - __main__ - Global step 900 Train loss 0.51 ACC 0.125 on epoch=299
03/17/2022 17:21:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=303
03/17/2022 17:21:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=306
03/17/2022 17:21:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=309
03/17/2022 17:21:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=313
03/17/2022 17:21:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=316
03/17/2022 17:21:35 - INFO - __main__ - Global step 950 Train loss 0.49 ACC 0.5 on epoch=316
03/17/2022 17:21:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=319
03/17/2022 17:21:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=323
03/17/2022 17:21:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=326
03/17/2022 17:21:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.56 on epoch=329
03/17/2022 17:21:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=333
03/17/2022 17:21:51 - INFO - __main__ - Global step 1000 Train loss 0.54 ACC 0.34375 on epoch=333
03/17/2022 17:21:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.54 on epoch=336
03/17/2022 17:21:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=339
03/17/2022 17:22:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=343
03/17/2022 17:22:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=346
03/17/2022 17:22:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=349
03/17/2022 17:22:06 - INFO - __main__ - Global step 1050 Train loss 0.49 ACC 0.4375 on epoch=349
03/17/2022 17:22:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=353
03/17/2022 17:22:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=356
03/17/2022 17:22:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.57 on epoch=359
03/17/2022 17:22:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=363
03/17/2022 17:22:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=366
03/17/2022 17:22:22 - INFO - __main__ - Global step 1100 Train loss 0.49 ACC 0.09375 on epoch=366
03/17/2022 17:22:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=369
03/17/2022 17:22:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=373
03/17/2022 17:22:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=376
03/17/2022 17:22:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.55 on epoch=379
03/17/2022 17:22:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.50 on epoch=383
03/17/2022 17:22:38 - INFO - __main__ - Global step 1150 Train loss 0.50 ACC 0.4375 on epoch=383
03/17/2022 17:22:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=386
03/17/2022 17:22:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=389
03/17/2022 17:22:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=393
03/17/2022 17:22:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=396
03/17/2022 17:22:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 4.14 on epoch=399
03/17/2022 17:22:55 - INFO - __main__ - Global step 1200 Train loss 1.19 ACC 0.53125 on epoch=399
03/17/2022 17:22:55 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=1200
03/17/2022 17:22:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 4.40 on epoch=403
03/17/2022 17:23:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 5.31 on epoch=406
03/17/2022 17:23:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.52 on epoch=409
03/17/2022 17:23:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.11 on epoch=413
03/17/2022 17:23:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.62 on epoch=416
03/17/2022 17:23:11 - INFO - __main__ - Global step 1250 Train loss 3.19 ACC 0.25 on epoch=416
03/17/2022 17:23:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.54 on epoch=419
03/17/2022 17:23:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.28 on epoch=423
03/17/2022 17:23:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.22 on epoch=426
03/17/2022 17:23:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.16 on epoch=429
03/17/2022 17:23:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.18 on epoch=433
03/17/2022 17:23:27 - INFO - __main__ - Global step 1300 Train loss 1.28 ACC 0.34375 on epoch=433
03/17/2022 17:23:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.98 on epoch=436
03/17/2022 17:23:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.93 on epoch=439
03/17/2022 17:23:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.91 on epoch=443
03/17/2022 17:23:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.95 on epoch=446
03/17/2022 17:23:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.87 on epoch=449
03/17/2022 17:23:43 - INFO - __main__ - Global step 1350 Train loss 0.93 ACC 0.25 on epoch=449
03/17/2022 17:23:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.80 on epoch=453
03/17/2022 17:23:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.88 on epoch=456
03/17/2022 17:23:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.81 on epoch=459
03/17/2022 17:23:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.74 on epoch=463
03/17/2022 17:23:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.70 on epoch=466
03/17/2022 17:23:59 - INFO - __main__ - Global step 1400 Train loss 0.78 ACC 0.125 on epoch=466
03/17/2022 17:24:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.79 on epoch=469
03/17/2022 17:24:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.76 on epoch=473
03/17/2022 17:24:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.72 on epoch=476
03/17/2022 17:24:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=479
03/17/2022 17:24:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.76 on epoch=483
03/17/2022 17:24:15 - INFO - __main__ - Global step 1450 Train loss 0.75 ACC 0.15625 on epoch=483
03/17/2022 17:24:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.71 on epoch=486
03/17/2022 17:24:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.72 on epoch=489
03/17/2022 17:24:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.67 on epoch=493
03/17/2022 17:24:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.58 on epoch=496
03/17/2022 17:24:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.57 on epoch=499
03/17/2022 17:24:31 - INFO - __main__ - Global step 1500 Train loss 0.65 ACC 0.125 on epoch=499
03/17/2022 17:24:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.68 on epoch=503
03/17/2022 17:24:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.66 on epoch=506
03/17/2022 17:24:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.62 on epoch=509
03/17/2022 17:24:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.67 on epoch=513
03/17/2022 17:24:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.11 on epoch=516
03/17/2022 17:24:47 - INFO - __main__ - Global step 1550 Train loss 0.75 ACC 0.09375 on epoch=516
03/17/2022 17:24:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.76 on epoch=519
03/17/2022 17:24:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.64 on epoch=523
03/17/2022 17:24:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.67 on epoch=526
03/17/2022 17:24:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.63 on epoch=529
03/17/2022 17:25:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.52 on epoch=533
03/17/2022 17:25:03 - INFO - __main__ - Global step 1600 Train loss 0.64 ACC 0.125 on epoch=533
03/17/2022 17:25:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.67 on epoch=536
03/17/2022 17:25:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.63 on epoch=539
03/17/2022 17:25:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.64 on epoch=543
03/17/2022 17:25:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=546
03/17/2022 17:25:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.65 on epoch=549
03/17/2022 17:25:19 - INFO - __main__ - Global step 1650 Train loss 0.63 ACC 0.125 on epoch=549
03/17/2022 17:25:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.56 on epoch=553
03/17/2022 17:25:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=556
03/17/2022 17:25:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=559
03/17/2022 17:25:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.59 on epoch=563
03/17/2022 17:25:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.54 on epoch=566
03/17/2022 17:25:35 - INFO - __main__ - Global step 1700 Train loss 0.57 ACC 0.1875 on epoch=566
03/17/2022 17:25:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.55 on epoch=569
03/17/2022 17:25:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=573
03/17/2022 17:25:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=576
03/17/2022 17:25:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=579
03/17/2022 17:25:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.58 on epoch=583
03/17/2022 17:25:51 - INFO - __main__ - Global step 1750 Train loss 0.56 ACC 0.125 on epoch=583
03/17/2022 17:25:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.54 on epoch=586
03/17/2022 17:25:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=589
03/17/2022 17:26:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=593
03/17/2022 17:26:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=596
03/17/2022 17:26:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.58 on epoch=599
03/17/2022 17:26:07 - INFO - __main__ - Global step 1800 Train loss 0.55 ACC 0.1875 on epoch=599
03/17/2022 17:26:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=603
03/17/2022 17:26:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=606
03/17/2022 17:26:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.57 on epoch=609
03/17/2022 17:26:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.56 on epoch=613
03/17/2022 17:26:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.57 on epoch=616
03/17/2022 17:26:23 - INFO - __main__ - Global step 1850 Train loss 0.57 ACC 0.21875 on epoch=616
03/17/2022 17:26:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.54 on epoch=619
03/17/2022 17:26:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.56 on epoch=623
03/17/2022 17:26:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=626
03/17/2022 17:26:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.55 on epoch=629
03/17/2022 17:26:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.56 on epoch=633
03/17/2022 17:26:39 - INFO - __main__ - Global step 1900 Train loss 0.54 ACC 0.25 on epoch=633
03/17/2022 17:26:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=636
03/17/2022 17:26:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.52 on epoch=639
03/17/2022 17:26:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=643
03/17/2022 17:26:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=646
03/17/2022 17:26:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.51 on epoch=649
03/17/2022 17:26:55 - INFO - __main__ - Global step 1950 Train loss 0.51 ACC 0.25 on epoch=649
03/17/2022 17:26:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.53 on epoch=653
03/17/2022 17:27:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.60 on epoch=656
03/17/2022 17:27:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.54 on epoch=659
03/17/2022 17:27:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.49 on epoch=663
03/17/2022 17:27:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=666
03/17/2022 17:27:11 - INFO - __main__ - Global step 2000 Train loss 0.53 ACC 0.15625 on epoch=666
03/17/2022 17:27:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.58 on epoch=669
03/17/2022 17:27:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.50 on epoch=673
03/17/2022 17:27:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.47 on epoch=676
03/17/2022 17:27:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.50 on epoch=679
03/17/2022 17:27:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.52 on epoch=683
03/17/2022 17:27:27 - INFO - __main__ - Global step 2050 Train loss 0.52 ACC 0.25 on epoch=683
03/17/2022 17:27:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.50 on epoch=686
03/17/2022 17:27:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.51 on epoch=689
03/17/2022 17:27:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.49 on epoch=693
03/17/2022 17:27:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.52 on epoch=696
03/17/2022 17:27:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.49 on epoch=699
03/17/2022 17:27:43 - INFO - __main__ - Global step 2100 Train loss 0.50 ACC 0.25 on epoch=699
03/17/2022 17:27:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.53 on epoch=703
03/17/2022 17:27:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=706
03/17/2022 17:27:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.52 on epoch=709
03/17/2022 17:27:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.53 on epoch=713
03/17/2022 17:27:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.50 on epoch=716
03/17/2022 17:27:59 - INFO - __main__ - Global step 2150 Train loss 0.50 ACC 0.3125 on epoch=716
03/17/2022 17:28:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.50 on epoch=719
03/17/2022 17:28:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.54 on epoch=723
03/17/2022 17:28:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=726
03/17/2022 17:28:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.49 on epoch=729
03/17/2022 17:28:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=733
03/17/2022 17:28:15 - INFO - __main__ - Global step 2200 Train loss 0.48 ACC 0.375 on epoch=733
03/17/2022 17:28:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.54 on epoch=736
03/17/2022 17:28:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.54 on epoch=739
03/17/2022 17:28:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.50 on epoch=743
03/17/2022 17:28:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.48 on epoch=746
03/17/2022 17:28:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.47 on epoch=749
03/17/2022 17:28:31 - INFO - __main__ - Global step 2250 Train loss 0.51 ACC 0.25 on epoch=749
03/17/2022 17:28:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.53 on epoch=753
03/17/2022 17:28:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=756
03/17/2022 17:28:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.50 on epoch=759
03/17/2022 17:28:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.49 on epoch=763
03/17/2022 17:28:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.51 on epoch=766
03/17/2022 17:28:48 - INFO - __main__ - Global step 2300 Train loss 0.49 ACC 0.375 on epoch=766
03/17/2022 17:28:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.56 on epoch=769
03/17/2022 17:28:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.51 on epoch=773
03/17/2022 17:28:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.49 on epoch=776
03/17/2022 17:29:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.50 on epoch=779
03/17/2022 17:29:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.53 on epoch=783
03/17/2022 17:29:04 - INFO - __main__ - Global step 2350 Train loss 0.52 ACC 0.21875 on epoch=783
03/17/2022 17:29:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.47 on epoch=786
03/17/2022 17:29:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.55 on epoch=789
03/17/2022 17:29:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.54 on epoch=793
03/17/2022 17:29:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.52 on epoch=796
03/17/2022 17:29:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.50 on epoch=799
03/17/2022 17:29:20 - INFO - __main__ - Global step 2400 Train loss 0.52 ACC 0.25 on epoch=799
03/17/2022 17:29:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.47 on epoch=803
03/17/2022 17:29:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.48 on epoch=806
03/17/2022 17:29:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.49 on epoch=809
03/17/2022 17:29:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.54 on epoch=813
03/17/2022 17:29:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.45 on epoch=816
03/17/2022 17:29:36 - INFO - __main__ - Global step 2450 Train loss 0.49 ACC 0.25 on epoch=816
03/17/2022 17:29:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.52 on epoch=819
03/17/2022 17:29:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=823
03/17/2022 17:29:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=826
03/17/2022 17:29:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.49 on epoch=829
03/17/2022 17:29:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=833
03/17/2022 17:29:52 - INFO - __main__ - Global step 2500 Train loss 0.47 ACC 0.375 on epoch=833
03/17/2022 17:29:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.50 on epoch=836
03/17/2022 17:29:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.51 on epoch=839
03/17/2022 17:30:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.51 on epoch=843
03/17/2022 17:30:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.51 on epoch=846
03/17/2022 17:30:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.54 on epoch=849
03/17/2022 17:30:09 - INFO - __main__ - Global step 2550 Train loss 0.51 ACC 0.3125 on epoch=849
03/17/2022 17:30:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.48 on epoch=853
03/17/2022 17:30:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.50 on epoch=856
03/17/2022 17:30:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.52 on epoch=859
03/17/2022 17:30:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=863
03/17/2022 17:30:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=866
03/17/2022 17:30:25 - INFO - __main__ - Global step 2600 Train loss 0.47 ACC 0.21875 on epoch=866
03/17/2022 17:30:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.54 on epoch=869
03/17/2022 17:30:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.48 on epoch=873
03/17/2022 17:30:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.52 on epoch=876
03/17/2022 17:30:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.48 on epoch=879
03/17/2022 17:30:40 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.46 on epoch=883
03/17/2022 17:30:41 - INFO - __main__ - Global step 2650 Train loss 0.49 ACC 0.34375 on epoch=883
03/17/2022 17:30:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.48 on epoch=886
03/17/2022 17:30:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.46 on epoch=889
03/17/2022 17:30:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.45 on epoch=893
03/17/2022 17:30:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=896
03/17/2022 17:30:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.47 on epoch=899
03/17/2022 17:30:57 - INFO - __main__ - Global step 2700 Train loss 0.46 ACC 0.34375 on epoch=899
03/17/2022 17:31:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.48 on epoch=903
03/17/2022 17:31:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.46 on epoch=906
03/17/2022 17:31:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.45 on epoch=909
03/17/2022 17:31:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=913
03/17/2022 17:31:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.47 on epoch=916
03/17/2022 17:31:13 - INFO - __main__ - Global step 2750 Train loss 0.46 ACC 0.4375 on epoch=916
03/17/2022 17:31:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.47 on epoch=919
03/17/2022 17:31:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=923
03/17/2022 17:31:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.48 on epoch=926
03/17/2022 17:31:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.53 on epoch=929
03/17/2022 17:31:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=933
03/17/2022 17:31:30 - INFO - __main__ - Global step 2800 Train loss 0.47 ACC 0.1875 on epoch=933
03/17/2022 17:31:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=936
03/17/2022 17:31:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.50 on epoch=939
03/17/2022 17:31:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=943
03/17/2022 17:31:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.50 on epoch=946
03/17/2022 17:31:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.50 on epoch=949
03/17/2022 17:31:46 - INFO - __main__ - Global step 2850 Train loss 0.47 ACC 0.21875 on epoch=949
03/17/2022 17:31:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.42 on epoch=953
03/17/2022 17:31:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.47 on epoch=956
03/17/2022 17:31:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.47 on epoch=959
03/17/2022 17:31:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=963
03/17/2022 17:32:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.48 on epoch=966
03/17/2022 17:32:02 - INFO - __main__ - Global step 2900 Train loss 0.45 ACC 0.3125 on epoch=966
03/17/2022 17:32:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.53 on epoch=969
03/17/2022 17:32:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=973
03/17/2022 17:32:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.44 on epoch=976
03/17/2022 17:32:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.53 on epoch=979
03/17/2022 17:32:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.50 on epoch=983
03/17/2022 17:32:19 - INFO - __main__ - Global step 2950 Train loss 0.48 ACC 0.28125 on epoch=983
03/17/2022 17:32:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.48 on epoch=986
03/17/2022 17:32:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.46 on epoch=989
03/17/2022 17:32:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=993
03/17/2022 17:32:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.44 on epoch=996
03/17/2022 17:32:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=999
03/17/2022 17:32:35 - INFO - __main__ - Global step 3000 Train loss 0.45 ACC 0.1875 on epoch=999
03/17/2022 17:32:35 - INFO - __main__ - save last model!
03/17/2022 17:32:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 17:32:35 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 17:32:35 - INFO - __main__ - Printing 3 examples
03/17/2022 17:32:35 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 17:32:35 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:35 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 17:32:35 - INFO - __main__ - ['neutral']
03/17/2022 17:32:35 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 17:32:35 - INFO - __main__ - ['entailment']
03/17/2022 17:32:35 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:32:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:32:35 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 17:32:36 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:32:36 - INFO - __main__ - Printing 3 examples
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 17:32:36 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:32:36 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:32:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:32:36 - INFO - __main__ - Printing 3 examples
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:32:36 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:36 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:32:36 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:32:36 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:32:37 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_21_0.5_8_predictions.txt
03/17/2022 17:32:37 - INFO - __main__ - ACC on test data: 0.1964
03/17/2022 17:32:38 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.19642857142857142
03/17/2022 17:32:38 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.4, bsz=8 ...
03/17/2022 17:32:38 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:32:38 - INFO - __main__ - Printing 3 examples
03/17/2022 17:32:38 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:32:38 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:38 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:32:38 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:38 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:32:38 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 17:32:38 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:32:39 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:32:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:32:39 - INFO - __main__ - Printing 3 examples
03/17/2022 17:32:39 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:32:39 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:39 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:32:39 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:39 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:32:39 - INFO - __main__ - ['contradiction']
03/17/2022 17:32:39 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:32:39 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:32:39 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:32:55 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:32:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:32:55 - INFO - __main__ - Starting training!
03/17/2022 17:32:57 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:32:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:32:58 - INFO - __main__ - Starting training!
03/17/2022 17:33:02 - INFO - __main__ - Step 10 Global step 10 Train loss 6.71 on epoch=3
03/17/2022 17:33:05 - INFO - __main__ - Step 20 Global step 20 Train loss 6.62 on epoch=6
03/17/2022 17:33:08 - INFO - __main__ - Step 30 Global step 30 Train loss 7.21 on epoch=9
03/17/2022 17:33:11 - INFO - __main__ - Step 40 Global step 40 Train loss 6.80 on epoch=13
03/17/2022 17:33:14 - INFO - __main__ - Step 50 Global step 50 Train loss 6.78 on epoch=16
03/17/2022 17:33:32 - INFO - __main__ - Global step 50 Train loss 6.82 ACC 0.0 on epoch=16
03/17/2022 17:33:32 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 17:33:35 - INFO - __main__ - Step 60 Global step 60 Train loss 6.94 on epoch=19
03/17/2022 17:33:38 - INFO - __main__ - Step 70 Global step 70 Train loss 6.52 on epoch=23
03/17/2022 17:33:41 - INFO - __main__ - Step 80 Global step 80 Train loss 6.36 on epoch=26
03/17/2022 17:33:44 - INFO - __main__ - Step 90 Global step 90 Train loss 6.35 on epoch=29
03/17/2022 17:33:47 - INFO - __main__ - Step 100 Global step 100 Train loss 6.15 on epoch=33
03/17/2022 17:34:03 - INFO - __main__ - Global step 100 Train loss 6.46 ACC 0.0 on epoch=33
03/17/2022 17:34:06 - INFO - __main__ - Step 110 Global step 110 Train loss 6.38 on epoch=36
03/17/2022 17:34:09 - INFO - __main__ - Step 120 Global step 120 Train loss 6.29 on epoch=39
03/17/2022 17:34:12 - INFO - __main__ - Step 130 Global step 130 Train loss 6.15 on epoch=43
03/17/2022 17:34:15 - INFO - __main__ - Step 140 Global step 140 Train loss 6.10 on epoch=46
03/17/2022 17:34:19 - INFO - __main__ - Step 150 Global step 150 Train loss 6.05 on epoch=49
03/17/2022 17:34:25 - INFO - __main__ - Global step 150 Train loss 6.19 ACC 0.0 on epoch=49
03/17/2022 17:34:28 - INFO - __main__ - Step 160 Global step 160 Train loss 5.93 on epoch=53
03/17/2022 17:34:31 - INFO - __main__ - Step 170 Global step 170 Train loss 5.87 on epoch=56
03/17/2022 17:34:34 - INFO - __main__ - Step 180 Global step 180 Train loss 5.91 on epoch=59
03/17/2022 17:34:37 - INFO - __main__ - Step 190 Global step 190 Train loss 5.76 on epoch=63
03/17/2022 17:34:40 - INFO - __main__ - Step 200 Global step 200 Train loss 5.68 on epoch=66
03/17/2022 17:34:49 - INFO - __main__ - Global step 200 Train loss 5.83 ACC 0.0 on epoch=66
03/17/2022 17:34:52 - INFO - __main__ - Step 210 Global step 210 Train loss 5.70 on epoch=69
03/17/2022 17:34:55 - INFO - __main__ - Step 220 Global step 220 Train loss 5.27 on epoch=73
03/17/2022 17:34:58 - INFO - __main__ - Step 230 Global step 230 Train loss 5.27 on epoch=76
03/17/2022 17:35:01 - INFO - __main__ - Step 240 Global step 240 Train loss 5.28 on epoch=79
03/17/2022 17:35:04 - INFO - __main__ - Step 250 Global step 250 Train loss 5.00 on epoch=83
03/17/2022 17:35:17 - INFO - __main__ - Global step 250 Train loss 5.30 ACC 0.0625 on epoch=83
03/17/2022 17:35:17 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=83, global_step=250
03/17/2022 17:35:20 - INFO - __main__ - Step 260 Global step 260 Train loss 5.01 on epoch=86
03/17/2022 17:35:24 - INFO - __main__ - Step 270 Global step 270 Train loss 4.76 on epoch=89
03/17/2022 17:35:27 - INFO - __main__ - Step 280 Global step 280 Train loss 4.43 on epoch=93
03/17/2022 17:35:30 - INFO - __main__ - Step 290 Global step 290 Train loss 4.15 on epoch=96
03/17/2022 17:35:33 - INFO - __main__ - Step 300 Global step 300 Train loss 4.01 on epoch=99
03/17/2022 17:35:46 - INFO - __main__ - Global step 300 Train loss 4.47 ACC 0.03125 on epoch=99
03/17/2022 17:35:49 - INFO - __main__ - Step 310 Global step 310 Train loss 3.68 on epoch=103
03/17/2022 17:35:52 - INFO - __main__ - Step 320 Global step 320 Train loss 3.43 on epoch=106
03/17/2022 17:35:55 - INFO - __main__ - Step 330 Global step 330 Train loss 3.35 on epoch=109
03/17/2022 17:35:58 - INFO - __main__ - Step 340 Global step 340 Train loss 3.01 on epoch=113
03/17/2022 17:36:01 - INFO - __main__ - Step 350 Global step 350 Train loss 2.72 on epoch=116
03/17/2022 17:36:03 - INFO - __main__ - Global step 350 Train loss 3.24 ACC 0.0 on epoch=116
03/17/2022 17:36:06 - INFO - __main__ - Step 360 Global step 360 Train loss 2.53 on epoch=119
03/17/2022 17:36:09 - INFO - __main__ - Step 370 Global step 370 Train loss 2.37 on epoch=123
03/17/2022 17:36:12 - INFO - __main__ - Step 380 Global step 380 Train loss 2.29 on epoch=126
03/17/2022 17:36:15 - INFO - __main__ - Step 390 Global step 390 Train loss 2.03 on epoch=129
03/17/2022 17:36:18 - INFO - __main__ - Step 400 Global step 400 Train loss 2.05 on epoch=133
03/17/2022 17:36:19 - INFO - __main__ - Global step 400 Train loss 2.25 ACC 0.0 on epoch=133
03/17/2022 17:36:22 - INFO - __main__ - Step 410 Global step 410 Train loss 1.95 on epoch=136
03/17/2022 17:36:25 - INFO - __main__ - Step 420 Global step 420 Train loss 1.79 on epoch=139
03/17/2022 17:36:28 - INFO - __main__ - Step 430 Global step 430 Train loss 1.68 on epoch=143
03/17/2022 17:36:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.63 on epoch=146
03/17/2022 17:36:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.43 on epoch=149
03/17/2022 17:36:36 - INFO - __main__ - Global step 450 Train loss 1.70 ACC 0.5 on epoch=149
03/17/2022 17:36:36 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.5 on epoch=149, global_step=450
03/17/2022 17:36:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.39 on epoch=153
03/17/2022 17:36:42 - INFO - __main__ - Step 470 Global step 470 Train loss 1.32 on epoch=156
03/17/2022 17:36:45 - INFO - __main__ - Step 480 Global step 480 Train loss 1.19 on epoch=159
03/17/2022 17:36:48 - INFO - __main__ - Step 490 Global step 490 Train loss 1.20 on epoch=163
03/17/2022 17:36:51 - INFO - __main__ - Step 500 Global step 500 Train loss 1.16 on epoch=166
03/17/2022 17:36:52 - INFO - __main__ - Global step 500 Train loss 1.25 ACC 0.5 on epoch=166
03/17/2022 17:36:55 - INFO - __main__ - Step 510 Global step 510 Train loss 1.07 on epoch=169
03/17/2022 17:36:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.95 on epoch=173
03/17/2022 17:37:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=176
03/17/2022 17:37:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.88 on epoch=179
03/17/2022 17:37:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=183
03/17/2022 17:37:09 - INFO - __main__ - Global step 550 Train loss 0.94 ACC 0.5 on epoch=183
03/17/2022 17:37:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=186
03/17/2022 17:37:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.81 on epoch=189
03/17/2022 17:37:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.78 on epoch=193
03/17/2022 17:37:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.78 on epoch=196
03/17/2022 17:37:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.78 on epoch=199
03/17/2022 17:37:25 - INFO - __main__ - Global step 600 Train loss 0.79 ACC 0.5 on epoch=199
03/17/2022 17:37:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=203
03/17/2022 17:37:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=206
03/17/2022 17:37:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.66 on epoch=209
03/17/2022 17:37:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.63 on epoch=213
03/17/2022 17:37:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=216
03/17/2022 17:37:41 - INFO - __main__ - Global step 650 Train loss 0.65 ACC 0.5 on epoch=216
03/17/2022 17:37:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.62 on epoch=219
03/17/2022 17:37:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=223
03/17/2022 17:37:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.64 on epoch=226
03/17/2022 17:37:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=229
03/17/2022 17:37:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=233
03/17/2022 17:37:57 - INFO - __main__ - Global step 700 Train loss 0.63 ACC 0.03125 on epoch=233
03/17/2022 17:38:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=236
03/17/2022 17:38:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=239
03/17/2022 17:38:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.62 on epoch=243
03/17/2022 17:38:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.64 on epoch=246
03/17/2022 17:38:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=249
03/17/2022 17:38:14 - INFO - __main__ - Global step 750 Train loss 0.60 ACC 0.5 on epoch=249
03/17/2022 17:38:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=253
03/17/2022 17:38:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=256
03/17/2022 17:38:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.56 on epoch=259
03/17/2022 17:38:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=263
03/17/2022 17:38:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.61 on epoch=266
03/17/2022 17:38:30 - INFO - __main__ - Global step 800 Train loss 0.57 ACC 0.4375 on epoch=266
03/17/2022 17:38:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.60 on epoch=269
03/17/2022 17:38:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=273
03/17/2022 17:38:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=276
03/17/2022 17:38:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=279
03/17/2022 17:38:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.59 on epoch=283
03/17/2022 17:38:46 - INFO - __main__ - Global step 850 Train loss 0.57 ACC 0.5 on epoch=283
03/17/2022 17:38:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=286
03/17/2022 17:38:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=289
03/17/2022 17:38:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.54 on epoch=293
03/17/2022 17:38:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.50 on epoch=296
03/17/2022 17:39:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=299
03/17/2022 17:39:02 - INFO - __main__ - Global step 900 Train loss 0.52 ACC 0.5 on epoch=299
03/17/2022 17:39:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=303
03/17/2022 17:39:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.56 on epoch=306
03/17/2022 17:39:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.60 on epoch=309
03/17/2022 17:39:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=313
03/17/2022 17:39:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=316
03/17/2022 17:39:19 - INFO - __main__ - Global step 950 Train loss 0.53 ACC 0.4375 on epoch=316
03/17/2022 17:39:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=319
03/17/2022 17:39:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=323
03/17/2022 17:39:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=326
03/17/2022 17:39:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=329
03/17/2022 17:39:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.54 on epoch=333
03/17/2022 17:39:35 - INFO - __main__ - Global step 1000 Train loss 0.53 ACC 0.4375 on epoch=333
03/17/2022 17:39:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=336
03/17/2022 17:39:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=339
03/17/2022 17:39:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.53 on epoch=343
03/17/2022 17:39:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=346
03/17/2022 17:39:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.50 on epoch=349
03/17/2022 17:39:51 - INFO - __main__ - Global step 1050 Train loss 0.50 ACC 0.09375 on epoch=349
03/17/2022 17:39:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=353
03/17/2022 17:39:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=356
03/17/2022 17:40:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=359
03/17/2022 17:40:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=363
03/17/2022 17:40:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=366
03/17/2022 17:40:08 - INFO - __main__ - Global step 1100 Train loss 0.47 ACC 0.21875 on epoch=366
03/17/2022 17:40:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=369
03/17/2022 17:40:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=373
03/17/2022 17:40:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=376
03/17/2022 17:40:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.47 on epoch=379
03/17/2022 17:40:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.52 on epoch=383
03/17/2022 17:40:24 - INFO - __main__ - Global step 1150 Train loss 0.50 ACC 0.3125 on epoch=383
03/17/2022 17:40:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=386
03/17/2022 17:40:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.52 on epoch=389
03/17/2022 17:40:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.53 on epoch=393
03/17/2022 17:40:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=396
03/17/2022 17:40:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=399
03/17/2022 17:40:41 - INFO - __main__ - Global step 1200 Train loss 0.50 ACC 0.09375 on epoch=399
03/17/2022 17:40:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=403
03/17/2022 17:40:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=406
03/17/2022 17:40:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=409
03/17/2022 17:40:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=413
03/17/2022 17:40:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=416
03/17/2022 17:40:57 - INFO - __main__ - Global step 1250 Train loss 0.47 ACC 0.5 on epoch=416
03/17/2022 17:41:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=419
03/17/2022 17:41:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=423
03/17/2022 17:41:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=426
03/17/2022 17:41:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=429
03/17/2022 17:41:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=433
03/17/2022 17:41:13 - INFO - __main__ - Global step 1300 Train loss 0.49 ACC 0.0625 on epoch=433
03/17/2022 17:41:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=436
03/17/2022 17:41:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=439
03/17/2022 17:41:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=443
03/17/2022 17:41:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=446
03/17/2022 17:41:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=449
03/17/2022 17:41:30 - INFO - __main__ - Global step 1350 Train loss 0.44 ACC 0.25 on epoch=449
03/17/2022 17:41:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=453
03/17/2022 17:41:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=456
03/17/2022 17:41:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=459
03/17/2022 17:41:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=463
03/17/2022 17:41:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=466
03/17/2022 17:41:46 - INFO - __main__ - Global step 1400 Train loss 0.44 ACC 0.15625 on epoch=466
03/17/2022 17:41:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=469
03/17/2022 17:41:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=473
03/17/2022 17:41:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=476
03/17/2022 17:41:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=479
03/17/2022 17:42:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=483
03/17/2022 17:42:03 - INFO - __main__ - Global step 1450 Train loss 0.44 ACC 0.09375 on epoch=483
03/17/2022 17:42:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=486
03/17/2022 17:42:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=489
03/17/2022 17:42:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=493
03/17/2022 17:42:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=496
03/17/2022 17:42:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=499
03/17/2022 17:42:19 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.125 on epoch=499
03/17/2022 17:42:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=503
03/17/2022 17:42:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=506
03/17/2022 17:42:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=509
03/17/2022 17:42:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=513
03/17/2022 17:42:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=516
03/17/2022 17:42:35 - INFO - __main__ - Global step 1550 Train loss 0.42 ACC 0.53125 on epoch=516
03/17/2022 17:42:35 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=516, global_step=1550
03/17/2022 17:42:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=519
03/17/2022 17:42:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=523
03/17/2022 17:42:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=526
03/17/2022 17:42:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=529
03/17/2022 17:42:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=533
03/17/2022 17:42:52 - INFO - __main__ - Global step 1600 Train loss 0.43 ACC 0.5 on epoch=533
03/17/2022 17:42:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=536
03/17/2022 17:42:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=539
03/17/2022 17:43:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=543
03/17/2022 17:43:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=546
03/17/2022 17:43:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=549
03/17/2022 17:43:09 - INFO - __main__ - Global step 1650 Train loss 0.41 ACC 0.4375 on epoch=549
03/17/2022 17:43:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=553
03/17/2022 17:43:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=556
03/17/2022 17:43:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=559
03/17/2022 17:43:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=563
03/17/2022 17:43:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=566
03/17/2022 17:43:25 - INFO - __main__ - Global step 1700 Train loss 0.39 ACC 0.53125 on epoch=566
03/17/2022 17:43:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=569
03/17/2022 17:43:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=573
03/17/2022 17:43:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=576
03/17/2022 17:43:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=579
03/17/2022 17:43:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=583
03/17/2022 17:43:42 - INFO - __main__ - Global step 1750 Train loss 0.40 ACC 0.46875 on epoch=583
03/17/2022 17:43:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=586
03/17/2022 17:43:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=589
03/17/2022 17:43:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=593
03/17/2022 17:43:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=596
03/17/2022 17:43:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=599
03/17/2022 17:43:58 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.46875 on epoch=599
03/17/2022 17:44:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=603
03/17/2022 17:44:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=606
03/17/2022 17:44:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=609
03/17/2022 17:44:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=613
03/17/2022 17:44:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=616
03/17/2022 17:44:15 - INFO - __main__ - Global step 1850 Train loss 0.34 ACC 0.46875 on epoch=616
03/17/2022 17:44:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=619
03/17/2022 17:44:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=623
03/17/2022 17:44:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=626
03/17/2022 17:44:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=629
03/17/2022 17:44:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=633
03/17/2022 17:44:31 - INFO - __main__ - Global step 1900 Train loss 0.34 ACC 0.53125 on epoch=633
03/17/2022 17:44:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=636
03/17/2022 17:44:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=639
03/17/2022 17:44:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=643
03/17/2022 17:44:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=646
03/17/2022 17:44:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=649
03/17/2022 17:44:48 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.46875 on epoch=649
03/17/2022 17:44:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=653
03/17/2022 17:44:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=656
03/17/2022 17:44:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=659
03/17/2022 17:45:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=663
03/17/2022 17:45:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=666
03/17/2022 17:45:05 - INFO - __main__ - Global step 2000 Train loss 0.33 ACC 0.46875 on epoch=666
03/17/2022 17:45:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=669
03/17/2022 17:45:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=673
03/17/2022 17:45:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=676
03/17/2022 17:45:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=679
03/17/2022 17:45:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=683
03/17/2022 17:45:21 - INFO - __main__ - Global step 2050 Train loss 0.33 ACC 0.625 on epoch=683
03/17/2022 17:45:21 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=683, global_step=2050
03/17/2022 17:45:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=686
03/17/2022 17:45:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=689
03/17/2022 17:45:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=693
03/17/2022 17:45:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=696
03/17/2022 17:45:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=699
03/17/2022 17:45:38 - INFO - __main__ - Global step 2100 Train loss 0.28 ACC 0.53125 on epoch=699
03/17/2022 17:45:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=703
03/17/2022 17:45:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=706
03/17/2022 17:45:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.28 on epoch=709
03/17/2022 17:45:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=713
03/17/2022 17:45:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=716
03/17/2022 17:45:54 - INFO - __main__ - Global step 2150 Train loss 0.25 ACC 0.71875 on epoch=716
03/17/2022 17:45:54 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.71875 on epoch=716, global_step=2150
03/17/2022 17:45:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=719
03/17/2022 17:46:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.28 on epoch=723
03/17/2022 17:46:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=726
03/17/2022 17:46:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=729
03/17/2022 17:46:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=733
03/17/2022 17:46:11 - INFO - __main__ - Global step 2200 Train loss 0.25 ACC 0.59375 on epoch=733
03/17/2022 17:46:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=736
03/17/2022 17:46:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.25 on epoch=739
03/17/2022 17:46:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=743
03/17/2022 17:46:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=746
03/17/2022 17:46:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=749
03/17/2022 17:46:28 - INFO - __main__ - Global step 2250 Train loss 0.24 ACC 0.8125 on epoch=749
03/17/2022 17:46:28 - INFO - __main__ - Saving model with best ACC: 0.71875 -> 0.8125 on epoch=749, global_step=2250
03/17/2022 17:46:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=753
03/17/2022 17:46:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.24 on epoch=756
03/17/2022 17:46:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=759
03/17/2022 17:46:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=763
03/17/2022 17:46:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=766
03/17/2022 17:46:44 - INFO - __main__ - Global step 2300 Train loss 0.24 ACC 0.65625 on epoch=766
03/17/2022 17:46:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=769
03/17/2022 17:46:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=773
03/17/2022 17:46:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=776
03/17/2022 17:46:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=779
03/17/2022 17:47:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=783
03/17/2022 17:47:01 - INFO - __main__ - Global step 2350 Train loss 0.20 ACC 0.4375 on epoch=783
03/17/2022 17:47:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=786
03/17/2022 17:47:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=789
03/17/2022 17:47:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=793
03/17/2022 17:47:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=796
03/17/2022 17:47:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.31 on epoch=799
03/17/2022 17:47:18 - INFO - __main__ - Global step 2400 Train loss 0.21 ACC 0.53125 on epoch=799
03/17/2022 17:47:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=803
03/17/2022 17:47:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=806
03/17/2022 17:47:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=809
03/17/2022 17:47:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=813
03/17/2022 17:47:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=816
03/17/2022 17:47:34 - INFO - __main__ - Global step 2450 Train loss 0.21 ACC 0.75 on epoch=816
03/17/2022 17:47:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=819
03/17/2022 17:47:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=823
03/17/2022 17:47:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=826
03/17/2022 17:47:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=829
03/17/2022 17:47:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=833
03/17/2022 17:47:51 - INFO - __main__ - Global step 2500 Train loss 0.19 ACC 0.59375 on epoch=833
03/17/2022 17:47:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=836
03/17/2022 17:47:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=839
03/17/2022 17:48:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=843
03/17/2022 17:48:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=846
03/17/2022 17:48:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=849
03/17/2022 17:48:07 - INFO - __main__ - Global step 2550 Train loss 0.17 ACC 0.65625 on epoch=849
03/17/2022 17:48:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=853
03/17/2022 17:48:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=856
03/17/2022 17:48:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=859
03/17/2022 17:48:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=863
03/17/2022 17:48:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=866
03/17/2022 17:48:24 - INFO - __main__ - Global step 2600 Train loss 0.13 ACC 0.65625 on epoch=866
03/17/2022 17:48:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=869
03/17/2022 17:48:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=873
03/17/2022 17:48:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=876
03/17/2022 17:48:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=879
03/17/2022 17:48:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=883
03/17/2022 17:48:40 - INFO - __main__ - Global step 2650 Train loss 0.13 ACC 0.5625 on epoch=883
03/17/2022 17:48:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=886
03/17/2022 17:48:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=889
03/17/2022 17:48:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=893
03/17/2022 17:48:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=896
03/17/2022 17:48:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=899
03/17/2022 17:48:57 - INFO - __main__ - Global step 2700 Train loss 0.11 ACC 0.5625 on epoch=899
03/17/2022 17:49:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=903
03/17/2022 17:49:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=906
03/17/2022 17:49:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=909
03/17/2022 17:49:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=913
03/17/2022 17:49:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=916
03/17/2022 17:49:13 - INFO - __main__ - Global step 2750 Train loss 0.10 ACC 0.4375 on epoch=916
03/17/2022 17:49:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=919
03/17/2022 17:49:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=923
03/17/2022 17:49:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=926
03/17/2022 17:49:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=929
03/17/2022 17:49:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=933
03/17/2022 17:49:30 - INFO - __main__ - Global step 2800 Train loss 0.09 ACC 0.75 on epoch=933
03/17/2022 17:49:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=936
03/17/2022 17:49:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=939
03/17/2022 17:49:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=943
03/17/2022 17:49:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=946
03/17/2022 17:49:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=949
03/17/2022 17:49:47 - INFO - __main__ - Global step 2850 Train loss 0.10 ACC 0.75 on epoch=949
03/17/2022 17:49:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=953
03/17/2022 17:49:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=956
03/17/2022 17:49:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=959
03/17/2022 17:50:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=963
03/17/2022 17:50:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=966
03/17/2022 17:50:04 - INFO - __main__ - Global step 2900 Train loss 0.09 ACC 0.28125 on epoch=966
03/17/2022 17:50:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=969
03/17/2022 17:50:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=973
03/17/2022 17:50:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=976
03/17/2022 17:50:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=979
03/17/2022 17:50:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=983
03/17/2022 17:50:20 - INFO - __main__ - Global step 2950 Train loss 0.10 ACC 0.625 on epoch=983
03/17/2022 17:50:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=986
03/17/2022 17:50:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=989
03/17/2022 17:50:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=993
03/17/2022 17:50:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=996
03/17/2022 17:50:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=999
03/17/2022 17:50:36 - INFO - __main__ - Global step 3000 Train loss 0.09 ACC 0.71875 on epoch=999
03/17/2022 17:50:36 - INFO - __main__ - save last model!
03/17/2022 17:50:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 17:50:37 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 17:50:37 - INFO - __main__ - Printing 3 examples
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 17:50:37 - INFO - __main__ - ['neutral']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 17:50:37 - INFO - __main__ - ['entailment']
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:50:37 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 17:50:37 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:50:37 - INFO - __main__ - Printing 3 examples
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:50:37 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:50:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:50:37 - INFO - __main__ - Printing 3 examples
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:50:37 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:50:37 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:50:37 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:50:39 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_21_0.4_8_predictions.txt
03/17/2022 17:50:39 - INFO - __main__ - ACC on test data: 0.6429
03/17/2022 17:50:39 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.4, bsz=8, dev_performance=0.8125, test_performance=0.6428571428571429
03/17/2022 17:50:39 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.3, bsz=8 ...
03/17/2022 17:50:40 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 17:50:40 - INFO - __main__ - Printing 3 examples
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 17:50:40 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:50:40 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 17:50:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 17:50:40 - INFO - __main__ - Printing 3 examples
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 17:50:40 - INFO - __main__ - ['contradiction']
03/17/2022 17:50:40 - INFO - __main__ - Tokenizing Input ...
03/17/2022 17:50:40 - INFO - __main__ - Tokenizing Output ...
03/17/2022 17:50:40 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 17:50:56 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:50:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:50:57 - INFO - __main__ - Starting training!
03/17/2022 17:50:59 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 17:51:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 17:51:00 - INFO - __main__ - Starting training!
03/17/2022 17:51:04 - INFO - __main__ - Step 10 Global step 10 Train loss 6.47 on epoch=3
03/17/2022 17:51:07 - INFO - __main__ - Step 20 Global step 20 Train loss 4.28 on epoch=6
03/17/2022 17:51:10 - INFO - __main__ - Step 30 Global step 30 Train loss 4.07 on epoch=9
03/17/2022 17:51:13 - INFO - __main__ - Step 40 Global step 40 Train loss 2.94 on epoch=13
03/17/2022 17:51:16 - INFO - __main__ - Step 50 Global step 50 Train loss 2.59 on epoch=16
03/17/2022 17:51:18 - INFO - __main__ - Global step 50 Train loss 4.07 ACC 0.28125 on epoch=16
03/17/2022 17:51:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.28125 on epoch=16, global_step=50
03/17/2022 17:51:21 - INFO - __main__ - Step 60 Global step 60 Train loss 2.31 on epoch=19
03/17/2022 17:51:24 - INFO - __main__ - Step 70 Global step 70 Train loss 1.99 on epoch=23
03/17/2022 17:51:27 - INFO - __main__ - Step 80 Global step 80 Train loss 4.52 on epoch=26
03/17/2022 17:51:30 - INFO - __main__ - Step 90 Global step 90 Train loss 4.36 on epoch=29
03/17/2022 17:51:33 - INFO - __main__ - Step 100 Global step 100 Train loss 2.48 on epoch=33
03/17/2022 17:51:34 - INFO - __main__ - Global step 100 Train loss 3.13 ACC 0.0 on epoch=33
03/17/2022 17:51:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.86 on epoch=36
03/17/2022 17:51:41 - INFO - __main__ - Step 120 Global step 120 Train loss 2.21 on epoch=39
03/17/2022 17:51:44 - INFO - __main__ - Step 130 Global step 130 Train loss 2.27 on epoch=43
03/17/2022 17:51:47 - INFO - __main__ - Step 140 Global step 140 Train loss 2.41 on epoch=46
03/17/2022 17:51:50 - INFO - __main__ - Step 150 Global step 150 Train loss 2.09 on epoch=49
03/17/2022 17:51:51 - INFO - __main__ - Global step 150 Train loss 2.37 ACC 0.1875 on epoch=49
03/17/2022 17:51:54 - INFO - __main__ - Step 160 Global step 160 Train loss 2.14 on epoch=53
03/17/2022 17:51:57 - INFO - __main__ - Step 170 Global step 170 Train loss 2.05 on epoch=56
03/17/2022 17:52:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.90 on epoch=59
03/17/2022 17:52:03 - INFO - __main__ - Step 190 Global step 190 Train loss 1.91 on epoch=63
03/17/2022 17:52:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.80 on epoch=66
03/17/2022 17:52:07 - INFO - __main__ - Global step 200 Train loss 1.96 ACC 0.0 on epoch=66
03/17/2022 17:52:10 - INFO - __main__ - Step 210 Global step 210 Train loss 1.93 on epoch=69
03/17/2022 17:52:13 - INFO - __main__ - Step 220 Global step 220 Train loss 1.82 on epoch=73
03/17/2022 17:52:16 - INFO - __main__ - Step 230 Global step 230 Train loss 1.87 on epoch=76
03/17/2022 17:52:19 - INFO - __main__ - Step 240 Global step 240 Train loss 2.05 on epoch=79
03/17/2022 17:52:22 - INFO - __main__ - Step 250 Global step 250 Train loss 1.77 on epoch=83
03/17/2022 17:52:24 - INFO - __main__ - Global step 250 Train loss 1.89 ACC 0.0 on epoch=83
03/17/2022 17:52:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.87 on epoch=86
03/17/2022 17:52:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.77 on epoch=89
03/17/2022 17:52:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.51 on epoch=93
03/17/2022 17:52:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=96
03/17/2022 17:52:39 - INFO - __main__ - Step 300 Global step 300 Train loss 1.47 on epoch=99
03/17/2022 17:52:40 - INFO - __main__ - Global step 300 Train loss 1.64 ACC 0.03125 on epoch=99
03/17/2022 17:52:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.47 on epoch=103
03/17/2022 17:52:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.66 on epoch=106
03/17/2022 17:52:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.56 on epoch=109
03/17/2022 17:52:52 - INFO - __main__ - Step 340 Global step 340 Train loss 1.32 on epoch=113
03/17/2022 17:52:55 - INFO - __main__ - Step 350 Global step 350 Train loss 1.39 on epoch=116
03/17/2022 17:52:56 - INFO - __main__ - Global step 350 Train loss 1.48 ACC 0.1875 on epoch=116
03/17/2022 17:52:59 - INFO - __main__ - Step 360 Global step 360 Train loss 2.42 on epoch=119
03/17/2022 17:53:02 - INFO - __main__ - Step 370 Global step 370 Train loss 3.92 on epoch=123
03/17/2022 17:53:05 - INFO - __main__ - Step 380 Global step 380 Train loss 4.40 on epoch=126
03/17/2022 17:53:08 - INFO - __main__ - Step 390 Global step 390 Train loss 3.83 on epoch=129
03/17/2022 17:53:11 - INFO - __main__ - Step 400 Global step 400 Train loss 3.49 on epoch=133
03/17/2022 17:53:13 - INFO - __main__ - Global step 400 Train loss 3.61 ACC 0.25 on epoch=133
03/17/2022 17:53:16 - INFO - __main__ - Step 410 Global step 410 Train loss 2.98 on epoch=136
03/17/2022 17:53:19 - INFO - __main__ - Step 420 Global step 420 Train loss 2.60 on epoch=139
03/17/2022 17:53:22 - INFO - __main__ - Step 430 Global step 430 Train loss 2.00 on epoch=143
03/17/2022 17:53:25 - INFO - __main__ - Step 440 Global step 440 Train loss 2.19 on epoch=146
03/17/2022 17:53:28 - INFO - __main__ - Step 450 Global step 450 Train loss 2.11 on epoch=149
03/17/2022 17:53:29 - INFO - __main__ - Global step 450 Train loss 2.38 ACC 0.03125 on epoch=149
03/17/2022 17:53:32 - INFO - __main__ - Step 460 Global step 460 Train loss 1.85 on epoch=153
03/17/2022 17:53:35 - INFO - __main__ - Step 470 Global step 470 Train loss 1.94 on epoch=156
03/17/2022 17:53:38 - INFO - __main__ - Step 480 Global step 480 Train loss 2.09 on epoch=159
03/17/2022 17:53:41 - INFO - __main__ - Step 490 Global step 490 Train loss 2.04 on epoch=163
03/17/2022 17:53:44 - INFO - __main__ - Step 500 Global step 500 Train loss 2.02 on epoch=166
03/17/2022 17:53:45 - INFO - __main__ - Global step 500 Train loss 1.99 ACC 0.1875 on epoch=166
03/17/2022 17:53:49 - INFO - __main__ - Step 510 Global step 510 Train loss 1.91 on epoch=169
03/17/2022 17:53:52 - INFO - __main__ - Step 520 Global step 520 Train loss 1.93 on epoch=173
03/17/2022 17:53:55 - INFO - __main__ - Step 530 Global step 530 Train loss 1.90 on epoch=176
03/17/2022 17:53:58 - INFO - __main__ - Step 540 Global step 540 Train loss 2.02 on epoch=179
03/17/2022 17:54:01 - INFO - __main__ - Step 550 Global step 550 Train loss 1.74 on epoch=183
03/17/2022 17:54:02 - INFO - __main__ - Global step 550 Train loss 1.90 ACC 0.125 on epoch=183
03/17/2022 17:54:05 - INFO - __main__ - Step 560 Global step 560 Train loss 1.84 on epoch=186
03/17/2022 17:54:08 - INFO - __main__ - Step 570 Global step 570 Train loss 1.75 on epoch=189
03/17/2022 17:54:11 - INFO - __main__ - Step 580 Global step 580 Train loss 1.76 on epoch=193
03/17/2022 17:54:14 - INFO - __main__ - Step 590 Global step 590 Train loss 1.78 on epoch=196
03/17/2022 17:54:17 - INFO - __main__ - Step 600 Global step 600 Train loss 1.87 on epoch=199
03/17/2022 17:54:18 - INFO - __main__ - Global step 600 Train loss 1.80 ACC 0.375 on epoch=199
03/17/2022 17:54:19 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.375 on epoch=199, global_step=600
03/17/2022 17:54:22 - INFO - __main__ - Step 610 Global step 610 Train loss 1.69 on epoch=203
03/17/2022 17:54:25 - INFO - __main__ - Step 620 Global step 620 Train loss 1.73 on epoch=206
03/17/2022 17:54:28 - INFO - __main__ - Step 630 Global step 630 Train loss 1.80 on epoch=209
03/17/2022 17:54:31 - INFO - __main__ - Step 640 Global step 640 Train loss 1.74 on epoch=213
03/17/2022 17:54:34 - INFO - __main__ - Step 650 Global step 650 Train loss 1.79 on epoch=216
03/17/2022 17:54:35 - INFO - __main__ - Global step 650 Train loss 1.75 ACC 0.3125 on epoch=216
03/17/2022 17:54:38 - INFO - __main__ - Step 660 Global step 660 Train loss 1.85 on epoch=219
03/17/2022 17:54:41 - INFO - __main__ - Step 670 Global step 670 Train loss 1.82 on epoch=223
03/17/2022 17:54:44 - INFO - __main__ - Step 680 Global step 680 Train loss 1.64 on epoch=226
03/17/2022 17:54:47 - INFO - __main__ - Step 690 Global step 690 Train loss 1.66 on epoch=229
03/17/2022 17:54:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.64 on epoch=233
03/17/2022 17:54:51 - INFO - __main__ - Global step 700 Train loss 1.72 ACC 0.15625 on epoch=233
03/17/2022 17:54:54 - INFO - __main__ - Step 710 Global step 710 Train loss 1.65 on epoch=236
03/17/2022 17:54:57 - INFO - __main__ - Step 720 Global step 720 Train loss 1.69 on epoch=239
03/17/2022 17:55:00 - INFO - __main__ - Step 730 Global step 730 Train loss 1.58 on epoch=243
03/17/2022 17:55:03 - INFO - __main__ - Step 740 Global step 740 Train loss 1.68 on epoch=246
03/17/2022 17:55:06 - INFO - __main__ - Step 750 Global step 750 Train loss 1.62 on epoch=249
03/17/2022 17:55:08 - INFO - __main__ - Global step 750 Train loss 1.64 ACC 0.1875 on epoch=249
03/17/2022 17:55:11 - INFO - __main__ - Step 760 Global step 760 Train loss 1.58 on epoch=253
03/17/2022 17:55:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.61 on epoch=256
03/17/2022 17:55:17 - INFO - __main__ - Step 780 Global step 780 Train loss 1.54 on epoch=259
03/17/2022 17:55:20 - INFO - __main__ - Step 790 Global step 790 Train loss 1.48 on epoch=263
03/17/2022 17:55:23 - INFO - __main__ - Step 800 Global step 800 Train loss 1.44 on epoch=266
03/17/2022 17:55:24 - INFO - __main__ - Global step 800 Train loss 1.53 ACC 0.46875 on epoch=266
03/17/2022 17:55:24 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=266, global_step=800
03/17/2022 17:55:27 - INFO - __main__ - Step 810 Global step 810 Train loss 1.55 on epoch=269
03/17/2022 17:55:30 - INFO - __main__ - Step 820 Global step 820 Train loss 1.39 on epoch=273
03/17/2022 17:55:33 - INFO - __main__ - Step 830 Global step 830 Train loss 1.45 on epoch=276
03/17/2022 17:55:36 - INFO - __main__ - Step 840 Global step 840 Train loss 1.47 on epoch=279
03/17/2022 17:55:39 - INFO - __main__ - Step 850 Global step 850 Train loss 1.40 on epoch=283
03/17/2022 17:55:40 - INFO - __main__ - Global step 850 Train loss 1.45 ACC 0.34375 on epoch=283
03/17/2022 17:55:43 - INFO - __main__ - Step 860 Global step 860 Train loss 1.30 on epoch=286
03/17/2022 17:55:47 - INFO - __main__ - Step 870 Global step 870 Train loss 1.43 on epoch=289
03/17/2022 17:55:50 - INFO - __main__ - Step 880 Global step 880 Train loss 1.40 on epoch=293
03/17/2022 17:55:53 - INFO - __main__ - Step 890 Global step 890 Train loss 1.30 on epoch=296
03/17/2022 17:55:56 - INFO - __main__ - Step 900 Global step 900 Train loss 1.36 on epoch=299
03/17/2022 17:55:57 - INFO - __main__ - Global step 900 Train loss 1.36 ACC 0.40625 on epoch=299
03/17/2022 17:56:00 - INFO - __main__ - Step 910 Global step 910 Train loss 1.31 on epoch=303
03/17/2022 17:56:03 - INFO - __main__ - Step 920 Global step 920 Train loss 1.37 on epoch=306
03/17/2022 17:56:06 - INFO - __main__ - Step 930 Global step 930 Train loss 1.40 on epoch=309
03/17/2022 17:56:09 - INFO - __main__ - Step 940 Global step 940 Train loss 1.27 on epoch=313
03/17/2022 17:56:12 - INFO - __main__ - Step 950 Global step 950 Train loss 1.25 on epoch=316
03/17/2022 17:56:13 - INFO - __main__ - Global step 950 Train loss 1.32 ACC 0.46875 on epoch=316
03/17/2022 17:56:16 - INFO - __main__ - Step 960 Global step 960 Train loss 1.30 on epoch=319
03/17/2022 17:56:19 - INFO - __main__ - Step 970 Global step 970 Train loss 1.33 on epoch=323
03/17/2022 17:56:22 - INFO - __main__ - Step 980 Global step 980 Train loss 1.21 on epoch=326
03/17/2022 17:56:25 - INFO - __main__ - Step 990 Global step 990 Train loss 1.26 on epoch=329
03/17/2022 17:56:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.22 on epoch=333
03/17/2022 17:56:29 - INFO - __main__ - Global step 1000 Train loss 1.27 ACC 0.21875 on epoch=333
03/17/2022 17:56:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.20 on epoch=336
03/17/2022 17:56:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.14 on epoch=339
03/17/2022 17:56:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.06 on epoch=343
03/17/2022 17:56:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.06 on epoch=346
03/17/2022 17:56:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.10 on epoch=349
03/17/2022 17:56:46 - INFO - __main__ - Global step 1050 Train loss 1.11 ACC 0.1875 on epoch=349
03/17/2022 17:56:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.10 on epoch=353
03/17/2022 17:56:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.15 on epoch=356
03/17/2022 17:56:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.11 on epoch=359
03/17/2022 17:56:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.05 on epoch=363
03/17/2022 17:57:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.02 on epoch=366
03/17/2022 17:57:02 - INFO - __main__ - Global step 1100 Train loss 1.08 ACC 0.46875 on epoch=366
03/17/2022 17:57:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.04 on epoch=369
03/17/2022 17:57:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.99 on epoch=373
03/17/2022 17:57:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.97 on epoch=376
03/17/2022 17:57:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.04 on epoch=379
03/17/2022 17:57:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.94 on epoch=383
03/17/2022 17:57:18 - INFO - __main__ - Global step 1150 Train loss 1.00 ACC 0.4375 on epoch=383
03/17/2022 17:57:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.86 on epoch=386
03/17/2022 17:57:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.92 on epoch=389
03/17/2022 17:57:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.87 on epoch=393
03/17/2022 17:57:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.85 on epoch=396
03/17/2022 17:57:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.91 on epoch=399
03/17/2022 17:57:35 - INFO - __main__ - Global step 1200 Train loss 0.88 ACC 0.21875 on epoch=399
03/17/2022 17:57:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.81 on epoch=403
03/17/2022 17:57:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.82 on epoch=406
03/17/2022 17:57:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.80 on epoch=409
03/17/2022 17:57:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.82 on epoch=413
03/17/2022 17:57:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.90 on epoch=416
03/17/2022 17:57:51 - INFO - __main__ - Global step 1250 Train loss 0.83 ACC 0.40625 on epoch=416
03/17/2022 17:57:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.81 on epoch=419
03/17/2022 17:57:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.82 on epoch=423
03/17/2022 17:58:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.84 on epoch=426
03/17/2022 17:58:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.73 on epoch=429
03/17/2022 17:58:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=433
03/17/2022 17:58:08 - INFO - __main__ - Global step 1300 Train loss 0.80 ACC 0.3125 on epoch=433
03/17/2022 17:58:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.73 on epoch=436
03/17/2022 17:58:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=439
03/17/2022 17:58:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.76 on epoch=443
03/17/2022 17:58:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.70 on epoch=446
03/17/2022 17:58:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.77 on epoch=449
03/17/2022 17:58:24 - INFO - __main__ - Global step 1350 Train loss 0.74 ACC 0.5 on epoch=449
03/17/2022 17:58:24 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=449, global_step=1350
03/17/2022 17:58:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.68 on epoch=453
03/17/2022 17:58:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.69 on epoch=456
03/17/2022 17:58:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.68 on epoch=459
03/17/2022 17:58:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.76 on epoch=463
03/17/2022 17:58:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.73 on epoch=466
03/17/2022 17:58:40 - INFO - __main__ - Global step 1400 Train loss 0.71 ACC 0.15625 on epoch=466
03/17/2022 17:58:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.68 on epoch=469
03/17/2022 17:58:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.77 on epoch=473
03/17/2022 17:58:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.67 on epoch=476
03/17/2022 17:58:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.76 on epoch=479
03/17/2022 17:58:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=483
03/17/2022 17:58:56 - INFO - __main__ - Global step 1450 Train loss 0.71 ACC 0.375 on epoch=483
03/17/2022 17:58:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.63 on epoch=486
03/17/2022 17:59:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.62 on epoch=489
03/17/2022 17:59:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.67 on epoch=493
03/17/2022 17:59:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.58 on epoch=496
03/17/2022 17:59:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.75 on epoch=499
03/17/2022 17:59:13 - INFO - __main__ - Global step 1500 Train loss 0.65 ACC 0.28125 on epoch=499
03/17/2022 17:59:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.66 on epoch=503
03/17/2022 17:59:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.65 on epoch=506
03/17/2022 17:59:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.63 on epoch=509
03/17/2022 17:59:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.64 on epoch=513
03/17/2022 17:59:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.69 on epoch=516
03/17/2022 17:59:29 - INFO - __main__ - Global step 1550 Train loss 0.65 ACC 0.375 on epoch=516
03/17/2022 17:59:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.65 on epoch=519
03/17/2022 17:59:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.62 on epoch=523
03/17/2022 17:59:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=526
03/17/2022 17:59:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.66 on epoch=529
03/17/2022 17:59:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.66 on epoch=533
03/17/2022 17:59:45 - INFO - __main__ - Global step 1600 Train loss 0.63 ACC 0.46875 on epoch=533
03/17/2022 17:59:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.66 on epoch=536
03/17/2022 17:59:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.74 on epoch=539
03/17/2022 17:59:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.63 on epoch=543
03/17/2022 17:59:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.55 on epoch=546
03/17/2022 18:00:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=549
03/17/2022 18:00:01 - INFO - __main__ - Global step 1650 Train loss 0.63 ACC 0.5 on epoch=549
03/17/2022 18:00:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.72 on epoch=553
03/17/2022 18:00:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=556
03/17/2022 18:00:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.56 on epoch=559
03/17/2022 18:00:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.58 on epoch=563
03/17/2022 18:00:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.62 on epoch=566
03/17/2022 18:00:17 - INFO - __main__ - Global step 1700 Train loss 0.62 ACC 0.5 on epoch=566
03/17/2022 18:00:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=569
03/17/2022 18:00:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=573
03/17/2022 18:00:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.64 on epoch=576
03/17/2022 18:00:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.57 on epoch=579
03/17/2022 18:00:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=583
03/17/2022 18:00:33 - INFO - __main__ - Global step 1750 Train loss 0.58 ACC 0.21875 on epoch=583
03/17/2022 18:00:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.63 on epoch=586
03/17/2022 18:00:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.61 on epoch=589
03/17/2022 18:00:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.56 on epoch=593
03/17/2022 18:00:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.56 on epoch=596
03/17/2022 18:00:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=599
03/17/2022 18:00:49 - INFO - __main__ - Global step 1800 Train loss 0.57 ACC 0.21875 on epoch=599
03/17/2022 18:00:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.54 on epoch=603
03/17/2022 18:00:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.61 on epoch=606
03/17/2022 18:00:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.58 on epoch=609
03/17/2022 18:01:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.57 on epoch=613
03/17/2022 18:01:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.56 on epoch=616
03/17/2022 18:01:05 - INFO - __main__ - Global step 1850 Train loss 0.57 ACC 0.5 on epoch=616
03/17/2022 18:01:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=619
03/17/2022 18:01:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.63 on epoch=623
03/17/2022 18:01:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.55 on epoch=626
03/17/2022 18:01:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.66 on epoch=629
03/17/2022 18:01:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=633
03/17/2022 18:01:21 - INFO - __main__ - Global step 1900 Train loss 0.58 ACC 0.46875 on epoch=633
03/17/2022 18:01:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.63 on epoch=636
03/17/2022 18:01:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.55 on epoch=639
03/17/2022 18:01:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=643
03/17/2022 18:01:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.57 on epoch=646
03/17/2022 18:01:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.57 on epoch=649
03/17/2022 18:01:37 - INFO - __main__ - Global step 1950 Train loss 0.57 ACC 0.25 on epoch=649
03/17/2022 18:01:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.57 on epoch=653
03/17/2022 18:01:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.57 on epoch=656
03/17/2022 18:01:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.60 on epoch=659
03/17/2022 18:01:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.59 on epoch=663
03/17/2022 18:01:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.51 on epoch=666
03/17/2022 18:01:54 - INFO - __main__ - Global step 2000 Train loss 0.57 ACC 0.40625 on epoch=666
03/17/2022 18:01:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.51 on epoch=669
03/17/2022 18:02:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.57 on epoch=673
03/17/2022 18:02:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.54 on epoch=676
03/17/2022 18:02:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.52 on epoch=679
03/17/2022 18:02:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.55 on epoch=683
03/17/2022 18:02:10 - INFO - __main__ - Global step 2050 Train loss 0.54 ACC 0.28125 on epoch=683
03/17/2022 18:02:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.64 on epoch=686
03/17/2022 18:02:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.64 on epoch=689
03/17/2022 18:02:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.51 on epoch=693
03/17/2022 18:02:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.49 on epoch=696
03/17/2022 18:02:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.56 on epoch=699
03/17/2022 18:02:26 - INFO - __main__ - Global step 2100 Train loss 0.57 ACC 0.25 on epoch=699
03/17/2022 18:02:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.57 on epoch=703
03/17/2022 18:02:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.52 on epoch=706
03/17/2022 18:02:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.48 on epoch=709
03/17/2022 18:02:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.49 on epoch=713
03/17/2022 18:02:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.49 on epoch=716
03/17/2022 18:02:42 - INFO - __main__ - Global step 2150 Train loss 0.51 ACC 0.5 on epoch=716
03/17/2022 18:02:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.57 on epoch=719
03/17/2022 18:02:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.60 on epoch=723
03/17/2022 18:02:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.45 on epoch=726
03/17/2022 18:02:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.49 on epoch=729
03/17/2022 18:02:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.52 on epoch=733
03/17/2022 18:02:58 - INFO - __main__ - Global step 2200 Train loss 0.53 ACC 0.25 on epoch=733
03/17/2022 18:03:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.50 on epoch=736
03/17/2022 18:03:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.53 on epoch=739
03/17/2022 18:03:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.52 on epoch=743
03/17/2022 18:03:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.54 on epoch=746
03/17/2022 18:03:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.54 on epoch=749
03/17/2022 18:03:14 - INFO - __main__ - Global step 2250 Train loss 0.53 ACC 0.25 on epoch=749
03/17/2022 18:03:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.53 on epoch=753
03/17/2022 18:03:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.50 on epoch=756
03/17/2022 18:03:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.53 on epoch=759
03/17/2022 18:03:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.52 on epoch=763
03/17/2022 18:03:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.52 on epoch=766
03/17/2022 18:03:30 - INFO - __main__ - Global step 2300 Train loss 0.52 ACC 0.5 on epoch=766
03/17/2022 18:03:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.53 on epoch=769
03/17/2022 18:03:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=773
03/17/2022 18:03:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.56 on epoch=776
03/17/2022 18:03:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.54 on epoch=779
03/17/2022 18:03:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.55 on epoch=783
03/17/2022 18:03:46 - INFO - __main__ - Global step 2350 Train loss 0.53 ACC 0.15625 on epoch=783
03/17/2022 18:03:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.51 on epoch=786
03/17/2022 18:03:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.51 on epoch=789
03/17/2022 18:03:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.47 on epoch=793
03/17/2022 18:03:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.57 on epoch=796
03/17/2022 18:04:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.56 on epoch=799
03/17/2022 18:04:02 - INFO - __main__ - Global step 2400 Train loss 0.52 ACC 0.5 on epoch=799
03/17/2022 18:04:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.51 on epoch=803
03/17/2022 18:04:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.54 on epoch=806
03/17/2022 18:04:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.52 on epoch=809
03/17/2022 18:04:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.48 on epoch=813
03/17/2022 18:04:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.47 on epoch=816
03/17/2022 18:04:19 - INFO - __main__ - Global step 2450 Train loss 0.51 ACC 0.28125 on epoch=816
03/17/2022 18:04:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.52 on epoch=819
03/17/2022 18:04:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.49 on epoch=823
03/17/2022 18:04:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.49 on epoch=826
03/17/2022 18:04:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.49 on epoch=829
03/17/2022 18:04:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.43 on epoch=833
03/17/2022 18:04:35 - INFO - __main__ - Global step 2500 Train loss 0.48 ACC 0.0625 on epoch=833
03/17/2022 18:04:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.58 on epoch=836
03/17/2022 18:04:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.51 on epoch=839
03/17/2022 18:04:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=843
03/17/2022 18:04:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.49 on epoch=846
03/17/2022 18:04:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.57 on epoch=849
03/17/2022 18:04:51 - INFO - __main__ - Global step 2550 Train loss 0.53 ACC 0.25 on epoch=849
03/17/2022 18:04:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.52 on epoch=853
03/17/2022 18:04:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=856
03/17/2022 18:05:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.55 on epoch=859
03/17/2022 18:05:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.47 on epoch=863
03/17/2022 18:05:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.46 on epoch=866
03/17/2022 18:05:07 - INFO - __main__ - Global step 2600 Train loss 0.49 ACC 0.5 on epoch=866
03/17/2022 18:05:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.50 on epoch=869
03/17/2022 18:05:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.48 on epoch=873
03/17/2022 18:05:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.51 on epoch=876
03/17/2022 18:05:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=879
03/17/2022 18:05:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.48 on epoch=883
03/17/2022 18:05:23 - INFO - __main__ - Global step 2650 Train loss 0.48 ACC 0.5 on epoch=883
03/17/2022 18:05:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.43 on epoch=886
03/17/2022 18:05:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.52 on epoch=889
03/17/2022 18:05:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.51 on epoch=893
03/17/2022 18:05:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=896
03/17/2022 18:05:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.43 on epoch=899
03/17/2022 18:05:39 - INFO - __main__ - Global step 2700 Train loss 0.46 ACC 0.40625 on epoch=899
03/17/2022 18:05:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.50 on epoch=903
03/17/2022 18:05:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.47 on epoch=906
03/17/2022 18:05:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=909
03/17/2022 18:05:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.52 on epoch=913
03/17/2022 18:05:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=916
03/17/2022 18:05:55 - INFO - __main__ - Global step 2750 Train loss 0.48 ACC 0.3125 on epoch=916
03/17/2022 18:05:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.51 on epoch=919
03/17/2022 18:06:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.45 on epoch=923
03/17/2022 18:06:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.41 on epoch=926
03/17/2022 18:06:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.48 on epoch=929
03/17/2022 18:06:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.46 on epoch=933
03/17/2022 18:06:11 - INFO - __main__ - Global step 2800 Train loss 0.46 ACC 0.15625 on epoch=933
03/17/2022 18:06:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.50 on epoch=936
03/17/2022 18:06:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.43 on epoch=939
03/17/2022 18:06:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=943
03/17/2022 18:06:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.50 on epoch=946
03/17/2022 18:06:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.50 on epoch=949
03/17/2022 18:06:27 - INFO - __main__ - Global step 2850 Train loss 0.48 ACC 0.34375 on epoch=949
03/17/2022 18:06:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.48 on epoch=953
03/17/2022 18:06:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.47 on epoch=956
03/17/2022 18:06:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=959
03/17/2022 18:06:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.47 on epoch=963
03/17/2022 18:06:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=966
03/17/2022 18:06:43 - INFO - __main__ - Global step 2900 Train loss 0.47 ACC 0.5 on epoch=966
03/17/2022 18:06:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.46 on epoch=969
03/17/2022 18:06:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.49 on epoch=973
03/17/2022 18:06:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.48 on epoch=976
03/17/2022 18:06:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.48 on epoch=979
03/17/2022 18:06:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.46 on epoch=983
03/17/2022 18:07:00 - INFO - __main__ - Global step 2950 Train loss 0.48 ACC 0.1875 on epoch=983
03/17/2022 18:07:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=986
03/17/2022 18:07:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.48 on epoch=989
03/17/2022 18:07:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=993
03/17/2022 18:07:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=996
03/17/2022 18:07:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.50 on epoch=999
03/17/2022 18:07:16 - INFO - __main__ - Global step 3000 Train loss 0.45 ACC 0.1875 on epoch=999
03/17/2022 18:07:16 - INFO - __main__ - save last model!
03/17/2022 18:07:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 18:07:16 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 18:07:16 - INFO - __main__ - Printing 3 examples
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 18:07:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 18:07:16 - INFO - __main__ - ['neutral']
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 18:07:16 - INFO - __main__ - ['entailment']
03/17/2022 18:07:16 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:07:16 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:07:16 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 18:07:16 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:07:16 - INFO - __main__ - Printing 3 examples
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 18:07:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 18:07:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:16 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 18:07:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 18:07:16 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:07:17 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:07:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:07:17 - INFO - __main__ - Printing 3 examples
03/17/2022 18:07:17 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 18:07:17 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:17 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 18:07:17 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:17 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 18:07:17 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:17 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:07:17 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:07:17 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:07:18 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_21_0.3_8_predictions.txt
03/17/2022 18:07:18 - INFO - __main__ - ACC on test data: 0.1786
03/17/2022 18:07:19 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.17857142857142858
03/17/2022 18:07:19 - INFO - __main__ - Running ... prefix=superglue-cb_16_21, lr=0.2, bsz=8 ...
03/17/2022 18:07:20 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:07:20 - INFO - __main__ - Printing 3 examples
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 18:07:20 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:07:20 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:07:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:07:20 - INFO - __main__ - Printing 3 examples
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: B: And the tanks came in and, you know, pretty much took care of that. A: Exactly. B: And, A: Yeah, uh, that, personally I don't see as Gorbachev as being maybe a threat, and I think he's actually, honestly trying to do some change. B: Uh-huh. A: But I don't believe that he, in this first pass around, you know, being the first one to really turn things around or attempt to is going to be allowed to get away with it either. [SEP] hypothesis: Gorbachev is going to be allowed to get away with doing some change
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: A: and if they weren't spending all the money on drug testing, people could have got a raise. So, see, you know, there's different, I think that's more of a personal view of mine other than a yes, sir, we should have drug testing because there's really a problem B: Uh-huh. A: and I know that. But then, I have other views to it. B: I didn't think it was that expensive because my son was in probably a week and a half period [SEP] hypothesis: it was that expensive
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. [SEP] hypothesis: some kid should be exempt from being spanked
03/17/2022 18:07:20 - INFO - __main__ - ['contradiction']
03/17/2022 18:07:20 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:07:20 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:07:20 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:07:35 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:07:35 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:07:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:07:36 - INFO - __main__ - Starting training!
03/17/2022 18:07:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:07:36 - INFO - __main__ - Starting training!
03/17/2022 18:07:40 - INFO - __main__ - Step 10 Global step 10 Train loss 6.52 on epoch=3
03/17/2022 18:07:43 - INFO - __main__ - Step 20 Global step 20 Train loss 5.80 on epoch=6
03/17/2022 18:07:46 - INFO - __main__ - Step 30 Global step 30 Train loss 6.43 on epoch=9
03/17/2022 18:07:49 - INFO - __main__ - Step 40 Global step 40 Train loss 6.69 on epoch=13
03/17/2022 18:07:52 - INFO - __main__ - Step 50 Global step 50 Train loss 6.74 on epoch=16
03/17/2022 18:08:09 - INFO - __main__ - Global step 50 Train loss 6.43 ACC 0.0 on epoch=16
03/17/2022 18:08:09 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 18:08:12 - INFO - __main__ - Step 60 Global step 60 Train loss 7.30 on epoch=19
03/17/2022 18:08:15 - INFO - __main__ - Step 70 Global step 70 Train loss 6.96 on epoch=23
03/17/2022 18:08:18 - INFO - __main__ - Step 80 Global step 80 Train loss 7.10 on epoch=26
03/17/2022 18:08:21 - INFO - __main__ - Step 90 Global step 90 Train loss 6.91 on epoch=29
03/17/2022 18:08:25 - INFO - __main__ - Step 100 Global step 100 Train loss 6.99 on epoch=33
03/17/2022 18:08:39 - INFO - __main__ - Global step 100 Train loss 7.05 ACC 0.0 on epoch=33
03/17/2022 18:08:42 - INFO - __main__ - Step 110 Global step 110 Train loss 6.90 on epoch=36
03/17/2022 18:08:45 - INFO - __main__ - Step 120 Global step 120 Train loss 7.09 on epoch=39
03/17/2022 18:08:48 - INFO - __main__ - Step 130 Global step 130 Train loss 7.01 on epoch=43
03/17/2022 18:08:51 - INFO - __main__ - Step 140 Global step 140 Train loss 6.94 on epoch=46
03/17/2022 18:08:54 - INFO - __main__ - Step 150 Global step 150 Train loss 7.06 on epoch=49
03/17/2022 18:09:00 - INFO - __main__ - Global step 150 Train loss 7.00 ACC 0.0 on epoch=49
03/17/2022 18:09:03 - INFO - __main__ - Step 160 Global step 160 Train loss 6.92 on epoch=53
03/17/2022 18:09:06 - INFO - __main__ - Step 170 Global step 170 Train loss 7.07 on epoch=56
03/17/2022 18:09:09 - INFO - __main__ - Step 180 Global step 180 Train loss 7.04 on epoch=59
03/17/2022 18:09:12 - INFO - __main__ - Step 190 Global step 190 Train loss 7.06 on epoch=63
03/17/2022 18:09:15 - INFO - __main__ - Step 200 Global step 200 Train loss 6.76 on epoch=66
03/17/2022 18:09:30 - INFO - __main__ - Global step 200 Train loss 6.97 ACC 0.0 on epoch=66
03/17/2022 18:09:33 - INFO - __main__ - Step 210 Global step 210 Train loss 6.68 on epoch=69
03/17/2022 18:09:36 - INFO - __main__ - Step 220 Global step 220 Train loss 6.34 on epoch=73
03/17/2022 18:09:39 - INFO - __main__ - Step 230 Global step 230 Train loss 6.45 on epoch=76
03/17/2022 18:09:42 - INFO - __main__ - Step 240 Global step 240 Train loss 6.39 on epoch=79
03/17/2022 18:09:45 - INFO - __main__ - Step 250 Global step 250 Train loss 6.54 on epoch=83
03/17/2022 18:10:05 - INFO - __main__ - Global step 250 Train loss 6.48 ACC 0.0 on epoch=83
03/17/2022 18:10:08 - INFO - __main__ - Step 260 Global step 260 Train loss 6.55 on epoch=86
03/17/2022 18:10:11 - INFO - __main__ - Step 270 Global step 270 Train loss 6.34 on epoch=89
03/17/2022 18:10:14 - INFO - __main__ - Step 280 Global step 280 Train loss 6.18 on epoch=93
03/17/2022 18:10:17 - INFO - __main__ - Step 290 Global step 290 Train loss 6.15 on epoch=96
03/17/2022 18:10:20 - INFO - __main__ - Step 300 Global step 300 Train loss 6.25 on epoch=99
03/17/2022 18:10:39 - INFO - __main__ - Global step 300 Train loss 6.30 ACC 0.0 on epoch=99
03/17/2022 18:10:42 - INFO - __main__ - Step 310 Global step 310 Train loss 5.94 on epoch=103
03/17/2022 18:10:45 - INFO - __main__ - Step 320 Global step 320 Train loss 6.05 on epoch=106
03/17/2022 18:10:48 - INFO - __main__ - Step 330 Global step 330 Train loss 6.17 on epoch=109
03/17/2022 18:10:51 - INFO - __main__ - Step 340 Global step 340 Train loss 5.98 on epoch=113
03/17/2022 18:10:54 - INFO - __main__ - Step 350 Global step 350 Train loss 6.09 on epoch=116
03/17/2022 18:11:13 - INFO - __main__ - Global step 350 Train loss 6.04 ACC 0.0 on epoch=116
03/17/2022 18:11:17 - INFO - __main__ - Step 360 Global step 360 Train loss 6.16 on epoch=119
03/17/2022 18:11:20 - INFO - __main__ - Step 370 Global step 370 Train loss 5.92 on epoch=123
03/17/2022 18:11:23 - INFO - __main__ - Step 380 Global step 380 Train loss 6.01 on epoch=126
03/17/2022 18:11:26 - INFO - __main__ - Step 390 Global step 390 Train loss 6.10 on epoch=129
03/17/2022 18:11:29 - INFO - __main__ - Step 400 Global step 400 Train loss 5.88 on epoch=133
03/17/2022 18:11:48 - INFO - __main__ - Global step 400 Train loss 6.01 ACC 0.0 on epoch=133
03/17/2022 18:11:51 - INFO - __main__ - Step 410 Global step 410 Train loss 6.06 on epoch=136
03/17/2022 18:11:54 - INFO - __main__ - Step 420 Global step 420 Train loss 6.01 on epoch=139
03/17/2022 18:11:57 - INFO - __main__ - Step 430 Global step 430 Train loss 5.82 on epoch=143
03/17/2022 18:12:00 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=146
03/17/2022 18:12:03 - INFO - __main__ - Step 450 Global step 450 Train loss 6.07 on epoch=149
03/17/2022 18:12:23 - INFO - __main__ - Global step 450 Train loss 5.97 ACC 0.0 on epoch=149
03/17/2022 18:12:26 - INFO - __main__ - Step 460 Global step 460 Train loss 5.86 on epoch=153
03/17/2022 18:12:29 - INFO - __main__ - Step 470 Global step 470 Train loss 5.83 on epoch=156
03/17/2022 18:12:32 - INFO - __main__ - Step 480 Global step 480 Train loss 5.82 on epoch=159
03/17/2022 18:12:35 - INFO - __main__ - Step 490 Global step 490 Train loss 5.74 on epoch=163
03/17/2022 18:12:38 - INFO - __main__ - Step 500 Global step 500 Train loss 5.68 on epoch=166
03/17/2022 18:12:58 - INFO - __main__ - Global step 500 Train loss 5.79 ACC 0.0 on epoch=166
03/17/2022 18:13:01 - INFO - __main__ - Step 510 Global step 510 Train loss 5.78 on epoch=169
03/17/2022 18:13:04 - INFO - __main__ - Step 520 Global step 520 Train loss 5.67 on epoch=173
03/17/2022 18:13:07 - INFO - __main__ - Step 530 Global step 530 Train loss 5.82 on epoch=176
03/17/2022 18:13:10 - INFO - __main__ - Step 540 Global step 540 Train loss 5.80 on epoch=179
03/17/2022 18:13:13 - INFO - __main__ - Step 550 Global step 550 Train loss 5.54 on epoch=183
03/17/2022 18:13:33 - INFO - __main__ - Global step 550 Train loss 5.72 ACC 0.0 on epoch=183
03/17/2022 18:13:36 - INFO - __main__ - Step 560 Global step 560 Train loss 5.86 on epoch=186
03/17/2022 18:13:39 - INFO - __main__ - Step 570 Global step 570 Train loss 5.78 on epoch=189
03/17/2022 18:13:42 - INFO - __main__ - Step 580 Global step 580 Train loss 5.55 on epoch=193
03/17/2022 18:13:45 - INFO - __main__ - Step 590 Global step 590 Train loss 5.65 on epoch=196
03/17/2022 18:13:48 - INFO - __main__ - Step 600 Global step 600 Train loss 5.60 on epoch=199
03/17/2022 18:14:08 - INFO - __main__ - Global step 600 Train loss 5.69 ACC 0.0 on epoch=199
03/17/2022 18:14:11 - INFO - __main__ - Step 610 Global step 610 Train loss 5.53 on epoch=203
03/17/2022 18:14:14 - INFO - __main__ - Step 620 Global step 620 Train loss 5.61 on epoch=206
03/17/2022 18:14:17 - INFO - __main__ - Step 630 Global step 630 Train loss 5.55 on epoch=209
03/17/2022 18:14:20 - INFO - __main__ - Step 640 Global step 640 Train loss 5.32 on epoch=213
03/17/2022 18:14:23 - INFO - __main__ - Step 650 Global step 650 Train loss 5.35 on epoch=216
03/17/2022 18:14:27 - INFO - __main__ - Global step 650 Train loss 5.47 ACC 0.0 on epoch=216
03/17/2022 18:14:30 - INFO - __main__ - Step 660 Global step 660 Train loss 5.31 on epoch=219
03/17/2022 18:14:33 - INFO - __main__ - Step 670 Global step 670 Train loss 5.16 on epoch=223
03/17/2022 18:14:36 - INFO - __main__ - Step 680 Global step 680 Train loss 5.14 on epoch=226
03/17/2022 18:14:39 - INFO - __main__ - Step 690 Global step 690 Train loss 5.18 on epoch=229
03/17/2022 18:14:42 - INFO - __main__ - Step 700 Global step 700 Train loss 5.03 on epoch=233
03/17/2022 18:14:44 - INFO - __main__ - Global step 700 Train loss 5.16 ACC 0.0 on epoch=233
03/17/2022 18:14:47 - INFO - __main__ - Step 710 Global step 710 Train loss 4.99 on epoch=236
03/17/2022 18:14:50 - INFO - __main__ - Step 720 Global step 720 Train loss 5.05 on epoch=239
03/17/2022 18:14:54 - INFO - __main__ - Step 730 Global step 730 Train loss 4.88 on epoch=243
03/17/2022 18:14:57 - INFO - __main__ - Step 740 Global step 740 Train loss 4.82 on epoch=246
03/17/2022 18:15:00 - INFO - __main__ - Step 750 Global step 750 Train loss 4.84 on epoch=249
03/17/2022 18:15:02 - INFO - __main__ - Global step 750 Train loss 4.92 ACC 0.09375 on epoch=249
03/17/2022 18:15:02 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=249, global_step=750
03/17/2022 18:15:05 - INFO - __main__ - Step 760 Global step 760 Train loss 4.53 on epoch=253
03/17/2022 18:15:08 - INFO - __main__ - Step 770 Global step 770 Train loss 4.56 on epoch=256
03/17/2022 18:15:11 - INFO - __main__ - Step 780 Global step 780 Train loss 4.61 on epoch=259
03/17/2022 18:15:14 - INFO - __main__ - Step 790 Global step 790 Train loss 4.37 on epoch=263
03/17/2022 18:15:17 - INFO - __main__ - Step 800 Global step 800 Train loss 4.41 on epoch=266
03/17/2022 18:15:19 - INFO - __main__ - Global step 800 Train loss 4.50 ACC 0.1875 on epoch=266
03/17/2022 18:15:19 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.1875 on epoch=266, global_step=800
03/17/2022 18:15:22 - INFO - __main__ - Step 810 Global step 810 Train loss 4.39 on epoch=269
03/17/2022 18:15:25 - INFO - __main__ - Step 820 Global step 820 Train loss 4.17 on epoch=273
03/17/2022 18:15:28 - INFO - __main__ - Step 830 Global step 830 Train loss 4.10 on epoch=276
03/17/2022 18:15:31 - INFO - __main__ - Step 840 Global step 840 Train loss 4.09 on epoch=279
03/17/2022 18:15:34 - INFO - __main__ - Step 850 Global step 850 Train loss 4.04 on epoch=283
03/17/2022 18:15:35 - INFO - __main__ - Global step 850 Train loss 4.16 ACC 0.40625 on epoch=283
03/17/2022 18:15:35 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.40625 on epoch=283, global_step=850
03/17/2022 18:15:38 - INFO - __main__ - Step 860 Global step 860 Train loss 3.93 on epoch=286
03/17/2022 18:15:41 - INFO - __main__ - Step 870 Global step 870 Train loss 3.93 on epoch=289
03/17/2022 18:15:45 - INFO - __main__ - Step 880 Global step 880 Train loss 3.77 on epoch=293
03/17/2022 18:15:48 - INFO - __main__ - Step 890 Global step 890 Train loss 3.71 on epoch=296
03/17/2022 18:15:51 - INFO - __main__ - Step 900 Global step 900 Train loss 3.59 on epoch=299
03/17/2022 18:15:52 - INFO - __main__ - Global step 900 Train loss 3.79 ACC 0.1875 on epoch=299
03/17/2022 18:15:55 - INFO - __main__ - Step 910 Global step 910 Train loss 3.57 on epoch=303
03/17/2022 18:15:58 - INFO - __main__ - Step 920 Global step 920 Train loss 3.61 on epoch=306
03/17/2022 18:16:01 - INFO - __main__ - Step 930 Global step 930 Train loss 3.47 on epoch=309
03/17/2022 18:16:04 - INFO - __main__ - Step 940 Global step 940 Train loss 3.30 on epoch=313
03/17/2022 18:16:07 - INFO - __main__ - Step 950 Global step 950 Train loss 3.32 on epoch=316
03/17/2022 18:16:09 - INFO - __main__ - Global step 950 Train loss 3.45 ACC 0.0 on epoch=316
03/17/2022 18:16:12 - INFO - __main__ - Step 960 Global step 960 Train loss 3.33 on epoch=319
03/17/2022 18:16:15 - INFO - __main__ - Step 970 Global step 970 Train loss 3.14 on epoch=323
03/17/2022 18:16:18 - INFO - __main__ - Step 980 Global step 980 Train loss 3.17 on epoch=326
03/17/2022 18:16:21 - INFO - __main__ - Step 990 Global step 990 Train loss 3.17 on epoch=329
03/17/2022 18:16:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.94 on epoch=333
03/17/2022 18:16:26 - INFO - __main__ - Global step 1000 Train loss 3.15 ACC 0.0 on epoch=333
03/17/2022 18:16:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.97 on epoch=336
03/17/2022 18:16:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.94 on epoch=339
03/17/2022 18:16:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.81 on epoch=343
03/17/2022 18:16:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.71 on epoch=346
03/17/2022 18:16:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.73 on epoch=349
03/17/2022 18:16:42 - INFO - __main__ - Global step 1050 Train loss 2.83 ACC 0.0 on epoch=349
03/17/2022 18:16:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.65 on epoch=353
03/17/2022 18:16:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.62 on epoch=356
03/17/2022 18:16:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.73 on epoch=359
03/17/2022 18:16:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.41 on epoch=363
03/17/2022 18:16:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.52 on epoch=366
03/17/2022 18:16:59 - INFO - __main__ - Global step 1100 Train loss 2.59 ACC 0.125 on epoch=366
03/17/2022 18:17:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.39 on epoch=369
03/17/2022 18:17:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 2.28 on epoch=373
03/17/2022 18:17:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.31 on epoch=376
03/17/2022 18:17:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.34 on epoch=379
03/17/2022 18:17:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 2.23 on epoch=383
03/17/2022 18:17:15 - INFO - __main__ - Global step 1150 Train loss 2.31 ACC 0.46875 on epoch=383
03/17/2022 18:17:15 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.46875 on epoch=383, global_step=1150
03/17/2022 18:17:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.12 on epoch=386
03/17/2022 18:17:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.27 on epoch=389
03/17/2022 18:17:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 2.05 on epoch=393
03/17/2022 18:17:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 2.09 on epoch=396
03/17/2022 18:17:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.12 on epoch=399
03/17/2022 18:17:31 - INFO - __main__ - Global step 1200 Train loss 2.13 ACC 0.5 on epoch=399
03/17/2022 18:17:31 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=399, global_step=1200
03/17/2022 18:17:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.06 on epoch=403
03/17/2022 18:17:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.04 on epoch=406
03/17/2022 18:17:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.93 on epoch=409
03/17/2022 18:17:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.90 on epoch=413
03/17/2022 18:17:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.84 on epoch=416
03/17/2022 18:17:48 - INFO - __main__ - Global step 1250 Train loss 1.95 ACC 0.34375 on epoch=416
03/17/2022 18:17:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.81 on epoch=419
03/17/2022 18:17:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.70 on epoch=423
03/17/2022 18:17:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.84 on epoch=426
03/17/2022 18:18:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.77 on epoch=429
03/17/2022 18:18:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.73 on epoch=433
03/17/2022 18:18:04 - INFO - __main__ - Global step 1300 Train loss 1.77 ACC 0.375 on epoch=433
03/17/2022 18:18:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.65 on epoch=436
03/17/2022 18:18:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.61 on epoch=439
03/17/2022 18:18:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.62 on epoch=443
03/17/2022 18:18:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.76 on epoch=446
03/17/2022 18:18:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.60 on epoch=449
03/17/2022 18:18:20 - INFO - __main__ - Global step 1350 Train loss 1.65 ACC 0.5 on epoch=449
03/17/2022 18:18:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.42 on epoch=453
03/17/2022 18:18:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.54 on epoch=456
03/17/2022 18:18:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.59 on epoch=459
03/17/2022 18:18:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.60 on epoch=463
03/17/2022 18:18:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.53 on epoch=466
03/17/2022 18:18:37 - INFO - __main__ - Global step 1400 Train loss 1.54 ACC 0.5 on epoch=466
03/17/2022 18:18:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.44 on epoch=469
03/17/2022 18:18:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.36 on epoch=473
03/17/2022 18:18:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.48 on epoch=476
03/17/2022 18:18:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.40 on epoch=479
03/17/2022 18:18:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.30 on epoch=483
03/17/2022 18:18:53 - INFO - __main__ - Global step 1450 Train loss 1.40 ACC 0.5 on epoch=483
03/17/2022 18:18:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.26 on epoch=486
03/17/2022 18:18:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.32 on epoch=489
03/17/2022 18:19:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.27 on epoch=493
03/17/2022 18:19:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.25 on epoch=496
03/17/2022 18:19:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.28 on epoch=499
03/17/2022 18:19:10 - INFO - __main__ - Global step 1500 Train loss 1.28 ACC 0.5 on epoch=499
03/17/2022 18:19:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.17 on epoch=503
03/17/2022 18:19:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.13 on epoch=506
03/17/2022 18:19:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.18 on epoch=509
03/17/2022 18:19:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.11 on epoch=513
03/17/2022 18:19:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.13 on epoch=516
03/17/2022 18:19:26 - INFO - __main__ - Global step 1550 Train loss 1.14 ACC 0.5 on epoch=516
03/17/2022 18:19:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.08 on epoch=519
03/17/2022 18:19:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.00 on epoch=523
03/17/2022 18:19:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.05 on epoch=526
03/17/2022 18:19:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.01 on epoch=529
03/17/2022 18:19:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.10 on epoch=533
03/17/2022 18:19:42 - INFO - __main__ - Global step 1600 Train loss 1.05 ACC 0.5 on epoch=533
03/17/2022 18:19:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.02 on epoch=536
03/17/2022 18:19:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.02 on epoch=539
03/17/2022 18:19:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.95 on epoch=543
03/17/2022 18:19:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.04 on epoch=546
03/17/2022 18:19:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.01 on epoch=549
03/17/2022 18:19:59 - INFO - __main__ - Global step 1650 Train loss 1.01 ACC 0.5 on epoch=549
03/17/2022 18:20:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.90 on epoch=553
03/17/2022 18:20:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.91 on epoch=556
03/17/2022 18:20:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.90 on epoch=559
03/17/2022 18:20:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.93 on epoch=563
03/17/2022 18:20:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.85 on epoch=566
03/17/2022 18:20:15 - INFO - __main__ - Global step 1700 Train loss 0.90 ACC 0.5 on epoch=566
03/17/2022 18:20:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.87 on epoch=569
03/17/2022 18:20:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.97 on epoch=573
03/17/2022 18:20:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.90 on epoch=576
03/17/2022 18:20:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.78 on epoch=579
03/17/2022 18:20:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.87 on epoch=583
03/17/2022 18:20:31 - INFO - __main__ - Global step 1750 Train loss 0.88 ACC 0.5 on epoch=583
03/17/2022 18:20:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.84 on epoch=586
03/17/2022 18:20:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.85 on epoch=589
03/17/2022 18:20:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.84 on epoch=593
03/17/2022 18:20:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.79 on epoch=596
03/17/2022 18:20:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.80 on epoch=599
03/17/2022 18:20:48 - INFO - __main__ - Global step 1800 Train loss 0.83 ACC 0.34375 on epoch=599
03/17/2022 18:20:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.83 on epoch=603
03/17/2022 18:20:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.77 on epoch=606
03/17/2022 18:20:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.78 on epoch=609
03/17/2022 18:21:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.84 on epoch=613
03/17/2022 18:21:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.81 on epoch=616
03/17/2022 18:21:04 - INFO - __main__ - Global step 1850 Train loss 0.81 ACC 0.5 on epoch=616
03/17/2022 18:21:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.76 on epoch=619
03/17/2022 18:21:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.67 on epoch=623
03/17/2022 18:21:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.83 on epoch=626
03/17/2022 18:21:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.83 on epoch=629
03/17/2022 18:21:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.64 on epoch=633
03/17/2022 18:21:20 - INFO - __main__ - Global step 1900 Train loss 0.74 ACC 0.5 on epoch=633
03/17/2022 18:21:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.70 on epoch=636
03/17/2022 18:21:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.75 on epoch=639
03/17/2022 18:21:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.64 on epoch=643
03/17/2022 18:21:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.68 on epoch=646
03/17/2022 18:21:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.74 on epoch=649
03/17/2022 18:21:36 - INFO - __main__ - Global step 1950 Train loss 0.70 ACC 0.5 on epoch=649
03/17/2022 18:21:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.64 on epoch=653
03/17/2022 18:21:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.63 on epoch=656
03/17/2022 18:21:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.64 on epoch=659
03/17/2022 18:21:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.71 on epoch=663
03/17/2022 18:21:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.74 on epoch=666
03/17/2022 18:21:52 - INFO - __main__ - Global step 2000 Train loss 0.67 ACC 0.5 on epoch=666
03/17/2022 18:21:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.65 on epoch=669
03/17/2022 18:21:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.64 on epoch=673
03/17/2022 18:22:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.70 on epoch=676
03/17/2022 18:22:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.72 on epoch=679
03/17/2022 18:22:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.71 on epoch=683
03/17/2022 18:22:08 - INFO - __main__ - Global step 2050 Train loss 0.68 ACC 0.5 on epoch=683
03/17/2022 18:22:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.65 on epoch=686
03/17/2022 18:22:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.64 on epoch=689
03/17/2022 18:22:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.64 on epoch=693
03/17/2022 18:22:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.62 on epoch=696
03/17/2022 18:22:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.65 on epoch=699
03/17/2022 18:22:24 - INFO - __main__ - Global step 2100 Train loss 0.64 ACC 0.5 on epoch=699
03/17/2022 18:22:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.62 on epoch=703
03/17/2022 18:22:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.68 on epoch=706
03/17/2022 18:22:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.63 on epoch=709
03/17/2022 18:22:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.61 on epoch=713
03/17/2022 18:22:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.58 on epoch=716
03/17/2022 18:22:40 - INFO - __main__ - Global step 2150 Train loss 0.62 ACC 0.5 on epoch=716
03/17/2022 18:22:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.68 on epoch=719
03/17/2022 18:22:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.56 on epoch=723
03/17/2022 18:22:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.69 on epoch=726
03/17/2022 18:22:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.64 on epoch=729
03/17/2022 18:22:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.63 on epoch=733
03/17/2022 18:22:57 - INFO - __main__ - Global step 2200 Train loss 0.64 ACC 0.4375 on epoch=733
03/17/2022 18:23:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.65 on epoch=736
03/17/2022 18:23:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.68 on epoch=739
03/17/2022 18:23:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.60 on epoch=743
03/17/2022 18:23:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.62 on epoch=746
03/17/2022 18:23:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.62 on epoch=749
03/17/2022 18:23:13 - INFO - __main__ - Global step 2250 Train loss 0.63 ACC 0.5 on epoch=749
03/17/2022 18:23:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.52 on epoch=753
03/17/2022 18:23:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.58 on epoch=756
03/17/2022 18:23:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.61 on epoch=759
03/17/2022 18:23:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.61 on epoch=763
03/17/2022 18:23:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.55 on epoch=766
03/17/2022 18:23:29 - INFO - __main__ - Global step 2300 Train loss 0.58 ACC 0.5 on epoch=766
03/17/2022 18:23:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.53 on epoch=769
03/17/2022 18:23:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.58 on epoch=773
03/17/2022 18:23:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.53 on epoch=776
03/17/2022 18:23:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.53 on epoch=779
03/17/2022 18:23:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.48 on epoch=783
03/17/2022 18:23:45 - INFO - __main__ - Global step 2350 Train loss 0.53 ACC 0.46875 on epoch=783
03/17/2022 18:23:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.61 on epoch=786
03/17/2022 18:23:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.61 on epoch=789
03/17/2022 18:23:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.52 on epoch=793
03/17/2022 18:23:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.49 on epoch=796
03/17/2022 18:24:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.54 on epoch=799
03/17/2022 18:24:01 - INFO - __main__ - Global step 2400 Train loss 0.56 ACC 0.3125 on epoch=799
03/17/2022 18:24:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.59 on epoch=803
03/17/2022 18:24:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.56 on epoch=806
03/17/2022 18:24:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.55 on epoch=809
03/17/2022 18:24:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.56 on epoch=813
03/17/2022 18:24:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.50 on epoch=816
03/17/2022 18:24:17 - INFO - __main__ - Global step 2450 Train loss 0.55 ACC 0.3125 on epoch=816
03/17/2022 18:24:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.56 on epoch=819
03/17/2022 18:24:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.47 on epoch=823
03/17/2022 18:24:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.56 on epoch=826
03/17/2022 18:24:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.50 on epoch=829
03/17/2022 18:24:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.60 on epoch=833
03/17/2022 18:24:33 - INFO - __main__ - Global step 2500 Train loss 0.54 ACC 0.5 on epoch=833
03/17/2022 18:24:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.52 on epoch=836
03/17/2022 18:24:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.51 on epoch=839
03/17/2022 18:24:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=843
03/17/2022 18:24:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.58 on epoch=846
03/17/2022 18:24:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.53 on epoch=849
03/17/2022 18:24:49 - INFO - __main__ - Global step 2550 Train loss 0.53 ACC 0.5 on epoch=849
03/17/2022 18:24:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.51 on epoch=853
03/17/2022 18:24:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.51 on epoch=856
03/17/2022 18:24:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.56 on epoch=859
03/17/2022 18:25:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.50 on epoch=863
03/17/2022 18:25:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.57 on epoch=866
03/17/2022 18:25:06 - INFO - __main__ - Global step 2600 Train loss 0.53 ACC 0.5 on epoch=866
03/17/2022 18:25:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.49 on epoch=869
03/17/2022 18:25:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.51 on epoch=873
03/17/2022 18:25:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.51 on epoch=876
03/17/2022 18:25:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.53 on epoch=879
03/17/2022 18:25:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.51 on epoch=883
03/17/2022 18:25:22 - INFO - __main__ - Global step 2650 Train loss 0.51 ACC 0.5 on epoch=883
03/17/2022 18:25:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.54 on epoch=886
03/17/2022 18:25:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.50 on epoch=889
03/17/2022 18:25:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.50 on epoch=893
03/17/2022 18:25:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.47 on epoch=896
03/17/2022 18:25:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.49 on epoch=899
03/17/2022 18:25:38 - INFO - __main__ - Global step 2700 Train loss 0.50 ACC 0.5 on epoch=899
03/17/2022 18:25:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.56 on epoch=903
03/17/2022 18:25:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.50 on epoch=906
03/17/2022 18:25:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.56 on epoch=909
03/17/2022 18:25:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=913
03/17/2022 18:25:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.51 on epoch=916
03/17/2022 18:25:54 - INFO - __main__ - Global step 2750 Train loss 0.52 ACC 0.5 on epoch=916
03/17/2022 18:25:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.53 on epoch=919
03/17/2022 18:26:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.56 on epoch=923
03/17/2022 18:26:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=926
03/17/2022 18:26:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.50 on epoch=929
03/17/2022 18:26:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.50 on epoch=933
03/17/2022 18:26:10 - INFO - __main__ - Global step 2800 Train loss 0.51 ACC 0.5 on epoch=933
03/17/2022 18:26:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.52 on epoch=936
03/17/2022 18:26:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.50 on epoch=939
03/17/2022 18:26:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.47 on epoch=943
03/17/2022 18:26:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.53 on epoch=946
03/17/2022 18:26:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.50 on epoch=949
03/17/2022 18:26:26 - INFO - __main__ - Global step 2850 Train loss 0.51 ACC 0.5 on epoch=949
03/17/2022 18:26:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=953
03/17/2022 18:26:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.47 on epoch=956
03/17/2022 18:26:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.45 on epoch=959
03/17/2022 18:26:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.47 on epoch=963
03/17/2022 18:26:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.51 on epoch=966
03/17/2022 18:26:43 - INFO - __main__ - Global step 2900 Train loss 0.48 ACC 0.5 on epoch=966
03/17/2022 18:26:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.53 on epoch=969
03/17/2022 18:26:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=973
03/17/2022 18:26:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.52 on epoch=976
03/17/2022 18:26:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.45 on epoch=979
03/17/2022 18:26:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.48 on epoch=983
03/17/2022 18:26:59 - INFO - __main__ - Global step 2950 Train loss 0.48 ACC 0.5 on epoch=983
03/17/2022 18:27:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.48 on epoch=986
03/17/2022 18:27:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=989
03/17/2022 18:27:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=993
03/17/2022 18:27:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.47 on epoch=996
03/17/2022 18:27:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.50 on epoch=999
03/17/2022 18:27:15 - INFO - __main__ - Global step 3000 Train loss 0.47 ACC 0.5 on epoch=999
03/17/2022 18:27:15 - INFO - __main__ - save last model!
03/17/2022 18:27:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 18:27:15 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 18:27:15 - INFO - __main__ - Printing 3 examples
03/17/2022 18:27:15 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 18:27:15 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:15 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 18:27:15 - INFO - __main__ - ['neutral']
03/17/2022 18:27:15 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 18:27:15 - INFO - __main__ - ['entailment']
03/17/2022 18:27:15 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:27:15 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:27:15 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 18:27:16 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:27:16 - INFO - __main__ - Printing 3 examples
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 18:27:16 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:27:16 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:27:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:27:16 - INFO - __main__ - Printing 3 examples
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 18:27:16 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:16 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:27:16 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:27:16 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:27:18 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_21_0.2_8_predictions.txt
03/17/2022 18:27:18 - INFO - __main__ - ACC on test data: 0.5000
03/17/2022 18:27:18 - INFO - __main__ - prefix=superglue-cb_16_21, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.5
03/17/2022 18:27:18 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.5, bsz=8 ...
03/17/2022 18:27:19 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:27:19 - INFO - __main__ - Printing 3 examples
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 18:27:19 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:27:19 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:27:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:27:19 - INFO - __main__ - Printing 3 examples
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 18:27:19 - INFO - __main__ - ['contradiction']
03/17/2022 18:27:19 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:27:19 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:27:19 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:27:31 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:27:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:27:32 - INFO - __main__ - Starting training!
03/17/2022 18:27:34 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:27:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:27:35 - INFO - __main__ - Starting training!
03/17/2022 18:27:39 - INFO - __main__ - Step 10 Global step 10 Train loss 5.58 on epoch=3
03/17/2022 18:27:42 - INFO - __main__ - Step 20 Global step 20 Train loss 6.48 on epoch=6
03/17/2022 18:27:45 - INFO - __main__ - Step 30 Global step 30 Train loss 6.76 on epoch=9
03/17/2022 18:27:48 - INFO - __main__ - Step 40 Global step 40 Train loss 6.35 on epoch=13
03/17/2022 18:27:51 - INFO - __main__ - Step 50 Global step 50 Train loss 6.65 on epoch=16
03/17/2022 18:28:02 - INFO - __main__ - Global step 50 Train loss 6.36 ACC 0.0 on epoch=16
03/17/2022 18:28:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=16, global_step=50
03/17/2022 18:28:05 - INFO - __main__ - Step 60 Global step 60 Train loss 6.44 on epoch=19
03/17/2022 18:28:08 - INFO - __main__ - Step 70 Global step 70 Train loss 5.78 on epoch=23
03/17/2022 18:28:11 - INFO - __main__ - Step 80 Global step 80 Train loss 6.18 on epoch=26
03/17/2022 18:28:14 - INFO - __main__ - Step 90 Global step 90 Train loss 5.66 on epoch=29
03/17/2022 18:28:17 - INFO - __main__ - Step 100 Global step 100 Train loss 5.42 on epoch=33
03/17/2022 18:28:30 - INFO - __main__ - Global step 100 Train loss 5.90 ACC 0.0 on epoch=33
03/17/2022 18:28:33 - INFO - __main__ - Step 110 Global step 110 Train loss 5.61 on epoch=36
03/17/2022 18:28:36 - INFO - __main__ - Step 120 Global step 120 Train loss 5.73 on epoch=39
03/17/2022 18:28:39 - INFO - __main__ - Step 130 Global step 130 Train loss 5.39 on epoch=43
03/17/2022 18:28:42 - INFO - __main__ - Step 140 Global step 140 Train loss 5.54 on epoch=46
03/17/2022 18:28:45 - INFO - __main__ - Step 150 Global step 150 Train loss 5.51 on epoch=49
03/17/2022 18:29:04 - INFO - __main__ - Global step 150 Train loss 5.55 ACC 0.0 on epoch=49
03/17/2022 18:29:07 - INFO - __main__ - Step 160 Global step 160 Train loss 5.02 on epoch=53
03/17/2022 18:29:10 - INFO - __main__ - Step 170 Global step 170 Train loss 5.06 on epoch=56
03/17/2022 18:29:13 - INFO - __main__ - Step 180 Global step 180 Train loss 4.94 on epoch=59
03/17/2022 18:29:16 - INFO - __main__ - Step 190 Global step 190 Train loss 4.66 on epoch=63
03/17/2022 18:29:19 - INFO - __main__ - Step 200 Global step 200 Train loss 4.60 on epoch=66
03/17/2022 18:29:27 - INFO - __main__ - Global step 200 Train loss 4.86 ACC 0.03125 on epoch=66
03/17/2022 18:29:27 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=66, global_step=200
03/17/2022 18:29:30 - INFO - __main__ - Step 210 Global step 210 Train loss 4.55 on epoch=69
03/17/2022 18:29:33 - INFO - __main__ - Step 220 Global step 220 Train loss 4.26 on epoch=73
03/17/2022 18:29:36 - INFO - __main__ - Step 230 Global step 230 Train loss 3.97 on epoch=76
03/17/2022 18:29:39 - INFO - __main__ - Step 240 Global step 240 Train loss 3.85 on epoch=79
03/17/2022 18:29:42 - INFO - __main__ - Step 250 Global step 250 Train loss 3.46 on epoch=83
03/17/2022 18:29:44 - INFO - __main__ - Global step 250 Train loss 4.02 ACC 0.375 on epoch=83
03/17/2022 18:29:44 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.375 on epoch=83, global_step=250
03/17/2022 18:29:47 - INFO - __main__ - Step 260 Global step 260 Train loss 3.34 on epoch=86
03/17/2022 18:29:50 - INFO - __main__ - Step 270 Global step 270 Train loss 3.22 on epoch=89
03/17/2022 18:29:53 - INFO - __main__ - Step 280 Global step 280 Train loss 2.94 on epoch=93
03/17/2022 18:29:56 - INFO - __main__ - Step 290 Global step 290 Train loss 2.87 on epoch=96
03/17/2022 18:29:59 - INFO - __main__ - Step 300 Global step 300 Train loss 2.63 on epoch=99
03/17/2022 18:30:00 - INFO - __main__ - Global step 300 Train loss 3.00 ACC 0.0 on epoch=99
03/17/2022 18:30:03 - INFO - __main__ - Step 310 Global step 310 Train loss 2.33 on epoch=103
03/17/2022 18:30:07 - INFO - __main__ - Step 320 Global step 320 Train loss 2.24 on epoch=106
03/17/2022 18:30:10 - INFO - __main__ - Step 330 Global step 330 Train loss 2.14 on epoch=109
03/17/2022 18:30:13 - INFO - __main__ - Step 340 Global step 340 Train loss 1.98 on epoch=113
03/17/2022 18:30:16 - INFO - __main__ - Step 350 Global step 350 Train loss 1.96 on epoch=116
03/17/2022 18:30:17 - INFO - __main__ - Global step 350 Train loss 2.13 ACC 0.4375 on epoch=116
03/17/2022 18:30:17 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.4375 on epoch=116, global_step=350
03/17/2022 18:30:20 - INFO - __main__ - Step 360 Global step 360 Train loss 1.94 on epoch=119
03/17/2022 18:30:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.85 on epoch=123
03/17/2022 18:30:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.66 on epoch=126
03/17/2022 18:30:29 - INFO - __main__ - Step 390 Global step 390 Train loss 1.57 on epoch=129
03/17/2022 18:30:32 - INFO - __main__ - Step 400 Global step 400 Train loss 1.58 on epoch=133
03/17/2022 18:30:33 - INFO - __main__ - Global step 400 Train loss 1.72 ACC 0.5 on epoch=133
03/17/2022 18:30:33 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=133, global_step=400
03/17/2022 18:30:36 - INFO - __main__ - Step 410 Global step 410 Train loss 1.57 on epoch=136
03/17/2022 18:30:39 - INFO - __main__ - Step 420 Global step 420 Train loss 1.34 on epoch=139
03/17/2022 18:30:42 - INFO - __main__ - Step 430 Global step 430 Train loss 1.29 on epoch=143
03/17/2022 18:30:45 - INFO - __main__ - Step 440 Global step 440 Train loss 1.33 on epoch=146
03/17/2022 18:30:48 - INFO - __main__ - Step 450 Global step 450 Train loss 1.26 on epoch=149
03/17/2022 18:30:49 - INFO - __main__ - Global step 450 Train loss 1.36 ACC 0.40625 on epoch=149
03/17/2022 18:30:52 - INFO - __main__ - Step 460 Global step 460 Train loss 1.06 on epoch=153
03/17/2022 18:30:55 - INFO - __main__ - Step 470 Global step 470 Train loss 1.04 on epoch=156
03/17/2022 18:30:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.98 on epoch=159
03/17/2022 18:31:01 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=163
03/17/2022 18:31:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=166
03/17/2022 18:31:05 - INFO - __main__ - Global step 500 Train loss 1.00 ACC 0.3125 on epoch=166
03/17/2022 18:31:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.89 on epoch=169
03/17/2022 18:31:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=173
03/17/2022 18:31:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=176
03/17/2022 18:31:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=179
03/17/2022 18:31:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=183
03/17/2022 18:31:21 - INFO - __main__ - Global step 550 Train loss 0.82 ACC 0.0 on epoch=183
03/17/2022 18:31:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=186
03/17/2022 18:31:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.74 on epoch=189
03/17/2022 18:31:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=193
03/17/2022 18:31:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=196
03/17/2022 18:31:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=199
03/17/2022 18:31:37 - INFO - __main__ - Global step 600 Train loss 0.69 ACC 0.25 on epoch=199
03/17/2022 18:31:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=203
03/17/2022 18:31:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.59 on epoch=206
03/17/2022 18:31:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.66 on epoch=209
03/17/2022 18:31:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.61 on epoch=213
03/17/2022 18:31:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=216
03/17/2022 18:31:53 - INFO - __main__ - Global step 650 Train loss 0.61 ACC 0.1875 on epoch=216
03/17/2022 18:31:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=219
03/17/2022 18:31:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.68 on epoch=223
03/17/2022 18:32:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.73 on epoch=226
03/17/2022 18:32:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=229
03/17/2022 18:32:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.63 on epoch=233
03/17/2022 18:32:09 - INFO - __main__ - Global step 700 Train loss 0.66 ACC 0.125 on epoch=233
03/17/2022 18:32:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=236
03/17/2022 18:32:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=239
03/17/2022 18:32:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.58 on epoch=243
03/17/2022 18:32:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=246
03/17/2022 18:32:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=249
03/17/2022 18:32:25 - INFO - __main__ - Global step 750 Train loss 0.57 ACC 0.5 on epoch=249
03/17/2022 18:32:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=253
03/17/2022 18:32:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=256
03/17/2022 18:32:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=259
03/17/2022 18:32:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=263
03/17/2022 18:32:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=266
03/17/2022 18:32:41 - INFO - __main__ - Global step 800 Train loss 0.53 ACC 0.5 on epoch=266
03/17/2022 18:32:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=269
03/17/2022 18:32:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=273
03/17/2022 18:32:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=276
03/17/2022 18:32:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=279
03/17/2022 18:32:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=283
03/17/2022 18:32:57 - INFO - __main__ - Global step 850 Train loss 0.52 ACC 0.5 on epoch=283
03/17/2022 18:33:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=286
03/17/2022 18:33:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=289
03/17/2022 18:33:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.54 on epoch=293
03/17/2022 18:33:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=296
03/17/2022 18:33:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=299
03/17/2022 18:33:13 - INFO - __main__ - Global step 900 Train loss 0.53 ACC 0.5 on epoch=299
03/17/2022 18:33:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=303
03/17/2022 18:33:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=306
03/17/2022 18:33:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=309
03/17/2022 18:33:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=313
03/17/2022 18:33:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=316
03/17/2022 18:33:29 - INFO - __main__ - Global step 950 Train loss 0.54 ACC 0.5 on epoch=316
03/17/2022 18:33:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=319
03/17/2022 18:33:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.55 on epoch=323
03/17/2022 18:33:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=326
03/17/2022 18:33:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=329
03/17/2022 18:33:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.56 on epoch=333
03/17/2022 18:33:44 - INFO - __main__ - Global step 1000 Train loss 0.52 ACC 0.0 on epoch=333
03/17/2022 18:33:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=336
03/17/2022 18:33:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=339
03/17/2022 18:33:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=343
03/17/2022 18:33:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=346
03/17/2022 18:34:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=349
03/17/2022 18:34:01 - INFO - __main__ - Global step 1050 Train loss 0.47 ACC 0.21875 on epoch=349
03/17/2022 18:34:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=353
03/17/2022 18:34:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=356
03/17/2022 18:34:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=359
03/17/2022 18:34:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.56 on epoch=363
03/17/2022 18:34:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.67 on epoch=366
03/17/2022 18:34:17 - INFO - __main__ - Global step 1100 Train loss 0.54 ACC 0.46875 on epoch=366
03/17/2022 18:34:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=369
03/17/2022 18:34:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=373
03/17/2022 18:34:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=376
03/17/2022 18:34:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=379
03/17/2022 18:34:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=383
03/17/2022 18:34:32 - INFO - __main__ - Global step 1150 Train loss 0.51 ACC 0.0625 on epoch=383
03/17/2022 18:34:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=386
03/17/2022 18:34:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=389
03/17/2022 18:34:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.52 on epoch=393
03/17/2022 18:34:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=396
03/17/2022 18:34:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.52 on epoch=399
03/17/2022 18:34:49 - INFO - __main__ - Global step 1200 Train loss 0.52 ACC 0.5 on epoch=399
03/17/2022 18:34:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=403
03/17/2022 18:34:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=406
03/17/2022 18:34:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=409
03/17/2022 18:35:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=413
03/17/2022 18:35:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=416
03/17/2022 18:35:05 - INFO - __main__ - Global step 1250 Train loss 0.50 ACC 0.4375 on epoch=416
03/17/2022 18:35:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=419
03/17/2022 18:35:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.80 on epoch=423
03/17/2022 18:35:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.70 on epoch=426
03/17/2022 18:35:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.83 on epoch=429
03/17/2022 18:35:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=433
03/17/2022 18:35:22 - INFO - __main__ - Global step 1300 Train loss 0.69 ACC 0.4375 on epoch=433
03/17/2022 18:35:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=436
03/17/2022 18:35:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=439
03/17/2022 18:35:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=443
03/17/2022 18:35:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=446
03/17/2022 18:35:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.52 on epoch=449
03/17/2022 18:35:38 - INFO - __main__ - Global step 1350 Train loss 0.51 ACC 0.34375 on epoch=449
03/17/2022 18:35:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=453
03/17/2022 18:35:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=456
03/17/2022 18:35:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=459
03/17/2022 18:35:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=463
03/17/2022 18:35:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=466
03/17/2022 18:35:54 - INFO - __main__ - Global step 1400 Train loss 0.49 ACC 0.40625 on epoch=466
03/17/2022 18:35:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=469
03/17/2022 18:36:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=473
03/17/2022 18:36:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=476
03/17/2022 18:36:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=479
03/17/2022 18:36:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=483
03/17/2022 18:36:10 - INFO - __main__ - Global step 1450 Train loss 0.47 ACC 0.25 on epoch=483
03/17/2022 18:36:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=486
03/17/2022 18:36:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=489
03/17/2022 18:36:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=493
03/17/2022 18:36:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=496
03/17/2022 18:36:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=499
03/17/2022 18:36:26 - INFO - __main__ - Global step 1500 Train loss 0.49 ACC 0.46875 on epoch=499
03/17/2022 18:36:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=503
03/17/2022 18:36:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=506
03/17/2022 18:36:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=509
03/17/2022 18:36:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=513
03/17/2022 18:36:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.48 on epoch=516
03/17/2022 18:36:42 - INFO - __main__ - Global step 1550 Train loss 0.47 ACC 0.25 on epoch=516
03/17/2022 18:36:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=519
03/17/2022 18:36:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=523
03/17/2022 18:36:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=526
03/17/2022 18:36:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=529
03/17/2022 18:36:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=533
03/17/2022 18:36:58 - INFO - __main__ - Global step 1600 Train loss 0.46 ACC 0.03125 on epoch=533
03/17/2022 18:37:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=536
03/17/2022 18:37:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=539
03/17/2022 18:37:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=543
03/17/2022 18:37:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=546
03/17/2022 18:37:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.49 on epoch=549
03/17/2022 18:37:14 - INFO - __main__ - Global step 1650 Train loss 0.48 ACC 0.34375 on epoch=549
03/17/2022 18:37:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=553
03/17/2022 18:37:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=556
03/17/2022 18:37:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=559
03/17/2022 18:37:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=563
03/17/2022 18:37:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.50 on epoch=566
03/17/2022 18:37:30 - INFO - __main__ - Global step 1700 Train loss 0.45 ACC 0.21875 on epoch=566
03/17/2022 18:37:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=569
03/17/2022 18:37:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=573
03/17/2022 18:37:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=576
03/17/2022 18:37:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=579
03/17/2022 18:37:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=583
03/17/2022 18:37:46 - INFO - __main__ - Global step 1750 Train loss 0.46 ACC 0.25 on epoch=583
03/17/2022 18:37:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=586
03/17/2022 18:37:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=589
03/17/2022 18:37:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=593
03/17/2022 18:37:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=596
03/17/2022 18:38:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=599
03/17/2022 18:38:02 - INFO - __main__ - Global step 1800 Train loss 0.43 ACC 0.25 on epoch=599
03/17/2022 18:38:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=603
03/17/2022 18:38:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=606
03/17/2022 18:38:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=609
03/17/2022 18:38:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=613
03/17/2022 18:38:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=616
03/17/2022 18:38:18 - INFO - __main__ - Global step 1850 Train loss 0.43 ACC 0.3125 on epoch=616
03/17/2022 18:38:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=619
03/17/2022 18:38:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=623
03/17/2022 18:38:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=626
03/17/2022 18:38:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=629
03/17/2022 18:38:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=633
03/17/2022 18:38:34 - INFO - __main__ - Global step 1900 Train loss 0.42 ACC 0.21875 on epoch=633
03/17/2022 18:38:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=636
03/17/2022 18:38:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=639
03/17/2022 18:38:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=643
03/17/2022 18:38:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=646
03/17/2022 18:38:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=649
03/17/2022 18:38:50 - INFO - __main__ - Global step 1950 Train loss 0.45 ACC 0.25 on epoch=649
03/17/2022 18:38:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=653
03/17/2022 18:38:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=656
03/17/2022 18:38:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=659
03/17/2022 18:39:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=663
03/17/2022 18:39:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=666
03/17/2022 18:39:06 - INFO - __main__ - Global step 2000 Train loss 0.41 ACC 0.5 on epoch=666
03/17/2022 18:39:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=669
03/17/2022 18:39:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=673
03/17/2022 18:39:15 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=676
03/17/2022 18:39:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=679
03/17/2022 18:39:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=683
03/17/2022 18:39:22 - INFO - __main__ - Global step 2050 Train loss 0.42 ACC 0.21875 on epoch=683
03/17/2022 18:39:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=686
03/17/2022 18:39:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=689
03/17/2022 18:39:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.46 on epoch=693
03/17/2022 18:39:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.40 on epoch=696
03/17/2022 18:39:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=699
03/17/2022 18:39:38 - INFO - __main__ - Global step 2100 Train loss 0.42 ACC 0.28125 on epoch=699
03/17/2022 18:39:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=703
03/17/2022 18:39:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=706
03/17/2022 18:39:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.49 on epoch=709
03/17/2022 18:39:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=713
03/17/2022 18:39:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=716
03/17/2022 18:39:54 - INFO - __main__ - Global step 2150 Train loss 0.43 ACC 0.1875 on epoch=716
03/17/2022 18:39:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=719
03/17/2022 18:40:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.50 on epoch=723
03/17/2022 18:40:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.46 on epoch=726
03/17/2022 18:40:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=729
03/17/2022 18:40:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=733
03/17/2022 18:40:10 - INFO - __main__ - Global step 2200 Train loss 0.45 ACC 0.375 on epoch=733
03/17/2022 18:40:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=736
03/17/2022 18:40:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.47 on epoch=739
03/17/2022 18:40:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=743
03/17/2022 18:40:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.48 on epoch=746
03/17/2022 18:40:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=749
03/17/2022 18:40:26 - INFO - __main__ - Global step 2250 Train loss 0.46 ACC 0.125 on epoch=749
03/17/2022 18:40:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=753
03/17/2022 18:40:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=756
03/17/2022 18:40:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.47 on epoch=759
03/17/2022 18:40:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=763
03/17/2022 18:40:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.44 on epoch=766
03/17/2022 18:40:42 - INFO - __main__ - Global step 2300 Train loss 0.45 ACC 0.34375 on epoch=766
03/17/2022 18:40:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=769
03/17/2022 18:40:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=773
03/17/2022 18:40:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=776
03/17/2022 18:40:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.40 on epoch=779
03/17/2022 18:40:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.32 on epoch=783
03/17/2022 18:40:58 - INFO - __main__ - Global step 2350 Train loss 0.37 ACC 0.25 on epoch=783
03/17/2022 18:41:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=786
03/17/2022 18:41:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=789
03/17/2022 18:41:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=793
03/17/2022 18:41:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.41 on epoch=796
03/17/2022 18:41:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=799
03/17/2022 18:41:14 - INFO - __main__ - Global step 2400 Train loss 0.40 ACC 0.4375 on epoch=799
03/17/2022 18:41:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=803
03/17/2022 18:41:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=806
03/17/2022 18:41:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=809
03/17/2022 18:41:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=813
03/17/2022 18:41:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=816
03/17/2022 18:41:30 - INFO - __main__ - Global step 2450 Train loss 0.34 ACC 0.46875 on epoch=816
03/17/2022 18:41:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=819
03/17/2022 18:41:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.34 on epoch=823
03/17/2022 18:41:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=826
03/17/2022 18:41:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.35 on epoch=829
03/17/2022 18:41:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.33 on epoch=833
03/17/2022 18:41:46 - INFO - __main__ - Global step 2500 Train loss 0.34 ACC 0.5 on epoch=833
03/17/2022 18:41:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=836
03/17/2022 18:41:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=839
03/17/2022 18:41:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=843
03/17/2022 18:41:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=846
03/17/2022 18:42:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.37 on epoch=849
03/17/2022 18:42:02 - INFO - __main__ - Global step 2550 Train loss 0.34 ACC 0.46875 on epoch=849
03/17/2022 18:42:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=853
03/17/2022 18:42:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=856
03/17/2022 18:42:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.29 on epoch=859
03/17/2022 18:42:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=863
03/17/2022 18:42:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=866
03/17/2022 18:42:18 - INFO - __main__ - Global step 2600 Train loss 0.34 ACC 0.375 on epoch=866
03/17/2022 18:42:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=869
03/17/2022 18:42:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=873
03/17/2022 18:42:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=876
03/17/2022 18:42:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=879
03/17/2022 18:42:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=883
03/17/2022 18:42:35 - INFO - __main__ - Global step 2650 Train loss 0.35 ACC 0.34375 on epoch=883
03/17/2022 18:42:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.28 on epoch=886
03/17/2022 18:42:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=889
03/17/2022 18:42:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=893
03/17/2022 18:42:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.34 on epoch=896
03/17/2022 18:42:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.37 on epoch=899
03/17/2022 18:42:51 - INFO - __main__ - Global step 2700 Train loss 0.33 ACC 0.5 on epoch=899
03/17/2022 18:42:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.35 on epoch=903
03/17/2022 18:42:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=906
03/17/2022 18:43:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.29 on epoch=909
03/17/2022 18:43:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=913
03/17/2022 18:43:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.30 on epoch=916
03/17/2022 18:43:07 - INFO - __main__ - Global step 2750 Train loss 0.30 ACC 0.375 on epoch=916
03/17/2022 18:43:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.32 on epoch=919
03/17/2022 18:43:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=923
03/17/2022 18:43:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.28 on epoch=926
03/17/2022 18:43:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=929
03/17/2022 18:43:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.30 on epoch=933
03/17/2022 18:43:23 - INFO - __main__ - Global step 2800 Train loss 0.30 ACC 0.53125 on epoch=933
03/17/2022 18:43:23 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=933, global_step=2800
03/17/2022 18:43:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=936
03/17/2022 18:43:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.34 on epoch=939
03/17/2022 18:43:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.25 on epoch=943
03/17/2022 18:43:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=946
03/17/2022 18:43:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.31 on epoch=949
03/17/2022 18:43:39 - INFO - __main__ - Global step 2850 Train loss 0.31 ACC 0.5 on epoch=949
03/17/2022 18:43:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.32 on epoch=953
03/17/2022 18:43:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.40 on epoch=956
03/17/2022 18:43:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.37 on epoch=959
03/17/2022 18:43:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=963
03/17/2022 18:43:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=966
03/17/2022 18:43:55 - INFO - __main__ - Global step 2900 Train loss 0.38 ACC 0.3125 on epoch=966
03/17/2022 18:43:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.44 on epoch=969
03/17/2022 18:44:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.36 on epoch=973
03/17/2022 18:44:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=976
03/17/2022 18:44:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=979
03/17/2022 18:44:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=983
03/17/2022 18:44:11 - INFO - __main__ - Global step 2950 Train loss 0.40 ACC 0.21875 on epoch=983
03/17/2022 18:44:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=986
03/17/2022 18:44:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=989
03/17/2022 18:44:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.34 on epoch=993
03/17/2022 18:44:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=996
03/17/2022 18:44:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.38 on epoch=999
03/17/2022 18:44:27 - INFO - __main__ - Global step 3000 Train loss 0.37 ACC 0.375 on epoch=999
03/17/2022 18:44:27 - INFO - __main__ - save last model!
03/17/2022 18:44:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 18:44:27 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 18:44:27 - INFO - __main__ - Printing 3 examples
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 18:44:27 - INFO - __main__ - ['neutral']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 18:44:27 - INFO - __main__ - ['entailment']
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:44:27 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 18:44:27 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:44:27 - INFO - __main__ - Printing 3 examples
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:44:27 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:44:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:44:27 - INFO - __main__ - Printing 3 examples
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 18:44:27 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:44:27 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:44:27 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:44:29 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_42_0.5_8_predictions.txt
03/17/2022 18:44:29 - INFO - __main__ - ACC on test data: 0.3571
03/17/2022 18:44:29 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.35714285714285715
03/17/2022 18:44:29 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.4, bsz=8 ...
03/17/2022 18:44:30 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 18:44:30 - INFO - __main__ - Printing 3 examples
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 18:44:30 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:44:30 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 18:44:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 18:44:30 - INFO - __main__ - Printing 3 examples
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 18:44:30 - INFO - __main__ - ['contradiction']
03/17/2022 18:44:30 - INFO - __main__ - Tokenizing Input ...
03/17/2022 18:44:30 - INFO - __main__ - Tokenizing Output ...
03/17/2022 18:44:30 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 18:44:43 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:44:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:44:43 - INFO - __main__ - Starting training!
03/17/2022 18:44:45 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 18:44:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 18:44:46 - INFO - __main__ - Starting training!
03/17/2022 18:44:52 - INFO - __main__ - Step 10 Global step 10 Train loss 5.37 on epoch=3
03/17/2022 18:44:55 - INFO - __main__ - Step 20 Global step 20 Train loss 2.41 on epoch=6
03/17/2022 18:44:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.45 on epoch=9
03/17/2022 18:45:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.04 on epoch=13
03/17/2022 18:45:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.85 on epoch=16
03/17/2022 18:45:05 - INFO - __main__ - Global step 50 Train loss 2.23 ACC 0.5 on epoch=16
03/17/2022 18:45:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 18:45:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=19
03/17/2022 18:45:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=23
03/17/2022 18:45:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.58 on epoch=26
03/17/2022 18:45:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
03/17/2022 18:45:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=33
03/17/2022 18:45:21 - INFO - __main__ - Global step 100 Train loss 0.62 ACC 0.09375 on epoch=33
03/17/2022 18:45:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=36
03/17/2022 18:45:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=39
03/17/2022 18:45:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
03/17/2022 18:45:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
03/17/2022 18:45:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=49
03/17/2022 18:45:37 - INFO - __main__ - Global step 150 Train loss 0.54 ACC 0.3125 on epoch=49
03/17/2022 18:45:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
03/17/2022 18:45:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
03/17/2022 18:45:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
03/17/2022 18:45:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
03/17/2022 18:45:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=66
03/17/2022 18:45:53 - INFO - __main__ - Global step 200 Train loss 0.47 ACC 0.5 on epoch=66
03/17/2022 18:45:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
03/17/2022 18:45:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
03/17/2022 18:46:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
03/17/2022 18:46:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
03/17/2022 18:46:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=83
03/17/2022 18:46:09 - INFO - __main__ - Global step 250 Train loss 0.47 ACC 0.5 on epoch=83
03/17/2022 18:46:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
03/17/2022 18:46:15 - INFO - __main__ - Step 270 Global step 270 Train loss 1.13 on epoch=89
03/17/2022 18:46:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=93
03/17/2022 18:46:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=96
03/17/2022 18:46:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=99
03/17/2022 18:46:25 - INFO - __main__ - Global step 300 Train loss 0.76 ACC 0.46875 on epoch=99
03/17/2022 18:46:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=103
03/17/2022 18:46:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.56 on epoch=106
03/17/2022 18:46:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=109
03/17/2022 18:46:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
03/17/2022 18:46:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=116
03/17/2022 18:46:41 - INFO - __main__ - Global step 350 Train loss 0.53 ACC 0.5 on epoch=116
03/17/2022 18:46:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
03/17/2022 18:46:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=123
03/17/2022 18:46:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=126
03/17/2022 18:46:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
03/17/2022 18:46:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=133
03/17/2022 18:46:57 - INFO - __main__ - Global step 400 Train loss 0.49 ACC 0.5 on epoch=133
03/17/2022 18:47:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=136
03/17/2022 18:47:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=139
03/17/2022 18:47:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
03/17/2022 18:47:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=146
03/17/2022 18:47:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=149
03/17/2022 18:47:13 - INFO - __main__ - Global step 450 Train loss 0.47 ACC 0.5 on epoch=149
03/17/2022 18:47:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=153
03/17/2022 18:47:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=156
03/17/2022 18:47:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=159
03/17/2022 18:47:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
03/17/2022 18:47:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=166
03/17/2022 18:47:29 - INFO - __main__ - Global step 500 Train loss 0.47 ACC 0.53125 on epoch=166
03/17/2022 18:47:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=166, global_step=500
03/17/2022 18:47:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=169
03/17/2022 18:47:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=173
03/17/2022 18:47:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=176
03/17/2022 18:47:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=179
03/17/2022 18:47:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=183
03/17/2022 18:47:45 - INFO - __main__ - Global step 550 Train loss 0.46 ACC 0.46875 on epoch=183
03/17/2022 18:47:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=186
03/17/2022 18:47:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=189
03/17/2022 18:47:54 - INFO - __main__ - Step 580 Global step 580 Train loss 1.84 on epoch=193
03/17/2022 18:47:57 - INFO - __main__ - Step 590 Global step 590 Train loss 2.82 on epoch=196
03/17/2022 18:48:00 - INFO - __main__ - Step 600 Global step 600 Train loss 3.75 on epoch=199
03/17/2022 18:48:01 - INFO - __main__ - Global step 600 Train loss 1.88 ACC 0.53125 on epoch=199
03/17/2022 18:48:04 - INFO - __main__ - Step 610 Global step 610 Train loss 1.87 on epoch=203
03/17/2022 18:48:07 - INFO - __main__ - Step 620 Global step 620 Train loss 1.28 on epoch=206
03/17/2022 18:48:10 - INFO - __main__ - Step 630 Global step 630 Train loss 1.46 on epoch=209
03/17/2022 18:48:13 - INFO - __main__ - Step 640 Global step 640 Train loss 3.25 on epoch=213
03/17/2022 18:48:16 - INFO - __main__ - Step 650 Global step 650 Train loss 1.10 on epoch=216
03/17/2022 18:48:17 - INFO - __main__ - Global step 650 Train loss 1.79 ACC 0.5 on epoch=216
03/17/2022 18:48:20 - INFO - __main__ - Step 660 Global step 660 Train loss 1.08 on epoch=219
03/17/2022 18:48:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.90 on epoch=223
03/17/2022 18:48:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.90 on epoch=226
03/17/2022 18:48:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=229
03/17/2022 18:48:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=233
03/17/2022 18:48:32 - INFO - __main__ - Global step 700 Train loss 0.81 ACC 0.5 on epoch=233
03/17/2022 18:48:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=236
03/17/2022 18:48:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.57 on epoch=239
03/17/2022 18:48:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=243
03/17/2022 18:48:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=246
03/17/2022 18:48:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.61 on epoch=249
03/17/2022 18:48:48 - INFO - __main__ - Global step 750 Train loss 0.56 ACC 0.5 on epoch=249
03/17/2022 18:48:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=253
03/17/2022 18:48:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=256
03/17/2022 18:48:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=259
03/17/2022 18:49:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=263
03/17/2022 18:49:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=266
03/17/2022 18:49:04 - INFO - __main__ - Global step 800 Train loss 0.52 ACC 0.5 on epoch=266
03/17/2022 18:49:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.59 on epoch=269
03/17/2022 18:49:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.56 on epoch=273
03/17/2022 18:49:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=276
03/17/2022 18:49:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=279
03/17/2022 18:49:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=283
03/17/2022 18:49:20 - INFO - __main__ - Global step 850 Train loss 0.54 ACC 0.46875 on epoch=283
03/17/2022 18:49:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=286
03/17/2022 18:49:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=289
03/17/2022 18:49:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.60 on epoch=293
03/17/2022 18:49:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=296
03/17/2022 18:49:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=299
03/17/2022 18:49:35 - INFO - __main__ - Global step 900 Train loss 0.53 ACC 0.5 on epoch=299
03/17/2022 18:49:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=303
03/17/2022 18:49:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=306
03/17/2022 18:49:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=309
03/17/2022 18:49:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=313
03/17/2022 18:49:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=316
03/17/2022 18:49:51 - INFO - __main__ - Global step 950 Train loss 0.49 ACC 0.5 on epoch=316
03/17/2022 18:49:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=319
03/17/2022 18:49:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=323
03/17/2022 18:50:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=326
03/17/2022 18:50:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=329
03/17/2022 18:50:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=333
03/17/2022 18:50:07 - INFO - __main__ - Global step 1000 Train loss 0.49 ACC 0.5 on epoch=333
03/17/2022 18:50:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.54 on epoch=336
03/17/2022 18:50:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=339
03/17/2022 18:50:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=343
03/17/2022 18:50:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=346
03/17/2022 18:50:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=349
03/17/2022 18:50:23 - INFO - __main__ - Global step 1050 Train loss 0.49 ACC 0.5 on epoch=349
03/17/2022 18:50:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=353
03/17/2022 18:50:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=356
03/17/2022 18:50:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=359
03/17/2022 18:50:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=363
03/17/2022 18:50:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=366
03/17/2022 18:50:39 - INFO - __main__ - Global step 1100 Train loss 0.49 ACC 0.5 on epoch=366
03/17/2022 18:50:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=369
03/17/2022 18:50:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=373
03/17/2022 18:50:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=376
03/17/2022 18:50:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=379
03/17/2022 18:50:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=383
03/17/2022 18:50:55 - INFO - __main__ - Global step 1150 Train loss 0.49 ACC 0.5 on epoch=383
03/17/2022 18:50:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=386
03/17/2022 18:51:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=389
03/17/2022 18:51:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=393
03/17/2022 18:51:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=396
03/17/2022 18:51:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=399
03/17/2022 18:51:11 - INFO - __main__ - Global step 1200 Train loss 0.48 ACC 0.5 on epoch=399
03/17/2022 18:51:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=403
03/17/2022 18:51:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=406
03/17/2022 18:51:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=409
03/17/2022 18:51:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=413
03/17/2022 18:51:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=416
03/17/2022 18:51:27 - INFO - __main__ - Global step 1250 Train loss 0.46 ACC 0.5 on epoch=416
03/17/2022 18:51:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=419
03/17/2022 18:51:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=423
03/17/2022 18:51:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=426
03/17/2022 18:51:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=429
03/17/2022 18:51:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=433
03/17/2022 18:51:43 - INFO - __main__ - Global step 1300 Train loss 0.48 ACC 0.5 on epoch=433
03/17/2022 18:51:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=436
03/17/2022 18:51:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=439
03/17/2022 18:51:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=443
03/17/2022 18:51:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=446
03/17/2022 18:51:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=449
03/17/2022 18:51:59 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=449
03/17/2022 18:52:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=453
03/17/2022 18:52:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=456
03/17/2022 18:52:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=459
03/17/2022 18:52:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=463
03/17/2022 18:52:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=466
03/17/2022 18:52:15 - INFO - __main__ - Global step 1400 Train loss 0.47 ACC 0.46875 on epoch=466
03/17/2022 18:52:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=469
03/17/2022 18:52:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=473
03/17/2022 18:52:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=476
03/17/2022 18:52:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=479
03/17/2022 18:52:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=483
03/17/2022 18:52:32 - INFO - __main__ - Global step 1450 Train loss 0.45 ACC 0.4375 on epoch=483
03/17/2022 18:52:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.53 on epoch=486
03/17/2022 18:52:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=489
03/17/2022 18:52:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=493
03/17/2022 18:52:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=496
03/17/2022 18:52:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.49 on epoch=499
03/17/2022 18:52:48 - INFO - __main__ - Global step 1500 Train loss 0.48 ACC 0.46875 on epoch=499
03/17/2022 18:52:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=503
03/17/2022 18:52:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=506
03/17/2022 18:52:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=509
03/17/2022 18:53:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=513
03/17/2022 18:53:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=516
03/17/2022 18:53:04 - INFO - __main__ - Global step 1550 Train loss 0.44 ACC 0.5 on epoch=516
03/17/2022 18:53:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=519
03/17/2022 18:53:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=523
03/17/2022 18:53:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=526
03/17/2022 18:53:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.54 on epoch=529
03/17/2022 18:53:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=533
03/17/2022 18:53:20 - INFO - __main__ - Global step 1600 Train loss 0.46 ACC 0.4375 on epoch=533
03/17/2022 18:53:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=536
03/17/2022 18:53:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=539
03/17/2022 18:53:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=543
03/17/2022 18:53:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=546
03/17/2022 18:53:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=549
03/17/2022 18:53:36 - INFO - __main__ - Global step 1650 Train loss 0.46 ACC 0.375 on epoch=549
03/17/2022 18:53:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=553
03/17/2022 18:53:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=556
03/17/2022 18:53:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=559
03/17/2022 18:53:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=563
03/17/2022 18:53:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=566
03/17/2022 18:53:52 - INFO - __main__ - Global step 1700 Train loss 0.44 ACC 0.46875 on epoch=566
03/17/2022 18:53:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=569
03/17/2022 18:53:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=573
03/17/2022 18:54:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=576
03/17/2022 18:54:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=579
03/17/2022 18:54:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=583
03/17/2022 18:54:08 - INFO - __main__ - Global step 1750 Train loss 0.44 ACC 0.46875 on epoch=583
03/17/2022 18:54:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=586
03/17/2022 18:54:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=589
03/17/2022 18:54:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=593
03/17/2022 18:54:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=596
03/17/2022 18:54:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=599
03/17/2022 18:54:24 - INFO - __main__ - Global step 1800 Train loss 0.42 ACC 0.46875 on epoch=599
03/17/2022 18:54:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=603
03/17/2022 18:54:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=606
03/17/2022 18:54:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=609
03/17/2022 18:54:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=613
03/17/2022 18:54:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=616
03/17/2022 18:54:40 - INFO - __main__ - Global step 1850 Train loss 0.42 ACC 0.5 on epoch=616
03/17/2022 18:54:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=619
03/17/2022 18:54:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=623
03/17/2022 18:54:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=626
03/17/2022 18:54:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=629
03/17/2022 18:54:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=633
03/17/2022 18:54:56 - INFO - __main__ - Global step 1900 Train loss 0.43 ACC 0.5 on epoch=633
03/17/2022 18:54:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=636
03/17/2022 18:55:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=639
03/17/2022 18:55:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=643
03/17/2022 18:55:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=646
03/17/2022 18:55:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=649
03/17/2022 18:55:12 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=649
03/17/2022 18:55:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=653
03/17/2022 18:55:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=656
03/17/2022 18:55:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=659
03/17/2022 18:55:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.47 on epoch=663
03/17/2022 18:55:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=666
03/17/2022 18:55:28 - INFO - __main__ - Global step 2000 Train loss 0.43 ACC 0.40625 on epoch=666
03/17/2022 18:55:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=669
03/17/2022 18:55:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=673
03/17/2022 18:55:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=676
03/17/2022 18:55:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=679
03/17/2022 18:55:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=683
03/17/2022 18:55:44 - INFO - __main__ - Global step 2050 Train loss 0.41 ACC 0.4375 on epoch=683
03/17/2022 18:55:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.45 on epoch=686
03/17/2022 18:55:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=689
03/17/2022 18:55:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=693
03/17/2022 18:55:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=696
03/17/2022 18:55:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.41 on epoch=699
03/17/2022 18:56:00 - INFO - __main__ - Global step 2100 Train loss 0.44 ACC 0.46875 on epoch=699
03/17/2022 18:56:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.45 on epoch=703
03/17/2022 18:56:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=706
03/17/2022 18:56:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=709
03/17/2022 18:56:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=713
03/17/2022 18:56:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=716
03/17/2022 18:56:16 - INFO - __main__ - Global step 2150 Train loss 0.42 ACC 0.46875 on epoch=716
03/17/2022 18:56:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=719
03/17/2022 18:56:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=723
03/17/2022 18:56:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=726
03/17/2022 18:56:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=729
03/17/2022 18:56:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=733
03/17/2022 18:56:32 - INFO - __main__ - Global step 2200 Train loss 0.43 ACC 0.28125 on epoch=733
03/17/2022 18:56:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.44 on epoch=736
03/17/2022 18:56:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=739
03/17/2022 18:56:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=743
03/17/2022 18:56:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=746
03/17/2022 18:56:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=749
03/17/2022 18:56:48 - INFO - __main__ - Global step 2250 Train loss 0.43 ACC 0.46875 on epoch=749
03/17/2022 18:56:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=753
03/17/2022 18:56:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=756
03/17/2022 18:56:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=759
03/17/2022 18:57:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=763
03/17/2022 18:57:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.38 on epoch=766
03/17/2022 18:57:04 - INFO - __main__ - Global step 2300 Train loss 0.41 ACC 0.5 on epoch=766
03/17/2022 18:57:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=769
03/17/2022 18:57:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=773
03/17/2022 18:57:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=776
03/17/2022 18:57:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.42 on epoch=779
03/17/2022 18:57:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.43 on epoch=783
03/17/2022 18:57:21 - INFO - __main__ - Global step 2350 Train loss 0.42 ACC 0.46875 on epoch=783
03/17/2022 18:57:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=786
03/17/2022 18:57:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=789
03/17/2022 18:57:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=793
03/17/2022 18:57:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.39 on epoch=796
03/17/2022 18:57:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=799
03/17/2022 18:57:37 - INFO - __main__ - Global step 2400 Train loss 0.39 ACC 0.46875 on epoch=799
03/17/2022 18:57:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=803
03/17/2022 18:57:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=806
03/17/2022 18:57:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=809
03/17/2022 18:57:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=813
03/17/2022 18:57:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.45 on epoch=816
03/17/2022 18:57:53 - INFO - __main__ - Global step 2450 Train loss 0.41 ACC 0.46875 on epoch=816
03/17/2022 18:57:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=819
03/17/2022 18:57:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=823
03/17/2022 18:58:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.39 on epoch=826
03/17/2022 18:58:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=829
03/17/2022 18:58:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.42 on epoch=833
03/17/2022 18:58:09 - INFO - __main__ - Global step 2500 Train loss 0.41 ACC 0.5 on epoch=833
03/17/2022 18:58:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.45 on epoch=836
03/17/2022 18:58:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=839
03/17/2022 18:58:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=843
03/17/2022 18:58:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=846
03/17/2022 18:58:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=849
03/17/2022 18:58:25 - INFO - __main__ - Global step 2550 Train loss 0.42 ACC 0.5 on epoch=849
03/17/2022 18:58:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=853
03/17/2022 18:58:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=856
03/17/2022 18:58:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=859
03/17/2022 18:58:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=863
03/17/2022 18:58:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=866
03/17/2022 18:58:41 - INFO - __main__ - Global step 2600 Train loss 0.40 ACC 0.375 on epoch=866
03/17/2022 18:58:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=869
03/17/2022 18:58:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.35 on epoch=873
03/17/2022 18:58:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.41 on epoch=876
03/17/2022 18:58:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.46 on epoch=879
03/17/2022 18:58:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=883
03/17/2022 18:58:57 - INFO - __main__ - Global step 2650 Train loss 0.39 ACC 0.5 on epoch=883
03/17/2022 18:59:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=886
03/17/2022 18:59:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=889
03/17/2022 18:59:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=893
03/17/2022 18:59:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.43 on epoch=896
03/17/2022 18:59:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=899
03/17/2022 18:59:13 - INFO - __main__ - Global step 2700 Train loss 0.40 ACC 0.4375 on epoch=899
03/17/2022 18:59:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.40 on epoch=903
03/17/2022 18:59:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=906
03/17/2022 18:59:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=909
03/17/2022 18:59:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=913
03/17/2022 18:59:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=916
03/17/2022 18:59:29 - INFO - __main__ - Global step 2750 Train loss 0.40 ACC 0.375 on epoch=916
03/17/2022 18:59:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=919
03/17/2022 18:59:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=923
03/17/2022 18:59:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=926
03/17/2022 18:59:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=929
03/17/2022 18:59:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.36 on epoch=933
03/17/2022 18:59:45 - INFO - __main__ - Global step 2800 Train loss 0.37 ACC 0.375 on epoch=933
03/17/2022 18:59:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=936
03/17/2022 18:59:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=939
03/17/2022 18:59:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.33 on epoch=943
03/17/2022 18:59:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.38 on epoch=946
03/17/2022 19:00:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=949
03/17/2022 19:00:02 - INFO - __main__ - Global step 2850 Train loss 0.36 ACC 0.3125 on epoch=949
03/17/2022 19:00:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=953
03/17/2022 19:00:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=956
03/17/2022 19:00:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=959
03/17/2022 19:00:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=963
03/17/2022 19:00:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.39 on epoch=966
03/17/2022 19:00:18 - INFO - __main__ - Global step 2900 Train loss 0.38 ACC 0.40625 on epoch=966
03/17/2022 19:00:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.39 on epoch=969
03/17/2022 19:00:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.36 on epoch=973
03/17/2022 19:00:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.45 on epoch=976
03/17/2022 19:00:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.42 on epoch=979
03/17/2022 19:00:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=983
03/17/2022 19:00:34 - INFO - __main__ - Global step 2950 Train loss 0.40 ACC 0.375 on epoch=983
03/17/2022 19:00:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=986
03/17/2022 19:00:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=989
03/17/2022 19:00:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.38 on epoch=993
03/17/2022 19:00:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=996
03/17/2022 19:00:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=999
03/17/2022 19:00:50 - INFO - __main__ - Global step 3000 Train loss 0.38 ACC 0.34375 on epoch=999
03/17/2022 19:00:50 - INFO - __main__ - save last model!
03/17/2022 19:00:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 19:00:50 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 19:00:50 - INFO - __main__ - Printing 3 examples
03/17/2022 19:00:50 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 19:00:50 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:50 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 19:00:50 - INFO - __main__ - ['neutral']
03/17/2022 19:00:50 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 19:00:50 - INFO - __main__ - ['entailment']
03/17/2022 19:00:50 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:00:50 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:00:50 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 19:00:51 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:00:51 - INFO - __main__ - Printing 3 examples
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 19:00:51 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:00:51 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:00:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:00:51 - INFO - __main__ - Printing 3 examples
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 19:00:51 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:51 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:00:51 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:00:51 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:00:53 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_42_0.4_8_predictions.txt
03/17/2022 19:00:53 - INFO - __main__ - ACC on test data: 0.3214
03/17/2022 19:00:53 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.32142857142857145
03/17/2022 19:00:53 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.3, bsz=8 ...
03/17/2022 19:00:54 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:00:54 - INFO - __main__ - Printing 3 examples
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 19:00:54 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:00:54 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:00:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:00:54 - INFO - __main__ - Printing 3 examples
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 19:00:54 - INFO - __main__ - ['contradiction']
03/17/2022 19:00:54 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:00:54 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:00:54 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:01:10 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:01:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:01:11 - INFO - __main__ - Starting training!
03/17/2022 19:01:13 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:01:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:01:13 - INFO - __main__ - Starting training!
03/17/2022 19:01:17 - INFO - __main__ - Step 10 Global step 10 Train loss 5.70 on epoch=3
03/17/2022 19:01:20 - INFO - __main__ - Step 20 Global step 20 Train loss 3.47 on epoch=6
03/17/2022 19:01:23 - INFO - __main__ - Step 30 Global step 30 Train loss 1.75 on epoch=9
03/17/2022 19:01:26 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=13
03/17/2022 19:01:29 - INFO - __main__ - Step 50 Global step 50 Train loss 0.83 on epoch=16
03/17/2022 19:01:30 - INFO - __main__ - Global step 50 Train loss 2.56 ACC 0.15625 on epoch=16
03/17/2022 19:01:30 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.15625 on epoch=16, global_step=50
03/17/2022 19:01:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=19
03/17/2022 19:01:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=23
03/17/2022 19:01:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=26
03/17/2022 19:01:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=29
03/17/2022 19:01:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=33
03/17/2022 19:01:46 - INFO - __main__ - Global step 100 Train loss 0.62 ACC 0.0625 on epoch=33
03/17/2022 19:01:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
03/17/2022 19:01:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
03/17/2022 19:01:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
03/17/2022 19:01:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=46
03/17/2022 19:02:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
03/17/2022 19:02:02 - INFO - __main__ - Global step 150 Train loss 0.50 ACC 0.4375 on epoch=49
03/17/2022 19:02:02 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.4375 on epoch=49, global_step=150
03/17/2022 19:02:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
03/17/2022 19:02:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
03/17/2022 19:02:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=59
03/17/2022 19:02:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
03/17/2022 19:02:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
03/17/2022 19:02:17 - INFO - __main__ - Global step 200 Train loss 0.45 ACC 0.5 on epoch=66
03/17/2022 19:02:17 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=66, global_step=200
03/17/2022 19:02:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
03/17/2022 19:02:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
03/17/2022 19:02:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
03/17/2022 19:02:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
03/17/2022 19:02:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=83
03/17/2022 19:02:33 - INFO - __main__ - Global step 250 Train loss 0.44 ACC 0.3125 on epoch=83
03/17/2022 19:02:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=86
03/17/2022 19:02:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
03/17/2022 19:02:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
03/17/2022 19:02:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
03/17/2022 19:02:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
03/17/2022 19:02:50 - INFO - __main__ - Global step 300 Train loss 0.42 ACC 0.3125 on epoch=99
03/17/2022 19:02:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
03/17/2022 19:02:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/17/2022 19:02:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
03/17/2022 19:03:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=113
03/17/2022 19:03:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
03/17/2022 19:03:06 - INFO - __main__ - Global step 350 Train loss 0.41 ACC 0.5 on epoch=116
03/17/2022 19:03:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
03/17/2022 19:03:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
03/17/2022 19:03:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=126
03/17/2022 19:03:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
03/17/2022 19:03:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
03/17/2022 19:03:22 - INFO - __main__ - Global step 400 Train loss 0.38 ACC 0.46875 on epoch=133
03/17/2022 19:03:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
03/17/2022 19:03:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=139
03/17/2022 19:03:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
03/17/2022 19:03:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
03/17/2022 19:03:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
03/17/2022 19:03:37 - INFO - __main__ - Global step 450 Train loss 0.35 ACC 0.5 on epoch=149
03/17/2022 19:03:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
03/17/2022 19:03:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
03/17/2022 19:03:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=159
03/17/2022 19:03:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
03/17/2022 19:03:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=166
03/17/2022 19:03:53 - INFO - __main__ - Global step 500 Train loss 0.33 ACC 0.53125 on epoch=166
03/17/2022 19:03:54 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=166, global_step=500
03/17/2022 19:03:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=169
03/17/2022 19:04:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=173
03/17/2022 19:04:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=176
03/17/2022 19:04:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
03/17/2022 19:04:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=183
03/17/2022 19:04:09 - INFO - __main__ - Global step 550 Train loss 0.29 ACC 0.5625 on epoch=183
03/17/2022 19:04:09 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=183, global_step=550
03/17/2022 19:04:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
03/17/2022 19:04:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=189
03/17/2022 19:04:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=193
03/17/2022 19:04:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=196
03/17/2022 19:04:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=199
03/17/2022 19:04:26 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.53125 on epoch=199
03/17/2022 19:04:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=203
03/17/2022 19:04:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=206
03/17/2022 19:04:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=209
03/17/2022 19:04:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=213
03/17/2022 19:04:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=216
03/17/2022 19:04:42 - INFO - __main__ - Global step 650 Train loss 0.26 ACC 0.6875 on epoch=216
03/17/2022 19:04:42 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.6875 on epoch=216, global_step=650
03/17/2022 19:04:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=219
03/17/2022 19:04:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=223
03/17/2022 19:04:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
03/17/2022 19:04:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=229
03/17/2022 19:04:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=233
03/17/2022 19:04:58 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.53125 on epoch=233
03/17/2022 19:05:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=236
03/17/2022 19:05:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=239
03/17/2022 19:05:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=243
03/17/2022 19:05:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=246
03/17/2022 19:05:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
03/17/2022 19:05:14 - INFO - __main__ - Global step 750 Train loss 0.22 ACC 0.53125 on epoch=249
03/17/2022 19:05:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
03/17/2022 19:05:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=256
03/17/2022 19:05:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=259
03/17/2022 19:05:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=263
03/17/2022 19:05:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=266
03/17/2022 19:05:30 - INFO - __main__ - Global step 800 Train loss 0.18 ACC 0.53125 on epoch=266
03/17/2022 19:05:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=269
03/17/2022 19:05:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=273
03/17/2022 19:05:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=276
03/17/2022 19:05:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=279
03/17/2022 19:05:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=283
03/17/2022 19:05:46 - INFO - __main__ - Global step 850 Train loss 0.20 ACC 0.53125 on epoch=283
03/17/2022 19:05:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=286
03/17/2022 19:05:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=289
03/17/2022 19:05:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=293
03/17/2022 19:05:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=296
03/17/2022 19:06:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=299
03/17/2022 19:06:02 - INFO - __main__ - Global step 900 Train loss 0.16 ACC 0.625 on epoch=299
03/17/2022 19:06:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
03/17/2022 19:06:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=306
03/17/2022 19:06:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=309
03/17/2022 19:06:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=313
03/17/2022 19:06:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=316
03/17/2022 19:06:18 - INFO - __main__ - Global step 950 Train loss 0.15 ACC 0.625 on epoch=316
03/17/2022 19:06:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=319
03/17/2022 19:06:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=323
03/17/2022 19:06:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=326
03/17/2022 19:06:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
03/17/2022 19:06:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=333
03/17/2022 19:06:34 - INFO - __main__ - Global step 1000 Train loss 0.12 ACC 0.625 on epoch=333
03/17/2022 19:06:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=336
03/17/2022 19:06:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=339
03/17/2022 19:06:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=343
03/17/2022 19:06:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=346
03/17/2022 19:06:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=349
03/17/2022 19:06:50 - INFO - __main__ - Global step 1050 Train loss 0.13 ACC 0.59375 on epoch=349
03/17/2022 19:06:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=353
03/17/2022 19:06:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=356
03/17/2022 19:07:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=359
03/17/2022 19:07:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
03/17/2022 19:07:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=366
03/17/2022 19:07:07 - INFO - __main__ - Global step 1100 Train loss 0.11 ACC 0.53125 on epoch=366
03/17/2022 19:07:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
03/17/2022 19:07:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=373
03/17/2022 19:07:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=376
03/17/2022 19:07:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=379
03/17/2022 19:07:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=383
03/17/2022 19:07:23 - INFO - __main__ - Global step 1150 Train loss 0.10 ACC 0.625 on epoch=383
03/17/2022 19:07:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
03/17/2022 19:07:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=389
03/17/2022 19:07:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=393
03/17/2022 19:07:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
03/17/2022 19:07:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
03/17/2022 19:07:39 - INFO - __main__ - Global step 1200 Train loss 0.09 ACC 0.59375 on epoch=399
03/17/2022 19:07:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
03/17/2022 19:07:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
03/17/2022 19:07:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
03/17/2022 19:07:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
03/17/2022 19:07:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=416
03/17/2022 19:07:55 - INFO - __main__ - Global step 1250 Train loss 0.07 ACC 0.59375 on epoch=416
03/17/2022 19:07:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
03/17/2022 19:08:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
03/17/2022 19:08:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
03/17/2022 19:08:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=429
03/17/2022 19:08:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
03/17/2022 19:08:11 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.59375 on epoch=433
03/17/2022 19:08:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
03/17/2022 19:08:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=439
03/17/2022 19:08:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=443
03/17/2022 19:08:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
03/17/2022 19:08:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
03/17/2022 19:08:28 - INFO - __main__ - Global step 1350 Train loss 0.07 ACC 0.59375 on epoch=449
03/17/2022 19:08:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
03/17/2022 19:08:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
03/17/2022 19:08:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
03/17/2022 19:08:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
03/17/2022 19:08:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
03/17/2022 19:08:43 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.53125 on epoch=466
03/17/2022 19:08:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
03/17/2022 19:08:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
03/17/2022 19:08:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
03/17/2022 19:08:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
03/17/2022 19:08:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
03/17/2022 19:08:59 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.625 on epoch=483
03/17/2022 19:09:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
03/17/2022 19:09:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
03/17/2022 19:09:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
03/17/2022 19:09:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
03/17/2022 19:09:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
03/17/2022 19:09:16 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.59375 on epoch=499
03/17/2022 19:09:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
03/17/2022 19:09:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
03/17/2022 19:09:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
03/17/2022 19:09:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
03/17/2022 19:09:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
03/17/2022 19:09:32 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.5625 on epoch=516
03/17/2022 19:09:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
03/17/2022 19:09:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
03/17/2022 19:09:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=526
03/17/2022 19:09:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
03/17/2022 19:09:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
03/17/2022 19:09:48 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.53125 on epoch=533
03/17/2022 19:09:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
03/17/2022 19:09:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
03/17/2022 19:09:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
03/17/2022 19:10:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=546
03/17/2022 19:10:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
03/17/2022 19:10:04 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.5625 on epoch=549
03/17/2022 19:10:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
03/17/2022 19:10:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
03/17/2022 19:10:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
03/17/2022 19:10:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
03/17/2022 19:10:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
03/17/2022 19:10:20 - INFO - __main__ - Global step 1700 Train loss 0.04 ACC 0.5 on epoch=566
03/17/2022 19:10:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
03/17/2022 19:10:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
03/17/2022 19:10:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
03/17/2022 19:10:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
03/17/2022 19:10:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
03/17/2022 19:10:36 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.53125 on epoch=583
03/17/2022 19:10:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
03/17/2022 19:10:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
03/17/2022 19:10:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
03/17/2022 19:10:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
03/17/2022 19:10:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
03/17/2022 19:10:52 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.53125 on epoch=599
03/17/2022 19:10:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
03/17/2022 19:10:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/17/2022 19:11:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
03/17/2022 19:11:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
03/17/2022 19:11:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
03/17/2022 19:11:08 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.59375 on epoch=616
03/17/2022 19:11:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
03/17/2022 19:11:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
03/17/2022 19:11:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
03/17/2022 19:11:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
03/17/2022 19:11:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
03/17/2022 19:11:24 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.53125 on epoch=633
03/17/2022 19:11:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
03/17/2022 19:11:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
03/17/2022 19:11:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
03/17/2022 19:11:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
03/17/2022 19:11:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/17/2022 19:11:40 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.53125 on epoch=649
03/17/2022 19:11:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
03/17/2022 19:11:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
03/17/2022 19:11:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
03/17/2022 19:11:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
03/17/2022 19:11:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
03/17/2022 19:11:56 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.5625 on epoch=666
03/17/2022 19:11:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
03/17/2022 19:12:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
03/17/2022 19:12:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
03/17/2022 19:12:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
03/17/2022 19:12:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
03/17/2022 19:12:12 - INFO - __main__ - Global step 2050 Train loss 0.01 ACC 0.5625 on epoch=683
03/17/2022 19:12:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
03/17/2022 19:12:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
03/17/2022 19:12:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
03/17/2022 19:12:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
03/17/2022 19:12:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
03/17/2022 19:12:28 - INFO - __main__ - Global step 2100 Train loss 0.00 ACC 0.5625 on epoch=699
03/17/2022 19:12:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
03/17/2022 19:12:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
03/17/2022 19:12:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
03/17/2022 19:12:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/17/2022 19:12:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/17/2022 19:12:44 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.53125 on epoch=716
03/17/2022 19:12:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
03/17/2022 19:12:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
03/17/2022 19:12:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/17/2022 19:12:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
03/17/2022 19:12:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
03/17/2022 19:13:00 - INFO - __main__ - Global step 2200 Train loss 0.00 ACC 0.59375 on epoch=733
03/17/2022 19:13:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
03/17/2022 19:13:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
03/17/2022 19:13:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
03/17/2022 19:13:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
03/17/2022 19:13:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
03/17/2022 19:13:16 - INFO - __main__ - Global step 2250 Train loss 0.01 ACC 0.53125 on epoch=749
03/17/2022 19:13:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
03/17/2022 19:13:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
03/17/2022 19:13:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
03/17/2022 19:13:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
03/17/2022 19:13:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
03/17/2022 19:13:32 - INFO - __main__ - Global step 2300 Train loss 0.01 ACC 0.5625 on epoch=766
03/17/2022 19:13:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/17/2022 19:13:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
03/17/2022 19:13:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
03/17/2022 19:13:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
03/17/2022 19:13:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
03/17/2022 19:13:49 - INFO - __main__ - Global step 2350 Train loss 0.01 ACC 0.53125 on epoch=783
03/17/2022 19:13:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
03/17/2022 19:13:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
03/17/2022 19:13:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
03/17/2022 19:14:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
03/17/2022 19:14:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
03/17/2022 19:14:06 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.46875 on epoch=799
03/17/2022 19:14:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
03/17/2022 19:14:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
03/17/2022 19:14:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
03/17/2022 19:14:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
03/17/2022 19:14:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/17/2022 19:14:22 - INFO - __main__ - Global step 2450 Train loss 0.01 ACC 0.53125 on epoch=816
03/17/2022 19:14:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
03/17/2022 19:14:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
03/17/2022 19:14:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/17/2022 19:14:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/17/2022 19:14:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
03/17/2022 19:14:38 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.53125 on epoch=833
03/17/2022 19:14:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
03/17/2022 19:14:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
03/17/2022 19:14:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/17/2022 19:14:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/17/2022 19:14:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/17/2022 19:14:55 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.625 on epoch=849
03/17/2022 19:14:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
03/17/2022 19:15:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/17/2022 19:15:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
03/17/2022 19:15:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/17/2022 19:15:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
03/17/2022 19:15:10 - INFO - __main__ - Global step 2600 Train loss 0.02 ACC 0.59375 on epoch=866
03/17/2022 19:15:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
03/17/2022 19:15:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
03/17/2022 19:15:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/17/2022 19:15:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
03/17/2022 19:15:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/17/2022 19:15:26 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.53125 on epoch=883
03/17/2022 19:15:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/17/2022 19:15:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
03/17/2022 19:15:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=893
03/17/2022 19:15:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/17/2022 19:15:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/17/2022 19:15:43 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.5 on epoch=899
03/17/2022 19:15:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/17/2022 19:15:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
03/17/2022 19:15:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
03/17/2022 19:15:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
03/17/2022 19:15:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
03/17/2022 19:15:59 - INFO - __main__ - Global step 2750 Train loss 0.00 ACC 0.46875 on epoch=916
03/17/2022 19:16:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/17/2022 19:16:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
03/17/2022 19:16:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/17/2022 19:16:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/17/2022 19:16:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/17/2022 19:16:14 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.5 on epoch=933
03/17/2022 19:16:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
03/17/2022 19:16:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
03/17/2022 19:16:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
03/17/2022 19:16:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/17/2022 19:16:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/17/2022 19:16:30 - INFO - __main__ - Global step 2850 Train loss 0.01 ACC 0.53125 on epoch=949
03/17/2022 19:16:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
03/17/2022 19:16:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
03/17/2022 19:16:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/17/2022 19:16:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
03/17/2022 19:16:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
03/17/2022 19:16:46 - INFO - __main__ - Global step 2900 Train loss 0.01 ACC 0.5625 on epoch=966
03/17/2022 19:16:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
03/17/2022 19:16:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
03/17/2022 19:16:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
03/17/2022 19:16:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/17/2022 19:17:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
03/17/2022 19:17:02 - INFO - __main__ - Global step 2950 Train loss 0.00 ACC 0.53125 on epoch=983
03/17/2022 19:17:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/17/2022 19:17:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
03/17/2022 19:17:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
03/17/2022 19:17:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
03/17/2022 19:17:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/17/2022 19:17:18 - INFO - __main__ - Global step 3000 Train loss 0.00 ACC 0.5 on epoch=999
03/17/2022 19:17:18 - INFO - __main__ - save last model!
03/17/2022 19:17:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 19:17:18 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 19:17:18 - INFO - __main__ - Printing 3 examples
03/17/2022 19:17:18 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 19:17:18 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:18 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 19:17:18 - INFO - __main__ - ['neutral']
03/17/2022 19:17:18 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 19:17:18 - INFO - __main__ - ['entailment']
03/17/2022 19:17:18 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:17:18 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:17:18 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 19:17:19 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:17:19 - INFO - __main__ - Printing 3 examples
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 19:17:19 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:17:19 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:17:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:17:19 - INFO - __main__ - Printing 3 examples
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 19:17:19 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:19 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:17:19 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:17:19 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:17:22 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_42_0.3_8_predictions.txt
03/17/2022 19:17:22 - INFO - __main__ - ACC on test data: 0.5000
03/17/2022 19:17:22 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.3, bsz=8, dev_performance=0.6875, test_performance=0.5
03/17/2022 19:17:22 - INFO - __main__ - Running ... prefix=superglue-cb_16_42, lr=0.2, bsz=8 ...
03/17/2022 19:17:23 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:17:23 - INFO - __main__ - Printing 3 examples
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 19:17:23 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:17:23 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:17:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:17:23 - INFO - __main__ - Printing 3 examples
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: B: I don't know how my parents did it. A: Yeah. B: I mean, there were five of us and I don't recall, you know, wanting anything in particular. Uh, but I don't know how my father did it. He worked at a truck line and he just didn't make that kind of money with five children. But we did okay. We had a house and a home and, but now, my wife and I both work and I don't believe we have as much as my parents did. [SEP] hypothesis: he and his wife have as much as his parents did
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: B: I think the, uh, I think a lot of the commentators on, like the major networks, like right, it's kind of appropriate right now because of the election stuff going on, but, um, it seems that, um, they kind of get to throw their opinions into how they, you know, report on the news. A: Right. And I think even in the elections, they choose who they're going to follow and who they're not, and basically you know, if a candidate can get them to follow, then the news will, you know, kind of publicize his name. B: Yeah.  Yeah, exactly. A: I don't think that the way I get the news is the right way to get it. [SEP] hypothesis: the way she gets the news is the right way to get it
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ -  [superglue-cb] premise: A: Do you go to museums in Europe? B: Uh, actually, no, I don't think I went to any of them. [SEP] hypothesis: she went to some of them
03/17/2022 19:17:23 - INFO - __main__ - ['contradiction']
03/17/2022 19:17:23 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:17:23 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:17:23 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:17:34 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:17:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:17:35 - INFO - __main__ - Starting training!
03/17/2022 19:17:40 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:17:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:17:40 - INFO - __main__ - Starting training!
03/17/2022 19:17:44 - INFO - __main__ - Step 10 Global step 10 Train loss 6.33 on epoch=3
03/17/2022 19:17:47 - INFO - __main__ - Step 20 Global step 20 Train loss 4.46 on epoch=6
03/17/2022 19:17:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.76 on epoch=9
03/17/2022 19:17:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=13
03/17/2022 19:17:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=16
03/17/2022 19:17:57 - INFO - __main__ - Global step 50 Train loss 3.28 ACC 0.5 on epoch=16
03/17/2022 19:17:57 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 19:18:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=19
03/17/2022 19:18:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.72 on epoch=23
03/17/2022 19:18:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.73 on epoch=26
03/17/2022 19:18:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=29
03/17/2022 19:18:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=33
03/17/2022 19:18:13 - INFO - __main__ - Global step 100 Train loss 0.74 ACC 0.09375 on epoch=33
03/17/2022 19:18:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=36
03/17/2022 19:18:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=39
03/17/2022 19:18:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
03/17/2022 19:18:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=46
03/17/2022 19:18:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
03/17/2022 19:18:29 - INFO - __main__ - Global step 150 Train loss 0.55 ACC 0.5 on epoch=49
03/17/2022 19:18:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=53
03/17/2022 19:18:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
03/17/2022 19:18:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
03/17/2022 19:18:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
03/17/2022 19:18:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
03/17/2022 19:18:45 - INFO - __main__ - Global step 200 Train loss 0.49 ACC 0.46875 on epoch=66
03/17/2022 19:18:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
03/17/2022 19:18:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
03/17/2022 19:18:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
03/17/2022 19:18:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
03/17/2022 19:19:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
03/17/2022 19:19:01 - INFO - __main__ - Global step 250 Train loss 0.47 ACC 0.3125 on epoch=83
03/17/2022 19:19:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
03/17/2022 19:19:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
03/17/2022 19:19:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=93
03/17/2022 19:19:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
03/17/2022 19:19:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
03/17/2022 19:19:18 - INFO - __main__ - Global step 300 Train loss 0.43 ACC 0.15625 on epoch=99
03/17/2022 19:19:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
03/17/2022 19:19:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
03/17/2022 19:19:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
03/17/2022 19:19:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=113
03/17/2022 19:19:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=116
03/17/2022 19:19:34 - INFO - __main__ - Global step 350 Train loss 0.46 ACC 0.28125 on epoch=116
03/17/2022 19:19:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
03/17/2022 19:19:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
03/17/2022 19:19:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
03/17/2022 19:19:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
03/17/2022 19:19:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=133
03/17/2022 19:19:50 - INFO - __main__ - Global step 400 Train loss 0.40 ACC 0.1875 on epoch=133
03/17/2022 19:19:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=136
03/17/2022 19:19:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=139
03/17/2022 19:19:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
03/17/2022 19:20:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
03/17/2022 19:20:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=149
03/17/2022 19:20:07 - INFO - __main__ - Global step 450 Train loss 0.38 ACC 0.28125 on epoch=149
03/17/2022 19:20:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=153
03/17/2022 19:20:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
03/17/2022 19:20:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=159
03/17/2022 19:20:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=163
03/17/2022 19:20:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
03/17/2022 19:20:23 - INFO - __main__ - Global step 500 Train loss 0.34 ACC 0.28125 on epoch=166
03/17/2022 19:20:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=169
03/17/2022 19:20:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=173
03/17/2022 19:20:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
03/17/2022 19:20:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=179
03/17/2022 19:20:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=183
03/17/2022 19:20:40 - INFO - __main__ - Global step 550 Train loss 0.33 ACC 0.25 on epoch=183
03/17/2022 19:20:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
03/17/2022 19:20:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
03/17/2022 19:20:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=193
03/17/2022 19:20:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=196
03/17/2022 19:20:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=199
03/17/2022 19:20:56 - INFO - __main__ - Global step 600 Train loss 0.35 ACC 0.3125 on epoch=199
03/17/2022 19:20:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=203
03/17/2022 19:21:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
03/17/2022 19:21:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=209
03/17/2022 19:21:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=213
03/17/2022 19:21:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=216
03/17/2022 19:21:12 - INFO - __main__ - Global step 650 Train loss 0.30 ACC 0.28125 on epoch=216
03/17/2022 19:21:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=219
03/17/2022 19:21:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=223
03/17/2022 19:21:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=226
03/17/2022 19:21:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=229
03/17/2022 19:21:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=233
03/17/2022 19:21:28 - INFO - __main__ - Global step 700 Train loss 0.28 ACC 0.5 on epoch=233
03/17/2022 19:21:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=236
03/17/2022 19:21:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=239
03/17/2022 19:21:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=243
03/17/2022 19:21:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
03/17/2022 19:21:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=249
03/17/2022 19:21:45 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.53125 on epoch=249
03/17/2022 19:21:45 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=249, global_step=750
03/17/2022 19:21:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=253
03/17/2022 19:21:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=256
03/17/2022 19:21:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=259
03/17/2022 19:21:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=263
03/17/2022 19:22:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=266
03/17/2022 19:22:01 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.4375 on epoch=266
03/17/2022 19:22:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
03/17/2022 19:22:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
03/17/2022 19:22:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=276
03/17/2022 19:22:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=279
03/17/2022 19:22:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=283
03/17/2022 19:22:17 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=283
03/17/2022 19:22:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=286
03/17/2022 19:22:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=289
03/17/2022 19:22:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=293
03/17/2022 19:22:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=296
03/17/2022 19:22:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=299
03/17/2022 19:22:34 - INFO - __main__ - Global step 900 Train loss 0.22 ACC 0.46875 on epoch=299
03/17/2022 19:22:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=303
03/17/2022 19:22:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
03/17/2022 19:22:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=309
03/17/2022 19:22:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=313
03/17/2022 19:22:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=316
03/17/2022 19:22:50 - INFO - __main__ - Global step 950 Train loss 0.20 ACC 0.5625 on epoch=316
03/17/2022 19:22:50 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=316, global_step=950
03/17/2022 19:22:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=319
03/17/2022 19:22:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=323
03/17/2022 19:22:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=326
03/17/2022 19:23:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=329
03/17/2022 19:23:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=333
03/17/2022 19:23:06 - INFO - __main__ - Global step 1000 Train loss 0.19 ACC 0.4375 on epoch=333
03/17/2022 19:23:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=336
03/17/2022 19:23:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=339
03/17/2022 19:23:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=343
03/17/2022 19:23:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=346
03/17/2022 19:23:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=349
03/17/2022 19:23:22 - INFO - __main__ - Global step 1050 Train loss 0.22 ACC 0.28125 on epoch=349
03/17/2022 19:23:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=353
03/17/2022 19:23:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=356
03/17/2022 19:23:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=359
03/17/2022 19:23:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
03/17/2022 19:23:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=366
03/17/2022 19:23:38 - INFO - __main__ - Global step 1100 Train loss 0.18 ACC 0.625 on epoch=366
03/17/2022 19:23:38 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.625 on epoch=366, global_step=1100
03/17/2022 19:23:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=369
03/17/2022 19:23:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=373
03/17/2022 19:23:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=376
03/17/2022 19:23:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=379
03/17/2022 19:23:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=383
03/17/2022 19:23:55 - INFO - __main__ - Global step 1150 Train loss 0.19 ACC 0.53125 on epoch=383
03/17/2022 19:23:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=386
03/17/2022 19:24:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=389
03/17/2022 19:24:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=393
03/17/2022 19:24:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=396
03/17/2022 19:24:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=399
03/17/2022 19:24:11 - INFO - __main__ - Global step 1200 Train loss 0.17 ACC 0.40625 on epoch=399
03/17/2022 19:24:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=403
03/17/2022 19:24:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=406
03/17/2022 19:24:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=409
03/17/2022 19:24:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=413
03/17/2022 19:24:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=416
03/17/2022 19:24:27 - INFO - __main__ - Global step 1250 Train loss 0.16 ACC 0.59375 on epoch=416
03/17/2022 19:24:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=419
03/17/2022 19:24:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=423
03/17/2022 19:24:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=426
03/17/2022 19:24:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=429
03/17/2022 19:24:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=433
03/17/2022 19:24:43 - INFO - __main__ - Global step 1300 Train loss 0.16 ACC 0.6875 on epoch=433
03/17/2022 19:24:43 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=433, global_step=1300
03/17/2022 19:24:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=436
03/17/2022 19:24:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=439
03/17/2022 19:24:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
03/17/2022 19:24:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=446
03/17/2022 19:24:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=449
03/17/2022 19:25:00 - INFO - __main__ - Global step 1350 Train loss 0.15 ACC 0.625 on epoch=449
03/17/2022 19:25:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=453
03/17/2022 19:25:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=456
03/17/2022 19:25:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=459
03/17/2022 19:25:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=463
03/17/2022 19:25:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=466
03/17/2022 19:25:16 - INFO - __main__ - Global step 1400 Train loss 0.14 ACC 0.65625 on epoch=466
03/17/2022 19:25:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=469
03/17/2022 19:25:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=473
03/17/2022 19:25:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=476
03/17/2022 19:25:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=479
03/17/2022 19:25:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=483
03/17/2022 19:25:32 - INFO - __main__ - Global step 1450 Train loss 0.12 ACC 0.5 on epoch=483
03/17/2022 19:25:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=486
03/17/2022 19:25:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
03/17/2022 19:25:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=493
03/17/2022 19:25:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=496
03/17/2022 19:25:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=499
03/17/2022 19:25:48 - INFO - __main__ - Global step 1500 Train loss 0.11 ACC 0.46875 on epoch=499
03/17/2022 19:25:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
03/17/2022 19:25:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=506
03/17/2022 19:25:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=509
03/17/2022 19:26:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
03/17/2022 19:26:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=516
03/17/2022 19:26:05 - INFO - __main__ - Global step 1550 Train loss 0.08 ACC 0.5625 on epoch=516
03/17/2022 19:26:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
03/17/2022 19:26:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
03/17/2022 19:26:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
03/17/2022 19:26:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
03/17/2022 19:26:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=533
03/17/2022 19:26:21 - INFO - __main__ - Global step 1600 Train loss 0.07 ACC 0.59375 on epoch=533
03/17/2022 19:26:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
03/17/2022 19:26:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=539
03/17/2022 19:26:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
03/17/2022 19:26:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=546
03/17/2022 19:26:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
03/17/2022 19:26:37 - INFO - __main__ - Global step 1650 Train loss 0.07 ACC 0.53125 on epoch=549
03/17/2022 19:26:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
03/17/2022 19:26:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
03/17/2022 19:26:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=559
03/17/2022 19:26:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=563
03/17/2022 19:26:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
03/17/2022 19:26:54 - INFO - __main__ - Global step 1700 Train loss 0.06 ACC 0.5625 on epoch=566
03/17/2022 19:26:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
03/17/2022 19:27:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
03/17/2022 19:27:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
03/17/2022 19:27:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=579
03/17/2022 19:27:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=583
03/17/2022 19:27:10 - INFO - __main__ - Global step 1750 Train loss 0.05 ACC 0.53125 on epoch=583
03/17/2022 19:27:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
03/17/2022 19:27:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
03/17/2022 19:27:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=593
03/17/2022 19:27:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=596
03/17/2022 19:27:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
03/17/2022 19:27:26 - INFO - __main__ - Global step 1800 Train loss 0.05 ACC 0.375 on epoch=599
03/17/2022 19:27:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
03/17/2022 19:27:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
03/17/2022 19:27:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
03/17/2022 19:27:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
03/17/2022 19:27:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
03/17/2022 19:27:42 - INFO - __main__ - Global step 1850 Train loss 0.03 ACC 0.5 on epoch=616
03/17/2022 19:27:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
03/17/2022 19:27:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
03/17/2022 19:27:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=626
03/17/2022 19:27:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
03/17/2022 19:27:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
03/17/2022 19:27:57 - INFO - __main__ - Global step 1900 Train loss 0.04 ACC 0.5 on epoch=633
03/17/2022 19:28:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
03/17/2022 19:28:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
03/17/2022 19:28:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
03/17/2022 19:28:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
03/17/2022 19:28:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
03/17/2022 19:28:13 - INFO - __main__ - Global step 1950 Train loss 0.02 ACC 0.53125 on epoch=649
03/17/2022 19:28:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
03/17/2022 19:28:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
03/17/2022 19:28:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
03/17/2022 19:28:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
03/17/2022 19:28:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
03/17/2022 19:28:29 - INFO - __main__ - Global step 2000 Train loss 0.02 ACC 0.53125 on epoch=666
03/17/2022 19:28:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
03/17/2022 19:28:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
03/17/2022 19:28:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
03/17/2022 19:28:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=679
03/17/2022 19:28:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
03/17/2022 19:28:44 - INFO - __main__ - Global step 2050 Train loss 0.03 ACC 0.40625 on epoch=683
03/17/2022 19:28:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
03/17/2022 19:28:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=689
03/17/2022 19:28:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
03/17/2022 19:28:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
03/17/2022 19:28:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
03/17/2022 19:29:00 - INFO - __main__ - Global step 2100 Train loss 0.04 ACC 0.5625 on epoch=699
03/17/2022 19:29:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
03/17/2022 19:29:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
03/17/2022 19:29:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
03/17/2022 19:29:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
03/17/2022 19:29:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
03/17/2022 19:29:16 - INFO - __main__ - Global step 2150 Train loss 0.01 ACC 0.5 on epoch=716
03/17/2022 19:29:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
03/17/2022 19:29:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
03/17/2022 19:29:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
03/17/2022 19:29:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
03/17/2022 19:29:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=733
03/17/2022 19:29:32 - INFO - __main__ - Global step 2200 Train loss 0.02 ACC 0.46875 on epoch=733
03/17/2022 19:29:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
03/17/2022 19:29:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=739
03/17/2022 19:29:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
03/17/2022 19:29:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
03/17/2022 19:29:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
03/17/2022 19:29:48 - INFO - __main__ - Global step 2250 Train loss 0.03 ACC 0.5 on epoch=749
03/17/2022 19:29:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
03/17/2022 19:29:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
03/17/2022 19:29:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
03/17/2022 19:29:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
03/17/2022 19:30:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
03/17/2022 19:30:03 - INFO - __main__ - Global step 2300 Train loss 0.02 ACC 0.46875 on epoch=766
03/17/2022 19:30:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
03/17/2022 19:30:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
03/17/2022 19:30:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
03/17/2022 19:30:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
03/17/2022 19:30:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
03/17/2022 19:30:19 - INFO - __main__ - Global step 2350 Train loss 0.02 ACC 0.5 on epoch=783
03/17/2022 19:30:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
03/17/2022 19:30:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
03/17/2022 19:30:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
03/17/2022 19:30:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
03/17/2022 19:30:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
03/17/2022 19:30:35 - INFO - __main__ - Global step 2400 Train loss 0.01 ACC 0.40625 on epoch=799
03/17/2022 19:30:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
03/17/2022 19:30:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=806
03/17/2022 19:30:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
03/17/2022 19:30:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
03/17/2022 19:30:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
03/17/2022 19:30:51 - INFO - __main__ - Global step 2450 Train loss 0.02 ACC 0.46875 on epoch=816
03/17/2022 19:30:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/17/2022 19:30:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
03/17/2022 19:30:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
03/17/2022 19:31:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
03/17/2022 19:31:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
03/17/2022 19:31:06 - INFO - __main__ - Global step 2500 Train loss 0.01 ACC 0.53125 on epoch=833
03/17/2022 19:31:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
03/17/2022 19:31:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/17/2022 19:31:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
03/17/2022 19:31:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
03/17/2022 19:31:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
03/17/2022 19:31:22 - INFO - __main__ - Global step 2550 Train loss 0.01 ACC 0.4375 on epoch=849
03/17/2022 19:31:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
03/17/2022 19:31:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
03/17/2022 19:31:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
03/17/2022 19:31:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
03/17/2022 19:31:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
03/17/2022 19:31:38 - INFO - __main__ - Global step 2600 Train loss 0.01 ACC 0.5625 on epoch=866
03/17/2022 19:31:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
03/17/2022 19:31:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
03/17/2022 19:31:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
03/17/2022 19:31:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
03/17/2022 19:31:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
03/17/2022 19:31:53 - INFO - __main__ - Global step 2650 Train loss 0.01 ACC 0.5 on epoch=883
03/17/2022 19:31:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
03/17/2022 19:31:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=889
03/17/2022 19:32:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/17/2022 19:32:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
03/17/2022 19:32:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
03/17/2022 19:32:09 - INFO - __main__ - Global step 2700 Train loss 0.01 ACC 0.46875 on epoch=899
03/17/2022 19:32:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
03/17/2022 19:32:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/17/2022 19:32:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
03/17/2022 19:32:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=913
03/17/2022 19:32:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
03/17/2022 19:32:25 - INFO - __main__ - Global step 2750 Train loss 0.01 ACC 0.5625 on epoch=916
03/17/2022 19:32:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
03/17/2022 19:32:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
03/17/2022 19:32:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
03/17/2022 19:32:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
03/17/2022 19:32:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
03/17/2022 19:32:40 - INFO - __main__ - Global step 2800 Train loss 0.00 ACC 0.5 on epoch=933
03/17/2022 19:32:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
03/17/2022 19:32:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=939
03/17/2022 19:32:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
03/17/2022 19:32:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
03/17/2022 19:32:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
03/17/2022 19:32:56 - INFO - __main__ - Global step 2850 Train loss 0.02 ACC 0.5 on epoch=949
03/17/2022 19:32:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
03/17/2022 19:33:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
03/17/2022 19:33:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
03/17/2022 19:33:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
03/17/2022 19:33:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=966
03/17/2022 19:33:12 - INFO - __main__ - Global step 2900 Train loss 0.02 ACC 0.5625 on epoch=966
03/17/2022 19:33:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
03/17/2022 19:33:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
03/17/2022 19:33:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
03/17/2022 19:33:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
03/17/2022 19:33:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/17/2022 19:33:27 - INFO - __main__ - Global step 2950 Train loss 0.01 ACC 0.46875 on epoch=983
03/17/2022 19:33:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
03/17/2022 19:33:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=989
03/17/2022 19:33:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/17/2022 19:33:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
03/17/2022 19:33:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
03/17/2022 19:33:43 - INFO - __main__ - Global step 3000 Train loss 0.02 ACC 0.46875 on epoch=999
03/17/2022 19:33:43 - INFO - __main__ - save last model!
03/17/2022 19:33:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 19:33:43 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 19:33:43 - INFO - __main__ - Printing 3 examples
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 19:33:43 - INFO - __main__ - ['neutral']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 19:33:43 - INFO - __main__ - ['entailment']
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:33:43 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 19:33:43 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:33:43 - INFO - __main__ - Printing 3 examples
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:33:43 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:33:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:33:43 - INFO - __main__ - Printing 3 examples
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 19:33:43 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:33:43 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:33:43 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:33:45 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_42_0.2_8_predictions.txt
03/17/2022 19:33:45 - INFO - __main__ - ACC on test data: 0.5179
03/17/2022 19:33:46 - INFO - __main__ - prefix=superglue-cb_16_42, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.5178571428571429
03/17/2022 19:33:46 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.5, bsz=8 ...
03/17/2022 19:33:46 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:33:46 - INFO - __main__ - Printing 3 examples
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 19:33:46 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:33:46 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:33:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:33:46 - INFO - __main__ - Printing 3 examples
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 19:33:46 - INFO - __main__ - ['contradiction']
03/17/2022 19:33:46 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:33:46 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:33:47 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:34:02 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:34:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:34:03 - INFO - __main__ - Starting training!
03/17/2022 19:34:06 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:34:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:34:07 - INFO - __main__ - Starting training!
03/17/2022 19:34:11 - INFO - __main__ - Step 10 Global step 10 Train loss 5.43 on epoch=3
03/17/2022 19:34:13 - INFO - __main__ - Step 20 Global step 20 Train loss 5.95 on epoch=6
03/17/2022 19:34:16 - INFO - __main__ - Step 30 Global step 30 Train loss 5.31 on epoch=9
03/17/2022 19:34:19 - INFO - __main__ - Step 40 Global step 40 Train loss 3.76 on epoch=13
03/17/2022 19:34:21 - INFO - __main__ - Step 50 Global step 50 Train loss 3.50 on epoch=16
03/17/2022 19:34:33 - INFO - __main__ - Global step 50 Train loss 4.79 ACC 0.25 on epoch=16
03/17/2022 19:34:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.25 on epoch=16, global_step=50
03/17/2022 19:34:35 - INFO - __main__ - Step 60 Global step 60 Train loss 3.32 on epoch=19
03/17/2022 19:34:38 - INFO - __main__ - Step 70 Global step 70 Train loss 3.00 on epoch=23
03/17/2022 19:34:41 - INFO - __main__ - Step 80 Global step 80 Train loss 2.60 on epoch=26
03/17/2022 19:34:43 - INFO - __main__ - Step 90 Global step 90 Train loss 2.54 on epoch=29
03/17/2022 19:34:46 - INFO - __main__ - Step 100 Global step 100 Train loss 2.20 on epoch=33
03/17/2022 19:34:47 - INFO - __main__ - Global step 100 Train loss 2.73 ACC 0.21875 on epoch=33
03/17/2022 19:34:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.92 on epoch=36
03/17/2022 19:34:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.86 on epoch=39
03/17/2022 19:34:55 - INFO - __main__ - Step 130 Global step 130 Train loss 1.61 on epoch=43
03/17/2022 19:34:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.50 on epoch=46
03/17/2022 19:35:00 - INFO - __main__ - Step 150 Global step 150 Train loss 1.45 on epoch=49
03/17/2022 19:35:01 - INFO - __main__ - Global step 150 Train loss 1.67 ACC 0.5 on epoch=49
03/17/2022 19:35:01 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.5 on epoch=49, global_step=150
03/17/2022 19:35:04 - INFO - __main__ - Step 160 Global step 160 Train loss 1.26 on epoch=53
03/17/2022 19:35:06 - INFO - __main__ - Step 170 Global step 170 Train loss 1.15 on epoch=56
03/17/2022 19:35:09 - INFO - __main__ - Step 180 Global step 180 Train loss 1.13 on epoch=59
03/17/2022 19:35:12 - INFO - __main__ - Step 190 Global step 190 Train loss 1.00 on epoch=63
03/17/2022 19:35:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=66
03/17/2022 19:35:15 - INFO - __main__ - Global step 200 Train loss 1.09 ACC 0.5 on epoch=66
03/17/2022 19:35:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=69
03/17/2022 19:35:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=73
03/17/2022 19:35:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=76
03/17/2022 19:35:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=79
03/17/2022 19:35:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=83
03/17/2022 19:35:30 - INFO - __main__ - Global step 250 Train loss 0.77 ACC 0.1875 on epoch=83
03/17/2022 19:35:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.73 on epoch=86
03/17/2022 19:35:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=89
03/17/2022 19:35:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=93
03/17/2022 19:35:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=96
03/17/2022 19:35:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=99
03/17/2022 19:35:43 - INFO - __main__ - Global step 300 Train loss 0.64 ACC 0.5 on epoch=99
03/17/2022 19:35:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=103
03/17/2022 19:35:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.56 on epoch=106
03/17/2022 19:35:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=109
03/17/2022 19:35:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=113
03/17/2022 19:35:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
03/17/2022 19:35:57 - INFO - __main__ - Global step 350 Train loss 0.57 ACC 0.5 on epoch=116
03/17/2022 19:36:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=119
03/17/2022 19:36:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=123
03/17/2022 19:36:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=126
03/17/2022 19:36:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=129
03/17/2022 19:36:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=133
03/17/2022 19:36:11 - INFO - __main__ - Global step 400 Train loss 0.52 ACC 0.5 on epoch=133
03/17/2022 19:36:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=136
03/17/2022 19:36:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=139
03/17/2022 19:36:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=143
03/17/2022 19:36:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=146
03/17/2022 19:36:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=149
03/17/2022 19:36:25 - INFO - __main__ - Global step 450 Train loss 0.53 ACC 0.15625 on epoch=149
03/17/2022 19:36:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=153
03/17/2022 19:36:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=156
03/17/2022 19:36:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=159
03/17/2022 19:36:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=163
03/17/2022 19:36:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=166
03/17/2022 19:36:40 - INFO - __main__ - Global step 500 Train loss 0.51 ACC 0.5 on epoch=166
03/17/2022 19:36:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=169
03/17/2022 19:36:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=173
03/17/2022 19:36:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=176
03/17/2022 19:36:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=179
03/17/2022 19:36:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=183
03/17/2022 19:36:54 - INFO - __main__ - Global step 550 Train loss 0.49 ACC 0.25 on epoch=183
03/17/2022 19:36:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=186
03/17/2022 19:36:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=189
03/17/2022 19:37:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=193
03/17/2022 19:37:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=196
03/17/2022 19:37:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=199
03/17/2022 19:37:08 - INFO - __main__ - Global step 600 Train loss 0.48 ACC 0.0 on epoch=199
03/17/2022 19:37:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=203
03/17/2022 19:37:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=206
03/17/2022 19:37:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=209
03/17/2022 19:37:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=213
03/17/2022 19:37:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=216
03/17/2022 19:37:22 - INFO - __main__ - Global step 650 Train loss 0.45 ACC 0.5 on epoch=216
03/17/2022 19:37:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=219
03/17/2022 19:37:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=223
03/17/2022 19:37:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=226
03/17/2022 19:37:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=229
03/17/2022 19:37:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=233
03/17/2022 19:37:36 - INFO - __main__ - Global step 700 Train loss 0.46 ACC 0.5 on epoch=233
03/17/2022 19:37:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=236
03/17/2022 19:37:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=239
03/17/2022 19:37:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=243
03/17/2022 19:37:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=246
03/17/2022 19:37:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=249
03/17/2022 19:37:51 - INFO - __main__ - Global step 750 Train loss 0.45 ACC 0.0625 on epoch=249
03/17/2022 19:37:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=253
03/17/2022 19:37:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=256
03/17/2022 19:37:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=259
03/17/2022 19:38:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=263
03/17/2022 19:38:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=266
03/17/2022 19:38:05 - INFO - __main__ - Global step 800 Train loss 0.45 ACC 0.5 on epoch=266
03/17/2022 19:38:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=269
03/17/2022 19:38:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=273
03/17/2022 19:38:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=276
03/17/2022 19:38:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=279
03/17/2022 19:38:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=283
03/17/2022 19:38:19 - INFO - __main__ - Global step 850 Train loss 0.45 ACC 0.5 on epoch=283
03/17/2022 19:38:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=286
03/17/2022 19:38:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=289
03/17/2022 19:38:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=293
03/17/2022 19:38:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=296
03/17/2022 19:38:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=299
03/17/2022 19:38:33 - INFO - __main__ - Global step 900 Train loss 0.42 ACC 0.3125 on epoch=299
03/17/2022 19:38:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=303
03/17/2022 19:38:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=306
03/17/2022 19:38:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=309
03/17/2022 19:38:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=313
03/17/2022 19:38:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=316
03/17/2022 19:38:48 - INFO - __main__ - Global step 950 Train loss 0.42 ACC 0.5 on epoch=316
03/17/2022 19:38:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=319
03/17/2022 19:38:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=323
03/17/2022 19:38:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=326
03/17/2022 19:38:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=329
03/17/2022 19:39:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=333
03/17/2022 19:39:02 - INFO - __main__ - Global step 1000 Train loss 0.42 ACC 0.0 on epoch=333
03/17/2022 19:39:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=336
03/17/2022 19:39:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=339
03/17/2022 19:39:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=343
03/17/2022 19:39:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=346
03/17/2022 19:39:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=349
03/17/2022 19:39:17 - INFO - __main__ - Global step 1050 Train loss 0.41 ACC 0.46875 on epoch=349
03/17/2022 19:39:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=353
03/17/2022 19:39:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=356
03/17/2022 19:39:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=359
03/17/2022 19:39:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=363
03/17/2022 19:39:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=366
03/17/2022 19:39:32 - INFO - __main__ - Global step 1100 Train loss 0.39 ACC 0.46875 on epoch=366
03/17/2022 19:39:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=369
03/17/2022 19:39:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=373
03/17/2022 19:39:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=376
03/17/2022 19:39:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=379
03/17/2022 19:39:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=383
03/17/2022 19:39:46 - INFO - __main__ - Global step 1150 Train loss 0.40 ACC 0.53125 on epoch=383
03/17/2022 19:39:46 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=383, global_step=1150
03/17/2022 19:39:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=386
03/17/2022 19:39:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=389
03/17/2022 19:39:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=393
03/17/2022 19:39:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=396
03/17/2022 19:39:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=399
03/17/2022 19:40:00 - INFO - __main__ - Global step 1200 Train loss 0.35 ACC 0.53125 on epoch=399
03/17/2022 19:40:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=403
03/17/2022 19:40:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=406
03/17/2022 19:40:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=409
03/17/2022 19:40:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=413
03/17/2022 19:40:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=416
03/17/2022 19:40:15 - INFO - __main__ - Global step 1250 Train loss 0.37 ACC 0.53125 on epoch=416
03/17/2022 19:40:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=419
03/17/2022 19:40:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=423
03/17/2022 19:40:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=426
03/17/2022 19:40:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=429
03/17/2022 19:40:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=433
03/17/2022 19:40:29 - INFO - __main__ - Global step 1300 Train loss 0.34 ACC 0.25 on epoch=433
03/17/2022 19:40:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=436
03/17/2022 19:40:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=439
03/17/2022 19:40:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=443
03/17/2022 19:40:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=446
03/17/2022 19:40:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=449
03/17/2022 19:40:44 - INFO - __main__ - Global step 1350 Train loss 0.32 ACC 0.375 on epoch=449
03/17/2022 19:40:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=453
03/17/2022 19:40:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=456
03/17/2022 19:40:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=459
03/17/2022 19:40:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=463
03/17/2022 19:40:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=466
03/17/2022 19:40:58 - INFO - __main__ - Global step 1400 Train loss 0.25 ACC 0.40625 on epoch=466
03/17/2022 19:41:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=469
03/17/2022 19:41:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=473
03/17/2022 19:41:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=476
03/17/2022 19:41:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=479
03/17/2022 19:41:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=483
03/17/2022 19:41:13 - INFO - __main__ - Global step 1450 Train loss 0.21 ACC 0.375 on epoch=483
03/17/2022 19:41:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=486
03/17/2022 19:41:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=489
03/17/2022 19:41:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=493
03/17/2022 19:41:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=496
03/17/2022 19:41:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=499
03/17/2022 19:41:27 - INFO - __main__ - Global step 1500 Train loss 0.19 ACC 0.5 on epoch=499
03/17/2022 19:41:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=503
03/17/2022 19:41:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=506
03/17/2022 19:41:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=509
03/17/2022 19:41:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=513
03/17/2022 19:41:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=516
03/17/2022 19:41:41 - INFO - __main__ - Global step 1550 Train loss 0.18 ACC 0.34375 on epoch=516
03/17/2022 19:41:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=519
03/17/2022 19:41:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=523
03/17/2022 19:41:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=526
03/17/2022 19:41:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=529
03/17/2022 19:41:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=533
03/17/2022 19:41:56 - INFO - __main__ - Global step 1600 Train loss 0.18 ACC 0.40625 on epoch=533
03/17/2022 19:41:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=536
03/17/2022 19:42:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=539
03/17/2022 19:42:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=543
03/17/2022 19:42:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=546
03/17/2022 19:42:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=549
03/17/2022 19:42:10 - INFO - __main__ - Global step 1650 Train loss 0.16 ACC 0.375 on epoch=549
03/17/2022 19:42:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=553
03/17/2022 19:42:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=556
03/17/2022 19:42:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=559
03/17/2022 19:42:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=563
03/17/2022 19:42:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=566
03/17/2022 19:42:25 - INFO - __main__ - Global step 1700 Train loss 0.15 ACC 0.46875 on epoch=566
03/17/2022 19:42:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=569
03/17/2022 19:42:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=573
03/17/2022 19:42:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=576
03/17/2022 19:42:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=579
03/17/2022 19:42:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=583
03/17/2022 19:42:39 - INFO - __main__ - Global step 1750 Train loss 0.14 ACC 0.375 on epoch=583
03/17/2022 19:42:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=586
03/17/2022 19:42:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=589
03/17/2022 19:42:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=593
03/17/2022 19:42:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=596
03/17/2022 19:42:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=599
03/17/2022 19:42:54 - INFO - __main__ - Global step 1800 Train loss 0.13 ACC 0.40625 on epoch=599
03/17/2022 19:42:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=603
03/17/2022 19:42:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=606
03/17/2022 19:43:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=609
03/17/2022 19:43:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=613
03/17/2022 19:43:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=616
03/17/2022 19:43:08 - INFO - __main__ - Global step 1850 Train loss 0.13 ACC 0.40625 on epoch=616
03/17/2022 19:43:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=619
03/17/2022 19:43:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=623
03/17/2022 19:43:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=626
03/17/2022 19:43:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=629
03/17/2022 19:43:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=633
03/17/2022 19:43:22 - INFO - __main__ - Global step 1900 Train loss 0.13 ACC 0.46875 on epoch=633
03/17/2022 19:43:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=636
03/17/2022 19:43:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=639
03/17/2022 19:43:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=643
03/17/2022 19:43:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=646
03/17/2022 19:43:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=649
03/17/2022 19:43:37 - INFO - __main__ - Global step 1950 Train loss 0.10 ACC 0.53125 on epoch=649
03/17/2022 19:43:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=653
03/17/2022 19:43:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=656
03/17/2022 19:43:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=659
03/17/2022 19:43:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=663
03/17/2022 19:43:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=666
03/17/2022 19:43:51 - INFO - __main__ - Global step 2000 Train loss 0.11 ACC 0.4375 on epoch=666
03/17/2022 19:43:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=669
03/17/2022 19:43:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
03/17/2022 19:43:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=676
03/17/2022 19:44:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=679
03/17/2022 19:44:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=683
03/17/2022 19:44:07 - INFO - __main__ - Global step 2050 Train loss 0.09 ACC 0.59375 on epoch=683
03/17/2022 19:44:07 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=683, global_step=2050
03/17/2022 19:44:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=686
03/17/2022 19:44:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=689
03/17/2022 19:44:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=693
03/17/2022 19:44:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=696
03/17/2022 19:44:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=699
03/17/2022 19:44:21 - INFO - __main__ - Global step 2100 Train loss 0.10 ACC 0.5625 on epoch=699
03/17/2022 19:44:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=703
03/17/2022 19:44:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=706
03/17/2022 19:44:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=709
03/17/2022 19:44:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=713
03/17/2022 19:44:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=716
03/17/2022 19:44:36 - INFO - __main__ - Global step 2150 Train loss 0.07 ACC 0.46875 on epoch=716
03/17/2022 19:44:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=719
03/17/2022 19:44:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=723
03/17/2022 19:44:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
03/17/2022 19:44:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=729
03/17/2022 19:44:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
03/17/2022 19:44:51 - INFO - __main__ - Global step 2200 Train loss 0.07 ACC 0.5 on epoch=733
03/17/2022 19:44:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
03/17/2022 19:44:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
03/17/2022 19:44:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=743
03/17/2022 19:45:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=746
03/17/2022 19:45:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
03/17/2022 19:45:04 - INFO - __main__ - Global step 2250 Train loss 0.05 ACC 0.4375 on epoch=749
03/17/2022 19:45:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
03/17/2022 19:45:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=756
03/17/2022 19:45:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=759
03/17/2022 19:45:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
03/17/2022 19:45:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
03/17/2022 19:45:19 - INFO - __main__ - Global step 2300 Train loss 0.06 ACC 0.40625 on epoch=766
03/17/2022 19:45:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=769
03/17/2022 19:45:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=773
03/17/2022 19:45:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
03/17/2022 19:45:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=779
03/17/2022 19:45:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=783
03/17/2022 19:45:33 - INFO - __main__ - Global step 2350 Train loss 0.06 ACC 0.5625 on epoch=783
03/17/2022 19:45:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
03/17/2022 19:45:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
03/17/2022 19:45:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=793
03/17/2022 19:45:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=796
03/17/2022 19:45:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=799
03/17/2022 19:45:47 - INFO - __main__ - Global step 2400 Train loss 0.06 ACC 0.375 on epoch=799
03/17/2022 19:45:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
03/17/2022 19:45:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
03/17/2022 19:45:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=809
03/17/2022 19:45:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
03/17/2022 19:46:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=816
03/17/2022 19:46:02 - INFO - __main__ - Global step 2450 Train loss 0.04 ACC 0.46875 on epoch=816
03/17/2022 19:46:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
03/17/2022 19:46:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=823
03/17/2022 19:46:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
03/17/2022 19:46:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
03/17/2022 19:46:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
03/17/2022 19:46:17 - INFO - __main__ - Global step 2500 Train loss 0.06 ACC 0.3125 on epoch=833
03/17/2022 19:46:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=836
03/17/2022 19:46:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
03/17/2022 19:46:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
03/17/2022 19:46:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=846
03/17/2022 19:46:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
03/17/2022 19:46:31 - INFO - __main__ - Global step 2550 Train loss 0.03 ACC 0.21875 on epoch=849
03/17/2022 19:46:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=853
03/17/2022 19:46:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
03/17/2022 19:46:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
03/17/2022 19:46:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=863
03/17/2022 19:46:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=866
03/17/2022 19:46:44 - INFO - __main__ - Global step 2600 Train loss 0.05 ACC 0.4375 on epoch=866
03/17/2022 19:46:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
03/17/2022 19:46:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
03/17/2022 19:46:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
03/17/2022 19:46:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
03/17/2022 19:46:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
03/17/2022 19:46:59 - INFO - __main__ - Global step 2650 Train loss 0.04 ACC 0.5625 on epoch=883
03/17/2022 19:47:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
03/17/2022 19:47:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
03/17/2022 19:47:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
03/17/2022 19:47:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
03/17/2022 19:47:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
03/17/2022 19:47:13 - INFO - __main__ - Global step 2700 Train loss 0.02 ACC 0.4375 on epoch=899
03/17/2022 19:47:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
03/17/2022 19:47:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
03/17/2022 19:47:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
03/17/2022 19:47:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
03/17/2022 19:47:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=916
03/17/2022 19:47:27 - INFO - __main__ - Global step 2750 Train loss 0.02 ACC 0.5625 on epoch=916
03/17/2022 19:47:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
03/17/2022 19:47:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=923
03/17/2022 19:47:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=926
03/17/2022 19:47:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=929
03/17/2022 19:47:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
03/17/2022 19:47:41 - INFO - __main__ - Global step 2800 Train loss 0.04 ACC 0.46875 on epoch=933
03/17/2022 19:47:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
03/17/2022 19:47:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
03/17/2022 19:47:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
03/17/2022 19:47:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
03/17/2022 19:47:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=949
03/17/2022 19:47:55 - INFO - __main__ - Global step 2850 Train loss 0.02 ACC 0.4375 on epoch=949
03/17/2022 19:47:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
03/17/2022 19:48:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
03/17/2022 19:48:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
03/17/2022 19:48:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
03/17/2022 19:48:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
03/17/2022 19:48:09 - INFO - __main__ - Global step 2900 Train loss 0.02 ACC 0.5 on epoch=966
03/17/2022 19:48:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=969
03/17/2022 19:48:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=973
03/17/2022 19:48:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=976
03/17/2022 19:48:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=979
03/17/2022 19:48:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
03/17/2022 19:48:24 - INFO - __main__ - Global step 2950 Train loss 0.05 ACC 0.46875 on epoch=983
03/17/2022 19:48:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
03/17/2022 19:48:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
03/17/2022 19:48:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
03/17/2022 19:48:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
03/17/2022 19:48:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
03/17/2022 19:48:38 - INFO - __main__ - Global step 3000 Train loss 0.02 ACC 0.375 on epoch=999
03/17/2022 19:48:38 - INFO - __main__ - save last model!
03/17/2022 19:48:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 19:48:38 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 19:48:38 - INFO - __main__ - Printing 3 examples
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 19:48:38 - INFO - __main__ - ['neutral']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 19:48:38 - INFO - __main__ - ['entailment']
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:48:38 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 19:48:38 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:48:38 - INFO - __main__ - Printing 3 examples
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:48:38 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:48:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:48:38 - INFO - __main__ - Printing 3 examples
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 19:48:38 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:48:38 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:48:38 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:48:40 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_87_0.5_8_predictions.txt
03/17/2022 19:48:40 - INFO - __main__ - ACC on test data: 0.3929
03/17/2022 19:48:41 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.39285714285714285
03/17/2022 19:48:41 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.4, bsz=8 ...
03/17/2022 19:48:42 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 19:48:42 - INFO - __main__ - Printing 3 examples
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 19:48:42 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:48:42 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 19:48:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 19:48:42 - INFO - __main__ - Printing 3 examples
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 19:48:42 - INFO - __main__ - ['contradiction']
03/17/2022 19:48:42 - INFO - __main__ - Tokenizing Input ...
03/17/2022 19:48:42 - INFO - __main__ - Tokenizing Output ...
03/17/2022 19:48:42 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 19:48:54 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:48:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:48:54 - INFO - __main__ - Starting training!
03/17/2022 19:48:57 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 19:48:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 19:48:57 - INFO - __main__ - Starting training!
03/17/2022 19:49:01 - INFO - __main__ - Step 10 Global step 10 Train loss 6.16 on epoch=3
03/17/2022 19:49:04 - INFO - __main__ - Step 20 Global step 20 Train loss 5.82 on epoch=6
03/17/2022 19:49:06 - INFO - __main__ - Step 30 Global step 30 Train loss 5.55 on epoch=9
03/17/2022 19:49:09 - INFO - __main__ - Step 40 Global step 40 Train loss 4.97 on epoch=13
03/17/2022 19:49:11 - INFO - __main__ - Step 50 Global step 50 Train loss 4.26 on epoch=16
03/17/2022 19:49:13 - INFO - __main__ - Global step 50 Train loss 5.35 ACC 0.1875 on epoch=16
03/17/2022 19:49:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.1875 on epoch=16, global_step=50
03/17/2022 19:49:15 - INFO - __main__ - Step 60 Global step 60 Train loss 3.24 on epoch=19
03/17/2022 19:49:18 - INFO - __main__ - Step 70 Global step 70 Train loss 2.27 on epoch=23
03/17/2022 19:49:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.97 on epoch=26
03/17/2022 19:49:23 - INFO - __main__ - Step 90 Global step 90 Train loss 1.60 on epoch=29
03/17/2022 19:49:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.36 on epoch=33
03/17/2022 19:49:27 - INFO - __main__ - Global step 100 Train loss 2.09 ACC 0.15625 on epoch=33
03/17/2022 19:49:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.21 on epoch=36
03/17/2022 19:49:32 - INFO - __main__ - Step 120 Global step 120 Train loss 1.12 on epoch=39
03/17/2022 19:49:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=43
03/17/2022 19:49:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=46
03/17/2022 19:49:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=49
03/17/2022 19:49:41 - INFO - __main__ - Global step 150 Train loss 1.01 ACC 0.09375 on epoch=49
03/17/2022 19:49:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=53
03/17/2022 19:49:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=56
03/17/2022 19:49:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=59
03/17/2022 19:49:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=63
03/17/2022 19:49:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=66
03/17/2022 19:49:54 - INFO - __main__ - Global step 200 Train loss 0.75 ACC 0.5 on epoch=66
03/17/2022 19:49:54 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.5 on epoch=66, global_step=200
03/17/2022 19:49:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=69
03/17/2022 19:50:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=73
03/17/2022 19:50:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=76
03/17/2022 19:50:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.66 on epoch=79
03/17/2022 19:50:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=83
03/17/2022 19:50:08 - INFO - __main__ - Global step 250 Train loss 0.67 ACC 0.46875 on epoch=83
03/17/2022 19:50:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=86
03/17/2022 19:50:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=89
03/17/2022 19:50:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=93
03/17/2022 19:50:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=96
03/17/2022 19:50:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=99
03/17/2022 19:50:22 - INFO - __main__ - Global step 300 Train loss 0.58 ACC 0.0 on epoch=99
03/17/2022 19:50:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=103
03/17/2022 19:50:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.61 on epoch=106
03/17/2022 19:50:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=109
03/17/2022 19:50:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=113
03/17/2022 19:50:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=116
03/17/2022 19:50:36 - INFO - __main__ - Global step 350 Train loss 0.59 ACC 0.5 on epoch=116
03/17/2022 19:50:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=119
03/17/2022 19:50:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.61 on epoch=123
03/17/2022 19:50:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=126
03/17/2022 19:50:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=129
03/17/2022 19:50:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=133
03/17/2022 19:50:49 - INFO - __main__ - Global step 400 Train loss 0.56 ACC 0.21875 on epoch=133
03/17/2022 19:50:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=136
03/17/2022 19:50:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=139
03/17/2022 19:50:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=143
03/17/2022 19:51:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=146
03/17/2022 19:51:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=149
03/17/2022 19:51:03 - INFO - __main__ - Global step 450 Train loss 0.53 ACC 0.5 on epoch=149
03/17/2022 19:51:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=153
03/17/2022 19:51:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=156
03/17/2022 19:51:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=159
03/17/2022 19:51:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=163
03/17/2022 19:51:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=166
03/17/2022 19:51:17 - INFO - __main__ - Global step 500 Train loss 0.52 ACC 0.5 on epoch=166
03/17/2022 19:51:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=169
03/17/2022 19:51:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=173
03/17/2022 19:51:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
03/17/2022 19:51:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=179
03/17/2022 19:51:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=183
03/17/2022 19:51:30 - INFO - __main__ - Global step 550 Train loss 0.51 ACC 0.5 on epoch=183
03/17/2022 19:51:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=186
03/17/2022 19:51:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=189
03/17/2022 19:51:38 - INFO - __main__ - Step 580 Global step 580 Train loss 1.31 on epoch=193
03/17/2022 19:51:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=196
03/17/2022 19:51:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.65 on epoch=199
03/17/2022 19:51:44 - INFO - __main__ - Global step 600 Train loss 0.78 ACC 0.5 on epoch=199
03/17/2022 19:51:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=203
03/17/2022 19:51:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=206
03/17/2022 19:51:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=209
03/17/2022 19:51:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=213
03/17/2022 19:51:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=216
03/17/2022 19:51:58 - INFO - __main__ - Global step 650 Train loss 0.53 ACC 0.5 on epoch=216
03/17/2022 19:52:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=219
03/17/2022 19:52:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=223
03/17/2022 19:52:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=226
03/17/2022 19:52:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=229
03/17/2022 19:52:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=233
03/17/2022 19:52:12 - INFO - __main__ - Global step 700 Train loss 0.51 ACC 0.5 on epoch=233
03/17/2022 19:52:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=236
03/17/2022 19:52:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=239
03/17/2022 19:52:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=243
03/17/2022 19:52:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=246
03/17/2022 19:52:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=249
03/17/2022 19:52:26 - INFO - __main__ - Global step 750 Train loss 0.51 ACC 0.5 on epoch=249
03/17/2022 19:52:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=253
03/17/2022 19:52:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=256
03/17/2022 19:52:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=259
03/17/2022 19:52:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=263
03/17/2022 19:52:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=266
03/17/2022 19:52:41 - INFO - __main__ - Global step 800 Train loss 0.49 ACC 0.5 on epoch=266
03/17/2022 19:52:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=269
03/17/2022 19:52:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=273
03/17/2022 19:52:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=276
03/17/2022 19:52:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=279
03/17/2022 19:52:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=283
03/17/2022 19:52:55 - INFO - __main__ - Global step 850 Train loss 0.48 ACC 0.5 on epoch=283
03/17/2022 19:52:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=286
03/17/2022 19:53:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=289
03/17/2022 19:53:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=293
03/17/2022 19:53:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=296
03/17/2022 19:53:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=299
03/17/2022 19:53:09 - INFO - __main__ - Global step 900 Train loss 0.44 ACC 0.5 on epoch=299
03/17/2022 19:53:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=303
03/17/2022 19:53:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=306
03/17/2022 19:53:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=309
03/17/2022 19:53:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=313
03/17/2022 19:53:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=316
03/17/2022 19:53:23 - INFO - __main__ - Global step 950 Train loss 0.46 ACC 0.5 on epoch=316
03/17/2022 19:53:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=319
03/17/2022 19:53:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=323
03/17/2022 19:53:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=326
03/17/2022 19:53:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=329
03/17/2022 19:53:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=333
03/17/2022 19:53:37 - INFO - __main__ - Global step 1000 Train loss 0.46 ACC 0.5 on epoch=333
03/17/2022 19:53:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=336
03/17/2022 19:53:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=339
03/17/2022 19:53:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=343
03/17/2022 19:53:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=346
03/17/2022 19:53:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=349
03/17/2022 19:53:52 - INFO - __main__ - Global step 1050 Train loss 0.44 ACC 0.5 on epoch=349
03/17/2022 19:53:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=353
03/17/2022 19:53:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=356
03/17/2022 19:54:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=359
03/17/2022 19:54:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=363
03/17/2022 19:54:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=366
03/17/2022 19:54:06 - INFO - __main__ - Global step 1100 Train loss 0.46 ACC 0.5 on epoch=366
03/17/2022 19:54:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=369
03/17/2022 19:54:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=373
03/17/2022 19:54:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=376
03/17/2022 19:54:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.86 on epoch=379
03/17/2022 19:54:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.41 on epoch=383
03/17/2022 19:54:25 - INFO - __main__ - Global step 1150 Train loss 1.13 ACC 0.15625 on epoch=383
03/17/2022 19:54:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.99 on epoch=386
03/17/2022 19:54:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.76 on epoch=389
03/17/2022 19:54:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.54 on epoch=393
03/17/2022 19:54:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=396
03/17/2022 19:54:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=399
03/17/2022 19:54:39 - INFO - __main__ - Global step 1200 Train loss 1.47 ACC 0.5 on epoch=399
03/17/2022 19:54:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=403
03/17/2022 19:54:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.49 on epoch=406
03/17/2022 19:54:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.54 on epoch=409
03/17/2022 19:54:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.59 on epoch=413
03/17/2022 19:54:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=416
03/17/2022 19:54:53 - INFO - __main__ - Global step 1250 Train loss 0.51 ACC 0.5 on epoch=416
03/17/2022 19:54:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.57 on epoch=419
03/17/2022 19:54:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=423
03/17/2022 19:55:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=426
03/17/2022 19:55:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.53 on epoch=429
03/17/2022 19:55:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=433
03/17/2022 19:55:07 - INFO - __main__ - Global step 1300 Train loss 0.52 ACC 0.5 on epoch=433
03/17/2022 19:55:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=436
03/17/2022 19:55:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.59 on epoch=439
03/17/2022 19:55:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=443
03/17/2022 19:55:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=446
03/17/2022 19:55:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=449
03/17/2022 19:55:20 - INFO - __main__ - Global step 1350 Train loss 0.51 ACC 0.5 on epoch=449
03/17/2022 19:55:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=453
03/17/2022 19:55:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=456
03/17/2022 19:55:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=459
03/17/2022 19:55:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=463
03/17/2022 19:55:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=466
03/17/2022 19:55:34 - INFO - __main__ - Global step 1400 Train loss 0.48 ACC 0.5 on epoch=466
03/17/2022 19:55:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=469
03/17/2022 19:55:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=473
03/17/2022 19:55:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=476
03/17/2022 19:55:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.53 on epoch=479
03/17/2022 19:55:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=483
03/17/2022 19:55:48 - INFO - __main__ - Global step 1450 Train loss 0.50 ACC 0.5 on epoch=483
03/17/2022 19:55:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=486
03/17/2022 19:55:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.55 on epoch=489
03/17/2022 19:55:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.50 on epoch=493
03/17/2022 19:55:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=496
03/17/2022 19:56:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=499
03/17/2022 19:56:02 - INFO - __main__ - Global step 1500 Train loss 0.49 ACC 0.5 on epoch=499
03/17/2022 19:56:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=503
03/17/2022 19:56:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.49 on epoch=506
03/17/2022 19:56:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.52 on epoch=509
03/17/2022 19:56:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=513
03/17/2022 19:56:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=516
03/17/2022 19:56:16 - INFO - __main__ - Global step 1550 Train loss 0.48 ACC 0.5 on epoch=516
03/17/2022 19:56:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=519
03/17/2022 19:56:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.46 on epoch=523
03/17/2022 19:56:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.51 on epoch=526
03/17/2022 19:56:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.50 on epoch=529
03/17/2022 19:56:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=533
03/17/2022 19:56:30 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=533
03/17/2022 19:56:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=536
03/17/2022 19:56:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.55 on epoch=539
03/17/2022 19:56:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=543
03/17/2022 19:56:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=546
03/17/2022 19:56:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.48 on epoch=549
03/17/2022 19:56:44 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.5 on epoch=549
03/17/2022 19:56:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=553
03/17/2022 19:56:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=556
03/17/2022 19:56:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=559
03/17/2022 19:56:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=563
03/17/2022 19:56:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=566
03/17/2022 19:56:57 - INFO - __main__ - Global step 1700 Train loss 0.47 ACC 0.5 on epoch=566
03/17/2022 19:57:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=569
03/17/2022 19:57:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=573
03/17/2022 19:57:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=576
03/17/2022 19:57:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=579
03/17/2022 19:57:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=583
03/17/2022 19:57:11 - INFO - __main__ - Global step 1750 Train loss 0.51 ACC 0.5 on epoch=583
03/17/2022 19:57:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=586
03/17/2022 19:57:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=589
03/17/2022 19:57:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=593
03/17/2022 19:57:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=596
03/17/2022 19:57:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=599
03/17/2022 19:57:25 - INFO - __main__ - Global step 1800 Train loss 0.48 ACC 0.5 on epoch=599
03/17/2022 19:57:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=603
03/17/2022 19:57:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.53 on epoch=606
03/17/2022 19:57:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.51 on epoch=609
03/17/2022 19:57:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=613
03/17/2022 19:57:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=616
03/17/2022 19:57:39 - INFO - __main__ - Global step 1850 Train loss 0.47 ACC 0.5 on epoch=616
03/17/2022 19:57:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=619
03/17/2022 19:57:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=623
03/17/2022 19:57:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=626
03/17/2022 19:57:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.56 on epoch=629
03/17/2022 19:57:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.55 on epoch=633
03/17/2022 19:57:53 - INFO - __main__ - Global step 1900 Train loss 0.51 ACC 0.5 on epoch=633
03/17/2022 19:57:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.48 on epoch=636
03/17/2022 19:57:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=639
03/17/2022 19:58:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=643
03/17/2022 19:58:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=646
03/17/2022 19:58:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=649
03/17/2022 19:58:07 - INFO - __main__ - Global step 1950 Train loss 0.45 ACC 0.5 on epoch=649
03/17/2022 19:58:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.49 on epoch=653
03/17/2022 19:58:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.49 on epoch=656
03/17/2022 19:58:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.52 on epoch=659
03/17/2022 19:58:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=663
03/17/2022 19:58:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=666
03/17/2022 19:58:21 - INFO - __main__ - Global step 2000 Train loss 0.48 ACC 0.5 on epoch=666
03/17/2022 19:58:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.46 on epoch=669
03/17/2022 19:58:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.38 on epoch=673
03/17/2022 19:58:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=676
03/17/2022 19:58:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=679
03/17/2022 19:58:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=683
03/17/2022 19:58:35 - INFO - __main__ - Global step 2050 Train loss 0.43 ACC 0.5 on epoch=683
03/17/2022 19:58:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=686
03/17/2022 19:58:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.47 on epoch=689
03/17/2022 19:58:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=693
03/17/2022 19:58:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.48 on epoch=696
03/17/2022 19:58:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=699
03/17/2022 19:58:49 - INFO - __main__ - Global step 2100 Train loss 0.46 ACC 0.5 on epoch=699
03/17/2022 19:58:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=703
03/17/2022 19:58:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.49 on epoch=706
03/17/2022 19:58:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.44 on epoch=709
03/17/2022 19:59:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=713
03/17/2022 19:59:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=716
03/17/2022 19:59:03 - INFO - __main__ - Global step 2150 Train loss 0.45 ACC 0.5 on epoch=716
03/17/2022 19:59:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.47 on epoch=719
03/17/2022 19:59:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=723
03/17/2022 19:59:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=726
03/17/2022 19:59:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=729
03/17/2022 19:59:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.47 on epoch=733
03/17/2022 19:59:17 - INFO - __main__ - Global step 2200 Train loss 0.46 ACC 0.5 on epoch=733
03/17/2022 19:59:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.49 on epoch=736
03/17/2022 19:59:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=739
03/17/2022 19:59:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=743
03/17/2022 19:59:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.49 on epoch=746
03/17/2022 19:59:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=749
03/17/2022 19:59:31 - INFO - __main__ - Global step 2250 Train loss 0.46 ACC 0.5 on epoch=749
03/17/2022 19:59:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=753
03/17/2022 19:59:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=756
03/17/2022 19:59:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.47 on epoch=759
03/17/2022 19:59:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=763
03/17/2022 19:59:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=766
03/17/2022 19:59:45 - INFO - __main__ - Global step 2300 Train loss 0.44 ACC 0.5 on epoch=766
03/17/2022 19:59:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.46 on epoch=769
03/17/2022 19:59:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=773
03/17/2022 19:59:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=776
03/17/2022 19:59:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=779
03/17/2022 19:59:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.46 on epoch=783
03/17/2022 19:59:59 - INFO - __main__ - Global step 2350 Train loss 0.45 ACC 0.375 on epoch=783
03/17/2022 20:00:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=786
03/17/2022 20:00:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=789
03/17/2022 20:00:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=793
03/17/2022 20:00:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.44 on epoch=796
03/17/2022 20:00:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=799
03/17/2022 20:00:13 - INFO - __main__ - Global step 2400 Train loss 0.43 ACC 0.46875 on epoch=799
03/17/2022 20:00:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=803
03/17/2022 20:00:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=806
03/17/2022 20:00:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=809
03/17/2022 20:00:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=813
03/17/2022 20:00:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=816
03/17/2022 20:00:27 - INFO - __main__ - Global step 2450 Train loss 0.42 ACC 0.5 on epoch=816
03/17/2022 20:00:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=819
03/17/2022 20:00:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=823
03/17/2022 20:00:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=826
03/17/2022 20:00:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=829
03/17/2022 20:00:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.42 on epoch=833
03/17/2022 20:00:41 - INFO - __main__ - Global step 2500 Train loss 0.44 ACC 0.5 on epoch=833
03/17/2022 20:00:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.46 on epoch=836
03/17/2022 20:00:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.47 on epoch=839
03/17/2022 20:00:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=843
03/17/2022 20:00:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=846
03/17/2022 20:00:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.45 on epoch=849
03/17/2022 20:00:55 - INFO - __main__ - Global step 2550 Train loss 0.46 ACC 0.5 on epoch=849
03/17/2022 20:00:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=853
03/17/2022 20:01:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.43 on epoch=856
03/17/2022 20:01:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=859
03/17/2022 20:01:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=863
03/17/2022 20:01:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.44 on epoch=866
03/17/2022 20:01:09 - INFO - __main__ - Global step 2600 Train loss 0.43 ACC 0.5 on epoch=866
03/17/2022 20:01:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.46 on epoch=869
03/17/2022 20:01:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=873
03/17/2022 20:01:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.45 on epoch=876
03/17/2022 20:01:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=879
03/17/2022 20:01:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.43 on epoch=883
03/17/2022 20:01:23 - INFO - __main__ - Global step 2650 Train loss 0.43 ACC 0.5 on epoch=883
03/17/2022 20:01:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.45 on epoch=886
03/17/2022 20:01:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.46 on epoch=889
03/17/2022 20:01:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.48 on epoch=893
03/17/2022 20:01:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=896
03/17/2022 20:01:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.45 on epoch=899
03/17/2022 20:01:37 - INFO - __main__ - Global step 2700 Train loss 0.45 ACC 0.5 on epoch=899
03/17/2022 20:01:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.44 on epoch=903
03/17/2022 20:01:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.47 on epoch=906
03/17/2022 20:01:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.48 on epoch=909
03/17/2022 20:01:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=913
03/17/2022 20:01:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=916
03/17/2022 20:01:51 - INFO - __main__ - Global step 2750 Train loss 0.45 ACC 0.5 on epoch=916
03/17/2022 20:01:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.50 on epoch=919
03/17/2022 20:01:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.38 on epoch=923
03/17/2022 20:01:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.47 on epoch=926
03/17/2022 20:02:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.44 on epoch=929
03/17/2022 20:02:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=933
03/17/2022 20:02:05 - INFO - __main__ - Global step 2800 Train loss 0.44 ACC 0.5 on epoch=933
03/17/2022 20:02:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.45 on epoch=936
03/17/2022 20:02:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.47 on epoch=939
03/17/2022 20:02:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.44 on epoch=943
03/17/2022 20:02:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=946
03/17/2022 20:02:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=949
03/17/2022 20:02:19 - INFO - __main__ - Global step 2850 Train loss 0.44 ACC 0.5 on epoch=949
03/17/2022 20:02:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=953
03/17/2022 20:02:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.44 on epoch=956
03/17/2022 20:02:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.47 on epoch=959
03/17/2022 20:02:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=963
03/17/2022 20:02:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=966
03/17/2022 20:02:33 - INFO - __main__ - Global step 2900 Train loss 0.43 ACC 0.5 on epoch=966
03/17/2022 20:02:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.55 on epoch=969
03/17/2022 20:02:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=973
03/17/2022 20:02:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.47 on epoch=976
03/17/2022 20:02:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.41 on epoch=979
03/17/2022 20:02:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.44 on epoch=983
03/17/2022 20:02:47 - INFO - __main__ - Global step 2950 Train loss 0.46 ACC 0.5 on epoch=983
03/17/2022 20:02:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=986
03/17/2022 20:02:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.43 on epoch=989
03/17/2022 20:02:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=993
03/17/2022 20:02:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.41 on epoch=996
03/17/2022 20:03:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.45 on epoch=999
03/17/2022 20:03:01 - INFO - __main__ - Global step 3000 Train loss 0.43 ACC 0.4375 on epoch=999
03/17/2022 20:03:01 - INFO - __main__ - save last model!
03/17/2022 20:03:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 20:03:01 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 20:03:01 - INFO - __main__ - Printing 3 examples
03/17/2022 20:03:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 20:03:01 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 20:03:01 - INFO - __main__ - ['neutral']
03/17/2022 20:03:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 20:03:01 - INFO - __main__ - ['entailment']
03/17/2022 20:03:01 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:03:02 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:03:02 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 20:03:02 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 20:03:02 - INFO - __main__ - Printing 3 examples
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 20:03:02 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:03:02 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 20:03:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 20:03:02 - INFO - __main__ - Printing 3 examples
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 20:03:02 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:02 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:03:02 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:03:02 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 20:03:04 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_87_0.4_8_predictions.txt
03/17/2022 20:03:04 - INFO - __main__ - ACC on test data: 0.4107
03/17/2022 20:03:04 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.4, bsz=8, dev_performance=0.5, test_performance=0.4107142857142857
03/17/2022 20:03:04 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.3, bsz=8 ...
03/17/2022 20:03:05 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 20:03:05 - INFO - __main__ - Printing 3 examples
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 20:03:05 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:03:05 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 20:03:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 20:03:05 - INFO - __main__ - Printing 3 examples
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 20:03:05 - INFO - __main__ - ['contradiction']
03/17/2022 20:03:05 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:03:05 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:03:05 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 20:03:17 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:03:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:03:18 - INFO - __main__ - Starting training!
03/17/2022 20:03:20 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:03:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:03:21 - INFO - __main__ - Starting training!
03/17/2022 20:03:24 - INFO - __main__ - Step 10 Global step 10 Train loss 6.23 on epoch=3
03/17/2022 20:03:27 - INFO - __main__ - Step 20 Global step 20 Train loss 5.57 on epoch=6
03/17/2022 20:03:29 - INFO - __main__ - Step 30 Global step 30 Train loss 4.90 on epoch=9
03/17/2022 20:03:32 - INFO - __main__ - Step 40 Global step 40 Train loss 3.90 on epoch=13
03/17/2022 20:03:35 - INFO - __main__ - Step 50 Global step 50 Train loss 3.07 on epoch=16
03/17/2022 20:03:36 - INFO - __main__ - Global step 50 Train loss 4.73 ACC 0.0625 on epoch=16
03/17/2022 20:03:36 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0625 on epoch=16, global_step=50
03/17/2022 20:03:39 - INFO - __main__ - Step 60 Global step 60 Train loss 3.14 on epoch=19
03/17/2022 20:03:41 - INFO - __main__ - Step 70 Global step 70 Train loss 5.40 on epoch=23
03/17/2022 20:03:44 - INFO - __main__ - Step 80 Global step 80 Train loss 5.31 on epoch=26
03/17/2022 20:03:47 - INFO - __main__ - Step 90 Global step 90 Train loss 4.80 on epoch=29
03/17/2022 20:03:49 - INFO - __main__ - Step 100 Global step 100 Train loss 3.10 on epoch=33
03/17/2022 20:03:51 - INFO - __main__ - Global step 100 Train loss 4.35 ACC 0.03125 on epoch=33
03/17/2022 20:03:53 - INFO - __main__ - Step 110 Global step 110 Train loss 2.86 on epoch=36
03/17/2022 20:03:56 - INFO - __main__ - Step 120 Global step 120 Train loss 2.81 on epoch=39
03/17/2022 20:03:59 - INFO - __main__ - Step 130 Global step 130 Train loss 3.02 on epoch=43
03/17/2022 20:04:01 - INFO - __main__ - Step 140 Global step 140 Train loss 2.79 on epoch=46
03/17/2022 20:04:04 - INFO - __main__ - Step 150 Global step 150 Train loss 2.79 on epoch=49
03/17/2022 20:04:05 - INFO - __main__ - Global step 150 Train loss 2.86 ACC 0.0 on epoch=49
03/17/2022 20:04:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.73 on epoch=53
03/17/2022 20:04:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.57 on epoch=56
03/17/2022 20:04:13 - INFO - __main__ - Step 180 Global step 180 Train loss 2.55 on epoch=59
03/17/2022 20:04:16 - INFO - __main__ - Step 190 Global step 190 Train loss 2.56 on epoch=63
03/17/2022 20:04:18 - INFO - __main__ - Step 200 Global step 200 Train loss 2.51 on epoch=66
03/17/2022 20:04:20 - INFO - __main__ - Global step 200 Train loss 2.58 ACC 0.1875 on epoch=66
03/17/2022 20:04:20 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.1875 on epoch=66, global_step=200
03/17/2022 20:04:22 - INFO - __main__ - Step 210 Global step 210 Train loss 2.39 on epoch=69
03/17/2022 20:04:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.13 on epoch=73
03/17/2022 20:04:28 - INFO - __main__ - Step 230 Global step 230 Train loss 2.27 on epoch=76
03/17/2022 20:04:30 - INFO - __main__ - Step 240 Global step 240 Train loss 2.15 on epoch=79
03/17/2022 20:04:33 - INFO - __main__ - Step 250 Global step 250 Train loss 2.07 on epoch=83
03/17/2022 20:04:34 - INFO - __main__ - Global step 250 Train loss 2.20 ACC 0.15625 on epoch=83
03/17/2022 20:04:37 - INFO - __main__ - Step 260 Global step 260 Train loss 2.00 on epoch=86
03/17/2022 20:04:39 - INFO - __main__ - Step 270 Global step 270 Train loss 2.00 on epoch=89
03/17/2022 20:04:42 - INFO - __main__ - Step 280 Global step 280 Train loss 1.94 on epoch=93
03/17/2022 20:04:45 - INFO - __main__ - Step 290 Global step 290 Train loss 1.85 on epoch=96
03/17/2022 20:04:47 - INFO - __main__ - Step 300 Global step 300 Train loss 1.98 on epoch=99
03/17/2022 20:04:48 - INFO - __main__ - Global step 300 Train loss 1.95 ACC 0.4375 on epoch=99
03/17/2022 20:04:48 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.4375 on epoch=99, global_step=300
03/17/2022 20:04:51 - INFO - __main__ - Step 310 Global step 310 Train loss 1.83 on epoch=103
03/17/2022 20:04:53 - INFO - __main__ - Step 320 Global step 320 Train loss 1.87 on epoch=106
03/17/2022 20:04:56 - INFO - __main__ - Step 330 Global step 330 Train loss 1.90 on epoch=109
03/17/2022 20:04:59 - INFO - __main__ - Step 340 Global step 340 Train loss 1.74 on epoch=113
03/17/2022 20:05:01 - INFO - __main__ - Step 350 Global step 350 Train loss 1.66 on epoch=116
03/17/2022 20:05:02 - INFO - __main__ - Global step 350 Train loss 1.80 ACC 0.40625 on epoch=116
03/17/2022 20:05:05 - INFO - __main__ - Step 360 Global step 360 Train loss 1.80 on epoch=119
03/17/2022 20:05:08 - INFO - __main__ - Step 370 Global step 370 Train loss 1.63 on epoch=123
03/17/2022 20:05:10 - INFO - __main__ - Step 380 Global step 380 Train loss 1.63 on epoch=126
03/17/2022 20:05:13 - INFO - __main__ - Step 390 Global step 390 Train loss 1.66 on epoch=129
03/17/2022 20:05:16 - INFO - __main__ - Step 400 Global step 400 Train loss 1.57 on epoch=133
03/17/2022 20:05:17 - INFO - __main__ - Global step 400 Train loss 1.66 ACC 0.40625 on epoch=133
03/17/2022 20:05:20 - INFO - __main__ - Step 410 Global step 410 Train loss 1.49 on epoch=136
03/17/2022 20:05:22 - INFO - __main__ - Step 420 Global step 420 Train loss 1.48 on epoch=139
03/17/2022 20:05:25 - INFO - __main__ - Step 430 Global step 430 Train loss 1.42 on epoch=143
03/17/2022 20:05:27 - INFO - __main__ - Step 440 Global step 440 Train loss 1.34 on epoch=146
03/17/2022 20:05:30 - INFO - __main__ - Step 450 Global step 450 Train loss 1.34 on epoch=149
03/17/2022 20:05:31 - INFO - __main__ - Global step 450 Train loss 1.41 ACC 0.5 on epoch=149
03/17/2022 20:05:31 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=149, global_step=450
03/17/2022 20:05:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.32 on epoch=153
03/17/2022 20:05:37 - INFO - __main__ - Step 470 Global step 470 Train loss 1.35 on epoch=156
03/17/2022 20:05:39 - INFO - __main__ - Step 480 Global step 480 Train loss 1.31 on epoch=159
03/17/2022 20:05:42 - INFO - __main__ - Step 490 Global step 490 Train loss 1.25 on epoch=163
03/17/2022 20:05:45 - INFO - __main__ - Step 500 Global step 500 Train loss 1.17 on epoch=166
03/17/2022 20:05:46 - INFO - __main__ - Global step 500 Train loss 1.28 ACC 0.5 on epoch=166
03/17/2022 20:05:48 - INFO - __main__ - Step 510 Global step 510 Train loss 1.22 on epoch=169
03/17/2022 20:05:51 - INFO - __main__ - Step 520 Global step 520 Train loss 1.16 on epoch=173
03/17/2022 20:05:54 - INFO - __main__ - Step 530 Global step 530 Train loss 1.12 on epoch=176
03/17/2022 20:05:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.14 on epoch=179
03/17/2022 20:05:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=183
03/17/2022 20:06:00 - INFO - __main__ - Global step 550 Train loss 1.12 ACC 0.5 on epoch=183
03/17/2022 20:06:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.92 on epoch=186
03/17/2022 20:06:05 - INFO - __main__ - Step 570 Global step 570 Train loss 1.05 on epoch=189
03/17/2022 20:06:08 - INFO - __main__ - Step 580 Global step 580 Train loss 1.02 on epoch=193
03/17/2022 20:06:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.94 on epoch=196
03/17/2022 20:06:13 - INFO - __main__ - Step 600 Global step 600 Train loss 1.00 on epoch=199
03/17/2022 20:06:14 - INFO - __main__ - Global step 600 Train loss 0.99 ACC 0.5 on epoch=199
03/17/2022 20:06:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=203
03/17/2022 20:06:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.92 on epoch=206
03/17/2022 20:06:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=209
03/17/2022 20:06:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.89 on epoch=213
03/17/2022 20:06:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.87 on epoch=216
03/17/2022 20:06:28 - INFO - __main__ - Global step 650 Train loss 0.87 ACC 0.4375 on epoch=216
03/17/2022 20:06:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.87 on epoch=219
03/17/2022 20:06:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.74 on epoch=223
03/17/2022 20:06:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.73 on epoch=226
03/17/2022 20:06:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=229
03/17/2022 20:06:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.72 on epoch=233
03/17/2022 20:06:42 - INFO - __main__ - Global step 700 Train loss 0.75 ACC 0.5 on epoch=233
03/17/2022 20:06:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.76 on epoch=236
03/17/2022 20:06:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.71 on epoch=239
03/17/2022 20:06:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.74 on epoch=243
03/17/2022 20:06:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.71 on epoch=246
03/17/2022 20:06:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.73 on epoch=249
03/17/2022 20:06:56 - INFO - __main__ - Global step 750 Train loss 0.73 ACC 0.5 on epoch=249
03/17/2022 20:06:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.65 on epoch=253
03/17/2022 20:07:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.69 on epoch=256
03/17/2022 20:07:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.64 on epoch=259
03/17/2022 20:07:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.60 on epoch=263
03/17/2022 20:07:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=266
03/17/2022 20:07:10 - INFO - __main__ - Global step 800 Train loss 0.63 ACC 0.4375 on epoch=266
03/17/2022 20:07:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=269
03/17/2022 20:07:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=273
03/17/2022 20:07:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=276
03/17/2022 20:07:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.62 on epoch=279
03/17/2022 20:07:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=283
03/17/2022 20:07:23 - INFO - __main__ - Global step 850 Train loss 0.59 ACC 0.25 on epoch=283
03/17/2022 20:07:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=286
03/17/2022 20:07:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.59 on epoch=289
03/17/2022 20:07:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=293
03/17/2022 20:07:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.61 on epoch=296
03/17/2022 20:07:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.66 on epoch=299
03/17/2022 20:07:37 - INFO - __main__ - Global step 900 Train loss 0.60 ACC 0.5 on epoch=299
03/17/2022 20:07:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=303
03/17/2022 20:07:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.59 on epoch=306
03/17/2022 20:07:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=309
03/17/2022 20:07:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.58 on epoch=313
03/17/2022 20:07:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=316
03/17/2022 20:07:51 - INFO - __main__ - Global step 950 Train loss 0.56 ACC 0.5 on epoch=316
03/17/2022 20:07:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.58 on epoch=319
03/17/2022 20:07:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.65 on epoch=323
03/17/2022 20:07:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=326
03/17/2022 20:08:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.55 on epoch=329
03/17/2022 20:08:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=333
03/17/2022 20:08:05 - INFO - __main__ - Global step 1000 Train loss 0.58 ACC 0.34375 on epoch=333
03/17/2022 20:08:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=336
03/17/2022 20:08:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.58 on epoch=339
03/17/2022 20:08:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=343
03/17/2022 20:08:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.64 on epoch=346
03/17/2022 20:08:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.56 on epoch=349
03/17/2022 20:08:19 - INFO - __main__ - Global step 1050 Train loss 0.57 ACC 0.15625 on epoch=349
03/17/2022 20:08:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=353
03/17/2022 20:08:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=356
03/17/2022 20:08:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=359
03/17/2022 20:08:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=363
03/17/2022 20:08:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=366
03/17/2022 20:08:33 - INFO - __main__ - Global step 1100 Train loss 0.50 ACC 0.5 on epoch=366
03/17/2022 20:08:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=369
03/17/2022 20:08:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.59 on epoch=373
03/17/2022 20:08:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=376
03/17/2022 20:08:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=379
03/17/2022 20:08:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=383
03/17/2022 20:08:47 - INFO - __main__ - Global step 1150 Train loss 0.53 ACC 0.5 on epoch=383
03/17/2022 20:08:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=386
03/17/2022 20:08:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=389
03/17/2022 20:08:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=393
03/17/2022 20:08:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=396
03/17/2022 20:09:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=399
03/17/2022 20:09:01 - INFO - __main__ - Global step 1200 Train loss 0.49 ACC 0.5 on epoch=399
03/17/2022 20:09:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.53 on epoch=403
03/17/2022 20:09:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.56 on epoch=406
03/17/2022 20:09:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=409
03/17/2022 20:09:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=413
03/17/2022 20:09:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.52 on epoch=416
03/17/2022 20:09:15 - INFO - __main__ - Global step 1250 Train loss 0.53 ACC 0.5 on epoch=416
03/17/2022 20:09:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=419
03/17/2022 20:09:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=423
03/17/2022 20:09:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=426
03/17/2022 20:09:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=429
03/17/2022 20:09:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=433
03/17/2022 20:09:30 - INFO - __main__ - Global step 1300 Train loss 0.50 ACC 0.5 on epoch=433
03/17/2022 20:09:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=436
03/17/2022 20:09:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=439
03/17/2022 20:09:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=443
03/17/2022 20:09:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.51 on epoch=446
03/17/2022 20:09:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=449
03/17/2022 20:09:44 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=449
03/17/2022 20:09:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=453
03/17/2022 20:09:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=456
03/17/2022 20:09:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=459
03/17/2022 20:09:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=463
03/17/2022 20:09:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=466
03/17/2022 20:09:58 - INFO - __main__ - Global step 1400 Train loss 0.47 ACC 0.5 on epoch=466
03/17/2022 20:10:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=469
03/17/2022 20:10:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=473
03/17/2022 20:10:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=476
03/17/2022 20:10:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=479
03/17/2022 20:10:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=483
03/17/2022 20:10:13 - INFO - __main__ - Global step 1450 Train loss 0.50 ACC 0.4375 on epoch=483
03/17/2022 20:10:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=486
03/17/2022 20:10:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=489
03/17/2022 20:10:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=493
03/17/2022 20:10:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=496
03/17/2022 20:10:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=499
03/17/2022 20:10:27 - INFO - __main__ - Global step 1500 Train loss 0.46 ACC 0.40625 on epoch=499
03/17/2022 20:10:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.45 on epoch=503
03/17/2022 20:10:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.49 on epoch=506
03/17/2022 20:10:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=509
03/17/2022 20:10:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=513
03/17/2022 20:10:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=516
03/17/2022 20:10:41 - INFO - __main__ - Global step 1550 Train loss 0.47 ACC 0.5 on epoch=516
03/17/2022 20:10:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=519
03/17/2022 20:10:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.59 on epoch=523
03/17/2022 20:10:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=526
03/17/2022 20:10:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=529
03/17/2022 20:10:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.49 on epoch=533
03/17/2022 20:10:55 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=533
03/17/2022 20:10:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=536
03/17/2022 20:11:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=539
03/17/2022 20:11:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=543
03/17/2022 20:11:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=546
03/17/2022 20:11:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=549
03/17/2022 20:11:09 - INFO - __main__ - Global step 1650 Train loss 0.43 ACC 0.5 on epoch=549
03/17/2022 20:11:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=553
03/17/2022 20:11:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=556
03/17/2022 20:11:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=559
03/17/2022 20:11:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=563
03/17/2022 20:11:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=566
03/17/2022 20:11:24 - INFO - __main__ - Global step 1700 Train loss 0.46 ACC 0.5 on epoch=566
03/17/2022 20:11:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=569
03/17/2022 20:11:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=573
03/17/2022 20:11:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=576
03/17/2022 20:11:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=579
03/17/2022 20:11:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=583
03/17/2022 20:11:38 - INFO - __main__ - Global step 1750 Train loss 0.45 ACC 0.5 on epoch=583
03/17/2022 20:11:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=586
03/17/2022 20:11:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=589
03/17/2022 20:11:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=593
03/17/2022 20:11:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=596
03/17/2022 20:11:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=599
03/17/2022 20:11:52 - INFO - __main__ - Global step 1800 Train loss 0.45 ACC 0.5 on epoch=599
03/17/2022 20:11:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=603
03/17/2022 20:11:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=606
03/17/2022 20:12:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=609
03/17/2022 20:12:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=613
03/17/2022 20:12:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=616
03/17/2022 20:12:06 - INFO - __main__ - Global step 1850 Train loss 0.43 ACC 0.5 on epoch=616
03/17/2022 20:12:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=619
03/17/2022 20:12:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.45 on epoch=623
03/17/2022 20:12:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=626
03/17/2022 20:12:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=629
03/17/2022 20:12:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=633
03/17/2022 20:12:20 - INFO - __main__ - Global step 1900 Train loss 0.43 ACC 0.5 on epoch=633
03/17/2022 20:12:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=636
03/17/2022 20:12:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=639
03/17/2022 20:12:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=643
03/17/2022 20:12:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=646
03/17/2022 20:12:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=649
03/17/2022 20:12:35 - INFO - __main__ - Global step 1950 Train loss 0.44 ACC 0.5 on epoch=649
03/17/2022 20:12:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=653
03/17/2022 20:12:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=656
03/17/2022 20:12:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=659
03/17/2022 20:12:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=663
03/17/2022 20:12:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=666
03/17/2022 20:12:49 - INFO - __main__ - Global step 2000 Train loss 0.42 ACC 0.5 on epoch=666
03/17/2022 20:12:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=669
03/17/2022 20:12:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=673
03/17/2022 20:12:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=676
03/17/2022 20:13:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=679
03/17/2022 20:13:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=683
03/17/2022 20:13:03 - INFO - __main__ - Global step 2050 Train loss 0.41 ACC 0.46875 on epoch=683
03/17/2022 20:13:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.45 on epoch=686
03/17/2022 20:13:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=689
03/17/2022 20:13:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.41 on epoch=693
03/17/2022 20:13:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.40 on epoch=696
03/17/2022 20:13:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=699
03/17/2022 20:13:17 - INFO - __main__ - Global step 2100 Train loss 0.41 ACC 0.5 on epoch=699
03/17/2022 20:13:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=703
03/17/2022 20:13:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=706
03/17/2022 20:13:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=709
03/17/2022 20:13:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=713
03/17/2022 20:13:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=716
03/17/2022 20:13:31 - INFO - __main__ - Global step 2150 Train loss 0.39 ACC 0.5 on epoch=716
03/17/2022 20:13:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=719
03/17/2022 20:13:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.42 on epoch=723
03/17/2022 20:13:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=726
03/17/2022 20:13:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=729
03/17/2022 20:13:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=733
03/17/2022 20:13:46 - INFO - __main__ - Global step 2200 Train loss 0.42 ACC 0.53125 on epoch=733
03/17/2022 20:13:46 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=733, global_step=2200
03/17/2022 20:13:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=736
03/17/2022 20:13:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.39 on epoch=739
03/17/2022 20:13:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=743
03/17/2022 20:13:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.37 on epoch=746
03/17/2022 20:13:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=749
03/17/2022 20:14:00 - INFO - __main__ - Global step 2250 Train loss 0.39 ACC 0.5 on epoch=749
03/17/2022 20:14:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.38 on epoch=753
03/17/2022 20:14:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=756
03/17/2022 20:14:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.36 on epoch=759
03/17/2022 20:14:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=763
03/17/2022 20:14:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=766
03/17/2022 20:14:14 - INFO - __main__ - Global step 2300 Train loss 0.39 ACC 0.53125 on epoch=766
03/17/2022 20:14:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=769
03/17/2022 20:14:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=773
03/17/2022 20:14:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=776
03/17/2022 20:14:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.35 on epoch=779
03/17/2022 20:14:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=783
03/17/2022 20:14:28 - INFO - __main__ - Global step 2350 Train loss 0.37 ACC 0.65625 on epoch=783
03/17/2022 20:14:28 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.65625 on epoch=783, global_step=2350
03/17/2022 20:14:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=786
03/17/2022 20:14:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=789
03/17/2022 20:14:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.31 on epoch=793
03/17/2022 20:14:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=796
03/17/2022 20:14:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=799
03/17/2022 20:14:42 - INFO - __main__ - Global step 2400 Train loss 0.35 ACC 0.5 on epoch=799
03/17/2022 20:14:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=803
03/17/2022 20:14:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=806
03/17/2022 20:14:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=809
03/17/2022 20:14:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.34 on epoch=813
03/17/2022 20:14:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=816
03/17/2022 20:14:57 - INFO - __main__ - Global step 2450 Train loss 0.35 ACC 0.625 on epoch=816
03/17/2022 20:14:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=819
03/17/2022 20:15:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=823
03/17/2022 20:15:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=826
03/17/2022 20:15:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=829
03/17/2022 20:15:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.31 on epoch=833
03/17/2022 20:15:11 - INFO - __main__ - Global step 2500 Train loss 0.35 ACC 0.5625 on epoch=833
03/17/2022 20:15:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=836
03/17/2022 20:15:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=839
03/17/2022 20:15:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=843
03/17/2022 20:15:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=846
03/17/2022 20:15:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.33 on epoch=849
03/17/2022 20:15:25 - INFO - __main__ - Global step 2550 Train loss 0.35 ACC 0.6875 on epoch=849
03/17/2022 20:15:25 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=849, global_step=2550
03/17/2022 20:15:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.29 on epoch=853
03/17/2022 20:15:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=856
03/17/2022 20:15:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=859
03/17/2022 20:15:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=863
03/17/2022 20:15:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=866
03/17/2022 20:15:39 - INFO - __main__ - Global step 2600 Train loss 0.31 ACC 0.5625 on epoch=866
03/17/2022 20:15:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=869
03/17/2022 20:15:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.31 on epoch=873
03/17/2022 20:15:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=876
03/17/2022 20:15:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=879
03/17/2022 20:15:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.26 on epoch=883
03/17/2022 20:15:54 - INFO - __main__ - Global step 2650 Train loss 0.29 ACC 0.3125 on epoch=883
03/17/2022 20:15:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=886
03/17/2022 20:15:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=889
03/17/2022 20:16:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.30 on epoch=893
03/17/2022 20:16:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=896
03/17/2022 20:16:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=899
03/17/2022 20:16:08 - INFO - __main__ - Global step 2700 Train loss 0.28 ACC 0.59375 on epoch=899
03/17/2022 20:16:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.35 on epoch=903
03/17/2022 20:16:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=906
03/17/2022 20:16:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.27 on epoch=909
03/17/2022 20:16:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=913
03/17/2022 20:16:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=916
03/17/2022 20:16:22 - INFO - __main__ - Global step 2750 Train loss 0.27 ACC 0.78125 on epoch=916
03/17/2022 20:16:22 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.78125 on epoch=916, global_step=2750
03/17/2022 20:16:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=919
03/17/2022 20:16:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=923
03/17/2022 20:16:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=926
03/17/2022 20:16:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.23 on epoch=929
03/17/2022 20:16:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.28 on epoch=933
03/17/2022 20:16:36 - INFO - __main__ - Global step 2800 Train loss 0.25 ACC 0.53125 on epoch=933
03/17/2022 20:16:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.22 on epoch=936
03/17/2022 20:16:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=939
03/17/2022 20:16:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.22 on epoch=943
03/17/2022 20:16:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=946
03/17/2022 20:16:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=949
03/17/2022 20:16:51 - INFO - __main__ - Global step 2850 Train loss 0.23 ACC 0.59375 on epoch=949
03/17/2022 20:16:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=953
03/17/2022 20:16:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.27 on epoch=956
03/17/2022 20:16:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=959
03/17/2022 20:17:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=963
03/17/2022 20:17:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=966
03/17/2022 20:17:05 - INFO - __main__ - Global step 2900 Train loss 0.21 ACC 0.75 on epoch=966
03/17/2022 20:17:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=969
03/17/2022 20:17:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=973
03/17/2022 20:17:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=976
03/17/2022 20:17:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=979
03/17/2022 20:17:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=983
03/17/2022 20:17:19 - INFO - __main__ - Global step 2950 Train loss 0.20 ACC 0.5 on epoch=983
03/17/2022 20:17:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=986
03/17/2022 20:17:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=989
03/17/2022 20:17:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=993
03/17/2022 20:17:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=996
03/17/2022 20:17:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.20 on epoch=999
03/17/2022 20:17:33 - INFO - __main__ - Global step 3000 Train loss 0.21 ACC 0.71875 on epoch=999
03/17/2022 20:17:33 - INFO - __main__ - save last model!
03/17/2022 20:17:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 20:17:33 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 20:17:33 - INFO - __main__ - Printing 3 examples
03/17/2022 20:17:33 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 20:17:33 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:33 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 20:17:33 - INFO - __main__ - ['neutral']
03/17/2022 20:17:33 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 20:17:33 - INFO - __main__ - ['entailment']
03/17/2022 20:17:33 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:17:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:17:33 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 20:17:34 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 20:17:34 - INFO - __main__ - Printing 3 examples
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 20:17:34 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:17:34 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 20:17:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 20:17:34 - INFO - __main__ - Printing 3 examples
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 20:17:34 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:34 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:17:34 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:17:34 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 20:17:36 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_87_0.3_8_predictions.txt
03/17/2022 20:17:36 - INFO - __main__ - ACC on test data: 0.5179
03/17/2022 20:17:36 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.3, bsz=8, dev_performance=0.78125, test_performance=0.5178571428571429
03/17/2022 20:17:36 - INFO - __main__ - Running ... prefix=superglue-cb_16_87, lr=0.2, bsz=8 ...
03/17/2022 20:17:37 - INFO - __main__ - Start tokenizing ... 48 instances
03/17/2022 20:17:37 - INFO - __main__ - Printing 3 examples
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 20:17:37 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:17:37 - INFO - __main__ - Loaded 48 examples from train data
use DistributedSampler
03/17/2022 20:17:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/17/2022 20:17:37 - INFO - __main__ - Printing 3 examples
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: A: It's divided, yeah. B: Wow! A: It really is, so we've got our Cowboys here and, uh, I don't think anybody roots differently [SEP] hypothesis: somebody roots differently
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of years, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really high improvement. [SEP] hypothesis: they have seen a really high improvement
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ -  [superglue-cb] premise: B: Well, you've got, well, any of the big cities you've got the different rival gangs and they're having their little turf wars over their little drug kingdoms and such, A: Uh-huh. B: And they get out their little Mac tens, they get out their little uzis and they're going to fight with them. And it doesn't matter what restrictions you put on that type of weapon or a class three firearm. If they want it they'll get it. I don't care if they've got to go down into New Mexico to get it they'll get it and they'll get across the border. Now my position, although, I have absolutely no use for a fully automatic weapon, anyway. A: Uh-huh. B: Since I am a law-abiding citizen and I have never had a felony, if I wanted to buy one, I don't think there should be that big of a restriction on it. [SEP] hypothesis: there should be that big of a restriction on it
03/17/2022 20:17:37 - INFO - __main__ - ['contradiction']
03/17/2022 20:17:37 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:17:37 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:17:37 - INFO - __main__ - Loaded 32 examples from dev data
03/17/2022 20:17:49 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:17:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:17:49 - INFO - __main__ - Starting training!
03/17/2022 20:17:56 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:17:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:17:56 - INFO - __main__ - Starting training!
03/17/2022 20:18:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.09 on epoch=3
03/17/2022 20:18:02 - INFO - __main__ - Step 20 Global step 20 Train loss 4.02 on epoch=6
03/17/2022 20:18:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=9
03/17/2022 20:18:08 - INFO - __main__ - Step 40 Global step 40 Train loss 1.71 on epoch=13
03/17/2022 20:18:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.23 on epoch=16
03/17/2022 20:18:11 - INFO - __main__ - Global step 50 Train loss 3.14 ACC 0.5 on epoch=16
03/17/2022 20:18:11 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=16, global_step=50
03/17/2022 20:18:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.21 on epoch=19
03/17/2022 20:18:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=23
03/17/2022 20:18:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=26
03/17/2022 20:18:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.75 on epoch=29
03/17/2022 20:18:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=33
03/17/2022 20:18:26 - INFO - __main__ - Global step 100 Train loss 0.85 ACC 0.3125 on epoch=33
03/17/2022 20:18:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=36
03/17/2022 20:18:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=39
03/17/2022 20:18:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=43
03/17/2022 20:18:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
03/17/2022 20:18:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.18 on epoch=49
03/17/2022 20:18:40 - INFO - __main__ - Global step 150 Train loss 0.70 ACC 0.5 on epoch=49
03/17/2022 20:18:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.22 on epoch=53
03/17/2022 20:18:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=56
03/17/2022 20:18:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=59
03/17/2022 20:18:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=63
03/17/2022 20:18:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=66
03/17/2022 20:18:54 - INFO - __main__ - Global step 200 Train loss 0.72 ACC 0.5 on epoch=66
03/17/2022 20:18:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=69
03/17/2022 20:18:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=73
03/17/2022 20:19:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=76
03/17/2022 20:19:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=79
03/17/2022 20:19:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=83
03/17/2022 20:19:08 - INFO - __main__ - Global step 250 Train loss 0.58 ACC 0.5 on epoch=83
03/17/2022 20:19:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=86
03/17/2022 20:19:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=89
03/17/2022 20:19:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=93
03/17/2022 20:19:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=96
03/17/2022 20:19:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=99
03/17/2022 20:19:22 - INFO - __main__ - Global step 300 Train loss 0.56 ACC 0.5 on epoch=99
03/17/2022 20:19:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=103
03/17/2022 20:19:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=106
03/17/2022 20:19:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=109
03/17/2022 20:19:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
03/17/2022 20:19:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=116
03/17/2022 20:19:36 - INFO - __main__ - Global step 350 Train loss 0.52 ACC 0.5 on epoch=116
03/17/2022 20:19:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=119
03/17/2022 20:19:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=123
03/17/2022 20:19:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=126
03/17/2022 20:19:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=129
03/17/2022 20:19:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=133
03/17/2022 20:19:50 - INFO - __main__ - Global step 400 Train loss 0.50 ACC 0.46875 on epoch=133
03/17/2022 20:19:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=136
03/17/2022 20:19:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=139
03/17/2022 20:19:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=143
03/17/2022 20:20:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=146
03/17/2022 20:20:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=149
03/17/2022 20:20:04 - INFO - __main__ - Global step 450 Train loss 0.52 ACC 0.5 on epoch=149
03/17/2022 20:20:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=153
03/17/2022 20:20:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=156
03/17/2022 20:20:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=159
03/17/2022 20:20:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=163
03/17/2022 20:20:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=166
03/17/2022 20:20:19 - INFO - __main__ - Global step 500 Train loss 0.51 ACC 0.5 on epoch=166
03/17/2022 20:20:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=169
03/17/2022 20:20:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=173
03/17/2022 20:20:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=176
03/17/2022 20:20:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=179
03/17/2022 20:20:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=183
03/17/2022 20:20:33 - INFO - __main__ - Global step 550 Train loss 0.49 ACC 0.5 on epoch=183
03/17/2022 20:20:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=186
03/17/2022 20:20:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=189
03/17/2022 20:20:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=193
03/17/2022 20:20:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=196
03/17/2022 20:20:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=199
03/17/2022 20:20:47 - INFO - __main__ - Global step 600 Train loss 0.45 ACC 0.5 on epoch=199
03/17/2022 20:20:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=203
03/17/2022 20:20:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=206
03/17/2022 20:20:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=209
03/17/2022 20:20:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=213
03/17/2022 20:21:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=216
03/17/2022 20:21:01 - INFO - __main__ - Global step 650 Train loss 0.48 ACC 0.4375 on epoch=216
03/17/2022 20:21:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=219
03/17/2022 20:21:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=223
03/17/2022 20:21:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=226
03/17/2022 20:21:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=229
03/17/2022 20:21:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=233
03/17/2022 20:21:16 - INFO - __main__ - Global step 700 Train loss 0.48 ACC 0.0 on epoch=233
03/17/2022 20:21:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=236
03/17/2022 20:21:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=239
03/17/2022 20:21:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=243
03/17/2022 20:21:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=246
03/17/2022 20:21:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=249
03/17/2022 20:21:30 - INFO - __main__ - Global step 750 Train loss 0.45 ACC 0.5 on epoch=249
03/17/2022 20:21:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=253
03/17/2022 20:21:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=256
03/17/2022 20:21:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=259
03/17/2022 20:21:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=263
03/17/2022 20:21:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=266
03/17/2022 20:21:44 - INFO - __main__ - Global step 800 Train loss 0.44 ACC 0.5 on epoch=266
03/17/2022 20:21:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=269
03/17/2022 20:21:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=273
03/17/2022 20:21:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=276
03/17/2022 20:21:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=279
03/17/2022 20:21:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=283
03/17/2022 20:21:59 - INFO - __main__ - Global step 850 Train loss 0.46 ACC 0.40625 on epoch=283
03/17/2022 20:22:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=286
03/17/2022 20:22:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=289
03/17/2022 20:22:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=293
03/17/2022 20:22:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=296
03/17/2022 20:22:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=299
03/17/2022 20:22:13 - INFO - __main__ - Global step 900 Train loss 0.41 ACC 0.5 on epoch=299
03/17/2022 20:22:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=303
03/17/2022 20:22:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=306
03/17/2022 20:22:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=309
03/17/2022 20:22:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=313
03/17/2022 20:22:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=316
03/17/2022 20:22:28 - INFO - __main__ - Global step 950 Train loss 0.42 ACC 0.46875 on epoch=316
03/17/2022 20:22:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=319
03/17/2022 20:22:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=323
03/17/2022 20:22:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=326
03/17/2022 20:22:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=329
03/17/2022 20:22:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=333
03/17/2022 20:22:42 - INFO - __main__ - Global step 1000 Train loss 0.42 ACC 0.46875 on epoch=333
03/17/2022 20:22:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=336
03/17/2022 20:22:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=339
03/17/2022 20:22:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=343
03/17/2022 20:22:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=346
03/17/2022 20:22:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=349
03/17/2022 20:22:57 - INFO - __main__ - Global step 1050 Train loss 0.41 ACC 0.46875 on epoch=349
03/17/2022 20:22:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=353
03/17/2022 20:23:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=356
03/17/2022 20:23:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=359
03/17/2022 20:23:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=363
03/17/2022 20:23:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=366
03/17/2022 20:23:11 - INFO - __main__ - Global step 1100 Train loss 0.44 ACC 0.40625 on epoch=366
03/17/2022 20:23:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=369
03/17/2022 20:23:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=373
03/17/2022 20:23:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=376
03/17/2022 20:23:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=379
03/17/2022 20:23:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=383
03/17/2022 20:23:25 - INFO - __main__ - Global step 1150 Train loss 0.42 ACC 0.40625 on epoch=383
03/17/2022 20:23:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=386
03/17/2022 20:23:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=389
03/17/2022 20:23:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=393
03/17/2022 20:23:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=396
03/17/2022 20:23:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=399
03/17/2022 20:23:39 - INFO - __main__ - Global step 1200 Train loss 0.43 ACC 0.46875 on epoch=399
03/17/2022 20:23:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=403
03/17/2022 20:23:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=406
03/17/2022 20:23:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=409
03/17/2022 20:23:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=413
03/17/2022 20:23:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=416
03/17/2022 20:23:53 - INFO - __main__ - Global step 1250 Train loss 0.38 ACC 0.4375 on epoch=416
03/17/2022 20:23:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=419
03/17/2022 20:23:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=423
03/17/2022 20:24:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=426
03/17/2022 20:24:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=429
03/17/2022 20:24:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=433
03/17/2022 20:24:08 - INFO - __main__ - Global step 1300 Train loss 0.39 ACC 0.34375 on epoch=433
03/17/2022 20:24:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=436
03/17/2022 20:24:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=439
03/17/2022 20:24:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=443
03/17/2022 20:24:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=446
03/17/2022 20:24:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=449
03/17/2022 20:24:22 - INFO - __main__ - Global step 1350 Train loss 0.39 ACC 0.5 on epoch=449
03/17/2022 20:24:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=453
03/17/2022 20:24:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=456
03/17/2022 20:24:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=459
03/17/2022 20:24:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=463
03/17/2022 20:24:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=466
03/17/2022 20:24:36 - INFO - __main__ - Global step 1400 Train loss 0.37 ACC 0.5 on epoch=466
03/17/2022 20:24:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=469
03/17/2022 20:24:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=473
03/17/2022 20:24:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=476
03/17/2022 20:24:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=479
03/17/2022 20:24:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=483
03/17/2022 20:24:50 - INFO - __main__ - Global step 1450 Train loss 0.40 ACC 0.46875 on epoch=483
03/17/2022 20:24:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=486
03/17/2022 20:24:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=489
03/17/2022 20:24:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=493
03/17/2022 20:25:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=496
03/17/2022 20:25:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=499
03/17/2022 20:25:05 - INFO - __main__ - Global step 1500 Train loss 0.34 ACC 0.5 on epoch=499
03/17/2022 20:25:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=503
03/17/2022 20:25:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=506
03/17/2022 20:25:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=509
03/17/2022 20:25:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=513
03/17/2022 20:25:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=516
03/17/2022 20:25:19 - INFO - __main__ - Global step 1550 Train loss 0.35 ACC 0.53125 on epoch=516
03/17/2022 20:25:19 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=516, global_step=1550
03/17/2022 20:25:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=519
03/17/2022 20:25:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=523
03/17/2022 20:25:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=526
03/17/2022 20:25:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=529
03/17/2022 20:25:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=533
03/17/2022 20:25:34 - INFO - __main__ - Global step 1600 Train loss 0.38 ACC 0.375 on epoch=533
03/17/2022 20:25:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=536
03/17/2022 20:25:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=539
03/17/2022 20:25:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=543
03/17/2022 20:25:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=546
03/17/2022 20:25:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=549
03/17/2022 20:25:48 - INFO - __main__ - Global step 1650 Train loss 0.34 ACC 0.34375 on epoch=549
03/17/2022 20:25:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=553
03/17/2022 20:25:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=556
03/17/2022 20:25:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=559
03/17/2022 20:25:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=563
03/17/2022 20:26:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=566
03/17/2022 20:26:02 - INFO - __main__ - Global step 1700 Train loss 0.33 ACC 0.375 on epoch=566
03/17/2022 20:26:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=569
03/17/2022 20:26:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=573
03/17/2022 20:26:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=576
03/17/2022 20:26:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=579
03/17/2022 20:26:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=583
03/17/2022 20:26:16 - INFO - __main__ - Global step 1750 Train loss 0.32 ACC 0.21875 on epoch=583
03/17/2022 20:26:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=586
03/17/2022 20:26:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=589
03/17/2022 20:26:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=593
03/17/2022 20:26:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=596
03/17/2022 20:26:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=599
03/17/2022 20:26:30 - INFO - __main__ - Global step 1800 Train loss 0.30 ACC 0.46875 on epoch=599
03/17/2022 20:26:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=603
03/17/2022 20:26:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=606
03/17/2022 20:26:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=609
03/17/2022 20:26:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=613
03/17/2022 20:26:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=616
03/17/2022 20:26:45 - INFO - __main__ - Global step 1850 Train loss 0.32 ACC 0.28125 on epoch=616
03/17/2022 20:26:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=619
03/17/2022 20:26:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=623
03/17/2022 20:26:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=626
03/17/2022 20:26:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=629
03/17/2022 20:26:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=633
03/17/2022 20:26:59 - INFO - __main__ - Global step 1900 Train loss 0.29 ACC 0.28125 on epoch=633
03/17/2022 20:27:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=636
03/17/2022 20:27:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=639
03/17/2022 20:27:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=643
03/17/2022 20:27:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=646
03/17/2022 20:27:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=649
03/17/2022 20:27:13 - INFO - __main__ - Global step 1950 Train loss 0.30 ACC 0.4375 on epoch=649
03/17/2022 20:27:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=653
03/17/2022 20:27:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=656
03/17/2022 20:27:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=659
03/17/2022 20:27:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=663
03/17/2022 20:27:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=666
03/17/2022 20:27:28 - INFO - __main__ - Global step 2000 Train loss 0.25 ACC 0.625 on epoch=666
03/17/2022 20:27:28 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=666, global_step=2000
03/17/2022 20:27:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=669
03/17/2022 20:27:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=673
03/17/2022 20:27:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=676
03/17/2022 20:27:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=679
03/17/2022 20:27:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=683
03/17/2022 20:27:42 - INFO - __main__ - Global step 2050 Train loss 0.25 ACC 0.28125 on epoch=683
03/17/2022 20:27:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=686
03/17/2022 20:27:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=689
03/17/2022 20:27:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=693
03/17/2022 20:27:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=696
03/17/2022 20:27:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=699
03/17/2022 20:27:56 - INFO - __main__ - Global step 2100 Train loss 0.22 ACC 0.5625 on epoch=699
03/17/2022 20:27:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=703
03/17/2022 20:28:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=706
03/17/2022 20:28:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=709
03/17/2022 20:28:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=713
03/17/2022 20:28:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=716
03/17/2022 20:28:10 - INFO - __main__ - Global step 2150 Train loss 0.20 ACC 0.53125 on epoch=716
03/17/2022 20:28:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=719
03/17/2022 20:28:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=723
03/17/2022 20:28:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=726
03/17/2022 20:28:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=729
03/17/2022 20:28:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=733
03/17/2022 20:28:25 - INFO - __main__ - Global step 2200 Train loss 0.17 ACC 0.46875 on epoch=733
03/17/2022 20:28:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=736
03/17/2022 20:28:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=739
03/17/2022 20:28:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=743
03/17/2022 20:28:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=746
03/17/2022 20:28:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=749
03/17/2022 20:28:39 - INFO - __main__ - Global step 2250 Train loss 0.15 ACC 0.5 on epoch=749
03/17/2022 20:28:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=753
03/17/2022 20:28:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=756
03/17/2022 20:28:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=759
03/17/2022 20:28:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=763
03/17/2022 20:28:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=766
03/17/2022 20:28:53 - INFO - __main__ - Global step 2300 Train loss 0.14 ACC 0.59375 on epoch=766
03/17/2022 20:28:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=769
03/17/2022 20:28:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=773
03/17/2022 20:29:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=776
03/17/2022 20:29:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=779
03/17/2022 20:29:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=783
03/17/2022 20:29:08 - INFO - __main__ - Global step 2350 Train loss 0.14 ACC 0.4375 on epoch=783
03/17/2022 20:29:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=786
03/17/2022 20:29:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=789
03/17/2022 20:29:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=793
03/17/2022 20:29:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=796
03/17/2022 20:29:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=799
03/17/2022 20:29:22 - INFO - __main__ - Global step 2400 Train loss 0.10 ACC 0.4375 on epoch=799
03/17/2022 20:29:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=803
03/17/2022 20:29:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=806
03/17/2022 20:29:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=809
03/17/2022 20:29:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=813
03/17/2022 20:29:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=816
03/17/2022 20:29:36 - INFO - __main__ - Global step 2450 Train loss 0.10 ACC 0.5 on epoch=816
03/17/2022 20:29:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=819
03/17/2022 20:29:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=823
03/17/2022 20:29:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=826
03/17/2022 20:29:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=829
03/17/2022 20:29:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=833
03/17/2022 20:29:50 - INFO - __main__ - Global step 2500 Train loss 0.09 ACC 0.3125 on epoch=833
03/17/2022 20:29:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=836
03/17/2022 20:29:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=839
03/17/2022 20:29:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=843
03/17/2022 20:30:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=846
03/17/2022 20:30:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=849
03/17/2022 20:30:05 - INFO - __main__ - Global step 2550 Train loss 0.07 ACC 0.3125 on epoch=849
03/17/2022 20:30:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=853
03/17/2022 20:30:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=856
03/17/2022 20:30:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
03/17/2022 20:30:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=863
03/17/2022 20:30:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=866
03/17/2022 20:30:19 - INFO - __main__ - Global step 2600 Train loss 0.06 ACC 0.40625 on epoch=866
03/17/2022 20:30:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=869
03/17/2022 20:30:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=873
03/17/2022 20:30:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
03/17/2022 20:30:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=879
03/17/2022 20:30:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=883
03/17/2022 20:30:33 - INFO - __main__ - Global step 2650 Train loss 0.05 ACC 0.375 on epoch=883
03/17/2022 20:30:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
03/17/2022 20:30:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=889
03/17/2022 20:30:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=893
03/17/2022 20:30:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
03/17/2022 20:30:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=899
03/17/2022 20:30:48 - INFO - __main__ - Global step 2700 Train loss 0.05 ACC 0.375 on epoch=899
03/17/2022 20:30:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=903
03/17/2022 20:30:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=906
03/17/2022 20:30:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=909
03/17/2022 20:30:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=913
03/17/2022 20:31:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
03/17/2022 20:31:02 - INFO - __main__ - Global step 2750 Train loss 0.05 ACC 0.34375 on epoch=916
03/17/2022 20:31:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=919
03/17/2022 20:31:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
03/17/2022 20:31:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=926
03/17/2022 20:31:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=929
03/17/2022 20:31:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=933
03/17/2022 20:31:16 - INFO - __main__ - Global step 2800 Train loss 0.06 ACC 0.40625 on epoch=933
03/17/2022 20:31:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=936
03/17/2022 20:31:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=939
03/17/2022 20:31:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
03/17/2022 20:31:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
03/17/2022 20:31:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
03/17/2022 20:31:30 - INFO - __main__ - Global step 2850 Train loss 0.06 ACC 0.40625 on epoch=949
03/17/2022 20:31:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=953
03/17/2022 20:31:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
03/17/2022 20:31:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=959
03/17/2022 20:31:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=963
03/17/2022 20:31:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=966
03/17/2022 20:31:45 - INFO - __main__ - Global step 2900 Train loss 0.05 ACC 0.40625 on epoch=966
03/17/2022 20:31:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
03/17/2022 20:31:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=973
03/17/2022 20:31:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
03/17/2022 20:31:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=979
03/17/2022 20:31:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=983
03/17/2022 20:31:59 - INFO - __main__ - Global step 2950 Train loss 0.04 ACC 0.40625 on epoch=983
03/17/2022 20:32:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
03/17/2022 20:32:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=989
03/17/2022 20:32:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
03/17/2022 20:32:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
03/17/2022 20:32:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
03/17/2022 20:32:14 - INFO - __main__ - Global step 3000 Train loss 0.04 ACC 0.3125 on epoch=999
03/17/2022 20:32:14 - INFO - __main__ - save last model!
03/17/2022 20:32:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 20:32:14 - INFO - __main__ - Start tokenizing ... 56 instances
03/17/2022 20:32:14 - INFO - __main__ - Printing 3 examples
03/17/2022 20:32:14 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
03/17/2022 20:32:14 - INFO - __main__ - ['contradiction']
03/17/2022 20:32:14 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
03/17/2022 20:32:14 - INFO - __main__ - ['neutral']
03/17/2022 20:32:14 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
03/17/2022 20:32:14 - INFO - __main__ - ['entailment']
03/17/2022 20:32:14 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:32:14 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:32:14 - INFO - __main__ - Loaded 56 examples from test data
03/17/2022 20:32:16 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-superglue-cb/superglue-cb_16_87_0.2_8_predictions.txt
03/17/2022 20:32:16 - INFO - __main__ - ACC on test data: 0.4107
03/17/2022 20:32:20 - INFO - __main__ - prefix=superglue-cb_16_87, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.4107142857142857
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (20830): No such process
Task: dbpedia_14, Checkpoint: models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10
03/17/2022 20:32:26 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/17/2022 20:32:26 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14
Output directory () already exists and is not empty.
03/17/2022 20:32:26 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/17/2022 20:32:26 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14
03/17/2022 20:32:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/17/2022 20:32:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/17/2022 20:32:27 - INFO - __main__ - args.device: cuda:0
03/17/2022 20:32:27 - INFO - __main__ - Using 2 gpus
03/17/2022 20:32:27 - INFO - __main__ - args.device: cuda:1
03/17/2022 20:32:27 - INFO - __main__ - Using 2 gpus
03/17/2022 20:32:27 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/17/2022 20:32:27 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/17/2022 20:32:32 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.5, bsz=8 ...
03/17/2022 20:32:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:32:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:32:33 - INFO - __main__ - Printing 3 examples
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 20:32:33 - INFO - __main__ - Printing 3 examples
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:32:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 20:32:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:32:33 - INFO - __main__ - Printing 3 examples
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:32:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 20:32:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:32:33 - INFO - __main__ - Printing 3 examples
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 20:32:33 - INFO - __main__ - ['Animal']
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:32:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:32:33 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 20:32:33 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 20:32:51 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:32:51 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:32:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:32:52 - INFO - __main__ - Starting training!
03/17/2022 20:32:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:32:56 - INFO - __main__ - Starting training!
03/17/2022 20:33:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.67 on epoch=0
03/17/2022 20:33:03 - INFO - __main__ - Step 20 Global step 20 Train loss 5.51 on epoch=1
03/17/2022 20:33:05 - INFO - __main__ - Step 30 Global step 30 Train loss 3.41 on epoch=2
03/17/2022 20:33:08 - INFO - __main__ - Step 40 Global step 40 Train loss 2.41 on epoch=2
03/17/2022 20:33:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.96 on epoch=3
03/17/2022 20:33:17 - INFO - __main__ - Global step 50 Train loss 3.99 Classification-F1 0.2597854446713188 on epoch=3
03/17/2022 20:33:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2597854446713188 on epoch=3, global_step=50
03/17/2022 20:33:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.61 on epoch=4
03/17/2022 20:33:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.33 on epoch=4
03/17/2022 20:33:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.15 on epoch=5
03/17/2022 20:33:27 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=6
03/17/2022 20:33:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=7
03/17/2022 20:33:37 - INFO - __main__ - Global step 100 Train loss 1.22 Classification-F1 0.4319940156915057 on epoch=7
03/17/2022 20:33:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2597854446713188 -> 0.4319940156915057 on epoch=7, global_step=100
03/17/2022 20:33:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=7
03/17/2022 20:33:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=8
03/17/2022 20:33:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=9
03/17/2022 20:33:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=9
03/17/2022 20:33:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=10
03/17/2022 20:33:57 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.6318344429987576 on epoch=10
03/17/2022 20:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4319940156915057 -> 0.6318344429987576 on epoch=10, global_step=150
03/17/2022 20:34:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=11
03/17/2022 20:34:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=12
03/17/2022 20:34:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=12
03/17/2022 20:34:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=13
03/17/2022 20:34:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=14
03/17/2022 20:34:17 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.6482500104342772 on epoch=14
03/17/2022 20:34:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6318344429987576 -> 0.6482500104342772 on epoch=14, global_step=200
03/17/2022 20:34:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=14
03/17/2022 20:34:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=15
03/17/2022 20:34:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=16
03/17/2022 20:34:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=17
03/17/2022 20:34:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=17
03/17/2022 20:34:37 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.5693269125056245 on epoch=17
03/17/2022 20:34:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=18
03/17/2022 20:34:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=19
03/17/2022 20:34:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=19
03/17/2022 20:34:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=20
03/17/2022 20:34:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=21
03/17/2022 20:34:57 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.8287966422882498 on epoch=21
03/17/2022 20:34:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6482500104342772 -> 0.8287966422882498 on epoch=21, global_step=300
03/17/2022 20:34:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=22
03/17/2022 20:35:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=22
03/17/2022 20:35:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=23
03/17/2022 20:35:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=24
03/17/2022 20:35:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=24
03/17/2022 20:35:16 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6200687593824542 on epoch=24
03/17/2022 20:35:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=25
03/17/2022 20:35:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=26
03/17/2022 20:35:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=27
03/17/2022 20:35:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=27
03/17/2022 20:35:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=28
03/17/2022 20:35:35 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.6104753066453649 on epoch=28
03/17/2022 20:35:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=29
03/17/2022 20:35:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=29
03/17/2022 20:35:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=30
03/17/2022 20:35:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=31
03/17/2022 20:35:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=32
03/17/2022 20:35:53 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.569660796675308 on epoch=32
03/17/2022 20:35:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=32
03/17/2022 20:35:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=33
03/17/2022 20:36:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=34
03/17/2022 20:36:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=34
03/17/2022 20:36:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=35
03/17/2022 20:36:12 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.9506174757292947 on epoch=35
03/17/2022 20:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.8287966422882498 -> 0.9506174757292947 on epoch=35, global_step=500
03/17/2022 20:36:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=36
03/17/2022 20:36:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=37
03/17/2022 20:36:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=37
03/17/2022 20:36:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=38
03/17/2022 20:36:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=39
03/17/2022 20:36:32 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.793375133006484 on epoch=39
03/17/2022 20:36:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=39
03/17/2022 20:36:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=40
03/17/2022 20:36:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=41
03/17/2022 20:36:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=42
03/17/2022 20:36:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=42
03/17/2022 20:36:52 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.7581829836799406 on epoch=42
03/17/2022 20:36:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=43
03/17/2022 20:36:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=44
03/17/2022 20:36:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=44
03/17/2022 20:37:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=45
03/17/2022 20:37:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=46
03/17/2022 20:37:10 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.7342460133575213 on epoch=46
03/17/2022 20:37:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=47
03/17/2022 20:37:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=47
03/17/2022 20:37:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=48
03/17/2022 20:37:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=49
03/17/2022 20:37:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=49
03/17/2022 20:37:29 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.5586418236528207 on epoch=49
03/17/2022 20:37:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=50
03/17/2022 20:37:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=51
03/17/2022 20:37:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=52
03/17/2022 20:37:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=52
03/17/2022 20:37:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=53
03/17/2022 20:37:48 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.5368505804010301 on epoch=53
03/17/2022 20:37:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=54
03/17/2022 20:37:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=54
03/17/2022 20:37:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=55
03/17/2022 20:37:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=56
03/17/2022 20:38:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=57
03/17/2022 20:38:06 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.6927672995151958 on epoch=57
03/17/2022 20:38:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=57
03/17/2022 20:38:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/17/2022 20:38:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=59
03/17/2022 20:38:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=59
03/17/2022 20:38:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=60
03/17/2022 20:38:25 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.8497588855766739 on epoch=60
03/17/2022 20:38:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=61
03/17/2022 20:38:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=62
03/17/2022 20:38:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=62
03/17/2022 20:38:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=63
03/17/2022 20:38:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=64
03/17/2022 20:38:44 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.5342285513706237 on epoch=64
03/17/2022 20:38:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=64
03/17/2022 20:38:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=65
03/17/2022 20:38:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=66
03/17/2022 20:38:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=67
03/17/2022 20:38:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=67
03/17/2022 20:39:02 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.5266100547300724 on epoch=67
03/17/2022 20:39:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=68
03/17/2022 20:39:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=69
03/17/2022 20:39:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=69
03/17/2022 20:39:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=70
03/17/2022 20:39:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=71
03/17/2022 20:39:21 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.5510628278477822 on epoch=71
03/17/2022 20:39:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=72
03/17/2022 20:39:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=72
03/17/2022 20:39:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=73
03/17/2022 20:39:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=74
03/17/2022 20:39:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=74
03/17/2022 20:39:40 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.5936814279987845 on epoch=74
03/17/2022 20:39:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=75
03/17/2022 20:39:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=76
03/17/2022 20:39:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=77
03/17/2022 20:39:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
03/17/2022 20:39:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=78
03/17/2022 20:39:58 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.5835947106883476 on epoch=78
03/17/2022 20:40:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=79
03/17/2022 20:40:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=79
03/17/2022 20:40:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=80
03/17/2022 20:40:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
03/17/2022 20:40:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=82
03/17/2022 20:40:16 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6141023944471519 on epoch=82
03/17/2022 20:40:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=82
03/17/2022 20:40:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=83
03/17/2022 20:40:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=84
03/17/2022 20:40:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=84
03/17/2022 20:40:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=85
03/17/2022 20:40:35 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.8430387010009283 on epoch=85
03/17/2022 20:40:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=86
03/17/2022 20:40:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=87
03/17/2022 20:40:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=87
03/17/2022 20:40:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=88
03/17/2022 20:40:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=89
03/17/2022 20:40:54 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.7580229669704086 on epoch=89
03/17/2022 20:40:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=89
03/17/2022 20:40:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
03/17/2022 20:41:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=91
03/17/2022 20:41:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=92
03/17/2022 20:41:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=92
03/17/2022 20:41:12 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7488414631085409 on epoch=92
03/17/2022 20:41:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=93
03/17/2022 20:41:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=94
03/17/2022 20:41:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=94
03/17/2022 20:41:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=95
03/17/2022 20:41:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=96
03/17/2022 20:41:31 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.637253875157101 on epoch=96
03/17/2022 20:41:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=97
03/17/2022 20:41:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=97
03/17/2022 20:41:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
03/17/2022 20:41:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
03/17/2022 20:41:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=99
03/17/2022 20:41:49 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.5271184029574685 on epoch=99
03/17/2022 20:41:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=100
03/17/2022 20:41:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=101
03/17/2022 20:41:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=102
03/17/2022 20:42:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=102
03/17/2022 20:42:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=103
03/17/2022 20:42:07 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5397669880624426 on epoch=103
03/17/2022 20:42:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
03/17/2022 20:42:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
03/17/2022 20:42:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=105
03/17/2022 20:42:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
03/17/2022 20:42:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=107
03/17/2022 20:42:26 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7840955213039758 on epoch=107
03/17/2022 20:42:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
03/17/2022 20:42:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=108
03/17/2022 20:42:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=109
03/17/2022 20:42:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=109
03/17/2022 20:42:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=110
03/17/2022 20:42:45 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.623720549803499 on epoch=110
03/17/2022 20:42:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=111
03/17/2022 20:42:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=112
03/17/2022 20:42:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
03/17/2022 20:42:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
03/17/2022 20:42:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
03/17/2022 20:43:03 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6079261714460955 on epoch=114
03/17/2022 20:43:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=114
03/17/2022 20:43:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=115
03/17/2022 20:43:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=116
03/17/2022 20:43:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
03/17/2022 20:43:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=117
03/17/2022 20:43:22 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6851121679695508 on epoch=117
03/17/2022 20:43:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=118
03/17/2022 20:43:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
03/17/2022 20:43:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=119
03/17/2022 20:43:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
03/17/2022 20:43:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=121
03/17/2022 20:43:41 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8513821804381577 on epoch=121
03/17/2022 20:43:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
03/17/2022 20:43:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=122
03/17/2022 20:43:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=123
03/17/2022 20:43:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=124
03/17/2022 20:43:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=124
03/17/2022 20:44:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.9097407894293322 on epoch=124
03/17/2022 20:44:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=125
03/17/2022 20:44:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=126
03/17/2022 20:44:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=127
03/17/2022 20:44:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
03/17/2022 20:44:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=128
03/17/2022 20:44:18 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7615184119188473 on epoch=128
03/17/2022 20:44:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=129
03/17/2022 20:44:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=129
03/17/2022 20:44:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=130
03/17/2022 20:44:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
03/17/2022 20:44:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=132
03/17/2022 20:44:36 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.8350651641211414 on epoch=132
03/17/2022 20:44:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=132
03/17/2022 20:44:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=133
03/17/2022 20:44:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=134
03/17/2022 20:44:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
03/17/2022 20:44:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=135
03/17/2022 20:44:55 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7636614803953514 on epoch=135
03/17/2022 20:44:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
03/17/2022 20:45:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
03/17/2022 20:45:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
03/17/2022 20:45:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/17/2022 20:45:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
03/17/2022 20:45:13 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6746806717930215 on epoch=139
03/17/2022 20:45:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
03/17/2022 20:45:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
03/17/2022 20:45:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
03/17/2022 20:45:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=142
03/17/2022 20:45:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
03/17/2022 20:45:31 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.45764477475578047 on epoch=142
03/17/2022 20:45:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
03/17/2022 20:45:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
03/17/2022 20:45:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
03/17/2022 20:45:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=145
03/17/2022 20:45:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=146
03/17/2022 20:45:50 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5063687852037411 on epoch=146
03/17/2022 20:45:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
03/17/2022 20:45:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
03/17/2022 20:45:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/17/2022 20:46:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
03/17/2022 20:46:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
03/17/2022 20:46:08 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5895026484641803 on epoch=149
03/17/2022 20:46:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
03/17/2022 20:46:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=151
03/17/2022 20:46:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=152
03/17/2022 20:46:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
03/17/2022 20:46:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=153
03/17/2022 20:46:26 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6210910594781562 on epoch=153
03/17/2022 20:46:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=154
03/17/2022 20:46:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/17/2022 20:46:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=155
03/17/2022 20:46:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
03/17/2022 20:46:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
03/17/2022 20:46:45 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6813808695608644 on epoch=157
03/17/2022 20:46:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
03/17/2022 20:46:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
03/17/2022 20:46:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/17/2022 20:46:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
03/17/2022 20:46:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
03/17/2022 20:47:03 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5681788684548733 on epoch=160
03/17/2022 20:47:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/17/2022 20:47:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
03/17/2022 20:47:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
03/17/2022 20:47:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
03/17/2022 20:47:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=164
03/17/2022 20:47:21 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7102971994992369 on epoch=164
03/17/2022 20:47:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
03/17/2022 20:47:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
03/17/2022 20:47:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
03/17/2022 20:47:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
03/17/2022 20:47:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
03/17/2022 20:47:40 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.4180785445491328 on epoch=167
03/17/2022 20:47:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=168
03/17/2022 20:47:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/17/2022 20:47:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/17/2022 20:47:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
03/17/2022 20:47:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=171
03/17/2022 20:47:59 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.571420589022403 on epoch=171
03/17/2022 20:48:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
03/17/2022 20:48:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
03/17/2022 20:48:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
03/17/2022 20:48:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
03/17/2022 20:48:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/17/2022 20:48:17 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7399587270555013 on epoch=174
03/17/2022 20:48:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
03/17/2022 20:48:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/17/2022 20:48:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
03/17/2022 20:48:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/17/2022 20:48:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
03/17/2022 20:48:36 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.72742125709635 on epoch=178
03/17/2022 20:48:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
03/17/2022 20:48:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/17/2022 20:48:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=180
03/17/2022 20:48:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=181
03/17/2022 20:48:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
03/17/2022 20:48:54 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8438403644285998 on epoch=182
03/17/2022 20:48:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/17/2022 20:48:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=183
03/17/2022 20:49:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=184
03/17/2022 20:49:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/17/2022 20:49:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
03/17/2022 20:49:13 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.620121988189215 on epoch=185
03/17/2022 20:49:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=186
03/17/2022 20:49:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
03/17/2022 20:49:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=187
03/17/2022 20:49:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/17/2022 20:49:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=189
03/17/2022 20:49:31 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6692198458502263 on epoch=189
03/17/2022 20:49:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=189
03/17/2022 20:49:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
03/17/2022 20:49:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
03/17/2022 20:49:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=192
03/17/2022 20:49:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/17/2022 20:49:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7936992426779232 on epoch=192
03/17/2022 20:49:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=193
03/17/2022 20:49:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/17/2022 20:49:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/17/2022 20:50:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/17/2022 20:50:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/17/2022 20:50:09 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6199241321621307 on epoch=196
03/17/2022 20:50:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
03/17/2022 20:50:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=197
03/17/2022 20:50:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/17/2022 20:50:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
03/17/2022 20:50:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
03/17/2022 20:50:27 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7035881638322531 on epoch=199
03/17/2022 20:50:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
03/17/2022 20:50:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
03/17/2022 20:50:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
03/17/2022 20:50:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/17/2022 20:50:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
03/17/2022 20:50:46 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6815927324201052 on epoch=203
03/17/2022 20:50:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/17/2022 20:50:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
03/17/2022 20:50:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/17/2022 20:50:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/17/2022 20:50:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
03/17/2022 20:51:04 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7177037006193586 on epoch=207
03/17/2022 20:51:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
03/17/2022 20:51:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/17/2022 20:51:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/17/2022 20:51:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
03/17/2022 20:51:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
03/17/2022 20:51:22 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7269717140684883 on epoch=210
03/17/2022 20:51:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=211
03/17/2022 20:51:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
03/17/2022 20:51:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/17/2022 20:51:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/17/2022 20:51:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
03/17/2022 20:51:36 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:51:36 - INFO - __main__ - Printing 3 examples
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 20:51:36 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:51:36 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 20:51:36 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:51:36 - INFO - __main__ - Printing 3 examples
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 20:51:36 - INFO - __main__ - ['Animal']
03/17/2022 20:51:36 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:51:36 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:51:37 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 20:51:40 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.8531647116324537 on epoch=214
03/17/2022 20:51:40 - INFO - __main__ - save last model!
03/17/2022 20:51:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 20:51:40 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 20:51:40 - INFO - __main__ - Printing 3 examples
03/17/2022 20:51:40 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 20:51:40 - INFO - __main__ - ['Animal']
03/17/2022 20:51:40 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 20:51:40 - INFO - __main__ - ['Animal']
03/17/2022 20:51:40 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 20:51:40 - INFO - __main__ - ['Village']
03/17/2022 20:51:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 20:51:42 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:51:46 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 20:51:52 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:51:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:51:52 - INFO - __main__ - Starting training!
03/17/2022 20:53:50 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_100_0.5_8_predictions.txt
03/17/2022 20:53:50 - INFO - __main__ - Classification-F1 on test data: 0.5682
03/17/2022 20:53:51 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.5, bsz=8, dev_performance=0.9506174757292947, test_performance=0.5681682576868735
03/17/2022 20:53:51 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.4, bsz=8 ...
03/17/2022 20:53:52 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:53:52 - INFO - __main__ - Printing 3 examples
03/17/2022 20:53:52 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 20:53:52 - INFO - __main__ - ['Animal']
03/17/2022 20:53:52 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 20:53:52 - INFO - __main__ - ['Animal']
03/17/2022 20:53:52 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 20:53:52 - INFO - __main__ - ['Animal']
03/17/2022 20:53:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 20:53:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:53:53 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 20:53:53 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 20:53:53 - INFO - __main__ - Printing 3 examples
03/17/2022 20:53:53 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 20:53:53 - INFO - __main__ - ['Animal']
03/17/2022 20:53:53 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 20:53:53 - INFO - __main__ - ['Animal']
03/17/2022 20:53:53 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 20:53:53 - INFO - __main__ - ['Animal']
03/17/2022 20:53:53 - INFO - __main__ - Tokenizing Input ...
03/17/2022 20:53:53 - INFO - __main__ - Tokenizing Output ...
03/17/2022 20:53:53 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 20:54:08 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 20:54:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 20:54:09 - INFO - __main__ - Starting training!
03/17/2022 20:54:13 - INFO - __main__ - Step 10 Global step 10 Train loss 6.76 on epoch=0
03/17/2022 20:54:16 - INFO - __main__ - Step 20 Global step 20 Train loss 6.14 on epoch=1
03/17/2022 20:54:18 - INFO - __main__ - Step 30 Global step 30 Train loss 5.12 on epoch=2
03/17/2022 20:54:21 - INFO - __main__ - Step 40 Global step 40 Train loss 3.56 on epoch=2
03/17/2022 20:54:23 - INFO - __main__ - Step 50 Global step 50 Train loss 2.70 on epoch=3
03/17/2022 20:54:37 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.0665378421900161 on epoch=3
03/17/2022 20:54:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0665378421900161 on epoch=3, global_step=50
03/17/2022 20:54:40 - INFO - __main__ - Step 60 Global step 60 Train loss 2.26 on epoch=4
03/17/2022 20:54:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.70 on epoch=4
03/17/2022 20:54:45 - INFO - __main__ - Step 80 Global step 80 Train loss 1.49 on epoch=5
03/17/2022 20:54:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=6
03/17/2022 20:54:50 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=7
03/17/2022 20:54:57 - INFO - __main__ - Global step 100 Train loss 1.60 Classification-F1 0.4242809062704788 on epoch=7
03/17/2022 20:54:57 - INFO - __main__ - Saving model with best Classification-F1: 0.0665378421900161 -> 0.4242809062704788 on epoch=7, global_step=100
03/17/2022 20:55:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=7
03/17/2022 20:55:03 - INFO - __main__ - Step 120 Global step 120 Train loss 1.04 on epoch=8
03/17/2022 20:55:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=9
03/17/2022 20:55:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.79 on epoch=9
03/17/2022 20:55:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=10
03/17/2022 20:55:19 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.6087270211907846 on epoch=10
03/17/2022 20:55:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4242809062704788 -> 0.6087270211907846 on epoch=10, global_step=150
03/17/2022 20:55:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=11
03/17/2022 20:55:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=12
03/17/2022 20:55:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.74 on epoch=12
03/17/2022 20:55:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=13
03/17/2022 20:55:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=14
03/17/2022 20:55:39 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.6014539987796497 on epoch=14
03/17/2022 20:55:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=14
03/17/2022 20:55:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.74 on epoch=15
03/17/2022 20:55:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.66 on epoch=16
03/17/2022 20:55:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=17
03/17/2022 20:55:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=17
03/17/2022 20:55:59 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.6914690996551764 on epoch=17
03/17/2022 20:55:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6087270211907846 -> 0.6914690996551764 on epoch=17, global_step=250
03/17/2022 20:56:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=18
03/17/2022 20:56:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=19
03/17/2022 20:56:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=19
03/17/2022 20:56:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=20
03/17/2022 20:56:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=21
03/17/2022 20:56:19 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.7314211214821984 on epoch=21
03/17/2022 20:56:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6914690996551764 -> 0.7314211214821984 on epoch=21, global_step=300
03/17/2022 20:56:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=22
03/17/2022 20:56:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=22
03/17/2022 20:56:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=23
03/17/2022 20:56:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=24
03/17/2022 20:56:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=24
03/17/2022 20:56:37 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.5541835419837983 on epoch=24
03/17/2022 20:56:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=25
03/17/2022 20:56:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=26
03/17/2022 20:56:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=27
03/17/2022 20:56:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=27
03/17/2022 20:56:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=28
03/17/2022 20:56:57 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.7075821069709277 on epoch=28
03/17/2022 20:57:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=29
03/17/2022 20:57:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=29
03/17/2022 20:57:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=30
03/17/2022 20:57:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=31
03/17/2022 20:57:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=32
03/17/2022 20:57:16 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.614967608898293 on epoch=32
03/17/2022 20:57:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=32
03/17/2022 20:57:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=33
03/17/2022 20:57:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=34
03/17/2022 20:57:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=34
03/17/2022 20:57:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=35
03/17/2022 20:57:36 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.719388235407138 on epoch=35
03/17/2022 20:57:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=36
03/17/2022 20:57:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=37
03/17/2022 20:57:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=37
03/17/2022 20:57:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=38
03/17/2022 20:57:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=39
03/17/2022 20:57:56 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.9046829475390972 on epoch=39
03/17/2022 20:57:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7314211214821984 -> 0.9046829475390972 on epoch=39, global_step=550
03/17/2022 20:57:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=39
03/17/2022 20:58:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=40
03/17/2022 20:58:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=41
03/17/2022 20:58:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=42
03/17/2022 20:58:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=42
03/17/2022 20:58:16 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.698137053321054 on epoch=42
03/17/2022 20:58:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=43
03/17/2022 20:58:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=44
03/17/2022 20:58:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=44
03/17/2022 20:58:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=45
03/17/2022 20:58:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=46
03/17/2022 20:58:35 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.8821514343762921 on epoch=46
03/17/2022 20:58:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=47
03/17/2022 20:58:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=47
03/17/2022 20:58:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=48
03/17/2022 20:58:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=49
03/17/2022 20:58:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=49
03/17/2022 20:58:55 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.7731621240871716 on epoch=49
03/17/2022 20:58:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=50
03/17/2022 20:59:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=51
03/17/2022 20:59:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=52
03/17/2022 20:59:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=52
03/17/2022 20:59:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=53
03/17/2022 20:59:14 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.8252807084564593 on epoch=53
03/17/2022 20:59:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=54
03/17/2022 20:59:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=54
03/17/2022 20:59:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=55
03/17/2022 20:59:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=56
03/17/2022 20:59:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=57
03/17/2022 20:59:33 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.9596485616190049 on epoch=57
03/17/2022 20:59:33 - INFO - __main__ - Saving model with best Classification-F1: 0.9046829475390972 -> 0.9596485616190049 on epoch=57, global_step=800
03/17/2022 20:59:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=57
03/17/2022 20:59:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=58
03/17/2022 20:59:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=59
03/17/2022 20:59:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=59
03/17/2022 20:59:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=60
03/17/2022 20:59:52 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.9679909720486436 on epoch=60
03/17/2022 20:59:52 - INFO - __main__ - Saving model with best Classification-F1: 0.9596485616190049 -> 0.9679909720486436 on epoch=60, global_step=850
03/17/2022 20:59:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=61
03/17/2022 20:59:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=62
03/17/2022 21:00:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=62
03/17/2022 21:00:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=63
03/17/2022 21:00:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=64
03/17/2022 21:00:11 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.8147595212938548 on epoch=64
03/17/2022 21:00:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=64
03/17/2022 21:00:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=65
03/17/2022 21:00:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=66
03/17/2022 21:00:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=67
03/17/2022 21:00:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=67
03/17/2022 21:00:30 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.9636496460754392 on epoch=67
03/17/2022 21:00:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=68
03/17/2022 21:00:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=69
03/17/2022 21:00:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
03/17/2022 21:00:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=70
03/17/2022 21:00:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=71
03/17/2022 21:00:49 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.9559156433834531 on epoch=71
03/17/2022 21:00:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=72
03/17/2022 21:00:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=72
03/17/2022 21:00:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=73
03/17/2022 21:01:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=74
03/17/2022 21:01:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=74
03/17/2022 21:01:08 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.8975022521227455 on epoch=74
03/17/2022 21:01:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=75
03/17/2022 21:01:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=76
03/17/2022 21:01:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=77
03/17/2022 21:01:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
03/17/2022 21:01:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=78
03/17/2022 21:01:27 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.977311719257427 on epoch=78
03/17/2022 21:01:27 - INFO - __main__ - Saving model with best Classification-F1: 0.9679909720486436 -> 0.977311719257427 on epoch=78, global_step=1100
03/17/2022 21:01:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=79
03/17/2022 21:01:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=79
03/17/2022 21:01:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=80
03/17/2022 21:01:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=81
03/17/2022 21:01:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=82
03/17/2022 21:01:46 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.9733915853731522 on epoch=82
03/17/2022 21:01:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=82
03/17/2022 21:01:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=83
03/17/2022 21:01:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=84
03/17/2022 21:01:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=84
03/17/2022 21:01:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=85
03/17/2022 21:02:05 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.9550861941500234 on epoch=85
03/17/2022 21:02:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=86
03/17/2022 21:02:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=87
03/17/2022 21:02:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=87
03/17/2022 21:02:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=88
03/17/2022 21:02:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=89
03/17/2022 21:02:23 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.8890910192460935 on epoch=89
03/17/2022 21:02:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=89
03/17/2022 21:02:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
03/17/2022 21:02:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=91
03/17/2022 21:02:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=92
03/17/2022 21:02:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=92
03/17/2022 21:02:42 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.9635563738590793 on epoch=92
03/17/2022 21:02:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=93
03/17/2022 21:02:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=94
03/17/2022 21:02:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=94
03/17/2022 21:02:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
03/17/2022 21:02:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=96
03/17/2022 21:03:01 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.973147851515973 on epoch=96
03/17/2022 21:03:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
03/17/2022 21:03:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=97
03/17/2022 21:03:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=98
03/17/2022 21:03:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
03/17/2022 21:03:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=99
03/17/2022 21:03:21 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.8975891496923035 on epoch=99
03/17/2022 21:03:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=100
03/17/2022 21:03:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=101
03/17/2022 21:03:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=102
03/17/2022 21:03:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=102
03/17/2022 21:03:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=103
03/17/2022 21:03:40 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.9548024708417093 on epoch=103
03/17/2022 21:03:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=104
03/17/2022 21:03:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=104
03/17/2022 21:03:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
03/17/2022 21:03:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=106
03/17/2022 21:03:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=107
03/17/2022 21:03:58 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.8932865410633192 on epoch=107
03/17/2022 21:04:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
03/17/2022 21:04:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=108
03/17/2022 21:04:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=109
03/17/2022 21:04:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=109
03/17/2022 21:04:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=110
03/17/2022 21:04:17 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.964160417681078 on epoch=110
03/17/2022 21:04:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=111
03/17/2022 21:04:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=112
03/17/2022 21:04:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=112
03/17/2022 21:04:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=113
03/17/2022 21:04:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=114
03/17/2022 21:04:36 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.9645816097700078 on epoch=114
03/17/2022 21:04:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=114
03/17/2022 21:04:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=115
03/17/2022 21:04:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=116
03/17/2022 21:04:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=117
03/17/2022 21:04:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=117
03/17/2022 21:04:55 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.9688065255427685 on epoch=117
03/17/2022 21:04:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=118
03/17/2022 21:05:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=119
03/17/2022 21:05:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=119
03/17/2022 21:05:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=120
03/17/2022 21:05:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
03/17/2022 21:05:14 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.9685017436542823 on epoch=121
03/17/2022 21:05:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=122
03/17/2022 21:05:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=122
03/17/2022 21:05:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=123
03/17/2022 21:05:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/17/2022 21:05:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=124
03/17/2022 21:05:32 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.968118295705379 on epoch=124
03/17/2022 21:05:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=125
03/17/2022 21:05:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=126
03/17/2022 21:05:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=127
03/17/2022 21:05:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
03/17/2022 21:05:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=128
03/17/2022 21:05:51 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9038610967358596 on epoch=128
03/17/2022 21:05:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
03/17/2022 21:05:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
03/17/2022 21:05:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/17/2022 21:06:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
03/17/2022 21:06:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=132
03/17/2022 21:06:09 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.9039628255994482 on epoch=132
03/17/2022 21:06:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
03/17/2022 21:06:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
03/17/2022 21:06:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=134
03/17/2022 21:06:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
03/17/2022 21:06:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=135
03/17/2022 21:06:29 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.9820991153059465 on epoch=135
03/17/2022 21:06:29 - INFO - __main__ - Saving model with best Classification-F1: 0.977311719257427 -> 0.9820991153059465 on epoch=135, global_step=1900
03/17/2022 21:06:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
03/17/2022 21:06:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=137
03/17/2022 21:06:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
03/17/2022 21:06:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=138
03/17/2022 21:06:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=139
03/17/2022 21:06:47 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.8970907434332481 on epoch=139
03/17/2022 21:06:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
03/17/2022 21:06:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=140
03/17/2022 21:06:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
03/17/2022 21:06:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
03/17/2022 21:07:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
03/17/2022 21:07:06 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.9778741995331858 on epoch=142
03/17/2022 21:07:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=143
03/17/2022 21:07:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=144
03/17/2022 21:07:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=144
03/17/2022 21:07:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
03/17/2022 21:07:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
03/17/2022 21:07:25 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.9735372374756354 on epoch=146
03/17/2022 21:07:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
03/17/2022 21:07:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
03/17/2022 21:07:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=148
03/17/2022 21:07:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=149
03/17/2022 21:07:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
03/17/2022 21:07:44 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.9777621532483962 on epoch=149
03/17/2022 21:07:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
03/17/2022 21:07:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
03/17/2022 21:07:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
03/17/2022 21:07:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
03/17/2022 21:07:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
03/17/2022 21:08:03 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.9690685878456955 on epoch=153
03/17/2022 21:08:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=154
03/17/2022 21:08:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=154
03/17/2022 21:08:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
03/17/2022 21:08:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
03/17/2022 21:08:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=157
03/17/2022 21:08:21 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.9690685878456955 on epoch=157
03/17/2022 21:08:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/17/2022 21:08:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
03/17/2022 21:08:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/17/2022 21:08:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
03/17/2022 21:08:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
03/17/2022 21:08:40 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.9732935036184561 on epoch=160
03/17/2022 21:08:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/17/2022 21:08:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=162
03/17/2022 21:08:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=162
03/17/2022 21:08:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/17/2022 21:08:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/17/2022 21:08:59 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.9104601518026565 on epoch=164
03/17/2022 21:09:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
03/17/2022 21:09:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=165
03/17/2022 21:09:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
03/17/2022 21:09:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
03/17/2022 21:09:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/17/2022 21:09:17 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.9103331704138155 on epoch=167
03/17/2022 21:09:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/17/2022 21:09:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/17/2022 21:09:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/17/2022 21:09:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
03/17/2022 21:09:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
03/17/2022 21:09:36 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.9020688862055087 on epoch=171
03/17/2022 21:09:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
03/17/2022 21:09:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
03/17/2022 21:09:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
03/17/2022 21:09:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
03/17/2022 21:09:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/17/2022 21:09:54 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.9777621532483962 on epoch=174
03/17/2022 21:09:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
03/17/2022 21:09:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/17/2022 21:10:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
03/17/2022 21:10:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
03/17/2022 21:10:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
03/17/2022 21:10:13 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.96885114016281 on epoch=178
03/17/2022 21:10:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
03/17/2022 21:10:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
03/17/2022 21:10:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/17/2022 21:10:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/17/2022 21:10:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
03/17/2022 21:10:32 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.9599121980170366 on epoch=182
03/17/2022 21:10:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/17/2022 21:10:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/17/2022 21:10:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
03/17/2022 21:10:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/17/2022 21:10:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=185
03/17/2022 21:10:51 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9732935036184563 on epoch=185
03/17/2022 21:10:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/17/2022 21:10:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
03/17/2022 21:10:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=187
03/17/2022 21:11:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/17/2022 21:11:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=189
03/17/2022 21:11:09 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.888897099069513 on epoch=189
03/17/2022 21:11:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
03/17/2022 21:11:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
03/17/2022 21:11:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
03/17/2022 21:11:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/17/2022 21:11:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
03/17/2022 21:11:28 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9081335652540586 on epoch=192
03/17/2022 21:11:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/17/2022 21:11:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/17/2022 21:11:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/17/2022 21:11:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
03/17/2022 21:11:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=196
03/17/2022 21:11:47 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9777621532483962 on epoch=196
03/17/2022 21:11:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
03/17/2022 21:11:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=197
03/17/2022 21:11:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/17/2022 21:11:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
03/17/2022 21:12:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
03/17/2022 21:12:06 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9777621532483962 on epoch=199
03/17/2022 21:12:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/17/2022 21:12:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
03/17/2022 21:12:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
03/17/2022 21:12:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
03/17/2022 21:12:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
03/17/2022 21:12:25 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9735372374756354 on epoch=203
03/17/2022 21:12:28 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/17/2022 21:12:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
03/17/2022 21:12:33 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/17/2022 21:12:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/17/2022 21:12:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/17/2022 21:12:44 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9732751751727083 on epoch=207
03/17/2022 21:12:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
03/17/2022 21:12:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/17/2022 21:12:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/17/2022 21:12:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=209
03/17/2022 21:12:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
03/17/2022 21:13:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.97331978979275 on epoch=210
03/17/2022 21:13:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=211
03/17/2022 21:13:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/17/2022 21:13:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/17/2022 21:13:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=213
03/17/2022 21:13:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/17/2022 21:13:18 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:13:18 - INFO - __main__ - Printing 3 examples
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 21:13:18 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:13:18 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 21:13:18 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:13:18 - INFO - __main__ - Printing 3 examples
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 21:13:18 - INFO - __main__ - ['Animal']
03/17/2022 21:13:18 - INFO - __main__ - Tokenizing Input ...
03/17/2022 21:13:18 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:13:19 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 21:13:22 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.97331978979275 on epoch=214
03/17/2022 21:13:22 - INFO - __main__ - save last model!
03/17/2022 21:13:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 21:13:22 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 21:13:22 - INFO - __main__ - Printing 3 examples
03/17/2022 21:13:22 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 21:13:22 - INFO - __main__ - ['Animal']
03/17/2022 21:13:22 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 21:13:22 - INFO - __main__ - ['Animal']
03/17/2022 21:13:22 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 21:13:22 - INFO - __main__ - ['Village']
03/17/2022 21:13:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 21:13:24 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:13:27 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 21:13:34 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 21:13:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 21:13:35 - INFO - __main__ - Starting training!
03/17/2022 21:15:33 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_100_0.4_8_predictions.txt
03/17/2022 21:15:33 - INFO - __main__ - Classification-F1 on test data: 0.6202
03/17/2022 21:15:34 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.4, bsz=8, dev_performance=0.9820991153059465, test_performance=0.6201899567180417
03/17/2022 21:15:34 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.3, bsz=8 ...
03/17/2022 21:15:35 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:15:35 - INFO - __main__ - Printing 3 examples
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 21:15:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:15:35 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 21:15:35 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:15:35 - INFO - __main__ - Printing 3 examples
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 21:15:35 - INFO - __main__ - ['Animal']
03/17/2022 21:15:35 - INFO - __main__ - Tokenizing Input ...
03/17/2022 21:15:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:15:36 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 21:15:50 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 21:15:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 21:15:51 - INFO - __main__ - Starting training!
03/17/2022 21:15:55 - INFO - __main__ - Step 10 Global step 10 Train loss 6.91 on epoch=0
03/17/2022 21:15:57 - INFO - __main__ - Step 20 Global step 20 Train loss 6.72 on epoch=1
03/17/2022 21:16:00 - INFO - __main__ - Step 30 Global step 30 Train loss 7.32 on epoch=2
03/17/2022 21:16:02 - INFO - __main__ - Step 40 Global step 40 Train loss 7.56 on epoch=2
03/17/2022 21:16:05 - INFO - __main__ - Step 50 Global step 50 Train loss 7.36 on epoch=3
03/17/2022 21:17:53 - INFO - __main__ - Global step 50 Train loss 7.17 Classification-F1 0.0 on epoch=3
03/17/2022 21:17:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/17/2022 21:17:56 - INFO - __main__ - Step 60 Global step 60 Train loss 6.49 on epoch=4
03/17/2022 21:17:59 - INFO - __main__ - Step 70 Global step 70 Train loss 6.57 on epoch=4
03/17/2022 21:18:01 - INFO - __main__ - Step 80 Global step 80 Train loss 6.51 on epoch=5
03/17/2022 21:18:04 - INFO - __main__ - Step 90 Global step 90 Train loss 6.56 on epoch=6
03/17/2022 21:18:06 - INFO - __main__ - Step 100 Global step 100 Train loss 6.37 on epoch=7
03/17/2022 21:19:51 - INFO - __main__ - Global step 100 Train loss 6.50 Classification-F1 0.0 on epoch=7
03/17/2022 21:19:54 - INFO - __main__ - Step 110 Global step 110 Train loss 6.46 on epoch=7
03/17/2022 21:19:56 - INFO - __main__ - Step 120 Global step 120 Train loss 6.63 on epoch=8
03/17/2022 21:19:59 - INFO - __main__ - Step 130 Global step 130 Train loss 6.45 on epoch=9
03/17/2022 21:20:01 - INFO - __main__ - Step 140 Global step 140 Train loss 6.62 on epoch=9
03/17/2022 21:20:04 - INFO - __main__ - Step 150 Global step 150 Train loss 6.83 on epoch=10
03/17/2022 21:21:38 - INFO - __main__ - Global step 150 Train loss 6.60 Classification-F1 0.0 on epoch=10
03/17/2022 21:21:40 - INFO - __main__ - Step 160 Global step 160 Train loss 7.06 on epoch=11
03/17/2022 21:21:43 - INFO - __main__ - Step 170 Global step 170 Train loss 6.59 on epoch=12
03/17/2022 21:21:45 - INFO - __main__ - Step 180 Global step 180 Train loss 6.47 on epoch=12
03/17/2022 21:21:48 - INFO - __main__ - Step 190 Global step 190 Train loss 6.71 on epoch=13
03/17/2022 21:21:51 - INFO - __main__ - Step 200 Global step 200 Train loss 6.46 on epoch=14
03/17/2022 21:23:42 - INFO - __main__ - Global step 200 Train loss 6.66 Classification-F1 0.0 on epoch=14
03/17/2022 21:23:45 - INFO - __main__ - Step 210 Global step 210 Train loss 6.65 on epoch=14
03/17/2022 21:23:47 - INFO - __main__ - Step 220 Global step 220 Train loss 6.61 on epoch=15
03/17/2022 21:23:50 - INFO - __main__ - Step 230 Global step 230 Train loss 6.65 on epoch=16
03/17/2022 21:23:52 - INFO - __main__ - Step 240 Global step 240 Train loss 6.43 on epoch=17
03/17/2022 21:23:55 - INFO - __main__ - Step 250 Global step 250 Train loss 6.31 on epoch=17
03/17/2022 21:25:41 - INFO - __main__ - Global step 250 Train loss 6.53 Classification-F1 0.0 on epoch=17
03/17/2022 21:25:44 - INFO - __main__ - Step 260 Global step 260 Train loss 6.52 on epoch=18
03/17/2022 21:25:46 - INFO - __main__ - Step 270 Global step 270 Train loss 6.38 on epoch=19
03/17/2022 21:25:49 - INFO - __main__ - Step 280 Global step 280 Train loss 6.53 on epoch=19
03/17/2022 21:25:51 - INFO - __main__ - Step 290 Global step 290 Train loss 6.33 on epoch=20
03/17/2022 21:25:54 - INFO - __main__ - Step 300 Global step 300 Train loss 6.42 on epoch=21
03/17/2022 21:27:55 - INFO - __main__ - Global step 300 Train loss 6.43 Classification-F1 0.0 on epoch=21
03/17/2022 21:27:58 - INFO - __main__ - Step 310 Global step 310 Train loss 6.39 on epoch=22
03/17/2022 21:28:00 - INFO - __main__ - Step 320 Global step 320 Train loss 6.29 on epoch=22
03/17/2022 21:28:03 - INFO - __main__ - Step 330 Global step 330 Train loss 6.41 on epoch=23
03/17/2022 21:28:05 - INFO - __main__ - Step 340 Global step 340 Train loss 6.32 on epoch=24
03/17/2022 21:28:08 - INFO - __main__ - Step 350 Global step 350 Train loss 6.42 on epoch=24
03/17/2022 21:30:07 - INFO - __main__ - Global step 350 Train loss 6.37 Classification-F1 0.0 on epoch=24
03/17/2022 21:30:10 - INFO - __main__ - Step 360 Global step 360 Train loss 6.41 on epoch=25
03/17/2022 21:30:12 - INFO - __main__ - Step 370 Global step 370 Train loss 6.49 on epoch=26
03/17/2022 21:30:15 - INFO - __main__ - Step 380 Global step 380 Train loss 6.29 on epoch=27
03/17/2022 21:30:17 - INFO - __main__ - Step 390 Global step 390 Train loss 6.29 on epoch=27
03/17/2022 21:30:20 - INFO - __main__ - Step 400 Global step 400 Train loss 6.30 on epoch=28
03/17/2022 21:32:21 - INFO - __main__ - Global step 400 Train loss 6.36 Classification-F1 0.0 on epoch=28
03/17/2022 21:32:23 - INFO - __main__ - Step 410 Global step 410 Train loss 6.16 on epoch=29
03/17/2022 21:32:26 - INFO - __main__ - Step 420 Global step 420 Train loss 6.38 on epoch=29
03/17/2022 21:32:28 - INFO - __main__ - Step 430 Global step 430 Train loss 6.27 on epoch=30
03/17/2022 21:32:31 - INFO - __main__ - Step 440 Global step 440 Train loss 6.24 on epoch=31
03/17/2022 21:32:33 - INFO - __main__ - Step 450 Global step 450 Train loss 6.23 on epoch=32
03/17/2022 21:34:31 - INFO - __main__ - Global step 450 Train loss 6.26 Classification-F1 0.0 on epoch=32
03/17/2022 21:34:33 - INFO - __main__ - Step 460 Global step 460 Train loss 6.11 on epoch=32
03/17/2022 21:34:36 - INFO - __main__ - Step 470 Global step 470 Train loss 6.19 on epoch=33
03/17/2022 21:34:38 - INFO - __main__ - Step 480 Global step 480 Train loss 5.99 on epoch=34
03/17/2022 21:34:41 - INFO - __main__ - Step 490 Global step 490 Train loss 6.16 on epoch=34
03/17/2022 21:34:43 - INFO - __main__ - Step 500 Global step 500 Train loss 6.07 on epoch=35
03/17/2022 21:36:32 - INFO - __main__ - Global step 500 Train loss 6.10 Classification-F1 0.0 on epoch=35
03/17/2022 21:36:34 - INFO - __main__ - Step 510 Global step 510 Train loss 6.11 on epoch=36
03/17/2022 21:36:37 - INFO - __main__ - Step 520 Global step 520 Train loss 6.01 on epoch=37
03/17/2022 21:36:40 - INFO - __main__ - Step 530 Global step 530 Train loss 5.85 on epoch=37
03/17/2022 21:36:42 - INFO - __main__ - Step 540 Global step 540 Train loss 5.94 on epoch=38
03/17/2022 21:36:45 - INFO - __main__ - Step 550 Global step 550 Train loss 5.78 on epoch=39
03/17/2022 21:38:23 - INFO - __main__ - Global step 550 Train loss 5.94 Classification-F1 0.0 on epoch=39
03/17/2022 21:38:26 - INFO - __main__ - Step 560 Global step 560 Train loss 5.91 on epoch=39
03/17/2022 21:38:29 - INFO - __main__ - Step 570 Global step 570 Train loss 5.86 on epoch=40
03/17/2022 21:38:31 - INFO - __main__ - Step 580 Global step 580 Train loss 5.79 on epoch=41
03/17/2022 21:38:34 - INFO - __main__ - Step 590 Global step 590 Train loss 5.62 on epoch=42
03/17/2022 21:38:36 - INFO - __main__ - Step 600 Global step 600 Train loss 5.58 on epoch=42
03/17/2022 21:40:14 - INFO - __main__ - Global step 600 Train loss 5.75 Classification-F1 0.0 on epoch=42
03/17/2022 21:40:16 - INFO - __main__ - Step 610 Global step 610 Train loss 5.48 on epoch=43
03/17/2022 21:40:19 - INFO - __main__ - Step 620 Global step 620 Train loss 5.40 on epoch=44
03/17/2022 21:40:21 - INFO - __main__ - Step 630 Global step 630 Train loss 5.39 on epoch=44
03/17/2022 21:40:24 - INFO - __main__ - Step 640 Global step 640 Train loss 5.16 on epoch=45
03/17/2022 21:40:26 - INFO - __main__ - Step 650 Global step 650 Train loss 5.11 on epoch=46
03/17/2022 21:41:58 - INFO - __main__ - Global step 650 Train loss 5.31 Classification-F1 0.0 on epoch=46
03/17/2022 21:42:01 - INFO - __main__ - Step 660 Global step 660 Train loss 4.93 on epoch=47
03/17/2022 21:42:04 - INFO - __main__ - Step 670 Global step 670 Train loss 4.71 on epoch=47
03/17/2022 21:42:06 - INFO - __main__ - Step 680 Global step 680 Train loss 4.52 on epoch=48
03/17/2022 21:42:09 - INFO - __main__ - Step 690 Global step 690 Train loss 4.51 on epoch=49
03/17/2022 21:42:11 - INFO - __main__ - Step 700 Global step 700 Train loss 4.28 on epoch=49
03/17/2022 21:43:34 - INFO - __main__ - Global step 700 Train loss 4.59 Classification-F1 0.0 on epoch=49
03/17/2022 21:43:36 - INFO - __main__ - Step 710 Global step 710 Train loss 4.07 on epoch=50
03/17/2022 21:43:39 - INFO - __main__ - Step 720 Global step 720 Train loss 4.09 on epoch=51
03/17/2022 21:43:41 - INFO - __main__ - Step 730 Global step 730 Train loss 3.79 on epoch=52
03/17/2022 21:43:44 - INFO - __main__ - Step 740 Global step 740 Train loss 3.73 on epoch=52
03/17/2022 21:43:46 - INFO - __main__ - Step 750 Global step 750 Train loss 3.52 on epoch=53
03/17/2022 21:44:50 - INFO - __main__ - Global step 750 Train loss 3.84 Classification-F1 0.0008783487044356609 on epoch=53
03/17/2022 21:44:50 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.0008783487044356609 on epoch=53, global_step=750
03/17/2022 21:44:53 - INFO - __main__ - Step 760 Global step 760 Train loss 3.52 on epoch=54
03/17/2022 21:44:55 - INFO - __main__ - Step 770 Global step 770 Train loss 3.33 on epoch=54
03/17/2022 21:44:58 - INFO - __main__ - Step 780 Global step 780 Train loss 3.10 on epoch=55
03/17/2022 21:45:01 - INFO - __main__ - Step 790 Global step 790 Train loss 3.00 on epoch=56
03/17/2022 21:45:03 - INFO - __main__ - Step 800 Global step 800 Train loss 2.96 on epoch=57
03/17/2022 21:45:10 - INFO - __main__ - Global step 800 Train loss 3.18 Classification-F1 0.00892608089260809 on epoch=57
03/17/2022 21:45:10 - INFO - __main__ - Saving model with best Classification-F1: 0.0008783487044356609 -> 0.00892608089260809 on epoch=57, global_step=800
03/17/2022 21:45:12 - INFO - __main__ - Step 810 Global step 810 Train loss 2.78 on epoch=57
03/17/2022 21:45:15 - INFO - __main__ - Step 820 Global step 820 Train loss 2.67 on epoch=58
03/17/2022 21:45:17 - INFO - __main__ - Step 830 Global step 830 Train loss 2.81 on epoch=59
03/17/2022 21:45:20 - INFO - __main__ - Step 840 Global step 840 Train loss 2.52 on epoch=59
03/17/2022 21:45:22 - INFO - __main__ - Step 850 Global step 850 Train loss 2.44 on epoch=60
03/17/2022 21:45:28 - INFO - __main__ - Global step 850 Train loss 2.64 Classification-F1 0.023626613038377747 on epoch=60
03/17/2022 21:45:28 - INFO - __main__ - Saving model with best Classification-F1: 0.00892608089260809 -> 0.023626613038377747 on epoch=60, global_step=850
03/17/2022 21:45:30 - INFO - __main__ - Step 860 Global step 860 Train loss 2.47 on epoch=61
03/17/2022 21:45:33 - INFO - __main__ - Step 870 Global step 870 Train loss 2.26 on epoch=62
03/17/2022 21:45:35 - INFO - __main__ - Step 880 Global step 880 Train loss 2.20 on epoch=62
03/17/2022 21:45:38 - INFO - __main__ - Step 890 Global step 890 Train loss 2.19 on epoch=63
03/17/2022 21:45:41 - INFO - __main__ - Step 900 Global step 900 Train loss 2.12 on epoch=64
03/17/2022 21:45:47 - INFO - __main__ - Global step 900 Train loss 2.25 Classification-F1 0.1950511800002434 on epoch=64
03/17/2022 21:45:47 - INFO - __main__ - Saving model with best Classification-F1: 0.023626613038377747 -> 0.1950511800002434 on epoch=64, global_step=900
03/17/2022 21:45:49 - INFO - __main__ - Step 910 Global step 910 Train loss 1.99 on epoch=64
03/17/2022 21:45:52 - INFO - __main__ - Step 920 Global step 920 Train loss 1.80 on epoch=65
03/17/2022 21:45:54 - INFO - __main__ - Step 930 Global step 930 Train loss 1.91 on epoch=66
03/17/2022 21:45:57 - INFO - __main__ - Step 940 Global step 940 Train loss 1.78 on epoch=67
03/17/2022 21:45:59 - INFO - __main__ - Step 950 Global step 950 Train loss 1.66 on epoch=67
03/17/2022 21:46:06 - INFO - __main__ - Global step 950 Train loss 1.83 Classification-F1 0.2120343966804585 on epoch=67
03/17/2022 21:46:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1950511800002434 -> 0.2120343966804585 on epoch=67, global_step=950
03/17/2022 21:46:09 - INFO - __main__ - Step 960 Global step 960 Train loss 1.72 on epoch=68
03/17/2022 21:46:12 - INFO - __main__ - Step 970 Global step 970 Train loss 1.58 on epoch=69
03/17/2022 21:46:14 - INFO - __main__ - Step 980 Global step 980 Train loss 1.52 on epoch=69
03/17/2022 21:46:17 - INFO - __main__ - Step 990 Global step 990 Train loss 1.54 on epoch=70
03/17/2022 21:46:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.49 on epoch=71
03/17/2022 21:46:26 - INFO - __main__ - Global step 1000 Train loss 1.57 Classification-F1 0.31062194991921127 on epoch=71
03/17/2022 21:46:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2120343966804585 -> 0.31062194991921127 on epoch=71, global_step=1000
03/17/2022 21:46:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.39 on epoch=72
03/17/2022 21:46:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.36 on epoch=72
03/17/2022 21:46:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.41 on epoch=73
03/17/2022 21:46:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.33 on epoch=74
03/17/2022 21:46:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.25 on epoch=74
03/17/2022 21:46:46 - INFO - __main__ - Global step 1050 Train loss 1.35 Classification-F1 0.3607245620790778 on epoch=74
03/17/2022 21:46:46 - INFO - __main__ - Saving model with best Classification-F1: 0.31062194991921127 -> 0.3607245620790778 on epoch=74, global_step=1050
03/17/2022 21:46:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.40 on epoch=75
03/17/2022 21:46:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.21 on epoch=76
03/17/2022 21:46:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.24 on epoch=77
03/17/2022 21:46:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.09 on epoch=77
03/17/2022 21:46:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.25 on epoch=78
03/17/2022 21:47:06 - INFO - __main__ - Global step 1100 Train loss 1.23 Classification-F1 0.40932464264102253 on epoch=78
03/17/2022 21:47:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3607245620790778 -> 0.40932464264102253 on epoch=78, global_step=1100
03/17/2022 21:47:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.12 on epoch=79
03/17/2022 21:47:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.21 on epoch=79
03/17/2022 21:47:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.07 on epoch=80
03/17/2022 21:47:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.09 on epoch=81
03/17/2022 21:47:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.09 on epoch=82
03/17/2022 21:47:25 - INFO - __main__ - Global step 1150 Train loss 1.12 Classification-F1 0.4440527128331833 on epoch=82
03/17/2022 21:47:25 - INFO - __main__ - Saving model with best Classification-F1: 0.40932464264102253 -> 0.4440527128331833 on epoch=82, global_step=1150
03/17/2022 21:47:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.93 on epoch=82
03/17/2022 21:47:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.98 on epoch=83
03/17/2022 21:47:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.01 on epoch=84
03/17/2022 21:47:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.06 on epoch=84
03/17/2022 21:47:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.82 on epoch=85
03/17/2022 21:47:45 - INFO - __main__ - Global step 1200 Train loss 0.96 Classification-F1 0.5192358009568739 on epoch=85
03/17/2022 21:47:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4440527128331833 -> 0.5192358009568739 on epoch=85, global_step=1200
03/17/2022 21:47:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.04 on epoch=86
03/17/2022 21:47:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.92 on epoch=87
03/17/2022 21:47:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.90 on epoch=87
03/17/2022 21:47:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.96 on epoch=88
03/17/2022 21:47:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.08 on epoch=89
03/17/2022 21:48:06 - INFO - __main__ - Global step 1250 Train loss 0.98 Classification-F1 0.5695229972296497 on epoch=89
03/17/2022 21:48:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5192358009568739 -> 0.5695229972296497 on epoch=89, global_step=1250
03/17/2022 21:48:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.91 on epoch=89
03/17/2022 21:48:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.88 on epoch=90
03/17/2022 21:48:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.78 on epoch=91
03/17/2022 21:48:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.81 on epoch=92
03/17/2022 21:48:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=92
03/17/2022 21:48:26 - INFO - __main__ - Global step 1300 Train loss 0.84 Classification-F1 0.5352661917190314 on epoch=92
03/17/2022 21:48:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.86 on epoch=93
03/17/2022 21:48:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.78 on epoch=94
03/17/2022 21:48:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.74 on epoch=94
03/17/2022 21:48:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.76 on epoch=95
03/17/2022 21:48:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.73 on epoch=96
03/17/2022 21:48:47 - INFO - __main__ - Global step 1350 Train loss 0.77 Classification-F1 0.5085196175787573 on epoch=96
03/17/2022 21:48:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.74 on epoch=97
03/17/2022 21:48:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.75 on epoch=97
03/17/2022 21:48:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.71 on epoch=98
03/17/2022 21:48:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.62 on epoch=99
03/17/2022 21:49:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.61 on epoch=99
03/17/2022 21:49:09 - INFO - __main__ - Global step 1400 Train loss 0.69 Classification-F1 0.4829611604965104 on epoch=99
03/17/2022 21:49:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.70 on epoch=100
03/17/2022 21:49:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.56 on epoch=101
03/17/2022 21:49:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.64 on epoch=102
03/17/2022 21:49:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=102
03/17/2022 21:49:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=103
03/17/2022 21:49:28 - INFO - __main__ - Global step 1450 Train loss 0.60 Classification-F1 0.674660372833997 on epoch=103
03/17/2022 21:49:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5695229972296497 -> 0.674660372833997 on epoch=103, global_step=1450
03/17/2022 21:49:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=104
03/17/2022 21:49:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.58 on epoch=104
03/17/2022 21:49:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.51 on epoch=105
03/17/2022 21:49:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=106
03/17/2022 21:49:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=107
03/17/2022 21:49:48 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.6653036563778892 on epoch=107
03/17/2022 21:49:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=107
03/17/2022 21:49:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.56 on epoch=108
03/17/2022 21:49:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=109
03/17/2022 21:49:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=109
03/17/2022 21:50:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=110
03/17/2022 21:50:08 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.6407011023549686 on epoch=110
03/17/2022 21:50:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=111
03/17/2022 21:50:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.55 on epoch=112
03/17/2022 21:50:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=112
03/17/2022 21:50:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=113
03/17/2022 21:50:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=114
03/17/2022 21:50:27 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.6834874223394147 on epoch=114
03/17/2022 21:50:27 - INFO - __main__ - Saving model with best Classification-F1: 0.674660372833997 -> 0.6834874223394147 on epoch=114, global_step=1600
03/17/2022 21:50:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=114
03/17/2022 21:50:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=115
03/17/2022 21:50:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=116
03/17/2022 21:50:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=117
03/17/2022 21:50:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=117
03/17/2022 21:50:47 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.6742326287785446 on epoch=117
03/17/2022 21:50:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=118
03/17/2022 21:50:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=119
03/17/2022 21:50:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=119
03/17/2022 21:50:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=120
03/17/2022 21:51:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=121
03/17/2022 21:51:06 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.7394650889907057 on epoch=121
03/17/2022 21:51:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6834874223394147 -> 0.7394650889907057 on epoch=121, global_step=1700
03/17/2022 21:51:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.50 on epoch=122
03/17/2022 21:51:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=122
03/17/2022 21:51:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=123
03/17/2022 21:51:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=124
03/17/2022 21:51:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=124
03/17/2022 21:51:26 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.7457511098892148 on epoch=124
03/17/2022 21:51:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7394650889907057 -> 0.7457511098892148 on epoch=124, global_step=1750
03/17/2022 21:51:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=125
03/17/2022 21:51:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=126
03/17/2022 21:51:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=127
03/17/2022 21:51:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=127
03/17/2022 21:51:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=128
03/17/2022 21:51:45 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.6341383564948486 on epoch=128
03/17/2022 21:51:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=129
03/17/2022 21:51:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=129
03/17/2022 21:51:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=130
03/17/2022 21:51:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=131
03/17/2022 21:51:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=132
03/17/2022 21:52:04 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.7246181736401676 on epoch=132
03/17/2022 21:52:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=132
03/17/2022 21:52:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=133
03/17/2022 21:52:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=134
03/17/2022 21:52:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=134
03/17/2022 21:52:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=135
03/17/2022 21:52:23 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.6730784477533702 on epoch=135
03/17/2022 21:52:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=136
03/17/2022 21:52:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=137
03/17/2022 21:52:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=137
03/17/2022 21:52:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=138
03/17/2022 21:52:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=139
03/17/2022 21:52:43 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.702433046979143 on epoch=139
03/17/2022 21:52:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=139
03/17/2022 21:52:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=140
03/17/2022 21:52:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=141
03/17/2022 21:52:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=142
03/17/2022 21:52:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=142
03/17/2022 21:53:02 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7114173610141351 on epoch=142
03/17/2022 21:53:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=143
03/17/2022 21:53:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=144
03/17/2022 21:53:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=144
03/17/2022 21:53:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=145
03/17/2022 21:53:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=146
03/17/2022 21:53:22 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7348881110671538 on epoch=146
03/17/2022 21:53:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=147
03/17/2022 21:53:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=147
03/17/2022 21:53:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=148
03/17/2022 21:53:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=149
03/17/2022 21:53:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=149
03/17/2022 21:53:42 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7876425584265412 on epoch=149
03/17/2022 21:53:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7457511098892148 -> 0.7876425584265412 on epoch=149, global_step=2100
03/17/2022 21:53:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=150
03/17/2022 21:53:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=151
03/17/2022 21:53:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=152
03/17/2022 21:53:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=152
03/17/2022 21:53:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=153
03/17/2022 21:54:01 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.6854813076999234 on epoch=153
03/17/2022 21:54:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=154
03/17/2022 21:54:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.15 on epoch=154
03/17/2022 21:54:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=155
03/17/2022 21:54:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=156
03/17/2022 21:54:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=157
03/17/2022 21:54:20 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.8107871245236016 on epoch=157
03/17/2022 21:54:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7876425584265412 -> 0.8107871245236016 on epoch=157, global_step=2200
03/17/2022 21:54:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=157
03/17/2022 21:54:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=158
03/17/2022 21:54:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=159
03/17/2022 21:54:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=159
03/17/2022 21:54:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=160
03/17/2022 21:54:40 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.7747218959268295 on epoch=160
03/17/2022 21:54:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=161
03/17/2022 21:54:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=162
03/17/2022 21:54:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=162
03/17/2022 21:54:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=163
03/17/2022 21:54:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=164
03/17/2022 21:54:59 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.7113839385542422 on epoch=164
03/17/2022 21:55:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=164
03/17/2022 21:55:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=165
03/17/2022 21:55:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=166
03/17/2022 21:55:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=167
03/17/2022 21:55:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=167
03/17/2022 21:55:18 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.765957621025247 on epoch=167
03/17/2022 21:55:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=168
03/17/2022 21:55:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=169
03/17/2022 21:55:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=169
03/17/2022 21:55:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=170
03/17/2022 21:55:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=171
03/17/2022 21:55:37 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7858051759603273 on epoch=171
03/17/2022 21:55:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=172
03/17/2022 21:55:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=172
03/17/2022 21:55:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=173
03/17/2022 21:55:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=174
03/17/2022 21:55:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=174
03/17/2022 21:55:56 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7751836261977384 on epoch=174
03/17/2022 21:55:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=175
03/17/2022 21:56:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=176
03/17/2022 21:56:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=177
03/17/2022 21:56:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=177
03/17/2022 21:56:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=178
03/17/2022 21:56:15 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.8239534597335749 on epoch=178
03/17/2022 21:56:15 - INFO - __main__ - Saving model with best Classification-F1: 0.8107871245236016 -> 0.8239534597335749 on epoch=178, global_step=2500
03/17/2022 21:56:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=179
03/17/2022 21:56:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=179
03/17/2022 21:56:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=180
03/17/2022 21:56:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=181
03/17/2022 21:56:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=182
03/17/2022 21:56:34 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.7691679785593726 on epoch=182
03/17/2022 21:56:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=182
03/17/2022 21:56:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=183
03/17/2022 21:56:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=184
03/17/2022 21:56:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=184
03/17/2022 21:56:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=185
03/17/2022 21:56:53 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.7443474529575553 on epoch=185
03/17/2022 21:56:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=186
03/17/2022 21:56:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=187
03/17/2022 21:57:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=187
03/17/2022 21:57:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=188
03/17/2022 21:57:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=189
03/17/2022 21:57:13 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.7829250900953023 on epoch=189
03/17/2022 21:57:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=189
03/17/2022 21:57:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=190
03/17/2022 21:57:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=191
03/17/2022 21:57:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=192
03/17/2022 21:57:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=192
03/17/2022 21:57:32 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.7599832993400915 on epoch=192
03/17/2022 21:57:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=193
03/17/2022 21:57:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=194
03/17/2022 21:57:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=194
03/17/2022 21:57:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=195
03/17/2022 21:57:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=196
03/17/2022 21:57:51 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.8394031951833104 on epoch=196
03/17/2022 21:57:51 - INFO - __main__ - Saving model with best Classification-F1: 0.8239534597335749 -> 0.8394031951833104 on epoch=196, global_step=2750
03/17/2022 21:57:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=197
03/17/2022 21:57:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=197
03/17/2022 21:57:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=198
03/17/2022 21:58:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=199
03/17/2022 21:58:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=199
03/17/2022 21:58:11 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.7919388858370008 on epoch=199
03/17/2022 21:58:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=200
03/17/2022 21:58:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=201
03/17/2022 21:58:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=202
03/17/2022 21:58:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=202
03/17/2022 21:58:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
03/17/2022 21:58:30 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.6686425107677479 on epoch=203
03/17/2022 21:58:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=204
03/17/2022 21:58:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=204
03/17/2022 21:58:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=205
03/17/2022 21:58:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=206
03/17/2022 21:58:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=207
03/17/2022 21:58:49 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7004691191660632 on epoch=207
03/17/2022 21:58:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=207
03/17/2022 21:58:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=208
03/17/2022 21:58:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=209
03/17/2022 21:58:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=209
03/17/2022 21:59:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=210
03/17/2022 21:59:08 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7797643593017443 on epoch=210
03/17/2022 21:59:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=211
03/17/2022 21:59:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=212
03/17/2022 21:59:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=212
03/17/2022 21:59:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=213
03/17/2022 21:59:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=214
03/17/2022 21:59:22 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:59:22 - INFO - __main__ - Printing 3 examples
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 21:59:22 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:59:22 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 21:59:22 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 21:59:22 - INFO - __main__ - Printing 3 examples
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 21:59:22 - INFO - __main__ - ['Animal']
03/17/2022 21:59:22 - INFO - __main__ - Tokenizing Input ...
03/17/2022 21:59:22 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:59:23 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 21:59:27 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7563480552738128 on epoch=214
03/17/2022 21:59:27 - INFO - __main__ - save last model!
03/17/2022 21:59:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 21:59:27 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 21:59:27 - INFO - __main__ - Printing 3 examples
03/17/2022 21:59:27 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 21:59:27 - INFO - __main__ - ['Animal']
03/17/2022 21:59:27 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 21:59:27 - INFO - __main__ - ['Animal']
03/17/2022 21:59:27 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 21:59:27 - INFO - __main__ - ['Village']
03/17/2022 21:59:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 21:59:29 - INFO - __main__ - Tokenizing Output ...
03/17/2022 21:59:32 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 21:59:41 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 21:59:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 21:59:42 - INFO - __main__ - Starting training!
03/17/2022 22:01:33 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_100_0.3_8_predictions.txt
03/17/2022 22:01:33 - INFO - __main__ - Classification-F1 on test data: 0.4864
03/17/2022 22:01:33 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.3, bsz=8, dev_performance=0.8394031951833104, test_performance=0.48643068369567743
03/17/2022 22:01:34 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.2, bsz=8 ...
03/17/2022 22:01:34 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:01:34 - INFO - __main__ - Printing 3 examples
03/17/2022 22:01:34 - INFO - __main__ -  [dbpedia_14] Linnaemyini is a tribe of flies in the family Tachinidae.
03/17/2022 22:01:34 - INFO - __main__ - ['Animal']
03/17/2022 22:01:34 - INFO - __main__ -  [dbpedia_14] Morula ambrosia is a species of sea snail a marine gastropod mollusk in the family Muricidae the murex snails or rock snails.
03/17/2022 22:01:34 - INFO - __main__ - ['Animal']
03/17/2022 22:01:34 - INFO - __main__ -  [dbpedia_14] Neoduma plagosus is a moth of the Arctiidae family. It was described by Rothschild in 1912. It is found in New Guinea.The length of the forewings 10 mm. The forewings are creamy white with a yellow costa. The basal half of the wings is edged with black and there are two olive-grey antemedian patches as well as one on the termen. The hindwings are buff.
03/17/2022 22:01:34 - INFO - __main__ - ['Animal']
03/17/2022 22:01:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 22:01:34 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:01:35 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 22:01:35 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:01:35 - INFO - __main__ - Printing 3 examples
03/17/2022 22:01:35 - INFO - __main__ -  [dbpedia_14] Mesoscincus is a genus comprising three species of skink native to Mexico and Central America. They were formerly included in the genus Eumeces.
03/17/2022 22:01:35 - INFO - __main__ - ['Animal']
03/17/2022 22:01:35 - INFO - __main__ -  [dbpedia_14] Oxynoemacheilus leontinae is a species of stone loach found in Israel Jordan Lebanon and Syria.Its natural habitat is rivers.
03/17/2022 22:01:35 - INFO - __main__ - ['Animal']
03/17/2022 22:01:35 - INFO - __main__ -  [dbpedia_14] Syrmoptera homeyerii is a butterfly in the Lycaenidae family. It is found in the Democratic Republic of Congo (Uele Sankuru Lualaba Lomani Tanganika and Maniema) and Angola.
03/17/2022 22:01:35 - INFO - __main__ - ['Animal']
03/17/2022 22:01:35 - INFO - __main__ - Tokenizing Input ...
03/17/2022 22:01:35 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:01:35 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 22:01:50 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 22:01:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 22:01:51 - INFO - __main__ - Starting training!
03/17/2022 22:01:55 - INFO - __main__ - Step 10 Global step 10 Train loss 7.02 on epoch=0
03/17/2022 22:01:58 - INFO - __main__ - Step 20 Global step 20 Train loss 6.41 on epoch=1
03/17/2022 22:02:00 - INFO - __main__ - Step 30 Global step 30 Train loss 5.69 on epoch=2
03/17/2022 22:02:03 - INFO - __main__ - Step 40 Global step 40 Train loss 4.66 on epoch=2
03/17/2022 22:02:05 - INFO - __main__ - Step 50 Global step 50 Train loss 3.97 on epoch=3
03/17/2022 22:03:01 - INFO - __main__ - Global step 50 Train loss 5.55 Classification-F1 0.0037841978796473797 on epoch=3
03/17/2022 22:03:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0037841978796473797 on epoch=3, global_step=50
03/17/2022 22:03:03 - INFO - __main__ - Step 60 Global step 60 Train loss 3.29 on epoch=4
03/17/2022 22:03:06 - INFO - __main__ - Step 70 Global step 70 Train loss 2.73 on epoch=4
03/17/2022 22:03:08 - INFO - __main__ - Step 80 Global step 80 Train loss 2.47 on epoch=5
03/17/2022 22:03:11 - INFO - __main__ - Step 90 Global step 90 Train loss 2.15 on epoch=6
03/17/2022 22:03:13 - INFO - __main__ - Step 100 Global step 100 Train loss 2.02 on epoch=7
03/17/2022 22:03:19 - INFO - __main__ - Global step 100 Train loss 2.53 Classification-F1 0.25290197637675327 on epoch=7
03/17/2022 22:03:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0037841978796473797 -> 0.25290197637675327 on epoch=7, global_step=100
03/17/2022 22:03:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.75 on epoch=7
03/17/2022 22:03:24 - INFO - __main__ - Step 120 Global step 120 Train loss 1.72 on epoch=8
03/17/2022 22:03:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.62 on epoch=9
03/17/2022 22:03:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.37 on epoch=9
03/17/2022 22:03:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.39 on epoch=10
03/17/2022 22:03:40 - INFO - __main__ - Global step 150 Train loss 1.57 Classification-F1 0.4313060209005931 on epoch=10
03/17/2022 22:03:40 - INFO - __main__ - Saving model with best Classification-F1: 0.25290197637675327 -> 0.4313060209005931 on epoch=10, global_step=150
03/17/2022 22:03:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.33 on epoch=11
03/17/2022 22:03:45 - INFO - __main__ - Step 170 Global step 170 Train loss 1.25 on epoch=12
03/17/2022 22:03:48 - INFO - __main__ - Step 180 Global step 180 Train loss 1.10 on epoch=12
03/17/2022 22:03:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=13
03/17/2022 22:03:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.01 on epoch=14
03/17/2022 22:04:01 - INFO - __main__ - Global step 200 Train loss 1.14 Classification-F1 0.4598925636872866 on epoch=14
03/17/2022 22:04:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4313060209005931 -> 0.4598925636872866 on epoch=14, global_step=200
03/17/2022 22:04:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=14
03/17/2022 22:04:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=15
03/17/2022 22:04:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.99 on epoch=16
03/17/2022 22:04:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.91 on epoch=17
03/17/2022 22:04:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=17
03/17/2022 22:04:21 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.6321826331652255 on epoch=17
03/17/2022 22:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4598925636872866 -> 0.6321826331652255 on epoch=17, global_step=250
03/17/2022 22:04:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=18
03/17/2022 22:04:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=19
03/17/2022 22:04:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.68 on epoch=19
03/17/2022 22:04:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=20
03/17/2022 22:04:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=21
03/17/2022 22:04:41 - INFO - __main__ - Global step 300 Train loss 0.78 Classification-F1 0.5936132178150367 on epoch=21
03/17/2022 22:04:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=22
03/17/2022 22:04:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=22
03/17/2022 22:04:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=23
03/17/2022 22:04:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=24
03/17/2022 22:04:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.66 on epoch=24
03/17/2022 22:05:03 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.5884296496047597 on epoch=24
03/17/2022 22:05:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=25
03/17/2022 22:05:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=26
03/17/2022 22:05:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=27
03/17/2022 22:05:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=27
03/17/2022 22:05:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=28
03/17/2022 22:05:24 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.6839231867401864 on epoch=28
03/17/2022 22:05:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6321826331652255 -> 0.6839231867401864 on epoch=28, global_step=400
03/17/2022 22:05:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=29
03/17/2022 22:05:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=29
03/17/2022 22:05:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.65 on epoch=30
03/17/2022 22:05:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=31
03/17/2022 22:05:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=32
03/17/2022 22:05:45 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.6473518650924054 on epoch=32
03/17/2022 22:05:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=32
03/17/2022 22:05:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=33
03/17/2022 22:05:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=34
03/17/2022 22:05:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=34
03/17/2022 22:05:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=35
03/17/2022 22:06:06 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.6677957340093787 on epoch=35
03/17/2022 22:06:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=36
03/17/2022 22:06:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=37
03/17/2022 22:06:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=37
03/17/2022 22:06:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=38
03/17/2022 22:06:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=39
03/17/2022 22:06:26 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.7229992805025167 on epoch=39
03/17/2022 22:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6839231867401864 -> 0.7229992805025167 on epoch=39, global_step=550
03/17/2022 22:06:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=39
03/17/2022 22:06:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=40
03/17/2022 22:06:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=41
03/17/2022 22:06:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=42
03/17/2022 22:06:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=42
03/17/2022 22:06:47 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.7061970836825924 on epoch=42
03/17/2022 22:06:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=43
03/17/2022 22:06:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=44
03/17/2022 22:06:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=44
03/17/2022 22:06:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=45
03/17/2022 22:07:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=46
03/17/2022 22:07:06 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.7634657098144919 on epoch=46
03/17/2022 22:07:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7229992805025167 -> 0.7634657098144919 on epoch=46, global_step=650
03/17/2022 22:07:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=47
03/17/2022 22:07:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=47
03/17/2022 22:07:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=48
03/17/2022 22:07:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=49
03/17/2022 22:07:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=49
03/17/2022 22:07:25 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.733398444578493 on epoch=49
03/17/2022 22:07:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=50
03/17/2022 22:07:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=51
03/17/2022 22:07:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=52
03/17/2022 22:07:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=52
03/17/2022 22:07:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=53
03/17/2022 22:07:44 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6351666169120838 on epoch=53
03/17/2022 22:07:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=54
03/17/2022 22:07:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=54
03/17/2022 22:07:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=55
03/17/2022 22:07:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=56
03/17/2022 22:07:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=57
03/17/2022 22:08:04 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.8102178572673568 on epoch=57
03/17/2022 22:08:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7634657098144919 -> 0.8102178572673568 on epoch=57, global_step=800
03/17/2022 22:08:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=57
03/17/2022 22:08:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=58
03/17/2022 22:08:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=59
03/17/2022 22:08:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=59
03/17/2022 22:08:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=60
03/17/2022 22:08:23 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.7519146630018902 on epoch=60
03/17/2022 22:08:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=61
03/17/2022 22:08:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=62
03/17/2022 22:08:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=62
03/17/2022 22:08:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=63
03/17/2022 22:08:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=64
03/17/2022 22:08:42 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.8103944730958079 on epoch=64
03/17/2022 22:08:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8102178572673568 -> 0.8103944730958079 on epoch=64, global_step=900
03/17/2022 22:08:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=64
03/17/2022 22:08:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=65
03/17/2022 22:08:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=66
03/17/2022 22:08:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=67
03/17/2022 22:08:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=67
03/17/2022 22:09:01 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.8160442378706136 on epoch=67
03/17/2022 22:09:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8103944730958079 -> 0.8160442378706136 on epoch=67, global_step=950
03/17/2022 22:09:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=68
03/17/2022 22:09:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=69
03/17/2022 22:09:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=69
03/17/2022 22:09:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=70
03/17/2022 22:09:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=71
03/17/2022 22:09:21 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.6615310526071705 on epoch=71
03/17/2022 22:09:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=72
03/17/2022 22:09:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=72
03/17/2022 22:09:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=73
03/17/2022 22:09:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=74
03/17/2022 22:09:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=74
03/17/2022 22:09:40 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.6809067923853426 on epoch=74
03/17/2022 22:09:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=75
03/17/2022 22:09:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=76
03/17/2022 22:09:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=77
03/17/2022 22:09:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=77
03/17/2022 22:09:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=78
03/17/2022 22:10:00 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.7455294391936316 on epoch=78
03/17/2022 22:10:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=79
03/17/2022 22:10:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=79
03/17/2022 22:10:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=80
03/17/2022 22:10:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=81
03/17/2022 22:10:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=82
03/17/2022 22:10:19 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.782736652906889 on epoch=82
03/17/2022 22:10:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=82
03/17/2022 22:10:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=83
03/17/2022 22:10:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=84
03/17/2022 22:10:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=84
03/17/2022 22:10:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=85
03/17/2022 22:10:39 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7618579494006439 on epoch=85
03/17/2022 22:10:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=86
03/17/2022 22:10:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=87
03/17/2022 22:10:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=87
03/17/2022 22:10:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=88
03/17/2022 22:10:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=89
03/17/2022 22:10:58 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.6631304574615771 on epoch=89
03/17/2022 22:11:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=89
03/17/2022 22:11:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=90
03/17/2022 22:11:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=91
03/17/2022 22:11:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=92
03/17/2022 22:11:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=92
03/17/2022 22:11:18 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.6954486993249103 on epoch=92
03/17/2022 22:11:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=93
03/17/2022 22:11:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=94
03/17/2022 22:11:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=94
03/17/2022 22:11:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=95
03/17/2022 22:11:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=96
03/17/2022 22:11:38 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.7835816711152052 on epoch=96
03/17/2022 22:11:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=97
03/17/2022 22:11:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=97
03/17/2022 22:11:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=98
03/17/2022 22:11:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=99
03/17/2022 22:11:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=99
03/17/2022 22:11:57 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7381443315602185 on epoch=99
03/17/2022 22:12:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=100
03/17/2022 22:12:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=101
03/17/2022 22:12:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=102
03/17/2022 22:12:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=102
03/17/2022 22:12:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=103
03/17/2022 22:12:16 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.7162042640089236 on epoch=103
03/17/2022 22:12:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=104
03/17/2022 22:12:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=104
03/17/2022 22:12:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=105
03/17/2022 22:12:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=106
03/17/2022 22:12:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=107
03/17/2022 22:12:36 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.7365160026213157 on epoch=107
03/17/2022 22:12:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=107
03/17/2022 22:12:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=108
03/17/2022 22:12:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=109
03/17/2022 22:12:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=109
03/17/2022 22:12:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=110
03/17/2022 22:12:55 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.7685066355939221 on epoch=110
03/17/2022 22:12:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=111
03/17/2022 22:13:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=112
03/17/2022 22:13:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=112
03/17/2022 22:13:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=113
03/17/2022 22:13:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=114
03/17/2022 22:13:14 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7019416209144562 on epoch=114
03/17/2022 22:13:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=114
03/17/2022 22:13:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=115
03/17/2022 22:13:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=116
03/17/2022 22:13:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=117
03/17/2022 22:13:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=117
03/17/2022 22:13:34 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7834891785277835 on epoch=117
03/17/2022 22:13:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=118
03/17/2022 22:13:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=119
03/17/2022 22:13:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=119
03/17/2022 22:13:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=120
03/17/2022 22:13:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=121
03/17/2022 22:13:53 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.6893667646626301 on epoch=121
03/17/2022 22:13:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=122
03/17/2022 22:13:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=122
03/17/2022 22:14:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=123
03/17/2022 22:14:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=124
03/17/2022 22:14:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=124
03/17/2022 22:14:13 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.7328647308486018 on epoch=124
03/17/2022 22:14:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=125
03/17/2022 22:14:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=126
03/17/2022 22:14:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=127
03/17/2022 22:14:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=127
03/17/2022 22:14:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=128
03/17/2022 22:14:32 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.703733231215005 on epoch=128
03/17/2022 22:14:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=129
03/17/2022 22:14:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=129
03/17/2022 22:14:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=130
03/17/2022 22:14:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=131
03/17/2022 22:14:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=132
03/17/2022 22:14:52 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.7794355621829728 on epoch=132
03/17/2022 22:14:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=132
03/17/2022 22:14:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=133
03/17/2022 22:14:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=134
03/17/2022 22:15:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=134
03/17/2022 22:15:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=135
03/17/2022 22:15:11 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.8135512755595368 on epoch=135
03/17/2022 22:15:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=136
03/17/2022 22:15:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=137
03/17/2022 22:15:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=137
03/17/2022 22:15:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=138
03/17/2022 22:15:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=139
03/17/2022 22:15:31 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.7444765107716227 on epoch=139
03/17/2022 22:15:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=139
03/17/2022 22:15:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=140
03/17/2022 22:15:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=141
03/17/2022 22:15:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=142
03/17/2022 22:15:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=142
03/17/2022 22:15:50 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.8710700045009409 on epoch=142
03/17/2022 22:15:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8160442378706136 -> 0.8710700045009409 on epoch=142, global_step=2000
03/17/2022 22:15:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=143
03/17/2022 22:15:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=144
03/17/2022 22:15:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=144
03/17/2022 22:16:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=145
03/17/2022 22:16:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
03/17/2022 22:16:09 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6548501912899856 on epoch=146
03/17/2022 22:16:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=147
03/17/2022 22:16:14 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
03/17/2022 22:16:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=148
03/17/2022 22:16:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
03/17/2022 22:16:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
03/17/2022 22:16:28 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7900553717065527 on epoch=149
03/17/2022 22:16:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=150
03/17/2022 22:16:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
03/17/2022 22:16:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=152
03/17/2022 22:16:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=152
03/17/2022 22:16:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=153
03/17/2022 22:16:47 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.8924834141880861 on epoch=153
03/17/2022 22:16:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8710700045009409 -> 0.8924834141880861 on epoch=153, global_step=2150
03/17/2022 22:16:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=154
03/17/2022 22:16:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
03/17/2022 22:16:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=155
03/17/2022 22:16:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=156
03/17/2022 22:17:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=157
03/17/2022 22:17:06 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.8416107649071358 on epoch=157
03/17/2022 22:17:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=157
03/17/2022 22:17:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=158
03/17/2022 22:17:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=159
03/17/2022 22:17:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=159
03/17/2022 22:17:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=160
03/17/2022 22:17:25 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.8303821178821179 on epoch=160
03/17/2022 22:17:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=161
03/17/2022 22:17:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=162
03/17/2022 22:17:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=162
03/17/2022 22:17:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/17/2022 22:17:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=164
03/17/2022 22:17:43 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.8227014941077442 on epoch=164
03/17/2022 22:17:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
03/17/2022 22:17:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=165
03/17/2022 22:17:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
03/17/2022 22:17:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=167
03/17/2022 22:17:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/17/2022 22:18:03 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.9055912293120301 on epoch=167
03/17/2022 22:18:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8924834141880861 -> 0.9055912293120301 on epoch=167, global_step=2350
03/17/2022 22:18:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=168
03/17/2022 22:18:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=169
03/17/2022 22:18:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=169
03/17/2022 22:18:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
03/17/2022 22:18:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=171
03/17/2022 22:18:22 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.851152401026393 on epoch=171
03/17/2022 22:18:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
03/17/2022 22:18:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=172
03/17/2022 22:18:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=173
03/17/2022 22:18:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=174
03/17/2022 22:18:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=174
03/17/2022 22:18:41 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.9020688862055085 on epoch=174
03/17/2022 22:18:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
03/17/2022 22:18:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=176
03/17/2022 22:18:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=177
03/17/2022 22:18:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=177
03/17/2022 22:18:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=178
03/17/2022 22:18:59 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7989181517407324 on epoch=178
03/17/2022 22:19:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=179
03/17/2022 22:19:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
03/17/2022 22:19:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=180
03/17/2022 22:19:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=181
03/17/2022 22:19:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=182
03/17/2022 22:19:17 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.8890622933886689 on epoch=182
03/17/2022 22:19:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=182
03/17/2022 22:19:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=183
03/17/2022 22:19:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=184
03/17/2022 22:19:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/17/2022 22:19:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=185
03/17/2022 22:19:36 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.977311719257427 on epoch=185
03/17/2022 22:19:36 - INFO - __main__ - Saving model with best Classification-F1: 0.9055912293120301 -> 0.977311719257427 on epoch=185, global_step=2600
03/17/2022 22:19:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=186
03/17/2022 22:19:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=187
03/17/2022 22:19:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=187
03/17/2022 22:19:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=188
03/17/2022 22:19:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=189
03/17/2022 22:19:54 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.8937965806068272 on epoch=189
03/17/2022 22:19:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
03/17/2022 22:19:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
03/17/2022 22:20:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
03/17/2022 22:20:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=192
03/17/2022 22:20:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=192
03/17/2022 22:20:13 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8067250831494814 on epoch=192
03/17/2022 22:20:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/17/2022 22:20:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=194
03/17/2022 22:20:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/17/2022 22:20:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=195
03/17/2022 22:20:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=196
03/17/2022 22:20:31 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6721280517145892 on epoch=196
03/17/2022 22:20:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=197
03/17/2022 22:20:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=197
03/17/2022 22:20:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
03/17/2022 22:20:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=199
03/17/2022 22:20:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/17/2022 22:20:50 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8270558406647117 on epoch=199
03/17/2022 22:20:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
03/17/2022 22:20:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=201
03/17/2022 22:20:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
03/17/2022 22:21:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=202
03/17/2022 22:21:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=203
03/17/2022 22:21:08 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.891856131721723 on epoch=203
03/17/2022 22:21:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/17/2022 22:21:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=204
03/17/2022 22:21:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=205
03/17/2022 22:21:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
03/17/2022 22:21:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
03/17/2022 22:21:27 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7744800000911782 on epoch=207
03/17/2022 22:21:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
03/17/2022 22:21:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=208
03/17/2022 22:21:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=209
03/17/2022 22:21:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=209
03/17/2022 22:21:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=210
03/17/2022 22:21:45 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7725860606972388 on epoch=210
03/17/2022 22:21:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
03/17/2022 22:21:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=212
03/17/2022 22:21:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
03/17/2022 22:21:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/17/2022 22:21:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=214
03/17/2022 22:21:59 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:21:59 - INFO - __main__ - Printing 3 examples
03/17/2022 22:21:59 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 22:21:59 - INFO - __main__ - ['Animal']
03/17/2022 22:21:59 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 22:21:59 - INFO - __main__ - ['Animal']
03/17/2022 22:21:59 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 22:21:59 - INFO - __main__ - ['Animal']
03/17/2022 22:21:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 22:21:59 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:22:00 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 22:22:00 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:22:00 - INFO - __main__ - Printing 3 examples
03/17/2022 22:22:00 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 22:22:00 - INFO - __main__ - ['Animal']
03/17/2022 22:22:00 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 22:22:00 - INFO - __main__ - ['Animal']
03/17/2022 22:22:00 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 22:22:00 - INFO - __main__ - ['Animal']
03/17/2022 22:22:00 - INFO - __main__ - Tokenizing Input ...
03/17/2022 22:22:00 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:22:00 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 22:22:04 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8987515915457092 on epoch=214
03/17/2022 22:22:04 - INFO - __main__ - save last model!
03/17/2022 22:22:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 22:22:04 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 22:22:04 - INFO - __main__ - Printing 3 examples
03/17/2022 22:22:04 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 22:22:04 - INFO - __main__ - ['Animal']
03/17/2022 22:22:04 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 22:22:04 - INFO - __main__ - ['Animal']
03/17/2022 22:22:04 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 22:22:04 - INFO - __main__ - ['Village']
03/17/2022 22:22:04 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 22:22:06 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:22:09 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 22:22:18 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 22:22:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 22:22:19 - INFO - __main__ - Starting training!
03/17/2022 22:24:19 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_100_0.2_8_predictions.txt
03/17/2022 22:24:19 - INFO - __main__ - Classification-F1 on test data: 0.6464
03/17/2022 22:24:20 - INFO - __main__ - prefix=dbpedia_14_16_100, lr=0.2, bsz=8, dev_performance=0.977311719257427, test_performance=0.6463601241248897
03/17/2022 22:24:20 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.5, bsz=8 ...
03/17/2022 22:24:21 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:24:21 - INFO - __main__ - Printing 3 examples
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 22:24:21 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:24:21 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 22:24:21 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:24:21 - INFO - __main__ - Printing 3 examples
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 22:24:21 - INFO - __main__ - ['Animal']
03/17/2022 22:24:21 - INFO - __main__ - Tokenizing Input ...
03/17/2022 22:24:21 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:24:22 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 22:24:36 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 22:24:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 22:24:37 - INFO - __main__ - Starting training!
03/17/2022 22:24:41 - INFO - __main__ - Step 10 Global step 10 Train loss 6.57 on epoch=0
03/17/2022 22:24:43 - INFO - __main__ - Step 20 Global step 20 Train loss 4.90 on epoch=1
03/17/2022 22:24:46 - INFO - __main__ - Step 30 Global step 30 Train loss 3.16 on epoch=2
03/17/2022 22:24:49 - INFO - __main__ - Step 40 Global step 40 Train loss 2.38 on epoch=2
03/17/2022 22:24:51 - INFO - __main__ - Step 50 Global step 50 Train loss 2.00 on epoch=3
03/17/2022 22:25:00 - INFO - __main__ - Global step 50 Train loss 3.80 Classification-F1 0.2899445380563462 on epoch=3
03/17/2022 22:25:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2899445380563462 on epoch=3, global_step=50
03/17/2022 22:25:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=4
03/17/2022 22:25:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.45 on epoch=4
03/17/2022 22:25:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=5
03/17/2022 22:25:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=6
03/17/2022 22:25:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.17 on epoch=7
03/17/2022 22:25:22 - INFO - __main__ - Global step 100 Train loss 1.29 Classification-F1 0.3923981411947056 on epoch=7
03/17/2022 22:25:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2899445380563462 -> 0.3923981411947056 on epoch=7, global_step=100
03/17/2022 22:25:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=7
03/17/2022 22:25:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.82 on epoch=8
03/17/2022 22:25:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=9
03/17/2022 22:25:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.69 on epoch=9
03/17/2022 22:25:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=10
03/17/2022 22:25:44 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.5136296263069349 on epoch=10
03/17/2022 22:25:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3923981411947056 -> 0.5136296263069349 on epoch=10, global_step=150
03/17/2022 22:25:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=11
03/17/2022 22:25:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=12
03/17/2022 22:25:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=12
03/17/2022 22:25:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.67 on epoch=13
03/17/2022 22:25:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=14
03/17/2022 22:26:05 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5962512308469963 on epoch=14
03/17/2022 22:26:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5136296263069349 -> 0.5962512308469963 on epoch=14, global_step=200
03/17/2022 22:26:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=14
03/17/2022 22:26:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=15
03/17/2022 22:26:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=16
03/17/2022 22:26:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=17
03/17/2022 22:26:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=17
03/17/2022 22:26:26 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.4744548184895855 on epoch=17
03/17/2022 22:26:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=18
03/17/2022 22:26:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=19
03/17/2022 22:26:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=19
03/17/2022 22:26:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=20
03/17/2022 22:26:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=21
03/17/2022 22:26:45 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.6286894272408929 on epoch=21
03/17/2022 22:26:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5962512308469963 -> 0.6286894272408929 on epoch=21, global_step=300
03/17/2022 22:26:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=22
03/17/2022 22:26:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=22
03/17/2022 22:26:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=23
03/17/2022 22:26:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=24
03/17/2022 22:26:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=24
03/17/2022 22:27:04 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.5702774727001373 on epoch=24
03/17/2022 22:27:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=25
03/17/2022 22:27:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=26
03/17/2022 22:27:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=27
03/17/2022 22:27:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=27
03/17/2022 22:27:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=28
03/17/2022 22:27:24 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.7586130881027074 on epoch=28
03/17/2022 22:27:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6286894272408929 -> 0.7586130881027074 on epoch=28, global_step=400
03/17/2022 22:27:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=29
03/17/2022 22:27:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=29
03/17/2022 22:27:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=30
03/17/2022 22:27:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=31
03/17/2022 22:27:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=32
03/17/2022 22:27:43 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.7425996894365797 on epoch=32
03/17/2022 22:27:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=32
03/17/2022 22:27:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=33
03/17/2022 22:27:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=34
03/17/2022 22:27:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=34
03/17/2022 22:27:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=35
03/17/2022 22:28:03 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.6692001768572959 on epoch=35
03/17/2022 22:28:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=36
03/17/2022 22:28:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=37
03/17/2022 22:28:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=37
03/17/2022 22:28:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=38
03/17/2022 22:28:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=39
03/17/2022 22:28:22 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.7492384443691356 on epoch=39
03/17/2022 22:28:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=39
03/17/2022 22:28:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=40
03/17/2022 22:28:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=41
03/17/2022 22:28:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=42
03/17/2022 22:28:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=42
03/17/2022 22:28:42 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.7707892736663169 on epoch=42
03/17/2022 22:28:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7586130881027074 -> 0.7707892736663169 on epoch=42, global_step=600
03/17/2022 22:28:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=43
03/17/2022 22:28:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=44
03/17/2022 22:28:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=44
03/17/2022 22:28:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=45
03/17/2022 22:28:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=46
03/17/2022 22:29:02 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.6669146408123084 on epoch=46
03/17/2022 22:29:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=47
03/17/2022 22:29:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=47
03/17/2022 22:29:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=48
03/17/2022 22:29:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=49
03/17/2022 22:29:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=49
03/17/2022 22:29:21 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.6792960709981083 on epoch=49
03/17/2022 22:29:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=50
03/17/2022 22:29:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.13 on epoch=51
03/17/2022 22:29:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=52
03/17/2022 22:29:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=52
03/17/2022 22:29:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=53
03/17/2022 22:29:41 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.9559074195121136 on epoch=53
03/17/2022 22:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7707892736663169 -> 0.9559074195121136 on epoch=53, global_step=750
03/17/2022 22:29:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=54
03/17/2022 22:29:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=54
03/17/2022 22:29:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=55
03/17/2022 22:29:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=56
03/17/2022 22:29:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=57
03/17/2022 22:30:01 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.8890797674447808 on epoch=57
03/17/2022 22:30:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=57
03/17/2022 22:30:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=58
03/17/2022 22:30:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=59
03/17/2022 22:30:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=59
03/17/2022 22:30:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=60
03/17/2022 22:30:20 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.7895455361621151 on epoch=60
03/17/2022 22:30:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=61
03/17/2022 22:30:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=62
03/17/2022 22:30:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=62
03/17/2022 22:30:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=63
03/17/2022 22:30:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=64
03/17/2022 22:30:39 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.8029900523259158 on epoch=64
03/17/2022 22:30:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=64
03/17/2022 22:30:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=65
03/17/2022 22:30:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=66
03/17/2022 22:30:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=67
03/17/2022 22:30:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=67
03/17/2022 22:30:58 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.8208011643959567 on epoch=67
03/17/2022 22:31:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=68
03/17/2022 22:31:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=69
03/17/2022 22:31:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=69
03/17/2022 22:31:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=70
03/17/2022 22:31:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=71
03/17/2022 22:31:17 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.897929357134029 on epoch=71
03/17/2022 22:31:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=72
03/17/2022 22:31:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=72
03/17/2022 22:31:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=73
03/17/2022 22:31:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=74
03/17/2022 22:31:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=74
03/17/2022 22:31:36 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.9640752708708116 on epoch=74
03/17/2022 22:31:36 - INFO - __main__ - Saving model with best Classification-F1: 0.9559074195121136 -> 0.9640752708708116 on epoch=74, global_step=1050
03/17/2022 22:31:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=75
03/17/2022 22:31:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=76
03/17/2022 22:31:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=77
03/17/2022 22:31:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=77
03/17/2022 22:31:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=78
03/17/2022 22:31:55 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.9640585853109572 on epoch=78
03/17/2022 22:31:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=79
03/17/2022 22:32:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=79
03/17/2022 22:32:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=80
03/17/2022 22:32:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=81
03/17/2022 22:32:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=82
03/17/2022 22:32:13 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.9730082062150373 on epoch=82
03/17/2022 22:32:13 - INFO - __main__ - Saving model with best Classification-F1: 0.9640752708708116 -> 0.9730082062150373 on epoch=82, global_step=1150
03/17/2022 22:32:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=82
03/17/2022 22:32:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=83
03/17/2022 22:32:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=84
03/17/2022 22:32:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=84
03/17/2022 22:32:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=85
03/17/2022 22:32:32 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.8408390998539672 on epoch=85
03/17/2022 22:32:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=86
03/17/2022 22:32:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=87
03/17/2022 22:32:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=87
03/17/2022 22:32:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=88
03/17/2022 22:32:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
03/17/2022 22:32:52 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.8977864668187249 on epoch=89
03/17/2022 22:32:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=89
03/17/2022 22:32:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=90
03/17/2022 22:33:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=91
03/17/2022 22:33:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=92
03/17/2022 22:33:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=92
03/17/2022 22:33:10 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.8446678321678323 on epoch=92
03/17/2022 22:33:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=93
03/17/2022 22:33:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=94
03/17/2022 22:33:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=94
03/17/2022 22:33:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=95
03/17/2022 22:33:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=96
03/17/2022 22:33:28 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.9101898012381883 on epoch=96
03/17/2022 22:33:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=97
03/17/2022 22:33:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=97
03/17/2022 22:33:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
03/17/2022 22:33:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
03/17/2022 22:33:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=99
03/17/2022 22:33:47 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.8469488296190657 on epoch=99
03/17/2022 22:33:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=100
03/17/2022 22:33:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=101
03/17/2022 22:33:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=102
03/17/2022 22:33:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
03/17/2022 22:34:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=103
03/17/2022 22:34:06 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.855058651026393 on epoch=103
03/17/2022 22:34:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=104
03/17/2022 22:34:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
03/17/2022 22:34:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=105
03/17/2022 22:34:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=106
03/17/2022 22:34:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=107
03/17/2022 22:34:24 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.9773495321882416 on epoch=107
03/17/2022 22:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.9730082062150373 -> 0.9773495321882416 on epoch=107, global_step=1500
03/17/2022 22:34:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
03/17/2022 22:34:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=108
03/17/2022 22:34:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=109
03/17/2022 22:34:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=109
03/17/2022 22:34:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
03/17/2022 22:34:44 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.8422822956667753 on epoch=110
03/17/2022 22:34:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=111
03/17/2022 22:34:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=112
03/17/2022 22:34:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=112
03/17/2022 22:34:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
03/17/2022 22:34:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=114
03/17/2022 22:35:03 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.855058651026393 on epoch=114
03/17/2022 22:35:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=114
03/17/2022 22:35:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
03/17/2022 22:35:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=116
03/17/2022 22:35:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=117
03/17/2022 22:35:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=117
03/17/2022 22:35:22 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.8433473991275142 on epoch=117
03/17/2022 22:35:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=118
03/17/2022 22:35:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
03/17/2022 22:35:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=119
03/17/2022 22:35:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=120
03/17/2022 22:35:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=121
03/17/2022 22:35:41 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.9726850958808034 on epoch=121
03/17/2022 22:35:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=122
03/17/2022 22:35:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=122
03/17/2022 22:35:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
03/17/2022 22:35:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=124
03/17/2022 22:35:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
03/17/2022 22:35:59 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.9100423590746171 on epoch=124
03/17/2022 22:36:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=125
03/17/2022 22:36:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=126
03/17/2022 22:36:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=127
03/17/2022 22:36:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=127
03/17/2022 22:36:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=128
03/17/2022 22:36:18 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.9771537455107434 on epoch=128
03/17/2022 22:36:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=129
03/17/2022 22:36:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=129
03/17/2022 22:36:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=130
03/17/2022 22:36:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
03/17/2022 22:36:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=132
03/17/2022 22:36:37 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.8436067665906376 on epoch=132
03/17/2022 22:36:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=132
03/17/2022 22:36:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=133
03/17/2022 22:36:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
03/17/2022 22:36:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=134
03/17/2022 22:36:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=135
03/17/2022 22:36:55 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7960814838745334 on epoch=135
03/17/2022 22:36:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=136
03/17/2022 22:37:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
03/17/2022 22:37:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=137
03/17/2022 22:37:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=138
03/17/2022 22:37:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=139
03/17/2022 22:37:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7286071157038899 on epoch=139
03/17/2022 22:37:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
03/17/2022 22:37:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=140
03/17/2022 22:37:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
03/17/2022 22:37:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=142
03/17/2022 22:37:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=142
03/17/2022 22:37:34 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.9101611944875702 on epoch=142
03/17/2022 22:37:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=143
03/17/2022 22:37:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
03/17/2022 22:37:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=144
03/17/2022 22:37:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
03/17/2022 22:37:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=146
03/17/2022 22:37:53 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7901313914192507 on epoch=146
03/17/2022 22:37:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=147
03/17/2022 22:37:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
03/17/2022 22:38:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/17/2022 22:38:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
03/17/2022 22:38:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=149
03/17/2022 22:38:11 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8591069464809384 on epoch=149
03/17/2022 22:38:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=150
03/17/2022 22:38:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=151
03/17/2022 22:38:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
03/17/2022 22:38:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
03/17/2022 22:38:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=153
03/17/2022 22:38:30 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8472574675537117 on epoch=153
03/17/2022 22:38:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=154
03/17/2022 22:38:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=154
03/17/2022 22:38:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=155
03/17/2022 22:38:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
03/17/2022 22:38:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=157
03/17/2022 22:38:49 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.806543614513254 on epoch=157
03/17/2022 22:38:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=157
03/17/2022 22:38:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
03/17/2022 22:38:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/17/2022 22:38:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
03/17/2022 22:39:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=160
03/17/2022 22:39:08 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.8029684048882622 on epoch=160
03/17/2022 22:39:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/17/2022 22:39:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=162
03/17/2022 22:39:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=162
03/17/2022 22:39:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/17/2022 22:39:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=164
03/17/2022 22:39:27 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.8509772692573845 on epoch=164
03/17/2022 22:39:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
03/17/2022 22:39:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=165
03/17/2022 22:39:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=166
03/17/2022 22:39:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
03/17/2022 22:39:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/17/2022 22:39:45 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.899347147123925 on epoch=167
03/17/2022 22:39:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=168
03/17/2022 22:39:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/17/2022 22:39:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=169
03/17/2022 22:39:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=170
03/17/2022 22:39:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=171
03/17/2022 22:40:04 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.9771537455107434 on epoch=171
03/17/2022 22:40:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
03/17/2022 22:40:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
03/17/2022 22:40:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
03/17/2022 22:40:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=174
03/17/2022 22:40:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
03/17/2022 22:40:23 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8553082862543845 on epoch=174
03/17/2022 22:40:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=175
03/17/2022 22:40:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/17/2022 22:40:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=177
03/17/2022 22:40:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/17/2022 22:40:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=178
03/17/2022 22:40:41 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.8470238697157496 on epoch=178
03/17/2022 22:40:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=179
03/17/2022 22:40:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/17/2022 22:40:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/17/2022 22:40:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=181
03/17/2022 22:40:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=182
03/17/2022 22:41:00 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8508428606552338 on epoch=182
03/17/2022 22:41:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=182
03/17/2022 22:41:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=183
03/17/2022 22:41:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=184
03/17/2022 22:41:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/17/2022 22:41:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
03/17/2022 22:41:18 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.8996270306754177 on epoch=185
03/17/2022 22:41:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=186
03/17/2022 22:41:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=187
03/17/2022 22:41:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/17/2022 22:41:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
03/17/2022 22:41:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=189
03/17/2022 22:41:36 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.9101611944875702 on epoch=189
03/17/2022 22:41:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=189
03/17/2022 22:41:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=190
03/17/2022 22:41:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=191
03/17/2022 22:41:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=192
03/17/2022 22:41:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/17/2022 22:41:54 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.8411968614931057 on epoch=192
03/17/2022 22:41:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=193
03/17/2022 22:41:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/17/2022 22:42:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=194
03/17/2022 22:42:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
03/17/2022 22:42:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/17/2022 22:42:13 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.9103086366511415 on epoch=196
03/17/2022 22:42:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=197
03/17/2022 22:42:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=197
03/17/2022 22:42:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/17/2022 22:42:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
03/17/2022 22:42:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
03/17/2022 22:42:31 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9037177275602324 on epoch=199
03/17/2022 22:42:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
03/17/2022 22:42:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
03/17/2022 22:42:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=202
03/17/2022 22:42:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
03/17/2022 22:42:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
03/17/2022 22:42:49 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7967321233395418 on epoch=203
03/17/2022 22:42:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
03/17/2022 22:42:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=204
03/17/2022 22:42:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
03/17/2022 22:42:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/17/2022 22:43:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
03/17/2022 22:43:07 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8994402234551421 on epoch=207
03/17/2022 22:43:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
03/17/2022 22:43:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=208
03/17/2022 22:43:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=209
03/17/2022 22:43:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/17/2022 22:43:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
03/17/2022 22:43:26 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.9776348295916607 on epoch=210
03/17/2022 22:43:26 - INFO - __main__ - Saving model with best Classification-F1: 0.9773495321882416 -> 0.9776348295916607 on epoch=210, global_step=2950
03/17/2022 22:43:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=211
03/17/2022 22:43:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
03/17/2022 22:43:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
03/17/2022 22:43:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
03/17/2022 22:43:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=214
03/17/2022 22:43:40 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:43:40 - INFO - __main__ - Printing 3 examples
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 22:43:40 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:43:40 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 22:43:40 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:43:40 - INFO - __main__ - Printing 3 examples
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 22:43:40 - INFO - __main__ - ['Animal']
03/17/2022 22:43:40 - INFO - __main__ - Tokenizing Input ...
03/17/2022 22:43:40 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:43:40 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 22:43:44 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9730082062150373 on epoch=214
03/17/2022 22:43:44 - INFO - __main__ - save last model!
03/17/2022 22:43:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 22:43:44 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 22:43:44 - INFO - __main__ - Printing 3 examples
03/17/2022 22:43:44 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 22:43:44 - INFO - __main__ - ['Animal']
03/17/2022 22:43:44 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 22:43:44 - INFO - __main__ - ['Animal']
03/17/2022 22:43:44 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 22:43:44 - INFO - __main__ - ['Village']
03/17/2022 22:43:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 22:43:46 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:43:49 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 22:43:59 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 22:44:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 22:44:00 - INFO - __main__ - Starting training!
03/17/2022 22:45:55 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_13_0.5_8_predictions.txt
03/17/2022 22:45:55 - INFO - __main__ - Classification-F1 on test data: 0.7235
03/17/2022 22:45:55 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.5, bsz=8, dev_performance=0.9776348295916607, test_performance=0.7235177729800423
03/17/2022 22:45:55 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.4, bsz=8 ...
03/17/2022 22:45:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:45:56 - INFO - __main__ - Printing 3 examples
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 22:45:56 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:45:56 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 22:45:56 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 22:45:56 - INFO - __main__ - Printing 3 examples
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 22:45:56 - INFO - __main__ - ['Animal']
03/17/2022 22:45:56 - INFO - __main__ - Tokenizing Input ...
03/17/2022 22:45:56 - INFO - __main__ - Tokenizing Output ...
03/17/2022 22:45:56 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 22:46:15 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 22:46:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 22:46:15 - INFO - __main__ - Starting training!
03/17/2022 22:46:23 - INFO - __main__ - Step 10 Global step 10 Train loss 6.63 on epoch=0
03/17/2022 22:46:25 - INFO - __main__ - Step 20 Global step 20 Train loss 5.97 on epoch=1
03/17/2022 22:46:28 - INFO - __main__ - Step 30 Global step 30 Train loss 5.51 on epoch=2
03/17/2022 22:46:31 - INFO - __main__ - Step 40 Global step 40 Train loss 5.01 on epoch=2
03/17/2022 22:46:33 - INFO - __main__ - Step 50 Global step 50 Train loss 4.74 on epoch=3
03/17/2022 22:48:14 - INFO - __main__ - Global step 50 Train loss 5.57 Classification-F1 0.0 on epoch=3
03/17/2022 22:48:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/17/2022 22:48:17 - INFO - __main__ - Step 60 Global step 60 Train loss 4.52 on epoch=4
03/17/2022 22:48:19 - INFO - __main__ - Step 70 Global step 70 Train loss 4.40 on epoch=4
03/17/2022 22:48:22 - INFO - __main__ - Step 80 Global step 80 Train loss 3.70 on epoch=5
03/17/2022 22:48:25 - INFO - __main__ - Step 90 Global step 90 Train loss 3.59 on epoch=6
03/17/2022 22:48:27 - INFO - __main__ - Step 100 Global step 100 Train loss 3.25 on epoch=7
03/17/2022 22:48:45 - INFO - __main__ - Global step 100 Train loss 3.89 Classification-F1 0.05661363470119231 on epoch=7
03/17/2022 22:48:45 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.05661363470119231 on epoch=7, global_step=100
03/17/2022 22:48:48 - INFO - __main__ - Step 110 Global step 110 Train loss 2.84 on epoch=7
03/17/2022 22:48:50 - INFO - __main__ - Step 120 Global step 120 Train loss 2.94 on epoch=8
03/17/2022 22:48:53 - INFO - __main__ - Step 130 Global step 130 Train loss 2.54 on epoch=9
03/17/2022 22:48:55 - INFO - __main__ - Step 140 Global step 140 Train loss 2.31 on epoch=9
03/17/2022 22:48:58 - INFO - __main__ - Step 150 Global step 150 Train loss 2.12 on epoch=10
03/17/2022 22:49:06 - INFO - __main__ - Global step 150 Train loss 2.55 Classification-F1 0.2305436315305195 on epoch=10
03/17/2022 22:49:06 - INFO - __main__ - Saving model with best Classification-F1: 0.05661363470119231 -> 0.2305436315305195 on epoch=10, global_step=150
03/17/2022 22:49:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.07 on epoch=11
03/17/2022 22:49:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.84 on epoch=12
03/17/2022 22:49:13 - INFO - __main__ - Step 180 Global step 180 Train loss 1.63 on epoch=12
03/17/2022 22:49:16 - INFO - __main__ - Step 190 Global step 190 Train loss 1.61 on epoch=13
03/17/2022 22:49:18 - INFO - __main__ - Step 200 Global step 200 Train loss 1.49 on epoch=14
03/17/2022 22:49:27 - INFO - __main__ - Global step 200 Train loss 1.73 Classification-F1 0.260286002941742 on epoch=14
03/17/2022 22:49:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2305436315305195 -> 0.260286002941742 on epoch=14, global_step=200
03/17/2022 22:49:29 - INFO - __main__ - Step 210 Global step 210 Train loss 1.38 on epoch=14
03/17/2022 22:49:32 - INFO - __main__ - Step 220 Global step 220 Train loss 1.25 on epoch=15
03/17/2022 22:49:35 - INFO - __main__ - Step 230 Global step 230 Train loss 1.19 on epoch=16
03/17/2022 22:49:37 - INFO - __main__ - Step 240 Global step 240 Train loss 1.08 on epoch=17
03/17/2022 22:49:40 - INFO - __main__ - Step 250 Global step 250 Train loss 1.18 on epoch=17
03/17/2022 22:49:48 - INFO - __main__ - Global step 250 Train loss 1.22 Classification-F1 0.3855088782156265 on epoch=17
03/17/2022 22:49:48 - INFO - __main__ - Saving model with best Classification-F1: 0.260286002941742 -> 0.3855088782156265 on epoch=17, global_step=250
03/17/2022 22:49:51 - INFO - __main__ - Step 260 Global step 260 Train loss 1.08 on epoch=18
03/17/2022 22:49:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.91 on epoch=19
03/17/2022 22:49:56 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=19
03/17/2022 22:49:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=20
03/17/2022 22:50:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=21
03/17/2022 22:50:12 - INFO - __main__ - Global step 300 Train loss 0.96 Classification-F1 0.46185502275032253 on epoch=21
03/17/2022 22:50:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3855088782156265 -> 0.46185502275032253 on epoch=21, global_step=300
03/17/2022 22:50:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=22
03/17/2022 22:50:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.82 on epoch=22
03/17/2022 22:50:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=23
03/17/2022 22:50:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=24
03/17/2022 22:50:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=24
03/17/2022 22:50:33 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.46506692580048314 on epoch=24
03/17/2022 22:50:33 - INFO - __main__ - Saving model with best Classification-F1: 0.46185502275032253 -> 0.46506692580048314 on epoch=24, global_step=350
03/17/2022 22:50:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=25
03/17/2022 22:50:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.75 on epoch=26
03/17/2022 22:50:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=27
03/17/2022 22:50:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=27
03/17/2022 22:50:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=28
03/17/2022 22:50:55 - INFO - __main__ - Global step 400 Train loss 0.73 Classification-F1 0.447376716280354 on epoch=28
03/17/2022 22:50:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=29
03/17/2022 22:51:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=29
03/17/2022 22:51:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=30
03/17/2022 22:51:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=31
03/17/2022 22:51:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.59 on epoch=32
03/17/2022 22:51:18 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.5658927906513117 on epoch=32
03/17/2022 22:51:18 - INFO - __main__ - Saving model with best Classification-F1: 0.46506692580048314 -> 0.5658927906513117 on epoch=32, global_step=450
03/17/2022 22:51:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.64 on epoch=32
03/17/2022 22:51:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=33
03/17/2022 22:51:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=34
03/17/2022 22:51:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=34
03/17/2022 22:51:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=35
03/17/2022 22:51:40 - INFO - __main__ - Global step 500 Train loss 0.58 Classification-F1 0.6339540205438667 on epoch=35
03/17/2022 22:51:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5658927906513117 -> 0.6339540205438667 on epoch=35, global_step=500
03/17/2022 22:51:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=36
03/17/2022 22:51:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=37
03/17/2022 22:51:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=37
03/17/2022 22:51:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=38
03/17/2022 22:51:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=39
03/17/2022 22:52:01 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.5840662943248334 on epoch=39
03/17/2022 22:52:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=39
03/17/2022 22:52:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=40
03/17/2022 22:52:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=41
03/17/2022 22:52:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=42
03/17/2022 22:52:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=42
03/17/2022 22:52:24 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.6700541905684778 on epoch=42
03/17/2022 22:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6339540205438667 -> 0.6700541905684778 on epoch=42, global_step=600
03/17/2022 22:52:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=43
03/17/2022 22:52:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=44
03/17/2022 22:52:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=44
03/17/2022 22:52:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=45
03/17/2022 22:52:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=46
03/17/2022 22:52:45 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.777197966507177 on epoch=46
03/17/2022 22:52:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6700541905684778 -> 0.777197966507177 on epoch=46, global_step=650
03/17/2022 22:52:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=47
03/17/2022 22:52:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=47
03/17/2022 22:52:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=48
03/17/2022 22:52:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=49
03/17/2022 22:52:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=49
03/17/2022 22:53:05 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.6926722016980899 on epoch=49
03/17/2022 22:53:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=50
03/17/2022 22:53:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=51
03/17/2022 22:53:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=52
03/17/2022 22:53:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=52
03/17/2022 22:53:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=53
03/17/2022 22:53:26 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.7834441520716031 on epoch=53
03/17/2022 22:53:26 - INFO - __main__ - Saving model with best Classification-F1: 0.777197966507177 -> 0.7834441520716031 on epoch=53, global_step=750
03/17/2022 22:53:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=54
03/17/2022 22:53:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=54
03/17/2022 22:53:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=55
03/17/2022 22:53:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=56
03/17/2022 22:53:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=57
03/17/2022 22:53:45 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.6278429960438878 on epoch=57
03/17/2022 22:53:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=57
03/17/2022 22:53:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=58
03/17/2022 22:53:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=59
03/17/2022 22:53:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=59
03/17/2022 22:53:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=60
03/17/2022 22:54:06 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.6965732168686372 on epoch=60
03/17/2022 22:54:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=61
03/17/2022 22:54:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=62
03/17/2022 22:54:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=62
03/17/2022 22:54:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=63
03/17/2022 22:54:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=64
03/17/2022 22:54:26 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.6126587499342685 on epoch=64
03/17/2022 22:54:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=64
03/17/2022 22:54:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=65
03/17/2022 22:54:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=66
03/17/2022 22:54:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=67
03/17/2022 22:54:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
03/17/2022 22:54:46 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.5951737667455049 on epoch=67
03/17/2022 22:54:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=68
03/17/2022 22:54:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=69
03/17/2022 22:54:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=69
03/17/2022 22:54:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=70
03/17/2022 22:54:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=71
03/17/2022 22:55:06 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.5282929636501066 on epoch=71
03/17/2022 22:55:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=72
03/17/2022 22:55:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=72
03/17/2022 22:55:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=73
03/17/2022 22:55:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=74
03/17/2022 22:55:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=74
03/17/2022 22:55:26 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.5155723710298218 on epoch=74
03/17/2022 22:55:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=75
03/17/2022 22:55:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=76
03/17/2022 22:55:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=77
03/17/2022 22:55:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=77
03/17/2022 22:55:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=78
03/17/2022 22:55:47 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.6210112248414525 on epoch=78
03/17/2022 22:55:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
03/17/2022 22:55:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=79
03/17/2022 22:55:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=80
03/17/2022 22:55:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=81
03/17/2022 22:56:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=82
03/17/2022 22:56:07 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.5564116100631131 on epoch=82
03/17/2022 22:56:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=82
03/17/2022 22:56:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
03/17/2022 22:56:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=84
03/17/2022 22:56:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=84
03/17/2022 22:56:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=85
03/17/2022 22:56:26 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.5988155565005661 on epoch=85
03/17/2022 22:56:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=86
03/17/2022 22:56:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=87
03/17/2022 22:56:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=87
03/17/2022 22:56:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=88
03/17/2022 22:56:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=89
03/17/2022 22:56:46 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.6412686438285826 on epoch=89
03/17/2022 22:56:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=89
03/17/2022 22:56:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
03/17/2022 22:56:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
03/17/2022 22:56:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=92
03/17/2022 22:56:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=92
03/17/2022 22:57:05 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.5905358642934706 on epoch=92
03/17/2022 22:57:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=93
03/17/2022 22:57:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=94
03/17/2022 22:57:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
03/17/2022 22:57:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=95
03/17/2022 22:57:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=96
03/17/2022 22:57:25 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7212591599937017 on epoch=96
03/17/2022 22:57:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
03/17/2022 22:57:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=97
03/17/2022 22:57:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=98
03/17/2022 22:57:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=99
03/17/2022 22:57:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=99
03/17/2022 22:57:44 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7241710350581319 on epoch=99
03/17/2022 22:57:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=100
03/17/2022 22:57:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=101
03/17/2022 22:57:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=102
03/17/2022 22:57:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=102
03/17/2022 22:57:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=103
03/17/2022 22:58:03 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.8285549147356384 on epoch=103
03/17/2022 22:58:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7834441520716031 -> 0.8285549147356384 on epoch=103, global_step=1450
03/17/2022 22:58:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=104
03/17/2022 22:58:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=104
03/17/2022 22:58:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=105
03/17/2022 22:58:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=106
03/17/2022 22:58:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=107
03/17/2022 22:58:23 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7069942297736898 on epoch=107
03/17/2022 22:58:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=107
03/17/2022 22:58:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
03/17/2022 22:58:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=109
03/17/2022 22:58:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=109
03/17/2022 22:58:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=110
03/17/2022 22:58:42 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7115825815950136 on epoch=110
03/17/2022 22:58:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=111
03/17/2022 22:58:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=112
03/17/2022 22:58:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
03/17/2022 22:58:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=113
03/17/2022 22:58:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
03/17/2022 22:59:02 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7273295726740732 on epoch=114
03/17/2022 22:59:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
03/17/2022 22:59:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
03/17/2022 22:59:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=116
03/17/2022 22:59:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=117
03/17/2022 22:59:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=117
03/17/2022 22:59:21 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7813391893088287 on epoch=117
03/17/2022 22:59:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=118
03/17/2022 22:59:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=119
03/17/2022 22:59:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=119
03/17/2022 22:59:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=120
03/17/2022 22:59:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
03/17/2022 22:59:40 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7551285604557627 on epoch=121
03/17/2022 22:59:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=122
03/17/2022 22:59:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=122
03/17/2022 22:59:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=123
03/17/2022 22:59:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/17/2022 22:59:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
03/17/2022 22:59:58 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7796537768494306 on epoch=124
03/17/2022 23:00:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=125
03/17/2022 23:00:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=126
03/17/2022 23:00:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=127
03/17/2022 23:00:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=127
03/17/2022 23:00:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=128
03/17/2022 23:00:17 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.9508579611606669 on epoch=128
03/17/2022 23:00:17 - INFO - __main__ - Saving model with best Classification-F1: 0.8285549147356384 -> 0.9508579611606669 on epoch=128, global_step=1800
03/17/2022 23:00:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=129
03/17/2022 23:00:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
03/17/2022 23:00:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=130
03/17/2022 23:00:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=131
03/17/2022 23:00:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=132
03/17/2022 23:00:35 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.9552337971795046 on epoch=132
03/17/2022 23:00:35 - INFO - __main__ - Saving model with best Classification-F1: 0.9508579611606669 -> 0.9552337971795046 on epoch=132, global_step=1850
03/17/2022 23:00:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
03/17/2022 23:00:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
03/17/2022 23:00:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=134
03/17/2022 23:00:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=134
03/17/2022 23:00:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
03/17/2022 23:00:53 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.9561469197186516 on epoch=135
03/17/2022 23:00:53 - INFO - __main__ - Saving model with best Classification-F1: 0.9552337971795046 -> 0.9561469197186516 on epoch=135, global_step=1900
03/17/2022 23:00:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
03/17/2022 23:00:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
03/17/2022 23:01:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=137
03/17/2022 23:01:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/17/2022 23:01:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=139
03/17/2022 23:01:12 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.9469998692511796 on epoch=139
03/17/2022 23:01:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=139
03/17/2022 23:01:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
03/17/2022 23:01:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
03/17/2022 23:01:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=142
03/17/2022 23:01:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=142
03/17/2022 23:01:30 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.8895660981777909 on epoch=142
03/17/2022 23:01:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=143
03/17/2022 23:01:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=144
03/17/2022 23:01:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=144
03/17/2022 23:01:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
03/17/2022 23:01:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=146
03/17/2022 23:01:48 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.8726620961077548 on epoch=146
03/17/2022 23:01:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
03/17/2022 23:01:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
03/17/2022 23:01:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=148
03/17/2022 23:01:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=149
03/17/2022 23:02:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
03/17/2022 23:02:07 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.9729660293685681 on epoch=149
03/17/2022 23:02:07 - INFO - __main__ - Saving model with best Classification-F1: 0.9561469197186516 -> 0.9729660293685681 on epoch=149, global_step=2100
03/17/2022 23:02:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=150
03/17/2022 23:02:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
03/17/2022 23:02:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
03/17/2022 23:02:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=152
03/17/2022 23:02:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
03/17/2022 23:02:25 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.8164480656097158 on epoch=153
03/17/2022 23:02:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
03/17/2022 23:02:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/17/2022 23:02:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=155
03/17/2022 23:02:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=156
03/17/2022 23:02:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=157
03/17/2022 23:02:44 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.6811882381905688 on epoch=157
03/17/2022 23:02:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=157
03/17/2022 23:02:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=158
03/17/2022 23:02:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
03/17/2022 23:02:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=159
03/17/2022 23:02:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=160
03/17/2022 23:03:02 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6578809540667583 on epoch=160
03/17/2022 23:03:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=161
03/17/2022 23:03:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=162
03/17/2022 23:03:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=162
03/17/2022 23:03:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/17/2022 23:03:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
03/17/2022 23:03:20 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.882012631683835 on epoch=164
03/17/2022 23:03:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=164
03/17/2022 23:03:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/17/2022 23:03:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=166
03/17/2022 23:03:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=167
03/17/2022 23:03:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=167
03/17/2022 23:03:38 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7436353780977576 on epoch=167
03/17/2022 23:03:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
03/17/2022 23:03:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
03/17/2022 23:03:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=169
03/17/2022 23:03:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
03/17/2022 23:03:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=171
03/17/2022 23:03:57 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.8301862563410831 on epoch=171
03/17/2022 23:03:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=172
03/17/2022 23:04:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
03/17/2022 23:04:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
03/17/2022 23:04:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=174
03/17/2022 23:04:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=174
03/17/2022 23:04:15 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.8895519731620847 on epoch=174
03/17/2022 23:04:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
03/17/2022 23:04:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
03/17/2022 23:04:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=177
03/17/2022 23:04:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/17/2022 23:04:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
03/17/2022 23:04:33 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8807444668972183 on epoch=178
03/17/2022 23:04:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=179
03/17/2022 23:04:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
03/17/2022 23:04:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
03/17/2022 23:04:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=181
03/17/2022 23:04:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
03/17/2022 23:04:52 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.8974818144806039 on epoch=182
03/17/2022 23:04:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
03/17/2022 23:04:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=183
03/17/2022 23:04:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=184
03/17/2022 23:05:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
03/17/2022 23:05:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
03/17/2022 23:05:10 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.9549362469226745 on epoch=185
03/17/2022 23:05:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/17/2022 23:05:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=187
03/17/2022 23:05:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
03/17/2022 23:05:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/17/2022 23:05:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
03/17/2022 23:05:28 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.8810472615783091 on epoch=189
03/17/2022 23:05:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/17/2022 23:05:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=190
03/17/2022 23:05:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
03/17/2022 23:05:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
03/17/2022 23:05:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
03/17/2022 23:05:47 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.8709584704411308 on epoch=192
03/17/2022 23:05:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
03/17/2022 23:05:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/17/2022 23:05:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
03/17/2022 23:05:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/17/2022 23:05:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=196
03/17/2022 23:06:05 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.8938882754031939 on epoch=196
03/17/2022 23:06:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
03/17/2022 23:06:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
03/17/2022 23:06:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=198
03/17/2022 23:06:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
03/17/2022 23:06:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/17/2022 23:06:24 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7572050313358417 on epoch=199
03/17/2022 23:06:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/17/2022 23:06:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
03/17/2022 23:06:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
03/17/2022 23:06:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/17/2022 23:06:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/17/2022 23:06:42 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7757322082219796 on epoch=203
03/17/2022 23:06:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/17/2022 23:06:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=204
03/17/2022 23:06:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/17/2022 23:06:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=206
03/17/2022 23:06:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/17/2022 23:07:00 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9550802561392643 on epoch=207
03/17/2022 23:07:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=207
03/17/2022 23:07:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/17/2022 23:07:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
03/17/2022 23:07:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=209
03/17/2022 23:07:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=210
03/17/2022 23:07:19 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.8357423476983404 on epoch=210
03/17/2022 23:07:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
03/17/2022 23:07:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
03/17/2022 23:07:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/17/2022 23:07:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/17/2022 23:07:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/17/2022 23:07:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:07:33 - INFO - __main__ - Printing 3 examples
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 23:07:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:07:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:07:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:07:33 - INFO - __main__ - Printing 3 examples
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 23:07:33 - INFO - __main__ - ['Animal']
03/17/2022 23:07:33 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:07:33 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:07:33 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:07:37 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9596282822462587 on epoch=214
03/17/2022 23:07:37 - INFO - __main__ - save last model!
03/17/2022 23:07:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 23:07:37 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 23:07:37 - INFO - __main__ - Printing 3 examples
03/17/2022 23:07:37 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 23:07:37 - INFO - __main__ - ['Animal']
03/17/2022 23:07:37 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 23:07:37 - INFO - __main__ - ['Animal']
03/17/2022 23:07:37 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 23:07:37 - INFO - __main__ - ['Village']
03/17/2022 23:07:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:07:39 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:07:43 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 23:07:51 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:07:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:07:52 - INFO - __main__ - Starting training!
03/17/2022 23:09:46 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_13_0.4_8_predictions.txt
03/17/2022 23:09:46 - INFO - __main__ - Classification-F1 on test data: 0.6840
03/17/2022 23:09:47 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.4, bsz=8, dev_performance=0.9729660293685681, test_performance=0.6839931011614823
03/17/2022 23:09:47 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.3, bsz=8 ...
03/17/2022 23:09:48 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:09:48 - INFO - __main__ - Printing 3 examples
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:09:48 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:09:48 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:09:48 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:09:48 - INFO - __main__ - Printing 3 examples
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 23:09:48 - INFO - __main__ - ['Animal']
03/17/2022 23:09:48 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:09:48 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:09:48 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:10:07 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:10:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:10:07 - INFO - __main__ - Starting training!
03/17/2022 23:10:11 - INFO - __main__ - Step 10 Global step 10 Train loss 6.75 on epoch=0
03/17/2022 23:10:13 - INFO - __main__ - Step 20 Global step 20 Train loss 6.27 on epoch=1
03/17/2022 23:10:16 - INFO - __main__ - Step 30 Global step 30 Train loss 5.33 on epoch=2
03/17/2022 23:10:19 - INFO - __main__ - Step 40 Global step 40 Train loss 4.46 on epoch=2
03/17/2022 23:10:21 - INFO - __main__ - Step 50 Global step 50 Train loss 3.56 on epoch=3
03/17/2022 23:10:44 - INFO - __main__ - Global step 50 Train loss 5.27 Classification-F1 0.009247718226046399 on epoch=3
03/17/2022 23:10:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.009247718226046399 on epoch=3, global_step=50
03/17/2022 23:10:47 - INFO - __main__ - Step 60 Global step 60 Train loss 2.96 on epoch=4
03/17/2022 23:10:49 - INFO - __main__ - Step 70 Global step 70 Train loss 2.34 on epoch=4
03/17/2022 23:10:52 - INFO - __main__ - Step 80 Global step 80 Train loss 2.03 on epoch=5
03/17/2022 23:10:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.88 on epoch=6
03/17/2022 23:10:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.62 on epoch=7
03/17/2022 23:11:07 - INFO - __main__ - Global step 100 Train loss 2.17 Classification-F1 0.2599066686414277 on epoch=7
03/17/2022 23:11:07 - INFO - __main__ - Saving model with best Classification-F1: 0.009247718226046399 -> 0.2599066686414277 on epoch=7, global_step=100
03/17/2022 23:11:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.52 on epoch=7
03/17/2022 23:11:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.35 on epoch=8
03/17/2022 23:11:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.34 on epoch=9
03/17/2022 23:11:17 - INFO - __main__ - Step 140 Global step 140 Train loss 1.14 on epoch=9
03/17/2022 23:11:20 - INFO - __main__ - Step 150 Global step 150 Train loss 1.16 on epoch=10
03/17/2022 23:11:29 - INFO - __main__ - Global step 150 Train loss 1.30 Classification-F1 0.41493402268922563 on epoch=10
03/17/2022 23:11:29 - INFO - __main__ - Saving model with best Classification-F1: 0.2599066686414277 -> 0.41493402268922563 on epoch=10, global_step=150
03/17/2022 23:11:32 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=11
03/17/2022 23:11:34 - INFO - __main__ - Step 170 Global step 170 Train loss 1.11 on epoch=12
03/17/2022 23:11:37 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=12
03/17/2022 23:11:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=13
03/17/2022 23:11:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=14
03/17/2022 23:11:51 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.4448141139720965 on epoch=14
03/17/2022 23:11:51 - INFO - __main__ - Saving model with best Classification-F1: 0.41493402268922563 -> 0.4448141139720965 on epoch=14, global_step=200
03/17/2022 23:11:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=14
03/17/2022 23:11:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=15
03/17/2022 23:11:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=16
03/17/2022 23:12:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=17
03/17/2022 23:12:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=17
03/17/2022 23:12:13 - INFO - __main__ - Global step 250 Train loss 0.79 Classification-F1 0.5009539295286108 on epoch=17
03/17/2022 23:12:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4448141139720965 -> 0.5009539295286108 on epoch=17, global_step=250
03/17/2022 23:12:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.77 on epoch=18
03/17/2022 23:12:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=19
03/17/2022 23:12:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=19
03/17/2022 23:12:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.67 on epoch=20
03/17/2022 23:12:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=21
03/17/2022 23:12:35 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.5560359435289086 on epoch=21
03/17/2022 23:12:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5009539295286108 -> 0.5560359435289086 on epoch=21, global_step=300
03/17/2022 23:12:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=22
03/17/2022 23:12:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.61 on epoch=22
03/17/2022 23:12:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=23
03/17/2022 23:12:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.68 on epoch=24
03/17/2022 23:12:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=24
03/17/2022 23:12:58 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5210474588445132 on epoch=24
03/17/2022 23:13:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=25
03/17/2022 23:13:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=26
03/17/2022 23:13:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=27
03/17/2022 23:13:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=27
03/17/2022 23:13:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=28
03/17/2022 23:13:19 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.6702135134335128 on epoch=28
03/17/2022 23:13:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5560359435289086 -> 0.6702135134335128 on epoch=28, global_step=400
03/17/2022 23:13:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=29
03/17/2022 23:13:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=29
03/17/2022 23:13:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=30
03/17/2022 23:13:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=31
03/17/2022 23:13:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=32
03/17/2022 23:13:41 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.6651268913449884 on epoch=32
03/17/2022 23:13:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=32
03/17/2022 23:13:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=33
03/17/2022 23:13:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=34
03/17/2022 23:13:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=34
03/17/2022 23:13:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=35
03/17/2022 23:14:01 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.808066563971887 on epoch=35
03/17/2022 23:14:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6702135134335128 -> 0.808066563971887 on epoch=35, global_step=500
03/17/2022 23:14:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=36
03/17/2022 23:14:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=37
03/17/2022 23:14:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=37
03/17/2022 23:14:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=38
03/17/2022 23:14:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=39
03/17/2022 23:14:22 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.8158385773053112 on epoch=39
03/17/2022 23:14:22 - INFO - __main__ - Saving model with best Classification-F1: 0.808066563971887 -> 0.8158385773053112 on epoch=39, global_step=550
03/17/2022 23:14:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=39
03/17/2022 23:14:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=40
03/17/2022 23:14:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=41
03/17/2022 23:14:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=42
03/17/2022 23:14:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=42
03/17/2022 23:14:43 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.7429254469325907 on epoch=42
03/17/2022 23:14:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=43
03/17/2022 23:14:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=44
03/17/2022 23:14:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=44
03/17/2022 23:14:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=45
03/17/2022 23:14:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=46
03/17/2022 23:15:03 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.7061949200357506 on epoch=46
03/17/2022 23:15:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=47
03/17/2022 23:15:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=47
03/17/2022 23:15:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=48
03/17/2022 23:15:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=49
03/17/2022 23:15:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=49
03/17/2022 23:15:22 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6993198852055849 on epoch=49
03/17/2022 23:15:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=50
03/17/2022 23:15:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=51
03/17/2022 23:15:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=52
03/17/2022 23:15:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=52
03/17/2022 23:15:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=53
03/17/2022 23:15:41 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.6450430885959436 on epoch=53
03/17/2022 23:15:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=54
03/17/2022 23:15:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=54
03/17/2022 23:15:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=55
03/17/2022 23:15:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=56
03/17/2022 23:15:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=57
03/17/2022 23:16:00 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.6578594152899145 on epoch=57
03/17/2022 23:16:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=57
03/17/2022 23:16:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=58
03/17/2022 23:16:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=59
03/17/2022 23:16:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=59
03/17/2022 23:16:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=60
03/17/2022 23:16:19 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.750443349420452 on epoch=60
03/17/2022 23:16:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=61
03/17/2022 23:16:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=62
03/17/2022 23:16:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=62
03/17/2022 23:16:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=63
03/17/2022 23:16:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=64
03/17/2022 23:16:38 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.7255263408932605 on epoch=64
03/17/2022 23:16:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=64
03/17/2022 23:16:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=65
03/17/2022 23:16:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=66
03/17/2022 23:16:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=67
03/17/2022 23:16:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=67
03/17/2022 23:16:57 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.7590383861879194 on epoch=67
03/17/2022 23:17:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=68
03/17/2022 23:17:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=69
03/17/2022 23:17:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=69
03/17/2022 23:17:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=70
03/17/2022 23:17:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=71
03/17/2022 23:17:17 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.6233600079295413 on epoch=71
03/17/2022 23:17:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=72
03/17/2022 23:17:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=72
03/17/2022 23:17:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=73
03/17/2022 23:17:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=74
03/17/2022 23:17:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=74
03/17/2022 23:17:36 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7029535689430125 on epoch=74
03/17/2022 23:17:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=75
03/17/2022 23:17:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=76
03/17/2022 23:17:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=77
03/17/2022 23:17:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=77
03/17/2022 23:17:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=78
03/17/2022 23:17:55 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.7175007300534847 on epoch=78
03/17/2022 23:17:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
03/17/2022 23:18:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=79
03/17/2022 23:18:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=80
03/17/2022 23:18:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=81
03/17/2022 23:18:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=82
03/17/2022 23:18:15 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.7603121066869799 on epoch=82
03/17/2022 23:18:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=82
03/17/2022 23:18:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=83
03/17/2022 23:18:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=84
03/17/2022 23:18:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=84
03/17/2022 23:18:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=85
03/17/2022 23:18:33 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.7582560687257082 on epoch=85
03/17/2022 23:18:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=86
03/17/2022 23:18:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=87
03/17/2022 23:18:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=87
03/17/2022 23:18:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=88
03/17/2022 23:18:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=89
03/17/2022 23:18:52 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.7231419097084937 on epoch=89
03/17/2022 23:18:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=89
03/17/2022 23:18:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
03/17/2022 23:19:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=91
03/17/2022 23:19:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=92
03/17/2022 23:19:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=92
03/17/2022 23:19:11 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.7352023431511021 on epoch=92
03/17/2022 23:19:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=93
03/17/2022 23:19:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=94
03/17/2022 23:19:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=94
03/17/2022 23:19:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=95
03/17/2022 23:19:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=96
03/17/2022 23:19:29 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.7423544945450774 on epoch=96
03/17/2022 23:19:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=97
03/17/2022 23:19:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=97
03/17/2022 23:19:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=98
03/17/2022 23:19:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=99
03/17/2022 23:19:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=99
03/17/2022 23:19:48 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.6517951929529809 on epoch=99
03/17/2022 23:19:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=100
03/17/2022 23:19:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=101
03/17/2022 23:19:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=102
03/17/2022 23:19:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=102
03/17/2022 23:20:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=103
03/17/2022 23:20:07 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.8975258232864723 on epoch=103
03/17/2022 23:20:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8158385773053112 -> 0.8975258232864723 on epoch=103, global_step=1450
03/17/2022 23:20:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=104
03/17/2022 23:20:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=104
03/17/2022 23:20:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=105
03/17/2022 23:20:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=106
03/17/2022 23:20:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=107
03/17/2022 23:20:27 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.8931275986984943 on epoch=107
03/17/2022 23:20:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=107
03/17/2022 23:20:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=108
03/17/2022 23:20:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=109
03/17/2022 23:20:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=109
03/17/2022 23:20:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=110
03/17/2022 23:20:46 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.7818866598348606 on epoch=110
03/17/2022 23:20:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=111
03/17/2022 23:20:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=112
03/17/2022 23:20:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
03/17/2022 23:20:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=113
03/17/2022 23:20:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=114
03/17/2022 23:21:05 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.9591525246462221 on epoch=114
03/17/2022 23:21:05 - INFO - __main__ - Saving model with best Classification-F1: 0.8975258232864723 -> 0.9591525246462221 on epoch=114, global_step=1600
03/17/2022 23:21:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=114
03/17/2022 23:21:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=115
03/17/2022 23:21:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=116
03/17/2022 23:21:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=117
03/17/2022 23:21:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=117
03/17/2022 23:21:24 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.9015303644968959 on epoch=117
03/17/2022 23:21:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=118
03/17/2022 23:21:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=119
03/17/2022 23:21:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=119
03/17/2022 23:21:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=120
03/17/2022 23:21:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=121
03/17/2022 23:21:42 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8974457805166762 on epoch=121
03/17/2022 23:21:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=122
03/17/2022 23:21:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=122
03/17/2022 23:21:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
03/17/2022 23:21:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=124
03/17/2022 23:21:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=124
03/17/2022 23:22:01 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.84313795042868 on epoch=124
03/17/2022 23:22:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=125
03/17/2022 23:22:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=126
03/17/2022 23:22:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=127
03/17/2022 23:22:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=127
03/17/2022 23:22:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
03/17/2022 23:22:20 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.8447714731559526 on epoch=128
03/17/2022 23:22:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=129
03/17/2022 23:22:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=129
03/17/2022 23:22:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=130
03/17/2022 23:22:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
03/17/2022 23:22:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=132
03/17/2022 23:22:39 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7874779984921106 on epoch=132
03/17/2022 23:22:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=132
03/17/2022 23:22:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=133
03/17/2022 23:22:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=134
03/17/2022 23:22:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=134
03/17/2022 23:22:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=135
03/17/2022 23:22:58 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.7806358189967333 on epoch=135
03/17/2022 23:23:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=136
03/17/2022 23:23:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=137
03/17/2022 23:23:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=137
03/17/2022 23:23:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=138
03/17/2022 23:23:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=139
03/17/2022 23:23:17 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.6777146223940822 on epoch=139
03/17/2022 23:23:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
03/17/2022 23:23:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=140
03/17/2022 23:23:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
03/17/2022 23:23:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=142
03/17/2022 23:23:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
03/17/2022 23:23:36 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7781665614990128 on epoch=142
03/17/2022 23:23:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
03/17/2022 23:23:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=144
03/17/2022 23:23:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=144
03/17/2022 23:23:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=145
03/17/2022 23:23:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
03/17/2022 23:23:55 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7816308751388483 on epoch=146
03/17/2022 23:23:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=147
03/17/2022 23:24:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
03/17/2022 23:24:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=148
03/17/2022 23:24:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
03/17/2022 23:24:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
03/17/2022 23:24:14 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.7758117733045661 on epoch=149
03/17/2022 23:24:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
03/17/2022 23:24:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
03/17/2022 23:24:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=152
03/17/2022 23:24:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=152
03/17/2022 23:24:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
03/17/2022 23:24:33 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.895522727815635 on epoch=153
03/17/2022 23:24:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=154
03/17/2022 23:24:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=154
03/17/2022 23:24:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/17/2022 23:24:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
03/17/2022 23:24:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
03/17/2022 23:24:53 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.8434856261558623 on epoch=157
03/17/2022 23:24:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=157
03/17/2022 23:24:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=158
03/17/2022 23:25:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=159
03/17/2022 23:25:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=159
03/17/2022 23:25:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
03/17/2022 23:25:14 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.9019209374854535 on epoch=160
03/17/2022 23:25:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=161
03/17/2022 23:25:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=162
03/17/2022 23:25:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=162
03/17/2022 23:25:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/17/2022 23:25:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=164
03/17/2022 23:25:34 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.8195424137902709 on epoch=164
03/17/2022 23:25:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=164
03/17/2022 23:25:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/17/2022 23:25:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=166
03/17/2022 23:25:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=167
03/17/2022 23:25:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
03/17/2022 23:25:52 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.8211812668236473 on epoch=167
03/17/2022 23:25:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=168
03/17/2022 23:25:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/17/2022 23:26:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=169
03/17/2022 23:26:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
03/17/2022 23:26:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=171
03/17/2022 23:26:11 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.838857494661329 on epoch=171
03/17/2022 23:26:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
03/17/2022 23:26:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
03/17/2022 23:26:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=173
03/17/2022 23:26:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
03/17/2022 23:26:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=174
03/17/2022 23:26:32 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.8384810876835819 on epoch=174
03/17/2022 23:26:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
03/17/2022 23:26:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/17/2022 23:26:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=177
03/17/2022 23:26:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=177
03/17/2022 23:26:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=178
03/17/2022 23:26:52 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8406417112299465 on epoch=178
03/17/2022 23:26:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
03/17/2022 23:26:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
03/17/2022 23:26:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
03/17/2022 23:27:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
03/17/2022 23:27:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
03/17/2022 23:27:10 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8268369715016929 on epoch=182
03/17/2022 23:27:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/17/2022 23:27:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
03/17/2022 23:27:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
03/17/2022 23:27:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/17/2022 23:27:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
03/17/2022 23:27:29 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.8926260414685461 on epoch=185
03/17/2022 23:27:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
03/17/2022 23:27:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
03/17/2022 23:27:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=187
03/17/2022 23:27:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
03/17/2022 23:27:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=189
03/17/2022 23:27:48 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9035662124087173 on epoch=189
03/17/2022 23:27:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=189
03/17/2022 23:27:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=190
03/17/2022 23:27:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
03/17/2022 23:27:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
03/17/2022 23:28:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=192
03/17/2022 23:28:07 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.833384749814719 on epoch=192
03/17/2022 23:28:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/17/2022 23:28:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/17/2022 23:28:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/17/2022 23:28:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/17/2022 23:28:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/17/2022 23:28:28 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7649528999833164 on epoch=196
03/17/2022 23:28:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=197
03/17/2022 23:28:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
03/17/2022 23:28:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/17/2022 23:28:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
03/17/2022 23:28:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
03/17/2022 23:28:46 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7945539808665402 on epoch=199
03/17/2022 23:28:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
03/17/2022 23:28:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
03/17/2022 23:28:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=202
03/17/2022 23:28:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=202
03/17/2022 23:28:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/17/2022 23:29:05 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.8820752154829375 on epoch=203
03/17/2022 23:29:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/17/2022 23:29:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/17/2022 23:29:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/17/2022 23:29:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=206
03/17/2022 23:29:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
03/17/2022 23:29:27 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.8107216145255757 on epoch=207
03/17/2022 23:29:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
03/17/2022 23:29:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/17/2022 23:29:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=209
03/17/2022 23:29:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/17/2022 23:29:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
03/17/2022 23:29:47 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.8967606312420813 on epoch=210
03/17/2022 23:29:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
03/17/2022 23:29:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/17/2022 23:29:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
03/17/2022 23:29:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/17/2022 23:29:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
03/17/2022 23:30:01 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:30:01 - INFO - __main__ - Printing 3 examples
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 23:30:01 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:30:01 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:30:01 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:30:01 - INFO - __main__ - Printing 3 examples
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 23:30:01 - INFO - __main__ - ['Animal']
03/17/2022 23:30:01 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:30:01 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:30:02 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:30:06 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7758236558856528 on epoch=214
03/17/2022 23:30:06 - INFO - __main__ - save last model!
03/17/2022 23:30:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 23:30:06 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 23:30:06 - INFO - __main__ - Printing 3 examples
03/17/2022 23:30:06 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 23:30:06 - INFO - __main__ - ['Animal']
03/17/2022 23:30:06 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 23:30:06 - INFO - __main__ - ['Animal']
03/17/2022 23:30:06 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 23:30:06 - INFO - __main__ - ['Village']
03/17/2022 23:30:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:30:08 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:30:11 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 23:30:17 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:30:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:30:18 - INFO - __main__ - Starting training!
03/17/2022 23:32:21 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_13_0.3_8_predictions.txt
03/17/2022 23:32:21 - INFO - __main__ - Classification-F1 on test data: 0.5493
03/17/2022 23:32:21 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.3, bsz=8, dev_performance=0.9591525246462221, test_performance=0.5493139810302042
03/17/2022 23:32:21 - INFO - __main__ - Running ... prefix=dbpedia_14_16_13, lr=0.2, bsz=8 ...
03/17/2022 23:32:22 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:32:22 - INFO - __main__ - Printing 3 examples
03/17/2022 23:32:22 - INFO - __main__ -  [dbpedia_14] Malkaridae is a small spider family with ten species in four genera.
03/17/2022 23:32:22 - INFO - __main__ - ['Animal']
03/17/2022 23:32:22 - INFO - __main__ -  [dbpedia_14] The Dahl's toad-headed turtle (Mesoclemmys dahli) is a species of turtle in the Chelidae family.It is endemic to Colombia.
03/17/2022 23:32:22 - INFO - __main__ - ['Animal']
03/17/2022 23:32:22 - INFO - __main__ -  [dbpedia_14] The Tersa Sphinx (Xylophanes tersa) is a moth of the Sphingidae family. It is found from the United States (Massachusetts south to southern Florida west to Nebraska New Mexico and southern Arizona) through Mexico the West Indies and Central America and into parts of South America (including Bolivia Paraguay Argentina and Brazil). An occasional stray can be found as far north as Canada.The wingspan is 60–80 mm.
03/17/2022 23:32:22 - INFO - __main__ - ['Animal']
03/17/2022 23:32:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:32:22 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:32:23 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:32:23 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:32:23 - INFO - __main__ - Printing 3 examples
03/17/2022 23:32:23 - INFO - __main__ -  [dbpedia_14] Nemadactylus is a genus of morwongs.
03/17/2022 23:32:23 - INFO - __main__ - ['Animal']
03/17/2022 23:32:23 - INFO - __main__ -  [dbpedia_14] Coleophora isomoera is a moth of the Coleophoridae family. It is found in Spain and Morocco Turkey Uzbekistan Mongolia and China.
03/17/2022 23:32:23 - INFO - __main__ - ['Animal']
03/17/2022 23:32:23 - INFO - __main__ -  [dbpedia_14] Bredana is a genus of jumping spiders that occurs in the USA.
03/17/2022 23:32:23 - INFO - __main__ - ['Animal']
03/17/2022 23:32:23 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:32:23 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:32:23 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:32:38 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:32:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:32:39 - INFO - __main__ - Starting training!
03/17/2022 23:32:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=0
03/17/2022 23:32:45 - INFO - __main__ - Step 20 Global step 20 Train loss 6.57 on epoch=1
03/17/2022 23:32:47 - INFO - __main__ - Step 30 Global step 30 Train loss 5.71 on epoch=2
03/17/2022 23:32:50 - INFO - __main__ - Step 40 Global step 40 Train loss 4.85 on epoch=2
03/17/2022 23:32:52 - INFO - __main__ - Step 50 Global step 50 Train loss 3.92 on epoch=3
03/17/2022 23:33:44 - INFO - __main__ - Global step 50 Train loss 5.58 Classification-F1 0.004922673058266278 on epoch=3
03/17/2022 23:33:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.004922673058266278 on epoch=3, global_step=50
03/17/2022 23:33:47 - INFO - __main__ - Step 60 Global step 60 Train loss 3.33 on epoch=4
03/17/2022 23:33:49 - INFO - __main__ - Step 70 Global step 70 Train loss 2.72 on epoch=4
03/17/2022 23:33:52 - INFO - __main__ - Step 80 Global step 80 Train loss 2.34 on epoch=5
03/17/2022 23:33:54 - INFO - __main__ - Step 90 Global step 90 Train loss 2.20 on epoch=6
03/17/2022 23:33:57 - INFO - __main__ - Step 100 Global step 100 Train loss 1.88 on epoch=7
03/17/2022 23:34:05 - INFO - __main__ - Global step 100 Train loss 2.49 Classification-F1 0.23089553243980152 on epoch=7
03/17/2022 23:34:05 - INFO - __main__ - Saving model with best Classification-F1: 0.004922673058266278 -> 0.23089553243980152 on epoch=7, global_step=100
03/17/2022 23:34:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.60 on epoch=7
03/17/2022 23:34:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.67 on epoch=8
03/17/2022 23:34:12 - INFO - __main__ - Step 130 Global step 130 Train loss 1.33 on epoch=9
03/17/2022 23:34:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.35 on epoch=9
03/17/2022 23:34:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.24 on epoch=10
03/17/2022 23:34:27 - INFO - __main__ - Global step 150 Train loss 1.44 Classification-F1 0.3488555825801932 on epoch=10
03/17/2022 23:34:27 - INFO - __main__ - Saving model with best Classification-F1: 0.23089553243980152 -> 0.3488555825801932 on epoch=10, global_step=150
03/17/2022 23:34:29 - INFO - __main__ - Step 160 Global step 160 Train loss 1.18 on epoch=11
03/17/2022 23:34:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.16 on epoch=12
03/17/2022 23:34:34 - INFO - __main__ - Step 180 Global step 180 Train loss 1.08 on epoch=12
03/17/2022 23:34:37 - INFO - __main__ - Step 190 Global step 190 Train loss 1.00 on epoch=13
03/17/2022 23:34:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=14
03/17/2022 23:34:50 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.38540750076064045 on epoch=14
03/17/2022 23:34:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3488555825801932 -> 0.38540750076064045 on epoch=14, global_step=200
03/17/2022 23:34:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=14
03/17/2022 23:34:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=15
03/17/2022 23:34:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=16
03/17/2022 23:35:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=17
03/17/2022 23:35:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=17
03/17/2022 23:35:12 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.5000894449674482 on epoch=17
03/17/2022 23:35:12 - INFO - __main__ - Saving model with best Classification-F1: 0.38540750076064045 -> 0.5000894449674482 on epoch=17, global_step=250
03/17/2022 23:35:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=18
03/17/2022 23:35:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=19
03/17/2022 23:35:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=19
03/17/2022 23:35:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=20
03/17/2022 23:35:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=21
03/17/2022 23:35:34 - INFO - __main__ - Global step 300 Train loss 0.82 Classification-F1 0.6172325784847721 on epoch=21
03/17/2022 23:35:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5000894449674482 -> 0.6172325784847721 on epoch=21, global_step=300
03/17/2022 23:35:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=22
03/17/2022 23:35:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=22
03/17/2022 23:35:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=23
03/17/2022 23:35:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=24
03/17/2022 23:35:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.66 on epoch=24
03/17/2022 23:35:57 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.5272221850019467 on epoch=24
03/17/2022 23:35:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.74 on epoch=25
03/17/2022 23:36:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=26
03/17/2022 23:36:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=27
03/17/2022 23:36:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.64 on epoch=27
03/17/2022 23:36:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=28
03/17/2022 23:36:19 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.625420922465341 on epoch=28
03/17/2022 23:36:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6172325784847721 -> 0.625420922465341 on epoch=28, global_step=400
03/17/2022 23:36:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=29
03/17/2022 23:36:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=29
03/17/2022 23:36:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.62 on epoch=30
03/17/2022 23:36:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=31
03/17/2022 23:36:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=32
03/17/2022 23:36:40 - INFO - __main__ - Global step 450 Train loss 0.58 Classification-F1 0.6033330301569996 on epoch=32
03/17/2022 23:36:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=32
03/17/2022 23:36:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=33
03/17/2022 23:36:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=34
03/17/2022 23:36:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=34
03/17/2022 23:36:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=35
03/17/2022 23:37:01 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.6697097342460343 on epoch=35
03/17/2022 23:37:01 - INFO - __main__ - Saving model with best Classification-F1: 0.625420922465341 -> 0.6697097342460343 on epoch=35, global_step=500
03/17/2022 23:37:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=36
03/17/2022 23:37:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=37
03/17/2022 23:37:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=37
03/17/2022 23:37:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=38
03/17/2022 23:37:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=39
03/17/2022 23:37:20 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.526097235111 on epoch=39
03/17/2022 23:37:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=39
03/17/2022 23:37:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=40
03/17/2022 23:37:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=41
03/17/2022 23:37:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=42
03/17/2022 23:37:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=42
03/17/2022 23:37:40 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.6699698792396117 on epoch=42
03/17/2022 23:37:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6697097342460343 -> 0.6699698792396117 on epoch=42, global_step=600
03/17/2022 23:37:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=43
03/17/2022 23:37:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=44
03/17/2022 23:37:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=44
03/17/2022 23:37:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=45
03/17/2022 23:37:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=46
03/17/2022 23:37:59 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.781387006420377 on epoch=46
03/17/2022 23:37:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6699698792396117 -> 0.781387006420377 on epoch=46, global_step=650
03/17/2022 23:38:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=47
03/17/2022 23:38:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=47
03/17/2022 23:38:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=48
03/17/2022 23:38:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=49
03/17/2022 23:38:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=49
03/17/2022 23:38:19 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.6758035299415875 on epoch=49
03/17/2022 23:38:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=50
03/17/2022 23:38:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=51
03/17/2022 23:38:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=52
03/17/2022 23:38:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=52
03/17/2022 23:38:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=53
03/17/2022 23:38:38 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.7145154283897448 on epoch=53
03/17/2022 23:38:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=54
03/17/2022 23:38:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=54
03/17/2022 23:38:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=55
03/17/2022 23:38:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=56
03/17/2022 23:38:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=57
03/17/2022 23:38:57 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.802257946578237 on epoch=57
03/17/2022 23:38:57 - INFO - __main__ - Saving model with best Classification-F1: 0.781387006420377 -> 0.802257946578237 on epoch=57, global_step=800
03/17/2022 23:39:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=57
03/17/2022 23:39:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/17/2022 23:39:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=59
03/17/2022 23:39:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=59
03/17/2022 23:39:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=60
03/17/2022 23:39:16 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.7683270467277821 on epoch=60
03/17/2022 23:39:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=61
03/17/2022 23:39:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=62
03/17/2022 23:39:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=62
03/17/2022 23:39:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=63
03/17/2022 23:39:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=64
03/17/2022 23:39:35 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.738951117158535 on epoch=64
03/17/2022 23:39:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=64
03/17/2022 23:39:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=65
03/17/2022 23:39:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=66
03/17/2022 23:39:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=67
03/17/2022 23:39:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=67
03/17/2022 23:39:54 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.7559090248606377 on epoch=67
03/17/2022 23:39:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=68
03/17/2022 23:39:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=69
03/17/2022 23:40:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=69
03/17/2022 23:40:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=70
03/17/2022 23:40:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=71
03/17/2022 23:40:13 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7916692615104679 on epoch=71
03/17/2022 23:40:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=72
03/17/2022 23:40:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=72
03/17/2022 23:40:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=73
03/17/2022 23:40:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=74
03/17/2022 23:40:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=74
03/17/2022 23:40:32 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.7511495170763811 on epoch=74
03/17/2022 23:40:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=75
03/17/2022 23:40:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=76
03/17/2022 23:40:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=77
03/17/2022 23:40:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=77
03/17/2022 23:40:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=78
03/17/2022 23:40:51 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.6717693401161144 on epoch=78
03/17/2022 23:40:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=79
03/17/2022 23:40:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=79
03/17/2022 23:40:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=80
03/17/2022 23:41:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=81
03/17/2022 23:41:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=82
03/17/2022 23:41:10 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.673224311957215 on epoch=82
03/17/2022 23:41:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=82
03/17/2022 23:41:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=83
03/17/2022 23:41:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=84
03/17/2022 23:41:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=84
03/17/2022 23:41:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=85
03/17/2022 23:41:30 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7664155789392981 on epoch=85
03/17/2022 23:41:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=86
03/17/2022 23:41:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=87
03/17/2022 23:41:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=87
03/17/2022 23:41:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=88
03/17/2022 23:41:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=89
03/17/2022 23:41:49 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.7147786646873863 on epoch=89
03/17/2022 23:41:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=89
03/17/2022 23:41:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=90
03/17/2022 23:41:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=91
03/17/2022 23:41:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=92
03/17/2022 23:42:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=92
03/17/2022 23:42:08 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.7426074423214172 on epoch=92
03/17/2022 23:42:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=93
03/17/2022 23:42:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=94
03/17/2022 23:42:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=94
03/17/2022 23:42:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=95
03/17/2022 23:42:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=96
03/17/2022 23:42:27 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.7627815012498014 on epoch=96
03/17/2022 23:42:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=97
03/17/2022 23:42:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=97
03/17/2022 23:42:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=98
03/17/2022 23:42:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=99
03/17/2022 23:42:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=99
03/17/2022 23:42:47 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.7643405818425352 on epoch=99
03/17/2022 23:42:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=100
03/17/2022 23:42:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=101
03/17/2022 23:42:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=102
03/17/2022 23:42:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=102
03/17/2022 23:42:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=103
03/17/2022 23:43:06 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.7464994887024721 on epoch=103
03/17/2022 23:43:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=104
03/17/2022 23:43:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=104
03/17/2022 23:43:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
03/17/2022 23:43:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=106
03/17/2022 23:43:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=107
03/17/2022 23:43:25 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.8034905699205391 on epoch=107
03/17/2022 23:43:25 - INFO - __main__ - Saving model with best Classification-F1: 0.802257946578237 -> 0.8034905699205391 on epoch=107, global_step=1500
03/17/2022 23:43:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=107
03/17/2022 23:43:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=108
03/17/2022 23:43:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
03/17/2022 23:43:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=109
03/17/2022 23:43:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=110
03/17/2022 23:43:44 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.6900868167918502 on epoch=110
03/17/2022 23:43:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=111
03/17/2022 23:43:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=112
03/17/2022 23:43:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=112
03/17/2022 23:43:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=113
03/17/2022 23:43:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=114
03/17/2022 23:44:03 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.7767367357500186 on epoch=114
03/17/2022 23:44:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=114
03/17/2022 23:44:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=115
03/17/2022 23:44:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=116
03/17/2022 23:44:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=117
03/17/2022 23:44:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=117
03/17/2022 23:44:22 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.7460561497326204 on epoch=117
03/17/2022 23:44:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=118
03/17/2022 23:44:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=119
03/17/2022 23:44:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=119
03/17/2022 23:44:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=120
03/17/2022 23:44:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=121
03/17/2022 23:44:41 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.682653632813228 on epoch=121
03/17/2022 23:44:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=122
03/17/2022 23:44:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=122
03/17/2022 23:44:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=123
03/17/2022 23:44:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=124
03/17/2022 23:44:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=124
03/17/2022 23:45:01 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.851129400623886 on epoch=124
03/17/2022 23:45:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8034905699205391 -> 0.851129400623886 on epoch=124, global_step=1750
03/17/2022 23:45:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=125
03/17/2022 23:45:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=126
03/17/2022 23:45:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=127
03/17/2022 23:45:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=127
03/17/2022 23:45:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=128
03/17/2022 23:45:20 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.8103576083414794 on epoch=128
03/17/2022 23:45:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
03/17/2022 23:45:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=129
03/17/2022 23:45:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=130
03/17/2022 23:45:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
03/17/2022 23:45:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=132
03/17/2022 23:45:39 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.8448060591961801 on epoch=132
03/17/2022 23:45:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
03/17/2022 23:45:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=133
03/17/2022 23:45:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=134
03/17/2022 23:45:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=134
03/17/2022 23:45:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=135
03/17/2022 23:45:58 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7609381956753903 on epoch=135
03/17/2022 23:46:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=136
03/17/2022 23:46:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=137
03/17/2022 23:46:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=137
03/17/2022 23:46:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=138
03/17/2022 23:46:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=139
03/17/2022 23:46:17 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.9053004179728317 on epoch=139
03/17/2022 23:46:17 - INFO - __main__ - Saving model with best Classification-F1: 0.851129400623886 -> 0.9053004179728317 on epoch=139, global_step=1950
03/17/2022 23:46:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=139
03/17/2022 23:46:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=140
03/17/2022 23:46:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
03/17/2022 23:46:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=142
03/17/2022 23:46:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=142
03/17/2022 23:46:36 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.9013673491441272 on epoch=142
03/17/2022 23:46:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=143
03/17/2022 23:46:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=144
03/17/2022 23:46:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=144
03/17/2022 23:46:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=145
03/17/2022 23:46:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
03/17/2022 23:46:55 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.8449204816229758 on epoch=146
03/17/2022 23:46:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=147
03/17/2022 23:47:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=147
03/17/2022 23:47:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=148
03/17/2022 23:47:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=149
03/17/2022 23:47:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
03/17/2022 23:47:14 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.8379141669701442 on epoch=149
03/17/2022 23:47:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=150
03/17/2022 23:47:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
03/17/2022 23:47:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=152
03/17/2022 23:47:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=152
03/17/2022 23:47:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
03/17/2022 23:47:33 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.8927604500706967 on epoch=153
03/17/2022 23:47:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
03/17/2022 23:47:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
03/17/2022 23:47:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=155
03/17/2022 23:47:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
03/17/2022 23:47:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=157
03/17/2022 23:47:52 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.8472193321976884 on epoch=157
03/17/2022 23:47:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=157
03/17/2022 23:47:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=158
03/17/2022 23:48:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=159
03/17/2022 23:48:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=159
03/17/2022 23:48:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=160
03/17/2022 23:48:11 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.9773538961038961 on epoch=160
03/17/2022 23:48:11 - INFO - __main__ - Saving model with best Classification-F1: 0.9053004179728317 -> 0.9773538961038961 on epoch=160, global_step=2250
03/17/2022 23:48:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=161
03/17/2022 23:48:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=162
03/17/2022 23:48:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
03/17/2022 23:48:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/17/2022 23:48:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=164
03/17/2022 23:48:30 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.973139893787427 on epoch=164
03/17/2022 23:48:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
03/17/2022 23:48:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=165
03/17/2022 23:48:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=166
03/17/2022 23:48:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=167
03/17/2022 23:48:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=167
03/17/2022 23:48:49 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.9035817855979145 on epoch=167
03/17/2022 23:48:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
03/17/2022 23:48:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
03/17/2022 23:48:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=169
03/17/2022 23:48:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
03/17/2022 23:49:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=171
03/17/2022 23:49:08 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.8957993864429665 on epoch=171
03/17/2022 23:49:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=172
03/17/2022 23:49:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
03/17/2022 23:49:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
03/17/2022 23:49:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=174
03/17/2022 23:49:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=174
03/17/2022 23:49:27 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.973139893787427 on epoch=174
03/17/2022 23:49:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
03/17/2022 23:49:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
03/17/2022 23:49:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
03/17/2022 23:49:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=177
03/17/2022 23:49:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=178
03/17/2022 23:49:46 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8691467672112834 on epoch=178
03/17/2022 23:49:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=179
03/17/2022 23:49:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
03/17/2022 23:49:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
03/17/2022 23:49:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=181
03/17/2022 23:49:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
03/17/2022 23:50:05 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.9734487734487735 on epoch=182
03/17/2022 23:50:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=182
03/17/2022 23:50:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=183
03/17/2022 23:50:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
03/17/2022 23:50:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
03/17/2022 23:50:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
03/17/2022 23:50:24 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.851152401026393 on epoch=185
03/17/2022 23:50:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
03/17/2022 23:50:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=187
03/17/2022 23:50:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/17/2022 23:50:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
03/17/2022 23:50:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
03/17/2022 23:50:43 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.9019293416178845 on epoch=189
03/17/2022 23:50:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=189
03/17/2022 23:50:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
03/17/2022 23:50:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
03/17/2022 23:50:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=192
03/17/2022 23:50:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
03/17/2022 23:51:02 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.973139893787427 on epoch=192
03/17/2022 23:51:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/17/2022 23:51:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
03/17/2022 23:51:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
03/17/2022 23:51:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
03/17/2022 23:51:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=196
03/17/2022 23:51:21 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.9054192533857849 on epoch=196
03/17/2022 23:51:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=197
03/17/2022 23:51:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=197
03/17/2022 23:51:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=198
03/17/2022 23:51:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=199
03/17/2022 23:51:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=199
03/17/2022 23:51:40 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.9690007381251622 on epoch=199
03/17/2022 23:51:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=200
03/17/2022 23:51:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
03/17/2022 23:51:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=202
03/17/2022 23:51:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
03/17/2022 23:51:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/17/2022 23:51:59 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.8511378011615204 on epoch=203
03/17/2022 23:52:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=204
03/17/2022 23:52:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/17/2022 23:52:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=205
03/17/2022 23:52:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/17/2022 23:52:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/17/2022 23:52:17 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.8984868473293521 on epoch=207
03/17/2022 23:52:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
03/17/2022 23:52:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/17/2022 23:52:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
03/17/2022 23:52:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=209
03/17/2022 23:52:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=210
03/17/2022 23:52:36 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.8427783445262017 on epoch=210
03/17/2022 23:52:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
03/17/2022 23:52:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
03/17/2022 23:52:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=212
03/17/2022 23:52:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/17/2022 23:52:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=214
03/17/2022 23:52:52 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:52:52 - INFO - __main__ - Printing 3 examples
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/17/2022 23:52:52 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:52:52 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:52:52 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:52:52 - INFO - __main__ - Printing 3 examples
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/17/2022 23:52:52 - INFO - __main__ - ['Plant']
03/17/2022 23:52:52 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:52:52 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:52:53 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:52:55 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.910046432062561 on epoch=214
03/17/2022 23:52:55 - INFO - __main__ - save last model!
03/17/2022 23:52:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/17/2022 23:52:55 - INFO - __main__ - Start tokenizing ... 3500 instances
03/17/2022 23:52:55 - INFO - __main__ - Printing 3 examples
03/17/2022 23:52:55 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/17/2022 23:52:55 - INFO - __main__ - ['Animal']
03/17/2022 23:52:55 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/17/2022 23:52:55 - INFO - __main__ - ['Animal']
03/17/2022 23:52:55 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/17/2022 23:52:55 - INFO - __main__ - ['Village']
03/17/2022 23:52:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:52:57 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:53:00 - INFO - __main__ - Loaded 3500 examples from test data
03/17/2022 23:53:11 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:53:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:53:12 - INFO - __main__ - Starting training!
03/17/2022 23:55:09 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_13_0.2_8_predictions.txt
03/17/2022 23:55:09 - INFO - __main__ - Classification-F1 on test data: 0.6513
03/17/2022 23:55:09 - INFO - __main__ - prefix=dbpedia_14_16_13, lr=0.2, bsz=8, dev_performance=0.9773538961038961, test_performance=0.6513333182650188
03/17/2022 23:55:09 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.5, bsz=8 ...
03/17/2022 23:55:10 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:55:10 - INFO - __main__ - Printing 3 examples
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/17/2022 23:55:10 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:55:10 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/17/2022 23:55:10 - INFO - __main__ - Start tokenizing ... 224 instances
03/17/2022 23:55:10 - INFO - __main__ - Printing 3 examples
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/17/2022 23:55:10 - INFO - __main__ - ['Plant']
03/17/2022 23:55:10 - INFO - __main__ - Tokenizing Input ...
03/17/2022 23:55:10 - INFO - __main__ - Tokenizing Output ...
03/17/2022 23:55:11 - INFO - __main__ - Loaded 224 examples from dev data
03/17/2022 23:55:26 - INFO - __main__ - load prompt embedding from ckpt
03/17/2022 23:55:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/17/2022 23:55:27 - INFO - __main__ - Starting training!
03/17/2022 23:55:30 - INFO - __main__ - Step 10 Global step 10 Train loss 7.24 on epoch=0
03/17/2022 23:55:33 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=1
03/17/2022 23:55:35 - INFO - __main__ - Step 30 Global step 30 Train loss 6.97 on epoch=2
03/17/2022 23:55:38 - INFO - __main__ - Step 40 Global step 40 Train loss 6.78 on epoch=2
03/17/2022 23:55:40 - INFO - __main__ - Step 50 Global step 50 Train loss 6.57 on epoch=3
03/17/2022 23:57:36 - INFO - __main__ - Global step 50 Train loss 6.89 Classification-F1 0.0 on epoch=3
03/17/2022 23:57:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/17/2022 23:57:38 - INFO - __main__ - Step 60 Global step 60 Train loss 6.79 on epoch=4
03/17/2022 23:57:41 - INFO - __main__ - Step 70 Global step 70 Train loss 6.46 on epoch=4
03/17/2022 23:57:43 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=5
03/17/2022 23:57:46 - INFO - __main__ - Step 90 Global step 90 Train loss 6.75 on epoch=6
03/17/2022 23:57:48 - INFO - __main__ - Step 100 Global step 100 Train loss 6.82 on epoch=7
03/17/2022 23:59:39 - INFO - __main__ - Global step 100 Train loss 6.71 Classification-F1 0.0 on epoch=7
03/17/2022 23:59:41 - INFO - __main__ - Step 110 Global step 110 Train loss 6.90 on epoch=7
03/17/2022 23:59:44 - INFO - __main__ - Step 120 Global step 120 Train loss 6.89 on epoch=8
03/17/2022 23:59:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.77 on epoch=9
03/17/2022 23:59:49 - INFO - __main__ - Step 140 Global step 140 Train loss 7.02 on epoch=9
03/17/2022 23:59:51 - INFO - __main__ - Step 150 Global step 150 Train loss 7.51 on epoch=10
03/18/2022 00:01:31 - INFO - __main__ - Global step 150 Train loss 7.02 Classification-F1 0.0 on epoch=10
03/18/2022 00:01:34 - INFO - __main__ - Step 160 Global step 160 Train loss 7.24 on epoch=11
03/18/2022 00:01:36 - INFO - __main__ - Step 170 Global step 170 Train loss 7.43 on epoch=12
03/18/2022 00:01:39 - INFO - __main__ - Step 180 Global step 180 Train loss 7.72 on epoch=12
03/18/2022 00:01:41 - INFO - __main__ - Step 190 Global step 190 Train loss 7.30 on epoch=13
03/18/2022 00:01:44 - INFO - __main__ - Step 200 Global step 200 Train loss 7.29 on epoch=14
03/18/2022 00:03:39 - INFO - __main__ - Global step 200 Train loss 7.40 Classification-F1 0.0 on epoch=14
03/18/2022 00:03:42 - INFO - __main__ - Step 210 Global step 210 Train loss 7.15 on epoch=14
03/18/2022 00:03:44 - INFO - __main__ - Step 220 Global step 220 Train loss 7.36 on epoch=15
03/18/2022 00:03:47 - INFO - __main__ - Step 230 Global step 230 Train loss 7.16 on epoch=16
03/18/2022 00:03:49 - INFO - __main__ - Step 240 Global step 240 Train loss 7.18 on epoch=17
03/18/2022 00:03:52 - INFO - __main__ - Step 250 Global step 250 Train loss 7.41 on epoch=17
03/18/2022 00:05:47 - INFO - __main__ - Global step 250 Train loss 7.25 Classification-F1 0.0 on epoch=17
03/18/2022 00:05:49 - INFO - __main__ - Step 260 Global step 260 Train loss 7.13 on epoch=18
03/18/2022 00:05:52 - INFO - __main__ - Step 270 Global step 270 Train loss 6.98 on epoch=19
03/18/2022 00:05:54 - INFO - __main__ - Step 280 Global step 280 Train loss 7.03 on epoch=19
03/18/2022 00:05:57 - INFO - __main__ - Step 290 Global step 290 Train loss 7.30 on epoch=20
03/18/2022 00:06:00 - INFO - __main__ - Step 300 Global step 300 Train loss 7.23 on epoch=21
03/18/2022 00:07:48 - INFO - __main__ - Global step 300 Train loss 7.14 Classification-F1 0.0 on epoch=21
03/18/2022 00:07:51 - INFO - __main__ - Step 310 Global step 310 Train loss 7.09 on epoch=22
03/18/2022 00:07:54 - INFO - __main__ - Step 320 Global step 320 Train loss 6.99 on epoch=22
03/18/2022 00:07:56 - INFO - __main__ - Step 330 Global step 330 Train loss 6.98 on epoch=23
03/18/2022 00:07:59 - INFO - __main__ - Step 340 Global step 340 Train loss 6.98 on epoch=24
03/18/2022 00:08:01 - INFO - __main__ - Step 350 Global step 350 Train loss 6.85 on epoch=24
03/18/2022 00:10:00 - INFO - __main__ - Global step 350 Train loss 6.98 Classification-F1 0.0 on epoch=24
03/18/2022 00:10:03 - INFO - __main__ - Step 360 Global step 360 Train loss 7.29 on epoch=25
03/18/2022 00:10:05 - INFO - __main__ - Step 370 Global step 370 Train loss 6.82 on epoch=26
03/18/2022 00:10:08 - INFO - __main__ - Step 380 Global step 380 Train loss 6.98 on epoch=27
03/18/2022 00:10:10 - INFO - __main__ - Step 390 Global step 390 Train loss 7.10 on epoch=27
03/18/2022 00:10:13 - INFO - __main__ - Step 400 Global step 400 Train loss 6.89 on epoch=28
03/18/2022 00:11:55 - INFO - __main__ - Global step 400 Train loss 7.01 Classification-F1 0.0 on epoch=28
03/18/2022 00:11:57 - INFO - __main__ - Step 410 Global step 410 Train loss 6.89 on epoch=29
03/18/2022 00:12:00 - INFO - __main__ - Step 420 Global step 420 Train loss 6.93 on epoch=29
03/18/2022 00:12:02 - INFO - __main__ - Step 430 Global step 430 Train loss 6.98 on epoch=30
03/18/2022 00:12:05 - INFO - __main__ - Step 440 Global step 440 Train loss 6.74 on epoch=31
03/18/2022 00:12:08 - INFO - __main__ - Step 450 Global step 450 Train loss 6.91 on epoch=32
03/18/2022 00:13:22 - INFO - __main__ - Global step 450 Train loss 6.89 Classification-F1 0.0 on epoch=32
03/18/2022 00:13:25 - INFO - __main__ - Step 460 Global step 460 Train loss 6.98 on epoch=32
03/18/2022 00:13:28 - INFO - __main__ - Step 470 Global step 470 Train loss 6.92 on epoch=33
03/18/2022 00:13:30 - INFO - __main__ - Step 480 Global step 480 Train loss 6.78 on epoch=34
03/18/2022 00:13:33 - INFO - __main__ - Step 490 Global step 490 Train loss 6.75 on epoch=34
03/18/2022 00:13:35 - INFO - __main__ - Step 500 Global step 500 Train loss 6.89 on epoch=35
03/18/2022 00:15:22 - INFO - __main__ - Global step 500 Train loss 6.87 Classification-F1 0.0 on epoch=35
03/18/2022 00:15:25 - INFO - __main__ - Step 510 Global step 510 Train loss 6.68 on epoch=36
03/18/2022 00:15:27 - INFO - __main__ - Step 520 Global step 520 Train loss 6.84 on epoch=37
03/18/2022 00:15:30 - INFO - __main__ - Step 530 Global step 530 Train loss 7.07 on epoch=37
03/18/2022 00:15:33 - INFO - __main__ - Step 540 Global step 540 Train loss 6.97 on epoch=38
03/18/2022 00:15:35 - INFO - __main__ - Step 550 Global step 550 Train loss 6.95 on epoch=39
03/18/2022 00:17:29 - INFO - __main__ - Global step 550 Train loss 6.90 Classification-F1 0.0 on epoch=39
03/18/2022 00:17:32 - INFO - __main__ - Step 560 Global step 560 Train loss 7.30 on epoch=39
03/18/2022 00:17:35 - INFO - __main__ - Step 570 Global step 570 Train loss 7.13 on epoch=40
03/18/2022 00:17:37 - INFO - __main__ - Step 580 Global step 580 Train loss 6.80 on epoch=41
03/18/2022 00:17:40 - INFO - __main__ - Step 590 Global step 590 Train loss 6.97 on epoch=42
03/18/2022 00:17:42 - INFO - __main__ - Step 600 Global step 600 Train loss 7.15 on epoch=42
03/18/2022 00:19:08 - INFO - __main__ - Global step 600 Train loss 7.07 Classification-F1 0.0 on epoch=42
03/18/2022 00:19:10 - INFO - __main__ - Step 610 Global step 610 Train loss 7.03 on epoch=43
03/18/2022 00:19:13 - INFO - __main__ - Step 620 Global step 620 Train loss 6.97 on epoch=44
03/18/2022 00:19:15 - INFO - __main__ - Step 630 Global step 630 Train loss 6.86 on epoch=44
03/18/2022 00:19:18 - INFO - __main__ - Step 640 Global step 640 Train loss 7.09 on epoch=45
03/18/2022 00:19:20 - INFO - __main__ - Step 650 Global step 650 Train loss 6.78 on epoch=46
03/18/2022 00:21:26 - INFO - __main__ - Global step 650 Train loss 6.95 Classification-F1 0.0 on epoch=46
03/18/2022 00:21:29 - INFO - __main__ - Step 660 Global step 660 Train loss 6.87 on epoch=47
03/18/2022 00:21:32 - INFO - __main__ - Step 670 Global step 670 Train loss 6.99 on epoch=47
03/18/2022 00:21:34 - INFO - __main__ - Step 680 Global step 680 Train loss 6.72 on epoch=48
03/18/2022 00:21:37 - INFO - __main__ - Step 690 Global step 690 Train loss 6.66 on epoch=49
03/18/2022 00:21:39 - INFO - __main__ - Step 700 Global step 700 Train loss 6.61 on epoch=49
03/18/2022 00:23:44 - INFO - __main__ - Global step 700 Train loss 6.77 Classification-F1 0.0 on epoch=49
03/18/2022 00:23:46 - INFO - __main__ - Step 710 Global step 710 Train loss 6.54 on epoch=50
03/18/2022 00:23:49 - INFO - __main__ - Step 720 Global step 720 Train loss 6.45 on epoch=51
03/18/2022 00:23:52 - INFO - __main__ - Step 730 Global step 730 Train loss 6.57 on epoch=52
03/18/2022 00:23:54 - INFO - __main__ - Step 740 Global step 740 Train loss 6.62 on epoch=52
03/18/2022 00:23:57 - INFO - __main__ - Step 750 Global step 750 Train loss 6.43 on epoch=53
03/18/2022 00:25:57 - INFO - __main__ - Global step 750 Train loss 6.52 Classification-F1 0.0 on epoch=53
03/18/2022 00:25:59 - INFO - __main__ - Step 760 Global step 760 Train loss 6.48 on epoch=54
03/18/2022 00:26:02 - INFO - __main__ - Step 770 Global step 770 Train loss 6.37 on epoch=54
03/18/2022 00:26:04 - INFO - __main__ - Step 780 Global step 780 Train loss 6.52 on epoch=55
03/18/2022 00:26:07 - INFO - __main__ - Step 790 Global step 790 Train loss 6.34 on epoch=56
03/18/2022 00:26:09 - INFO - __main__ - Step 800 Global step 800 Train loss 6.42 on epoch=57
03/18/2022 00:28:02 - INFO - __main__ - Global step 800 Train loss 6.42 Classification-F1 0.0 on epoch=57
03/18/2022 00:28:04 - INFO - __main__ - Step 810 Global step 810 Train loss 6.60 on epoch=57
03/18/2022 00:28:07 - INFO - __main__ - Step 820 Global step 820 Train loss 6.33 on epoch=58
03/18/2022 00:28:09 - INFO - __main__ - Step 830 Global step 830 Train loss 6.32 on epoch=59
03/18/2022 00:28:12 - INFO - __main__ - Step 840 Global step 840 Train loss 6.24 on epoch=59
03/18/2022 00:28:15 - INFO - __main__ - Step 850 Global step 850 Train loss 6.28 on epoch=60
03/18/2022 00:30:12 - INFO - __main__ - Global step 850 Train loss 6.35 Classification-F1 0.0 on epoch=60
03/18/2022 00:30:15 - INFO - __main__ - Step 860 Global step 860 Train loss 6.18 on epoch=61
03/18/2022 00:30:17 - INFO - __main__ - Step 870 Global step 870 Train loss 6.31 on epoch=62
03/18/2022 00:30:20 - INFO - __main__ - Step 880 Global step 880 Train loss 6.30 on epoch=62
03/18/2022 00:30:22 - INFO - __main__ - Step 890 Global step 890 Train loss 6.15 on epoch=63
03/18/2022 00:30:25 - INFO - __main__ - Step 900 Global step 900 Train loss 6.13 on epoch=64
03/18/2022 00:32:20 - INFO - __main__ - Global step 900 Train loss 6.21 Classification-F1 0.0 on epoch=64
03/18/2022 00:32:23 - INFO - __main__ - Step 910 Global step 910 Train loss 6.00 on epoch=64
03/18/2022 00:32:25 - INFO - __main__ - Step 920 Global step 920 Train loss 6.20 on epoch=65
03/18/2022 00:32:28 - INFO - __main__ - Step 930 Global step 930 Train loss 6.63 on epoch=66
03/18/2022 00:32:31 - INFO - __main__ - Step 940 Global step 940 Train loss 6.68 on epoch=67
03/18/2022 00:32:33 - INFO - __main__ - Step 950 Global step 950 Train loss 6.77 on epoch=67
03/18/2022 00:34:14 - INFO - __main__ - Global step 950 Train loss 6.46 Classification-F1 0.0 on epoch=67
03/18/2022 00:34:17 - INFO - __main__ - Step 960 Global step 960 Train loss 6.67 on epoch=68
03/18/2022 00:34:19 - INFO - __main__ - Step 970 Global step 970 Train loss 6.50 on epoch=69
03/18/2022 00:34:22 - INFO - __main__ - Step 980 Global step 980 Train loss 6.31 on epoch=69
03/18/2022 00:34:24 - INFO - __main__ - Step 990 Global step 990 Train loss 6.45 on epoch=70
03/18/2022 00:34:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 6.61 on epoch=71
03/18/2022 00:36:16 - INFO - __main__ - Global step 1000 Train loss 6.51 Classification-F1 0.0 on epoch=71
03/18/2022 00:36:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 6.66 on epoch=72
03/18/2022 00:36:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 6.48 on epoch=72
03/18/2022 00:36:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 6.22 on epoch=73
03/18/2022 00:36:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 6.44 on epoch=74
03/18/2022 00:36:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 6.38 on epoch=74
03/18/2022 00:38:28 - INFO - __main__ - Global step 1050 Train loss 6.43 Classification-F1 0.0 on epoch=74
03/18/2022 00:38:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 6.42 on epoch=75
03/18/2022 00:38:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 6.12 on epoch=76
03/18/2022 00:38:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 6.19 on epoch=77
03/18/2022 00:38:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 6.27 on epoch=77
03/18/2022 00:38:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 6.10 on epoch=78
03/18/2022 00:40:39 - INFO - __main__ - Global step 1100 Train loss 6.22 Classification-F1 0.0 on epoch=78
03/18/2022 00:40:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 6.11 on epoch=79
03/18/2022 00:40:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 6.08 on epoch=79
03/18/2022 00:40:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 6.12 on epoch=80
03/18/2022 00:40:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 5.96 on epoch=81
03/18/2022 00:40:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 5.98 on epoch=82
03/18/2022 00:42:35 - INFO - __main__ - Global step 1150 Train loss 6.05 Classification-F1 0.0 on epoch=82
03/18/2022 00:42:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 6.04 on epoch=82
03/18/2022 00:42:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 6.01 on epoch=83
03/18/2022 00:42:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 5.89 on epoch=84
03/18/2022 00:42:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 5.82 on epoch=84
03/18/2022 00:42:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 5.93 on epoch=85
03/18/2022 00:44:44 - INFO - __main__ - Global step 1200 Train loss 5.94 Classification-F1 0.0 on epoch=85
03/18/2022 00:44:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 5.70 on epoch=86
03/18/2022 00:44:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 5.86 on epoch=87
03/18/2022 00:44:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 5.75 on epoch=87
03/18/2022 00:44:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 5.65 on epoch=88
03/18/2022 00:44:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 5.69 on epoch=89
03/18/2022 00:46:58 - INFO - __main__ - Global step 1250 Train loss 5.73 Classification-F1 0.0 on epoch=89
03/18/2022 00:47:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 5.57 on epoch=89
03/18/2022 00:47:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 5.63 on epoch=90
03/18/2022 00:47:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 5.55 on epoch=91
03/18/2022 00:47:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 5.47 on epoch=92
03/18/2022 00:47:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 5.44 on epoch=92
03/18/2022 00:49:04 - INFO - __main__ - Global step 1300 Train loss 5.53 Classification-F1 0.0 on epoch=92
03/18/2022 00:49:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 5.36 on epoch=93
03/18/2022 00:49:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 5.32 on epoch=94
03/18/2022 00:49:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 5.22 on epoch=94
03/18/2022 00:49:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 5.16 on epoch=95
03/18/2022 00:49:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 5.05 on epoch=96
03/18/2022 00:51:09 - INFO - __main__ - Global step 1350 Train loss 5.22 Classification-F1 0.0 on epoch=96
03/18/2022 00:51:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 5.11 on epoch=97
03/18/2022 00:51:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 5.06 on epoch=97
03/18/2022 00:51:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 4.86 on epoch=98
03/18/2022 00:51:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 4.94 on epoch=99
03/18/2022 00:51:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 4.80 on epoch=99
03/18/2022 00:52:58 - INFO - __main__ - Global step 1400 Train loss 4.95 Classification-F1 0.0 on epoch=99
03/18/2022 00:53:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 4.77 on epoch=100
03/18/2022 00:53:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 4.62 on epoch=101
03/18/2022 00:53:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 4.63 on epoch=102
03/18/2022 00:53:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 4.59 on epoch=102
03/18/2022 00:53:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 4.54 on epoch=103
03/18/2022 00:54:31 - INFO - __main__ - Global step 1450 Train loss 4.63 Classification-F1 0.0 on epoch=103
03/18/2022 00:54:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 4.44 on epoch=104
03/18/2022 00:54:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 4.36 on epoch=104
03/18/2022 00:54:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 4.38 on epoch=105
03/18/2022 00:54:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 4.34 on epoch=106
03/18/2022 00:54:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 4.31 on epoch=107
03/18/2022 00:55:26 - INFO - __main__ - Global step 1500 Train loss 4.37 Classification-F1 0.0 on epoch=107
03/18/2022 00:55:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 4.28 on epoch=107
03/18/2022 00:55:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 4.17 on epoch=108
03/18/2022 00:55:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 4.18 on epoch=109
03/18/2022 00:55:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 4.01 on epoch=109
03/18/2022 00:55:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 4.06 on epoch=110
03/18/2022 00:55:54 - INFO - __main__ - Global step 1550 Train loss 4.14 Classification-F1 0.006026365348399246 on epoch=110
03/18/2022 00:55:54 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.006026365348399246 on epoch=110, global_step=1550
03/18/2022 00:55:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.93 on epoch=111
03/18/2022 00:55:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.97 on epoch=112
03/18/2022 00:56:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.99 on epoch=112
03/18/2022 00:56:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.80 on epoch=113
03/18/2022 00:56:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.83 on epoch=114
03/18/2022 00:56:18 - INFO - __main__ - Global step 1600 Train loss 3.90 Classification-F1 0.01513480392156863 on epoch=114
03/18/2022 00:56:18 - INFO - __main__ - Saving model with best Classification-F1: 0.006026365348399246 -> 0.01513480392156863 on epoch=114, global_step=1600
03/18/2022 00:56:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 3.75 on epoch=114
03/18/2022 00:56:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 3.77 on epoch=115
03/18/2022 00:56:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 3.72 on epoch=116
03/18/2022 00:56:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 3.68 on epoch=117
03/18/2022 00:56:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 3.74 on epoch=117
03/18/2022 00:56:38 - INFO - __main__ - Global step 1650 Train loss 3.73 Classification-F1 0.03896706378661878 on epoch=117
03/18/2022 00:56:38 - INFO - __main__ - Saving model with best Classification-F1: 0.01513480392156863 -> 0.03896706378661878 on epoch=117, global_step=1650
03/18/2022 00:56:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.66 on epoch=118
03/18/2022 00:56:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 3.51 on epoch=119
03/18/2022 00:56:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 3.52 on epoch=119
03/18/2022 00:56:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 3.45 on epoch=120
03/18/2022 00:56:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 3.44 on epoch=121
03/18/2022 00:56:55 - INFO - __main__ - Global step 1700 Train loss 3.52 Classification-F1 0.04511511879932932 on epoch=121
03/18/2022 00:56:55 - INFO - __main__ - Saving model with best Classification-F1: 0.03896706378661878 -> 0.04511511879932932 on epoch=121, global_step=1700
03/18/2022 00:56:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 3.53 on epoch=122
03/18/2022 00:57:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 3.34 on epoch=122
03/18/2022 00:57:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 3.34 on epoch=123
03/18/2022 00:57:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 3.31 on epoch=124
03/18/2022 00:57:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 3.24 on epoch=124
03/18/2022 00:57:12 - INFO - __main__ - Global step 1750 Train loss 3.35 Classification-F1 0.02748362445414847 on epoch=124
03/18/2022 00:57:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 3.23 on epoch=125
03/18/2022 00:57:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 3.06 on epoch=126
03/18/2022 00:57:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 3.09 on epoch=127
03/18/2022 00:57:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 3.05 on epoch=127
03/18/2022 00:57:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 3.00 on epoch=128
03/18/2022 00:57:29 - INFO - __main__ - Global step 1800 Train loss 3.09 Classification-F1 0.029195402298850575 on epoch=128
03/18/2022 00:57:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.94 on epoch=129
03/18/2022 00:57:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.94 on epoch=129
03/18/2022 00:57:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 3.03 on epoch=130
03/18/2022 00:57:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.90 on epoch=131
03/18/2022 00:57:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.86 on epoch=132
03/18/2022 00:57:45 - INFO - __main__ - Global step 1850 Train loss 2.93 Classification-F1 0.03566507243363139 on epoch=132
03/18/2022 00:57:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.75 on epoch=132
03/18/2022 00:57:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.71 on epoch=133
03/18/2022 00:57:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.73 on epoch=134
03/18/2022 00:57:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.62 on epoch=134
03/18/2022 00:57:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.65 on epoch=135
03/18/2022 00:58:01 - INFO - __main__ - Global step 1900 Train loss 2.69 Classification-F1 0.04137935814791709 on epoch=135
03/18/2022 00:58:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.66 on epoch=136
03/18/2022 00:58:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.65 on epoch=137
03/18/2022 00:58:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.58 on epoch=137
03/18/2022 00:58:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.67 on epoch=138
03/18/2022 00:58:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.59 on epoch=139
03/18/2022 00:58:18 - INFO - __main__ - Global step 1950 Train loss 2.63 Classification-F1 0.06208497491137376 on epoch=139
03/18/2022 00:58:18 - INFO - __main__ - Saving model with best Classification-F1: 0.04511511879932932 -> 0.06208497491137376 on epoch=139, global_step=1950
03/18/2022 00:58:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.44 on epoch=139
03/18/2022 00:58:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.52 on epoch=140
03/18/2022 00:58:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 2.47 on epoch=141
03/18/2022 00:58:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.45 on epoch=142
03/18/2022 00:58:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.30 on epoch=142
03/18/2022 00:58:34 - INFO - __main__ - Global step 2000 Train loss 2.43 Classification-F1 0.09348752845147647 on epoch=142
03/18/2022 00:58:34 - INFO - __main__ - Saving model with best Classification-F1: 0.06208497491137376 -> 0.09348752845147647 on epoch=142, global_step=2000
03/18/2022 00:58:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 2.32 on epoch=143
03/18/2022 00:58:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 2.32 on epoch=144
03/18/2022 00:58:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 2.32 on epoch=144
03/18/2022 00:58:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 2.23 on epoch=145
03/18/2022 00:58:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 2.13 on epoch=146
03/18/2022 00:58:50 - INFO - __main__ - Global step 2050 Train loss 2.26 Classification-F1 0.10919965702574398 on epoch=146
03/18/2022 00:58:50 - INFO - __main__ - Saving model with best Classification-F1: 0.09348752845147647 -> 0.10919965702574398 on epoch=146, global_step=2050
03/18/2022 00:58:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 2.09 on epoch=147
03/18/2022 00:58:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 2.15 on epoch=147
03/18/2022 00:58:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 2.18 on epoch=148
03/18/2022 00:59:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 2.10 on epoch=149
03/18/2022 00:59:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 1.97 on epoch=149
03/18/2022 00:59:07 - INFO - __main__ - Global step 2100 Train loss 2.10 Classification-F1 0.12360214242567183 on epoch=149
03/18/2022 00:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.10919965702574398 -> 0.12360214242567183 on epoch=149, global_step=2100
03/18/2022 00:59:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 1.96 on epoch=150
03/18/2022 00:59:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 2.00 on epoch=151
03/18/2022 00:59:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 2.01 on epoch=152
03/18/2022 00:59:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 1.88 on epoch=152
03/18/2022 00:59:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 1.88 on epoch=153
03/18/2022 00:59:24 - INFO - __main__ - Global step 2150 Train loss 1.94 Classification-F1 0.14128658109030795 on epoch=153
03/18/2022 00:59:24 - INFO - __main__ - Saving model with best Classification-F1: 0.12360214242567183 -> 0.14128658109030795 on epoch=153, global_step=2150
03/18/2022 00:59:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 1.92 on epoch=154
03/18/2022 00:59:29 - INFO - __main__ - Step 2170 Global step 2170 Train loss 1.88 on epoch=154
03/18/2022 00:59:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 1.89 on epoch=155
03/18/2022 00:59:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 1.90 on epoch=156
03/18/2022 00:59:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 1.79 on epoch=157
03/18/2022 00:59:41 - INFO - __main__ - Global step 2200 Train loss 1.88 Classification-F1 0.1838864439133434 on epoch=157
03/18/2022 00:59:41 - INFO - __main__ - Saving model with best Classification-F1: 0.14128658109030795 -> 0.1838864439133434 on epoch=157, global_step=2200
03/18/2022 00:59:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 1.84 on epoch=157
03/18/2022 00:59:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 1.72 on epoch=158
03/18/2022 00:59:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 1.82 on epoch=159
03/18/2022 00:59:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 1.71 on epoch=159
03/18/2022 00:59:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 1.68 on epoch=160
03/18/2022 00:59:59 - INFO - __main__ - Global step 2250 Train loss 1.75 Classification-F1 0.2109360061865826 on epoch=160
03/18/2022 00:59:59 - INFO - __main__ - Saving model with best Classification-F1: 0.1838864439133434 -> 0.2109360061865826 on epoch=160, global_step=2250
03/18/2022 01:00:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 1.75 on epoch=161
03/18/2022 01:00:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 1.70 on epoch=162
03/18/2022 01:00:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 1.75 on epoch=162
03/18/2022 01:00:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 1.60 on epoch=163
03/18/2022 01:00:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 1.63 on epoch=164
03/18/2022 01:00:17 - INFO - __main__ - Global step 2300 Train loss 1.69 Classification-F1 0.2078210017928019 on epoch=164
03/18/2022 01:00:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 1.51 on epoch=164
03/18/2022 01:00:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 1.73 on epoch=165
03/18/2022 01:00:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 1.53 on epoch=166
03/18/2022 01:00:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 1.65 on epoch=167
03/18/2022 01:00:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 1.54 on epoch=167
03/18/2022 01:00:35 - INFO - __main__ - Global step 2350 Train loss 1.59 Classification-F1 0.22205649645028142 on epoch=167
03/18/2022 01:00:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2109360061865826 -> 0.22205649645028142 on epoch=167, global_step=2350
03/18/2022 01:00:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 1.55 on epoch=168
03/18/2022 01:00:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 1.50 on epoch=169
03/18/2022 01:00:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 1.48 on epoch=169
03/18/2022 01:00:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 1.50 on epoch=170
03/18/2022 01:00:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 1.44 on epoch=171
03/18/2022 01:00:53 - INFO - __main__ - Global step 2400 Train loss 1.49 Classification-F1 0.21072601579217307 on epoch=171
03/18/2022 01:00:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 1.40 on epoch=172
03/18/2022 01:00:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 1.45 on epoch=172
03/18/2022 01:01:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 1.41 on epoch=173
03/18/2022 01:01:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 1.44 on epoch=174
03/18/2022 01:01:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 1.38 on epoch=174
03/18/2022 01:01:12 - INFO - __main__ - Global step 2450 Train loss 1.42 Classification-F1 0.26283400909208215 on epoch=174
03/18/2022 01:01:12 - INFO - __main__ - Saving model with best Classification-F1: 0.22205649645028142 -> 0.26283400909208215 on epoch=174, global_step=2450
03/18/2022 01:01:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 1.44 on epoch=175
03/18/2022 01:01:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 1.34 on epoch=176
03/18/2022 01:01:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 1.43 on epoch=177
03/18/2022 01:01:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 1.36 on epoch=177
03/18/2022 01:01:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 1.32 on epoch=178
03/18/2022 01:01:30 - INFO - __main__ - Global step 2500 Train loss 1.38 Classification-F1 0.2582414168005226 on epoch=178
03/18/2022 01:01:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 1.34 on epoch=179
03/18/2022 01:01:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 1.20 on epoch=179
03/18/2022 01:01:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 1.24 on epoch=180
03/18/2022 01:01:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 1.18 on epoch=181
03/18/2022 01:01:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 1.27 on epoch=182
03/18/2022 01:01:49 - INFO - __main__ - Global step 2550 Train loss 1.25 Classification-F1 0.29905859518394423 on epoch=182
03/18/2022 01:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.26283400909208215 -> 0.29905859518394423 on epoch=182, global_step=2550
03/18/2022 01:01:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 1.15 on epoch=182
03/18/2022 01:01:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 1.13 on epoch=183
03/18/2022 01:01:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 1.15 on epoch=184
03/18/2022 01:02:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 1.27 on epoch=184
03/18/2022 01:02:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 1.26 on epoch=185
03/18/2022 01:02:09 - INFO - __main__ - Global step 2600 Train loss 1.19 Classification-F1 0.2813283031002936 on epoch=185
03/18/2022 01:02:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 1.38 on epoch=186
03/18/2022 01:02:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 1.27 on epoch=187
03/18/2022 01:02:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 1.22 on epoch=187
03/18/2022 01:02:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 1.23 on epoch=188
03/18/2022 01:02:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 1.17 on epoch=189
03/18/2022 01:02:28 - INFO - __main__ - Global step 2650 Train loss 1.25 Classification-F1 0.32603996266938223 on epoch=189
03/18/2022 01:02:28 - INFO - __main__ - Saving model with best Classification-F1: 0.29905859518394423 -> 0.32603996266938223 on epoch=189, global_step=2650
03/18/2022 01:02:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 1.21 on epoch=189
03/18/2022 01:02:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 1.20 on epoch=190
03/18/2022 01:02:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 1.12 on epoch=191
03/18/2022 01:02:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 1.17 on epoch=192
03/18/2022 01:02:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 1.08 on epoch=192
03/18/2022 01:02:47 - INFO - __main__ - Global step 2700 Train loss 1.16 Classification-F1 0.40383387398705745 on epoch=192
03/18/2022 01:02:47 - INFO - __main__ - Saving model with best Classification-F1: 0.32603996266938223 -> 0.40383387398705745 on epoch=192, global_step=2700
03/18/2022 01:02:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 1.05 on epoch=193
03/18/2022 01:02:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 1.05 on epoch=194
03/18/2022 01:02:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 1.10 on epoch=194
03/18/2022 01:02:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 1.08 on epoch=195
03/18/2022 01:03:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 1.01 on epoch=196
03/18/2022 01:03:07 - INFO - __main__ - Global step 2750 Train loss 1.06 Classification-F1 0.47849905297509937 on epoch=196
03/18/2022 01:03:07 - INFO - __main__ - Saving model with best Classification-F1: 0.40383387398705745 -> 0.47849905297509937 on epoch=196, global_step=2750
03/18/2022 01:03:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 1.01 on epoch=197
03/18/2022 01:03:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 1.09 on epoch=197
03/18/2022 01:03:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 1.02 on epoch=198
03/18/2022 01:03:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.91 on epoch=199
03/18/2022 01:03:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 1.00 on epoch=199
03/18/2022 01:03:27 - INFO - __main__ - Global step 2800 Train loss 1.01 Classification-F1 0.5668867532742806 on epoch=199
03/18/2022 01:03:27 - INFO - __main__ - Saving model with best Classification-F1: 0.47849905297509937 -> 0.5668867532742806 on epoch=199, global_step=2800
03/18/2022 01:03:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 1.01 on epoch=200
03/18/2022 01:03:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.93 on epoch=201
03/18/2022 01:03:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.99 on epoch=202
03/18/2022 01:03:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.78 on epoch=202
03/18/2022 01:03:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 1.04 on epoch=203
03/18/2022 01:03:46 - INFO - __main__ - Global step 2850 Train loss 0.95 Classification-F1 0.5486757752844192 on epoch=203
03/18/2022 01:03:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.96 on epoch=204
03/18/2022 01:03:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.99 on epoch=204
03/18/2022 01:03:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.95 on epoch=205
03/18/2022 01:03:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.93 on epoch=206
03/18/2022 01:03:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.92 on epoch=207
03/18/2022 01:04:06 - INFO - __main__ - Global step 2900 Train loss 0.95 Classification-F1 0.5579884595150574 on epoch=207
03/18/2022 01:04:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.88 on epoch=207
03/18/2022 01:04:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.87 on epoch=208
03/18/2022 01:04:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.96 on epoch=209
03/18/2022 01:04:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.93 on epoch=209
03/18/2022 01:04:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.92 on epoch=210
03/18/2022 01:04:25 - INFO - __main__ - Global step 2950 Train loss 0.91 Classification-F1 0.4497240267214964 on epoch=210
03/18/2022 01:04:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.82 on epoch=211
03/18/2022 01:04:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.86 on epoch=212
03/18/2022 01:04:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.95 on epoch=212
03/18/2022 01:04:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.83 on epoch=213
03/18/2022 01:04:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.88 on epoch=214
03/18/2022 01:04:39 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:04:39 - INFO - __main__ - Printing 3 examples
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 01:04:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:04:39 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:04:39 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:04:39 - INFO - __main__ - Printing 3 examples
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:04:39 - INFO - __main__ - ['Plant']
03/18/2022 01:04:39 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:04:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:04:39 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:04:44 - INFO - __main__ - Global step 3000 Train loss 0.87 Classification-F1 0.5415913074309298 on epoch=214
03/18/2022 01:04:44 - INFO - __main__ - save last model!
03/18/2022 01:04:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 01:04:45 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 01:04:45 - INFO - __main__ - Printing 3 examples
03/18/2022 01:04:45 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 01:04:45 - INFO - __main__ - ['Animal']
03/18/2022 01:04:45 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 01:04:45 - INFO - __main__ - ['Animal']
03/18/2022 01:04:45 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 01:04:45 - INFO - __main__ - ['Village']
03/18/2022 01:04:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:04:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:04:50 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 01:04:58 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:04:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:04:59 - INFO - __main__ - Starting training!
03/18/2022 01:06:47 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_21_0.5_8_predictions.txt
03/18/2022 01:06:47 - INFO - __main__ - Classification-F1 on test data: 0.2293
03/18/2022 01:06:49 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.5, bsz=8, dev_performance=0.5668867532742806, test_performance=0.2293142413773697
03/18/2022 01:06:49 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.4, bsz=8 ...
03/18/2022 01:06:50 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:06:50 - INFO - __main__ - Printing 3 examples
03/18/2022 01:06:50 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:06:50 - INFO - __main__ - ['Plant']
03/18/2022 01:06:50 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:06:50 - INFO - __main__ - ['Plant']
03/18/2022 01:06:50 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:06:50 - INFO - __main__ - ['Plant']
03/18/2022 01:06:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:06:50 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:06:51 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:06:51 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:06:51 - INFO - __main__ - Printing 3 examples
03/18/2022 01:06:51 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:06:51 - INFO - __main__ - ['Plant']
03/18/2022 01:06:51 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:06:51 - INFO - __main__ - ['Plant']
03/18/2022 01:06:51 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:06:51 - INFO - __main__ - ['Plant']
03/18/2022 01:06:51 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:06:51 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:06:51 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:07:06 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:07:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:07:07 - INFO - __main__ - Starting training!
03/18/2022 01:07:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.17 on epoch=0
03/18/2022 01:07:13 - INFO - __main__ - Step 20 Global step 20 Train loss 6.07 on epoch=1
03/18/2022 01:07:15 - INFO - __main__ - Step 30 Global step 30 Train loss 4.53 on epoch=2
03/18/2022 01:07:18 - INFO - __main__ - Step 40 Global step 40 Train loss 3.08 on epoch=2
03/18/2022 01:07:21 - INFO - __main__ - Step 50 Global step 50 Train loss 2.50 on epoch=3
03/18/2022 01:07:39 - INFO - __main__ - Global step 50 Train loss 4.67 Classification-F1 0.10000394041596145 on epoch=3
03/18/2022 01:07:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10000394041596145 on epoch=3, global_step=50
03/18/2022 01:07:41 - INFO - __main__ - Step 60 Global step 60 Train loss 2.16 on epoch=4
03/18/2022 01:07:44 - INFO - __main__ - Step 70 Global step 70 Train loss 1.81 on epoch=4
03/18/2022 01:07:46 - INFO - __main__ - Step 80 Global step 80 Train loss 1.48 on epoch=5
03/18/2022 01:07:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=6
03/18/2022 01:07:51 - INFO - __main__ - Step 100 Global step 100 Train loss 1.13 on epoch=7
03/18/2022 01:07:58 - INFO - __main__ - Global step 100 Train loss 1.58 Classification-F1 0.36363223377621745 on epoch=7
03/18/2022 01:07:58 - INFO - __main__ - Saving model with best Classification-F1: 0.10000394041596145 -> 0.36363223377621745 on epoch=7, global_step=100
03/18/2022 01:08:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=7
03/18/2022 01:08:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=8
03/18/2022 01:08:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=9
03/18/2022 01:08:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=9
03/18/2022 01:08:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=10
03/18/2022 01:08:17 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.5613465811183506 on epoch=10
03/18/2022 01:08:18 - INFO - __main__ - Saving model with best Classification-F1: 0.36363223377621745 -> 0.5613465811183506 on epoch=10, global_step=150
03/18/2022 01:08:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.69 on epoch=11
03/18/2022 01:08:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=12
03/18/2022 01:08:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=12
03/18/2022 01:08:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.68 on epoch=13
03/18/2022 01:08:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=14
03/18/2022 01:08:37 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5515649836841524 on epoch=14
03/18/2022 01:08:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=14
03/18/2022 01:08:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=15
03/18/2022 01:08:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=16
03/18/2022 01:08:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.66 on epoch=17
03/18/2022 01:08:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=17
03/18/2022 01:08:55 - INFO - __main__ - Global step 250 Train loss 0.61 Classification-F1 0.6802576046976375 on epoch=17
03/18/2022 01:08:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5613465811183506 -> 0.6802576046976375 on epoch=17, global_step=250
03/18/2022 01:08:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=18
03/18/2022 01:09:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=19
03/18/2022 01:09:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=19
03/18/2022 01:09:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=20
03/18/2022 01:09:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=21
03/18/2022 01:09:15 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.6440212150156488 on epoch=21
03/18/2022 01:09:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=22
03/18/2022 01:09:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=22
03/18/2022 01:09:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=23
03/18/2022 01:09:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=24
03/18/2022 01:09:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=24
03/18/2022 01:09:34 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.9002030183185715 on epoch=24
03/18/2022 01:09:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6802576046976375 -> 0.9002030183185715 on epoch=24, global_step=350
03/18/2022 01:09:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=25
03/18/2022 01:09:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=26
03/18/2022 01:09:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=27
03/18/2022 01:09:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=27
03/18/2022 01:09:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=28
03/18/2022 01:09:53 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.8561656688713963 on epoch=28
03/18/2022 01:09:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=29
03/18/2022 01:09:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=29
03/18/2022 01:10:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=30
03/18/2022 01:10:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=31
03/18/2022 01:10:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=32
03/18/2022 01:10:12 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.925586132383396 on epoch=32
03/18/2022 01:10:12 - INFO - __main__ - Saving model with best Classification-F1: 0.9002030183185715 -> 0.925586132383396 on epoch=32, global_step=450
03/18/2022 01:10:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=32
03/18/2022 01:10:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=33
03/18/2022 01:10:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=34
03/18/2022 01:10:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=34
03/18/2022 01:10:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=35
03/18/2022 01:10:31 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.762102937403564 on epoch=35
03/18/2022 01:10:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=36
03/18/2022 01:10:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=37
03/18/2022 01:10:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=37
03/18/2022 01:10:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=38
03/18/2022 01:10:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=39
03/18/2022 01:10:51 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.6515178885497472 on epoch=39
03/18/2022 01:10:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=39
03/18/2022 01:10:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=40
03/18/2022 01:10:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=41
03/18/2022 01:11:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=42
03/18/2022 01:11:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=42
03/18/2022 01:11:11 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.7337457426152278 on epoch=42
03/18/2022 01:11:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=43
03/18/2022 01:11:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=44
03/18/2022 01:11:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=44
03/18/2022 01:11:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=45
03/18/2022 01:11:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=46
03/18/2022 01:11:30 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.7205098873544011 on epoch=46
03/18/2022 01:11:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=47
03/18/2022 01:11:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=47
03/18/2022 01:11:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=48
03/18/2022 01:11:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=49
03/18/2022 01:11:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=49
03/18/2022 01:11:51 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.8065467462644882 on epoch=49
03/18/2022 01:11:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=50
03/18/2022 01:11:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=51
03/18/2022 01:11:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=52
03/18/2022 01:12:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=52
03/18/2022 01:12:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=53
03/18/2022 01:12:10 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.7552464414404921 on epoch=53
03/18/2022 01:12:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=54
03/18/2022 01:12:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=54
03/18/2022 01:12:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=55
03/18/2022 01:12:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=56
03/18/2022 01:12:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=57
03/18/2022 01:12:29 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.6897565537202897 on epoch=57
03/18/2022 01:12:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=57
03/18/2022 01:12:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/18/2022 01:12:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=59
03/18/2022 01:12:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=59
03/18/2022 01:12:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=60
03/18/2022 01:12:48 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.7504325259515571 on epoch=60
03/18/2022 01:12:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=61
03/18/2022 01:12:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=62
03/18/2022 01:12:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=62
03/18/2022 01:12:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=63
03/18/2022 01:13:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=64
03/18/2022 01:13:07 - INFO - __main__ - Global step 900 Train loss 0.14 Classification-F1 0.6413381343674722 on epoch=64
03/18/2022 01:13:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=64
03/18/2022 01:13:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=65
03/18/2022 01:13:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=66
03/18/2022 01:13:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
03/18/2022 01:13:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=67
03/18/2022 01:13:26 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.7946767538912405 on epoch=67
03/18/2022 01:13:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=68
03/18/2022 01:13:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=69
03/18/2022 01:13:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=69
03/18/2022 01:13:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=70
03/18/2022 01:13:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=71
03/18/2022 01:13:45 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.5138031922563531 on epoch=71
03/18/2022 01:13:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=72
03/18/2022 01:13:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=72
03/18/2022 01:13:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=73
03/18/2022 01:13:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=74
03/18/2022 01:13:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=74
03/18/2022 01:14:05 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.8192380831643002 on epoch=74
03/18/2022 01:14:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=75
03/18/2022 01:14:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=76
03/18/2022 01:14:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=77
03/18/2022 01:14:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=77
03/18/2022 01:14:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
03/18/2022 01:14:23 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.8147876572124 on epoch=78
03/18/2022 01:14:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
03/18/2022 01:14:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=79
03/18/2022 01:14:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=80
03/18/2022 01:14:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
03/18/2022 01:14:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=82
03/18/2022 01:14:43 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.7793348614838083 on epoch=82
03/18/2022 01:14:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=82
03/18/2022 01:14:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=83
03/18/2022 01:14:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=84
03/18/2022 01:14:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=84
03/18/2022 01:14:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=85
03/18/2022 01:15:02 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.7468727076630346 on epoch=85
03/18/2022 01:15:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=86
03/18/2022 01:15:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=87
03/18/2022 01:15:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=87
03/18/2022 01:15:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=88
03/18/2022 01:15:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=89
03/18/2022 01:15:20 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.833706979064842 on epoch=89
03/18/2022 01:15:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=89
03/18/2022 01:15:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
03/18/2022 01:15:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=91
03/18/2022 01:15:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=92
03/18/2022 01:15:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=92
03/18/2022 01:15:39 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.9013009123492994 on epoch=92
03/18/2022 01:15:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=93
03/18/2022 01:15:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=94
03/18/2022 01:15:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
03/18/2022 01:15:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=95
03/18/2022 01:15:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=96
03/18/2022 01:15:58 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.7147182017029687 on epoch=96
03/18/2022 01:16:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=97
03/18/2022 01:16:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=97
03/18/2022 01:16:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=98
03/18/2022 01:16:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=99
03/18/2022 01:16:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=99
03/18/2022 01:16:17 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.9732792729455689 on epoch=99
03/18/2022 01:16:17 - INFO - __main__ - Saving model with best Classification-F1: 0.925586132383396 -> 0.9732792729455689 on epoch=99, global_step=1400
03/18/2022 01:16:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=100
03/18/2022 01:16:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=101
03/18/2022 01:16:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=102
03/18/2022 01:16:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=102
03/18/2022 01:16:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=103
03/18/2022 01:16:36 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.8969303319129923 on epoch=103
03/18/2022 01:16:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=104
03/18/2022 01:16:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
03/18/2022 01:16:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=105
03/18/2022 01:16:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=106
03/18/2022 01:16:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=107
03/18/2022 01:16:55 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.9730221707451311 on epoch=107
03/18/2022 01:16:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=107
03/18/2022 01:17:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
03/18/2022 01:17:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=109
03/18/2022 01:17:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=109
03/18/2022 01:17:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
03/18/2022 01:17:13 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.789631531977266 on epoch=110
03/18/2022 01:17:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=111
03/18/2022 01:17:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=112
03/18/2022 01:17:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=112
03/18/2022 01:17:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=113
03/18/2022 01:17:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=114
03/18/2022 01:17:32 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6511350627207405 on epoch=114
03/18/2022 01:17:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
03/18/2022 01:17:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=115
03/18/2022 01:17:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=116
03/18/2022 01:17:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=117
03/18/2022 01:17:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=117
03/18/2022 01:17:51 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.9103127096390854 on epoch=117
03/18/2022 01:17:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=118
03/18/2022 01:17:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
03/18/2022 01:17:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=119
03/18/2022 01:18:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=120
03/18/2022 01:18:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=121
03/18/2022 01:18:10 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7513801610575805 on epoch=121
03/18/2022 01:18:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=122
03/18/2022 01:18:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=122
03/18/2022 01:18:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=123
03/18/2022 01:18:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/18/2022 01:18:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
03/18/2022 01:18:29 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.9821297653958945 on epoch=124
03/18/2022 01:18:29 - INFO - __main__ - Saving model with best Classification-F1: 0.9732792729455689 -> 0.9821297653958945 on epoch=124, global_step=1750
03/18/2022 01:18:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
03/18/2022 01:18:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=126
03/18/2022 01:18:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=127
03/18/2022 01:18:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=127
03/18/2022 01:18:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=128
03/18/2022 01:18:48 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7061570057613133 on epoch=128
03/18/2022 01:18:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=129
03/18/2022 01:18:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
03/18/2022 01:18:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/18/2022 01:18:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=131
03/18/2022 01:19:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
03/18/2022 01:19:07 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.9185272075594656 on epoch=132
03/18/2022 01:19:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=132
03/18/2022 01:19:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=133
03/18/2022 01:19:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=134
03/18/2022 01:19:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
03/18/2022 01:19:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=135
03/18/2022 01:19:26 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.8511562194525905 on epoch=135
03/18/2022 01:19:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=136
03/18/2022 01:19:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=137
03/18/2022 01:19:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
03/18/2022 01:19:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/18/2022 01:19:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=139
03/18/2022 01:19:45 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6344658187768287 on epoch=139
03/18/2022 01:19:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=139
03/18/2022 01:19:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
03/18/2022 01:19:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
03/18/2022 01:19:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=142
03/18/2022 01:19:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=142
03/18/2022 01:20:04 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.8570908174486804 on epoch=142
03/18/2022 01:20:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 01:20:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=144
03/18/2022 01:20:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=144
03/18/2022 01:20:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=145
03/18/2022 01:20:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
03/18/2022 01:20:23 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.9865984150258343 on epoch=146
03/18/2022 01:20:23 - INFO - __main__ - Saving model with best Classification-F1: 0.9821297653958945 -> 0.9865984150258343 on epoch=146, global_step=2050
03/18/2022 01:20:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
03/18/2022 01:20:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
03/18/2022 01:20:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/18/2022 01:20:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
03/18/2022 01:20:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
03/18/2022 01:20:42 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.968810623315629 on epoch=149
03/18/2022 01:20:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=150
03/18/2022 01:20:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
03/18/2022 01:20:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
03/18/2022 01:20:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=152
03/18/2022 01:20:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
03/18/2022 01:21:00 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.855335105083089 on epoch=153
03/18/2022 01:21:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=154
03/18/2022 01:21:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/18/2022 01:21:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/18/2022 01:21:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=156
03/18/2022 01:21:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
03/18/2022 01:21:19 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7358131041598784 on epoch=157
03/18/2022 01:21:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/18/2022 01:21:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=158
03/18/2022 01:21:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=159
03/18/2022 01:21:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
03/18/2022 01:21:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
03/18/2022 01:21:38 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7266101879005105 on epoch=160
03/18/2022 01:21:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=161
03/18/2022 01:21:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
03/18/2022 01:21:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=162
03/18/2022 01:21:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
03/18/2022 01:21:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/18/2022 01:21:56 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7155954909013863 on epoch=164
03/18/2022 01:21:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
03/18/2022 01:22:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/18/2022 01:22:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
03/18/2022 01:22:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
03/18/2022 01:22:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
03/18/2022 01:22:15 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6065670843117305 on epoch=167
03/18/2022 01:22:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=168
03/18/2022 01:22:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/18/2022 01:22:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/18/2022 01:22:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=170
03/18/2022 01:22:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
03/18/2022 01:22:33 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7951266791399618 on epoch=171
03/18/2022 01:22:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=172
03/18/2022 01:22:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=172
03/18/2022 01:22:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=173
03/18/2022 01:22:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=174
03/18/2022 01:22:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/18/2022 01:22:52 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7470996718256746 on epoch=174
03/18/2022 01:22:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
03/18/2022 01:22:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=176
03/18/2022 01:23:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
03/18/2022 01:23:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=177
03/18/2022 01:23:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=178
03/18/2022 01:23:11 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6066839318170162 on epoch=178
03/18/2022 01:23:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
03/18/2022 01:23:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/18/2022 01:23:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/18/2022 01:23:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/18/2022 01:23:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
03/18/2022 01:23:29 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7797994132240114 on epoch=182
03/18/2022 01:23:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
03/18/2022 01:23:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 01:23:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
03/18/2022 01:23:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/18/2022 01:23:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
03/18/2022 01:23:48 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9819717916492109 on epoch=185
03/18/2022 01:23:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=186
03/18/2022 01:23:53 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
03/18/2022 01:23:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/18/2022 01:23:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=188
03/18/2022 01:24:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
03/18/2022 01:24:07 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.9865940511101802 on epoch=189
03/18/2022 01:24:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=189
03/18/2022 01:24:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
03/18/2022 01:24:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
03/18/2022 01:24:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/18/2022 01:24:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=192
03/18/2022 01:24:25 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.8448060591961801 on epoch=192
03/18/2022 01:24:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/18/2022 01:24:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/18/2022 01:24:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=194
03/18/2022 01:24:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/18/2022 01:24:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=196
03/18/2022 01:24:44 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.851004088086089 on epoch=196
03/18/2022 01:24:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
03/18/2022 01:24:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 01:24:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/18/2022 01:24:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
03/18/2022 01:24:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
03/18/2022 01:25:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9021775357259227 on epoch=199
03/18/2022 01:25:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/18/2022 01:25:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
03/18/2022 01:25:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
03/18/2022 01:25:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=202
03/18/2022 01:25:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/18/2022 01:25:21 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9051422175031741 on epoch=203
03/18/2022 01:25:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
03/18/2022 01:25:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/18/2022 01:25:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
03/18/2022 01:25:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=206
03/18/2022 01:25:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/18/2022 01:25:40 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.906987269003398 on epoch=207
03/18/2022 01:25:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=207
03/18/2022 01:25:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/18/2022 01:25:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
03/18/2022 01:25:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
03/18/2022 01:25:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=210
03/18/2022 01:25:59 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7765721161674951 on epoch=210
03/18/2022 01:26:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=211
03/18/2022 01:26:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/18/2022 01:26:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=212
03/18/2022 01:26:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/18/2022 01:26:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=214
03/18/2022 01:26:13 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:26:13 - INFO - __main__ - Printing 3 examples
03/18/2022 01:26:13 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:26:13 - INFO - __main__ - ['Plant']
03/18/2022 01:26:13 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:26:13 - INFO - __main__ - ['Plant']
03/18/2022 01:26:13 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:26:13 - INFO - __main__ - ['Plant']
03/18/2022 01:26:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 01:26:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:26:14 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:26:14 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:26:14 - INFO - __main__ - Printing 3 examples
03/18/2022 01:26:14 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:26:14 - INFO - __main__ - ['Plant']
03/18/2022 01:26:14 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:26:14 - INFO - __main__ - ['Plant']
03/18/2022 01:26:14 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:26:14 - INFO - __main__ - ['Plant']
03/18/2022 01:26:14 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:26:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:26:14 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:26:18 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.9077131601958207 on epoch=214
03/18/2022 01:26:18 - INFO - __main__ - save last model!
03/18/2022 01:26:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 01:26:18 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 01:26:18 - INFO - __main__ - Printing 3 examples
03/18/2022 01:26:18 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 01:26:18 - INFO - __main__ - ['Animal']
03/18/2022 01:26:18 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 01:26:18 - INFO - __main__ - ['Animal']
03/18/2022 01:26:18 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 01:26:18 - INFO - __main__ - ['Village']
03/18/2022 01:26:18 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:26:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:26:23 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 01:26:30 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:26:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:26:31 - INFO - __main__ - Starting training!
03/18/2022 01:28:25 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_21_0.4_8_predictions.txt
03/18/2022 01:28:25 - INFO - __main__ - Classification-F1 on test data: 0.5371
03/18/2022 01:28:26 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.4, bsz=8, dev_performance=0.9865984150258343, test_performance=0.5371332550433522
03/18/2022 01:28:26 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.3, bsz=8 ...
03/18/2022 01:28:27 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:28:27 - INFO - __main__ - Printing 3 examples
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:28:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:28:27 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:28:27 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:28:27 - INFO - __main__ - Printing 3 examples
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:28:27 - INFO - __main__ - ['Plant']
03/18/2022 01:28:27 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:28:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:28:27 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:28:42 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:28:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:28:43 - INFO - __main__ - Starting training!
03/18/2022 01:28:48 - INFO - __main__ - Step 10 Global step 10 Train loss 7.14 on epoch=0
03/18/2022 01:28:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.42 on epoch=1
03/18/2022 01:28:53 - INFO - __main__ - Step 30 Global step 30 Train loss 6.55 on epoch=2
03/18/2022 01:28:56 - INFO - __main__ - Step 40 Global step 40 Train loss 6.51 on epoch=2
03/18/2022 01:28:58 - INFO - __main__ - Step 50 Global step 50 Train loss 6.24 on epoch=3
03/18/2022 01:31:01 - INFO - __main__ - Global step 50 Train loss 6.57 Classification-F1 0.0 on epoch=3
03/18/2022 01:31:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/18/2022 01:31:04 - INFO - __main__ - Step 60 Global step 60 Train loss 6.32 on epoch=4
03/18/2022 01:31:06 - INFO - __main__ - Step 70 Global step 70 Train loss 6.17 on epoch=4
03/18/2022 01:31:09 - INFO - __main__ - Step 80 Global step 80 Train loss 6.23 on epoch=5
03/18/2022 01:31:11 - INFO - __main__ - Step 90 Global step 90 Train loss 5.99 on epoch=6
03/18/2022 01:31:14 - INFO - __main__ - Step 100 Global step 100 Train loss 6.03 on epoch=7
03/18/2022 01:33:15 - INFO - __main__ - Global step 100 Train loss 6.15 Classification-F1 0.0 on epoch=7
03/18/2022 01:33:17 - INFO - __main__ - Step 110 Global step 110 Train loss 6.01 on epoch=7
03/18/2022 01:33:20 - INFO - __main__ - Step 120 Global step 120 Train loss 5.78 on epoch=8
03/18/2022 01:33:22 - INFO - __main__ - Step 130 Global step 130 Train loss 5.76 on epoch=9
03/18/2022 01:33:25 - INFO - __main__ - Step 140 Global step 140 Train loss 5.41 on epoch=9
03/18/2022 01:33:27 - INFO - __main__ - Step 150 Global step 150 Train loss 5.12 on epoch=10
03/18/2022 01:35:25 - INFO - __main__ - Global step 150 Train loss 5.62 Classification-F1 0.0 on epoch=10
03/18/2022 01:35:28 - INFO - __main__ - Step 160 Global step 160 Train loss 4.68 on epoch=11
03/18/2022 01:35:30 - INFO - __main__ - Step 170 Global step 170 Train loss 4.39 on epoch=12
03/18/2022 01:35:33 - INFO - __main__ - Step 180 Global step 180 Train loss 4.05 on epoch=12
03/18/2022 01:35:35 - INFO - __main__ - Step 190 Global step 190 Train loss 3.75 on epoch=13
03/18/2022 01:35:38 - INFO - __main__ - Step 200 Global step 200 Train loss 3.37 on epoch=14
03/18/2022 01:36:25 - INFO - __main__ - Global step 200 Train loss 4.05 Classification-F1 0.015089949972368786 on epoch=14
03/18/2022 01:36:25 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.015089949972368786 on epoch=14, global_step=200
03/18/2022 01:36:27 - INFO - __main__ - Step 210 Global step 210 Train loss 3.13 on epoch=14
03/18/2022 01:36:30 - INFO - __main__ - Step 220 Global step 220 Train loss 2.85 on epoch=15
03/18/2022 01:36:32 - INFO - __main__ - Step 230 Global step 230 Train loss 2.82 on epoch=16
03/18/2022 01:36:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.62 on epoch=17
03/18/2022 01:36:37 - INFO - __main__ - Step 250 Global step 250 Train loss 2.54 on epoch=17
03/18/2022 01:37:03 - INFO - __main__ - Global step 250 Train loss 2.79 Classification-F1 0.07278523082718887 on epoch=17
03/18/2022 01:37:03 - INFO - __main__ - Saving model with best Classification-F1: 0.015089949972368786 -> 0.07278523082718887 on epoch=17, global_step=250
03/18/2022 01:37:05 - INFO - __main__ - Step 260 Global step 260 Train loss 2.40 on epoch=18
03/18/2022 01:37:08 - INFO - __main__ - Step 270 Global step 270 Train loss 2.30 on epoch=19
03/18/2022 01:37:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.95 on epoch=19
03/18/2022 01:37:13 - INFO - __main__ - Step 290 Global step 290 Train loss 2.02 on epoch=20
03/18/2022 01:37:15 - INFO - __main__ - Step 300 Global step 300 Train loss 1.80 on epoch=21
03/18/2022 01:37:29 - INFO - __main__ - Global step 300 Train loss 2.09 Classification-F1 0.21095424266446414 on epoch=21
03/18/2022 01:37:29 - INFO - __main__ - Saving model with best Classification-F1: 0.07278523082718887 -> 0.21095424266446414 on epoch=21, global_step=300
03/18/2022 01:37:31 - INFO - __main__ - Step 310 Global step 310 Train loss 1.90 on epoch=22
03/18/2022 01:37:34 - INFO - __main__ - Step 320 Global step 320 Train loss 1.70 on epoch=22
03/18/2022 01:37:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.73 on epoch=23
03/18/2022 01:37:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.50 on epoch=24
03/18/2022 01:37:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.38 on epoch=24
03/18/2022 01:37:52 - INFO - __main__ - Global step 350 Train loss 1.64 Classification-F1 0.35250696154064237 on epoch=24
03/18/2022 01:37:52 - INFO - __main__ - Saving model with best Classification-F1: 0.21095424266446414 -> 0.35250696154064237 on epoch=24, global_step=350
03/18/2022 01:37:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.31 on epoch=25
03/18/2022 01:37:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.38 on epoch=26
03/18/2022 01:38:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.22 on epoch=27
03/18/2022 01:38:02 - INFO - __main__ - Step 390 Global step 390 Train loss 1.14 on epoch=27
03/18/2022 01:38:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.04 on epoch=28
03/18/2022 01:38:13 - INFO - __main__ - Global step 400 Train loss 1.22 Classification-F1 0.4440908091492946 on epoch=28
03/18/2022 01:38:13 - INFO - __main__ - Saving model with best Classification-F1: 0.35250696154064237 -> 0.4440908091492946 on epoch=28, global_step=400
03/18/2022 01:38:16 - INFO - __main__ - Step 410 Global step 410 Train loss 1.15 on epoch=29
03/18/2022 01:38:19 - INFO - __main__ - Step 420 Global step 420 Train loss 1.07 on epoch=29
03/18/2022 01:38:21 - INFO - __main__ - Step 430 Global step 430 Train loss 1.01 on epoch=30
03/18/2022 01:38:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=31
03/18/2022 01:38:26 - INFO - __main__ - Step 450 Global step 450 Train loss 1.00 on epoch=32
03/18/2022 01:38:37 - INFO - __main__ - Global step 450 Train loss 1.03 Classification-F1 0.41555230374400753 on epoch=32
03/18/2022 01:38:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.92 on epoch=32
03/18/2022 01:38:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.95 on epoch=33
03/18/2022 01:38:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=34
03/18/2022 01:38:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.81 on epoch=34
03/18/2022 01:38:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=35
03/18/2022 01:38:55 - INFO - __main__ - Global step 500 Train loss 0.89 Classification-F1 0.4619467182617088 on epoch=35
03/18/2022 01:38:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4440908091492946 -> 0.4619467182617088 on epoch=35, global_step=500
03/18/2022 01:38:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.75 on epoch=36
03/18/2022 01:39:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=37
03/18/2022 01:39:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=37
03/18/2022 01:39:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=38
03/18/2022 01:39:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.79 on epoch=39
03/18/2022 01:39:15 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.5876031901259451 on epoch=39
03/18/2022 01:39:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4619467182617088 -> 0.5876031901259451 on epoch=39, global_step=550
03/18/2022 01:39:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=39
03/18/2022 01:39:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.69 on epoch=40
03/18/2022 01:39:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=41
03/18/2022 01:39:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.62 on epoch=42
03/18/2022 01:39:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=42
03/18/2022 01:39:35 - INFO - __main__ - Global step 600 Train loss 0.64 Classification-F1 0.5862992699899721 on epoch=42
03/18/2022 01:39:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=43
03/18/2022 01:39:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=44
03/18/2022 01:39:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=44
03/18/2022 01:39:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=45
03/18/2022 01:39:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.58 on epoch=46
03/18/2022 01:39:54 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.5796087236664487 on epoch=46
03/18/2022 01:39:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=47
03/18/2022 01:39:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=47
03/18/2022 01:40:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=48
03/18/2022 01:40:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.60 on epoch=49
03/18/2022 01:40:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=49
03/18/2022 01:40:13 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.6292766126372242 on epoch=49
03/18/2022 01:40:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5876031901259451 -> 0.6292766126372242 on epoch=49, global_step=700
03/18/2022 01:40:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=50
03/18/2022 01:40:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=51
03/18/2022 01:40:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=52
03/18/2022 01:40:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=52
03/18/2022 01:40:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=53
03/18/2022 01:40:32 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.7077311322256176 on epoch=53
03/18/2022 01:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6292766126372242 -> 0.7077311322256176 on epoch=53, global_step=750
03/18/2022 01:40:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=54
03/18/2022 01:40:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=54
03/18/2022 01:40:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=55
03/18/2022 01:40:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=56
03/18/2022 01:40:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=57
03/18/2022 01:40:51 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.7043795741342165 on epoch=57
03/18/2022 01:40:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=57
03/18/2022 01:40:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=58
03/18/2022 01:40:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=59
03/18/2022 01:41:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=59
03/18/2022 01:41:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=60
03/18/2022 01:41:11 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.6739597458375547 on epoch=60
03/18/2022 01:41:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=61
03/18/2022 01:41:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=62
03/18/2022 01:41:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=62
03/18/2022 01:41:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=63
03/18/2022 01:41:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=64
03/18/2022 01:41:30 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.6940918167617577 on epoch=64
03/18/2022 01:41:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=64
03/18/2022 01:41:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=65
03/18/2022 01:41:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=66
03/18/2022 01:41:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=67
03/18/2022 01:41:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=67
03/18/2022 01:41:49 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.8240326360715465 on epoch=67
03/18/2022 01:41:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7077311322256176 -> 0.8240326360715465 on epoch=67, global_step=950
03/18/2022 01:41:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=68
03/18/2022 01:41:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=69
03/18/2022 01:41:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=69
03/18/2022 01:42:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=70
03/18/2022 01:42:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=71
03/18/2022 01:42:09 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.6426714859427333 on epoch=71
03/18/2022 01:42:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=72
03/18/2022 01:42:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=72
03/18/2022 01:42:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=73
03/18/2022 01:42:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=74
03/18/2022 01:42:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=74
03/18/2022 01:42:28 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.8718772287456796 on epoch=74
03/18/2022 01:42:28 - INFO - __main__ - Saving model with best Classification-F1: 0.8240326360715465 -> 0.8718772287456796 on epoch=74, global_step=1050
03/18/2022 01:42:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=75
03/18/2022 01:42:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=76
03/18/2022 01:42:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=77
03/18/2022 01:42:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=77
03/18/2022 01:42:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=78
03/18/2022 01:42:48 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.6964789263800548 on epoch=78
03/18/2022 01:42:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=79
03/18/2022 01:42:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=79
03/18/2022 01:42:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=80
03/18/2022 01:42:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=81
03/18/2022 01:43:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=82
03/18/2022 01:43:07 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.7287035679374388 on epoch=82
03/18/2022 01:43:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=82
03/18/2022 01:43:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=83
03/18/2022 01:43:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=84
03/18/2022 01:43:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=84
03/18/2022 01:43:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=85
03/18/2022 01:43:26 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.6567766757795152 on epoch=85
03/18/2022 01:43:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=86
03/18/2022 01:43:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=87
03/18/2022 01:43:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=87
03/18/2022 01:43:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=88
03/18/2022 01:43:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=89
03/18/2022 01:43:45 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.8282538149563091 on epoch=89
03/18/2022 01:43:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=89
03/18/2022 01:43:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=90
03/18/2022 01:43:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=91
03/18/2022 01:43:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=92
03/18/2022 01:43:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=92
03/18/2022 01:44:03 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.6718706016556741 on epoch=92
03/18/2022 01:44:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=93
03/18/2022 01:44:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=94
03/18/2022 01:44:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
03/18/2022 01:44:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=95
03/18/2022 01:44:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=96
03/18/2022 01:44:22 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.7909023140047808 on epoch=96
03/18/2022 01:44:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
03/18/2022 01:44:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=97
03/18/2022 01:44:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=98
03/18/2022 01:44:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=99
03/18/2022 01:44:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=99
03/18/2022 01:44:41 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.8432659326560537 on epoch=99
03/18/2022 01:44:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=100
03/18/2022 01:44:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=101
03/18/2022 01:44:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=102
03/18/2022 01:44:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=102
03/18/2022 01:44:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=103
03/18/2022 01:45:00 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.767421040324843 on epoch=103
03/18/2022 01:45:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=104
03/18/2022 01:45:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=104
03/18/2022 01:45:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=105
03/18/2022 01:45:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=106
03/18/2022 01:45:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=107
03/18/2022 01:45:18 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.9686792018860331 on epoch=107
03/18/2022 01:45:18 - INFO - __main__ - Saving model with best Classification-F1: 0.8718772287456796 -> 0.9686792018860331 on epoch=107, global_step=1500
03/18/2022 01:45:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=107
03/18/2022 01:45:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=108
03/18/2022 01:45:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=109
03/18/2022 01:45:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=109
03/18/2022 01:45:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=110
03/18/2022 01:45:37 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.9185272075594656 on epoch=110
03/18/2022 01:45:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=111
03/18/2022 01:45:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=112
03/18/2022 01:45:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=112
03/18/2022 01:45:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=113
03/18/2022 01:45:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=114
03/18/2022 01:45:55 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.8313433877950007 on epoch=114
03/18/2022 01:45:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=114
03/18/2022 01:46:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=115
03/18/2022 01:46:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=116
03/18/2022 01:46:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=117
03/18/2022 01:46:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=117
03/18/2022 01:46:14 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.8471323619257087 on epoch=117
03/18/2022 01:46:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=118
03/18/2022 01:46:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=119
03/18/2022 01:46:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=119
03/18/2022 01:46:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=120
03/18/2022 01:46:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=121
03/18/2022 01:46:33 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8448060591961801 on epoch=121
03/18/2022 01:46:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=122
03/18/2022 01:46:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=122
03/18/2022 01:46:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=123
03/18/2022 01:46:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=124
03/18/2022 01:46:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=124
03/18/2022 01:46:51 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.75453587104441 on epoch=124
03/18/2022 01:46:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=125
03/18/2022 01:46:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=126
03/18/2022 01:46:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=127
03/18/2022 01:47:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=127
03/18/2022 01:47:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
03/18/2022 01:47:10 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7956293755016193 on epoch=128
03/18/2022 01:47:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=129
03/18/2022 01:47:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=129
03/18/2022 01:47:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/18/2022 01:47:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=131
03/18/2022 01:47:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=132
03/18/2022 01:47:28 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.8349287808965228 on epoch=132
03/18/2022 01:47:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=132
03/18/2022 01:47:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=133
03/18/2022 01:47:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=134
03/18/2022 01:47:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=134
03/18/2022 01:47:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=135
03/18/2022 01:47:47 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.9146301726946888 on epoch=135
03/18/2022 01:47:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=136
03/18/2022 01:47:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
03/18/2022 01:47:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=137
03/18/2022 01:47:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=138
03/18/2022 01:48:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=139
03/18/2022 01:48:06 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.7501245304359876 on epoch=139
03/18/2022 01:48:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=139
03/18/2022 01:48:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=140
03/18/2022 01:48:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=141
03/18/2022 01:48:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=142
03/18/2022 01:48:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=142
03/18/2022 01:48:24 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.7089690162695256 on epoch=142
03/18/2022 01:48:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 01:48:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=144
03/18/2022 01:48:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=144
03/18/2022 01:48:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=145
03/18/2022 01:48:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=146
03/18/2022 01:48:43 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7006700147616955 on epoch=146
03/18/2022 01:48:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=147
03/18/2022 01:48:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=147
03/18/2022 01:48:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=148
03/18/2022 01:48:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
03/18/2022 01:48:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
03/18/2022 01:49:02 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.914058971247514 on epoch=149
03/18/2022 01:49:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=150
03/18/2022 01:49:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
03/18/2022 01:49:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=152
03/18/2022 01:49:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=152
03/18/2022 01:49:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=153
03/18/2022 01:49:20 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.8488566733511587 on epoch=153
03/18/2022 01:49:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
03/18/2022 01:49:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
03/18/2022 01:49:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=155
03/18/2022 01:49:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=156
03/18/2022 01:49:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=157
03/18/2022 01:49:39 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.838185365075486 on epoch=157
03/18/2022 01:49:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=157
03/18/2022 01:49:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=158
03/18/2022 01:49:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=159
03/18/2022 01:49:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=159
03/18/2022 01:49:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=160
03/18/2022 01:49:58 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.756250678831324 on epoch=160
03/18/2022 01:50:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=161
03/18/2022 01:50:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
03/18/2022 01:50:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=162
03/18/2022 01:50:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/18/2022 01:50:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=164
03/18/2022 01:50:16 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.9185272075594656 on epoch=164
03/18/2022 01:50:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
03/18/2022 01:50:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/18/2022 01:50:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
03/18/2022 01:50:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
03/18/2022 01:50:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=167
03/18/2022 01:50:35 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.8529325513196482 on epoch=167
03/18/2022 01:50:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
03/18/2022 01:50:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=169
03/18/2022 01:50:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
03/18/2022 01:50:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
03/18/2022 01:50:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=171
03/18/2022 01:50:53 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.9014156747743084 on epoch=171
03/18/2022 01:50:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
03/18/2022 01:50:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
03/18/2022 01:51:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
03/18/2022 01:51:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
03/18/2022 01:51:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
03/18/2022 01:51:12 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.9100684261974583 on epoch=174
03/18/2022 01:51:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
03/18/2022 01:51:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
03/18/2022 01:51:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
03/18/2022 01:51:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=177
03/18/2022 01:51:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=178
03/18/2022 01:51:31 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.9057215649262368 on epoch=178
03/18/2022 01:51:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
03/18/2022 01:51:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=179
03/18/2022 01:51:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=180
03/18/2022 01:51:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=181
03/18/2022 01:51:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
03/18/2022 01:51:49 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.9076032853563443 on epoch=182
03/18/2022 01:51:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=182
03/18/2022 01:51:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 01:51:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=184
03/18/2022 01:51:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
03/18/2022 01:52:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=185
03/18/2022 01:52:08 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.8463111369535175 on epoch=185
03/18/2022 01:52:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=186
03/18/2022 01:52:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=187
03/18/2022 01:52:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
03/18/2022 01:52:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
03/18/2022 01:52:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=189
03/18/2022 01:52:26 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.8458362789764 on epoch=189
03/18/2022 01:52:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 01:52:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
03/18/2022 01:52:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=191
03/18/2022 01:52:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/18/2022 01:52:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/18/2022 01:52:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.910202834799609 on epoch=192
03/18/2022 01:52:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=193
03/18/2022 01:52:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/18/2022 01:52:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/18/2022 01:52:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=195
03/18/2022 01:52:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=196
03/18/2022 01:53:04 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.8424888283759252 on epoch=196
03/18/2022 01:53:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=197
03/18/2022 01:53:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 01:53:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
03/18/2022 01:53:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=199
03/18/2022 01:53:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
03/18/2022 01:53:22 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.9022844936823432 on epoch=199
03/18/2022 01:53:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/18/2022 01:53:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=201
03/18/2022 01:53:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=202
03/18/2022 01:53:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
03/18/2022 01:53:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=203
03/18/2022 01:53:41 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.9684042751998161 on epoch=203
03/18/2022 01:53:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=204
03/18/2022 01:53:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=204
03/18/2022 01:53:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=205
03/18/2022 01:53:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
03/18/2022 01:53:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=207
03/18/2022 01:54:00 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.9059017595307919 on epoch=207
03/18/2022 01:54:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
03/18/2022 01:54:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/18/2022 01:54:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=209
03/18/2022 01:54:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/18/2022 01:54:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
03/18/2022 01:54:19 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9819901200949587 on epoch=210
03/18/2022 01:54:19 - INFO - __main__ - Saving model with best Classification-F1: 0.9686792018860331 -> 0.9819901200949587 on epoch=210, global_step=2950
03/18/2022 01:54:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
03/18/2022 01:54:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/18/2022 01:54:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/18/2022 01:54:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/18/2022 01:54:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=214
03/18/2022 01:54:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:54:33 - INFO - __main__ - Printing 3 examples
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 01:54:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:54:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:54:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:54:33 - INFO - __main__ - Printing 3 examples
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:54:33 - INFO - __main__ - ['Plant']
03/18/2022 01:54:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:54:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:54:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:54:38 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.9775075059349252 on epoch=214
03/18/2022 01:54:38 - INFO - __main__ - save last model!
03/18/2022 01:54:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 01:54:38 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 01:54:38 - INFO - __main__ - Printing 3 examples
03/18/2022 01:54:38 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 01:54:38 - INFO - __main__ - ['Animal']
03/18/2022 01:54:38 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 01:54:38 - INFO - __main__ - ['Animal']
03/18/2022 01:54:38 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 01:54:38 - INFO - __main__ - ['Village']
03/18/2022 01:54:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:54:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:54:43 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 01:54:49 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:54:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:54:50 - INFO - __main__ - Starting training!
03/18/2022 01:56:48 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_21_0.3_8_predictions.txt
03/18/2022 01:56:48 - INFO - __main__ - Classification-F1 on test data: 0.6504
03/18/2022 01:56:50 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.3, bsz=8, dev_performance=0.9819901200949587, test_performance=0.6503704189945287
03/18/2022 01:56:50 - INFO - __main__ - Running ... prefix=dbpedia_14_16_21, lr=0.2, bsz=8 ...
03/18/2022 01:56:51 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:56:51 - INFO - __main__ - Printing 3 examples
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] Symplocos octopetala is a species of plant in the Symplocaceae family. It is endemic to Jamaica.
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] Walsura is a genus of plant in family Meliaceae. It contains the following species (but this list may be incomplete): Walsura gardneri Thwaites Walsura pinnata Hassk. Walsura trifoliate Walsura
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] Cystopteris is a genus of ferns in the family Cystopteridaceae. These are known generally as bladderferns or fragile ferns. They are found in temperate areas worldwide. This is a very diverse genus and within a species individuals can look quite different especially in harsh environments where they experience stress and remain small and stunted. Also they hybridize easily with each other. Identifying an individual can be challenging.
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 01:56:51 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:56:51 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 01:56:51 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 01:56:51 - INFO - __main__ - Printing 3 examples
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] Bellis annua or the annual daisy is a species of the genus Bellis.
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] Carduus acanthoides known as the spiny plumeless thistle welted thistle and plumeless thistle is a biennial plant species of thistle in the Asteraceae—sunflower family. The plant is native to Europe and Asia.
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ -  [dbpedia_14] 'Gympie Gold' is a hybrid cultivar of the genus Aechmea in the Bromeliad family.
03/18/2022 01:56:51 - INFO - __main__ - ['Plant']
03/18/2022 01:56:51 - INFO - __main__ - Tokenizing Input ...
03/18/2022 01:56:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 01:56:52 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 01:57:10 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 01:57:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 01:57:11 - INFO - __main__ - Starting training!
03/18/2022 01:57:15 - INFO - __main__ - Step 10 Global step 10 Train loss 7.39 on epoch=0
03/18/2022 01:57:17 - INFO - __main__ - Step 20 Global step 20 Train loss 6.61 on epoch=1
03/18/2022 01:57:20 - INFO - __main__ - Step 30 Global step 30 Train loss 6.11 on epoch=2
03/18/2022 01:57:23 - INFO - __main__ - Step 40 Global step 40 Train loss 5.31 on epoch=2
03/18/2022 01:57:25 - INFO - __main__ - Step 50 Global step 50 Train loss 4.21 on epoch=3
03/18/2022 01:58:25 - INFO - __main__ - Global step 50 Train loss 5.93 Classification-F1 0.006119211467643984 on epoch=3
03/18/2022 01:58:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.006119211467643984 on epoch=3, global_step=50
03/18/2022 01:58:28 - INFO - __main__ - Step 60 Global step 60 Train loss 3.46 on epoch=4
03/18/2022 01:58:31 - INFO - __main__ - Step 70 Global step 70 Train loss 2.95 on epoch=4
03/18/2022 01:58:33 - INFO - __main__ - Step 80 Global step 80 Train loss 2.54 on epoch=5
03/18/2022 01:58:36 - INFO - __main__ - Step 90 Global step 90 Train loss 2.18 on epoch=6
03/18/2022 01:58:38 - INFO - __main__ - Step 100 Global step 100 Train loss 2.06 on epoch=7
03/18/2022 01:58:45 - INFO - __main__ - Global step 100 Train loss 2.64 Classification-F1 0.20169895239894498 on epoch=7
03/18/2022 01:58:45 - INFO - __main__ - Saving model with best Classification-F1: 0.006119211467643984 -> 0.20169895239894498 on epoch=7, global_step=100
03/18/2022 01:58:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.79 on epoch=7
03/18/2022 01:58:50 - INFO - __main__ - Step 120 Global step 120 Train loss 1.54 on epoch=8
03/18/2022 01:58:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.46 on epoch=9
03/18/2022 01:58:55 - INFO - __main__ - Step 140 Global step 140 Train loss 1.38 on epoch=9
03/18/2022 01:58:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.39 on epoch=10
03/18/2022 01:59:04 - INFO - __main__ - Global step 150 Train loss 1.51 Classification-F1 0.41835185102047945 on epoch=10
03/18/2022 01:59:04 - INFO - __main__ - Saving model with best Classification-F1: 0.20169895239894498 -> 0.41835185102047945 on epoch=10, global_step=150
03/18/2022 01:59:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.20 on epoch=11
03/18/2022 01:59:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.17 on epoch=12
03/18/2022 01:59:12 - INFO - __main__ - Step 180 Global step 180 Train loss 1.13 on epoch=12
03/18/2022 01:59:14 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=13
03/18/2022 01:59:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=14
03/18/2022 01:59:27 - INFO - __main__ - Global step 200 Train loss 1.11 Classification-F1 0.3531646250600418 on epoch=14
03/18/2022 01:59:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=14
03/18/2022 01:59:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.99 on epoch=15
03/18/2022 01:59:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=16
03/18/2022 01:59:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=17
03/18/2022 01:59:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=17
03/18/2022 01:59:48 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.4733083474921428 on epoch=17
03/18/2022 01:59:48 - INFO - __main__ - Saving model with best Classification-F1: 0.41835185102047945 -> 0.4733083474921428 on epoch=17, global_step=250
03/18/2022 01:59:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=18
03/18/2022 01:59:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=19
03/18/2022 01:59:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=19
03/18/2022 01:59:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=20
03/18/2022 02:00:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=21
03/18/2022 02:00:08 - INFO - __main__ - Global step 300 Train loss 0.78 Classification-F1 0.5313272495855108 on epoch=21
03/18/2022 02:00:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4733083474921428 -> 0.5313272495855108 on epoch=21, global_step=300
03/18/2022 02:00:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.89 on epoch=22
03/18/2022 02:00:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=22
03/18/2022 02:00:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=23
03/18/2022 02:00:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=24
03/18/2022 02:00:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=24
03/18/2022 02:00:28 - INFO - __main__ - Global step 350 Train loss 0.75 Classification-F1 0.6688175495879455 on epoch=24
03/18/2022 02:00:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5313272495855108 -> 0.6688175495879455 on epoch=24, global_step=350
03/18/2022 02:00:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.70 on epoch=25
03/18/2022 02:00:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.66 on epoch=26
03/18/2022 02:00:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=27
03/18/2022 02:00:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=27
03/18/2022 02:00:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=28
03/18/2022 02:00:47 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.6035722471621427 on epoch=28
03/18/2022 02:00:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=29
03/18/2022 02:00:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=29
03/18/2022 02:00:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=30
03/18/2022 02:00:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=31
03/18/2022 02:01:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=32
03/18/2022 02:01:06 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.6910770125931417 on epoch=32
03/18/2022 02:01:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6688175495879455 -> 0.6910770125931417 on epoch=32, global_step=450
03/18/2022 02:01:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=32
03/18/2022 02:01:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=33
03/18/2022 02:01:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=34
03/18/2022 02:01:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.58 on epoch=34
03/18/2022 02:01:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=35
03/18/2022 02:01:25 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.721559267967068 on epoch=35
03/18/2022 02:01:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6910770125931417 -> 0.721559267967068 on epoch=35, global_step=500
03/18/2022 02:01:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=36
03/18/2022 02:01:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.59 on epoch=37
03/18/2022 02:01:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=37
03/18/2022 02:01:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=38
03/18/2022 02:01:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=39
03/18/2022 02:01:45 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.7564129443755928 on epoch=39
03/18/2022 02:01:45 - INFO - __main__ - Saving model with best Classification-F1: 0.721559267967068 -> 0.7564129443755928 on epoch=39, global_step=550
03/18/2022 02:01:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.57 on epoch=39
03/18/2022 02:01:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=40
03/18/2022 02:01:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=41
03/18/2022 02:01:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=42
03/18/2022 02:01:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=42
03/18/2022 02:02:04 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.7605654223983728 on epoch=42
03/18/2022 02:02:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7564129443755928 -> 0.7605654223983728 on epoch=42, global_step=600
03/18/2022 02:02:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=43
03/18/2022 02:02:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=44
03/18/2022 02:02:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=44
03/18/2022 02:02:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=45
03/18/2022 02:02:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=46
03/18/2022 02:02:24 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.7084173576593917 on epoch=46
03/18/2022 02:02:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=47
03/18/2022 02:02:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=47
03/18/2022 02:02:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=48
03/18/2022 02:02:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=49
03/18/2022 02:02:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=49
03/18/2022 02:02:43 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.7200290657606896 on epoch=49
03/18/2022 02:02:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=50
03/18/2022 02:02:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=51
03/18/2022 02:02:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=52
03/18/2022 02:02:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=52
03/18/2022 02:02:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=53
03/18/2022 02:03:02 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.7714929898588643 on epoch=53
03/18/2022 02:03:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7605654223983728 -> 0.7714929898588643 on epoch=53, global_step=750
03/18/2022 02:03:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=54
03/18/2022 02:03:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=54
03/18/2022 02:03:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=55
03/18/2022 02:03:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=56
03/18/2022 02:03:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=57
03/18/2022 02:03:21 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.7863473085285505 on epoch=57
03/18/2022 02:03:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7714929898588643 -> 0.7863473085285505 on epoch=57, global_step=800
03/18/2022 02:03:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=57
03/18/2022 02:03:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=58
03/18/2022 02:03:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=59
03/18/2022 02:03:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=59
03/18/2022 02:03:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=60
03/18/2022 02:03:41 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.7430874573575512 on epoch=60
03/18/2022 02:03:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=61
03/18/2022 02:03:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=62
03/18/2022 02:03:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=62
03/18/2022 02:03:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=63
03/18/2022 02:03:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=64
03/18/2022 02:04:00 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.7092266065087173 on epoch=64
03/18/2022 02:04:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=64
03/18/2022 02:04:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=65
03/18/2022 02:04:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=66
03/18/2022 02:04:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=67
03/18/2022 02:04:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=67
03/18/2022 02:04:19 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.6448339183949557 on epoch=67
03/18/2022 02:04:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=68
03/18/2022 02:04:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=69
03/18/2022 02:04:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=69
03/18/2022 02:04:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=70
03/18/2022 02:04:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=71
03/18/2022 02:04:38 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.5982817170253504 on epoch=71
03/18/2022 02:04:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=72
03/18/2022 02:04:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=72
03/18/2022 02:04:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=73
03/18/2022 02:04:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=74
03/18/2022 02:04:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=74
03/18/2022 02:05:01 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7524922705356751 on epoch=74
03/18/2022 02:05:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=75
03/18/2022 02:05:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=76
03/18/2022 02:05:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=77
03/18/2022 02:05:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=77
03/18/2022 02:05:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=78
03/18/2022 02:05:21 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.5372663256626159 on epoch=78
03/18/2022 02:05:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=79
03/18/2022 02:05:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=79
03/18/2022 02:05:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=80
03/18/2022 02:05:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=81
03/18/2022 02:05:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=82
03/18/2022 02:05:41 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.6588598403290165 on epoch=82
03/18/2022 02:05:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=82
03/18/2022 02:05:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=83
03/18/2022 02:05:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=84
03/18/2022 02:05:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=84
03/18/2022 02:05:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=85
03/18/2022 02:06:00 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.5523150227038385 on epoch=85
03/18/2022 02:06:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=86
03/18/2022 02:06:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=87
03/18/2022 02:06:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=87
03/18/2022 02:06:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=88
03/18/2022 02:06:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=89
03/18/2022 02:06:19 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.5358085551675993 on epoch=89
03/18/2022 02:06:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=89
03/18/2022 02:06:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=90
03/18/2022 02:06:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=91
03/18/2022 02:06:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=92
03/18/2022 02:06:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=92
03/18/2022 02:06:39 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.7806948349839145 on epoch=92
03/18/2022 02:06:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=93
03/18/2022 02:06:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=94
03/18/2022 02:06:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=94
03/18/2022 02:06:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=95
03/18/2022 02:06:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=96
03/18/2022 02:07:00 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.7361735011716473 on epoch=96
03/18/2022 02:07:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=97
03/18/2022 02:07:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=97
03/18/2022 02:07:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=98
03/18/2022 02:07:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=99
03/18/2022 02:07:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=99
03/18/2022 02:07:23 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.6311972153065432 on epoch=99
03/18/2022 02:07:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=100
03/18/2022 02:07:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=101
03/18/2022 02:07:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=102
03/18/2022 02:07:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=102
03/18/2022 02:07:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=103
03/18/2022 02:07:43 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.6034453377228771 on epoch=103
03/18/2022 02:07:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=104
03/18/2022 02:07:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=104
03/18/2022 02:07:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=105
03/18/2022 02:07:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=106
03/18/2022 02:07:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=107
03/18/2022 02:08:04 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.8232819747141216 on epoch=107
03/18/2022 02:08:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7863473085285505 -> 0.8232819747141216 on epoch=107, global_step=1500
03/18/2022 02:08:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=107
03/18/2022 02:08:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=108
03/18/2022 02:08:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=109
03/18/2022 02:08:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=109
03/18/2022 02:08:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=110
03/18/2022 02:08:23 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.7717334558932876 on epoch=110
03/18/2022 02:08:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=111
03/18/2022 02:08:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=112
03/18/2022 02:08:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=112
03/18/2022 02:08:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=113
03/18/2022 02:08:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=114
03/18/2022 02:08:42 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.5864311678058013 on epoch=114
03/18/2022 02:08:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=114
03/18/2022 02:08:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=115
03/18/2022 02:08:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=116
03/18/2022 02:08:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=117
03/18/2022 02:08:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=117
03/18/2022 02:09:01 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.6972423727941554 on epoch=117
03/18/2022 02:09:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=118
03/18/2022 02:09:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=119
03/18/2022 02:09:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=119
03/18/2022 02:09:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=120
03/18/2022 02:09:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=121
03/18/2022 02:09:20 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7988305732850326 on epoch=121
03/18/2022 02:09:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=122
03/18/2022 02:09:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=122
03/18/2022 02:09:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=123
03/18/2022 02:09:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=124
03/18/2022 02:09:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=124
03/18/2022 02:09:40 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.7993740604746298 on epoch=124
03/18/2022 02:09:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
03/18/2022 02:09:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=126
03/18/2022 02:09:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=127
03/18/2022 02:09:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=127
03/18/2022 02:09:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=128
03/18/2022 02:09:59 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.759845769523189 on epoch=128
03/18/2022 02:10:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=129
03/18/2022 02:10:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=129
03/18/2022 02:10:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=130
03/18/2022 02:10:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=131
03/18/2022 02:10:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=132
03/18/2022 02:10:22 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.8396325603083112 on epoch=132
03/18/2022 02:10:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8232819747141216 -> 0.8396325603083112 on epoch=132, global_step=1850
03/18/2022 02:10:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=132
03/18/2022 02:10:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=133
03/18/2022 02:10:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=134
03/18/2022 02:10:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=134
03/18/2022 02:10:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=135
03/18/2022 02:10:41 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.8554534762952102 on epoch=135
03/18/2022 02:10:41 - INFO - __main__ - Saving model with best Classification-F1: 0.8396325603083112 -> 0.8554534762952102 on epoch=135, global_step=1900
03/18/2022 02:10:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=136
03/18/2022 02:10:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
03/18/2022 02:10:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=137
03/18/2022 02:10:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=138
03/18/2022 02:10:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=139
03/18/2022 02:11:01 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.9776304656760065 on epoch=139
03/18/2022 02:11:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8554534762952102 -> 0.9776304656760065 on epoch=139, global_step=1950
03/18/2022 02:11:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=139
03/18/2022 02:11:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
03/18/2022 02:11:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=141
03/18/2022 02:11:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=142
03/18/2022 02:11:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=142
03/18/2022 02:11:20 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.8028707377379104 on epoch=142
03/18/2022 02:11:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 02:11:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
03/18/2022 02:11:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=144
03/18/2022 02:11:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=145
03/18/2022 02:11:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=146
03/18/2022 02:11:39 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.9186746497230369 on epoch=146
03/18/2022 02:11:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
03/18/2022 02:11:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=147
03/18/2022 02:11:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=148
03/18/2022 02:11:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=149
03/18/2022 02:11:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
03/18/2022 02:11:58 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.9163766699250571 on epoch=149
03/18/2022 02:12:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
03/18/2022 02:12:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=151
03/18/2022 02:12:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=152
03/18/2022 02:12:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=152
03/18/2022 02:12:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=153
03/18/2022 02:12:17 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.851259990799839 on epoch=153
03/18/2022 02:12:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
03/18/2022 02:12:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/18/2022 02:12:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=155
03/18/2022 02:12:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=156
03/18/2022 02:12:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
03/18/2022 02:12:37 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.8452116037030648 on epoch=157
03/18/2022 02:12:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=157
03/18/2022 02:12:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
03/18/2022 02:12:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=159
03/18/2022 02:12:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
03/18/2022 02:12:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=160
03/18/2022 02:12:56 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7232494726552452 on epoch=160
03/18/2022 02:12:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=161
03/18/2022 02:13:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=162
03/18/2022 02:13:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
03/18/2022 02:13:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/18/2022 02:13:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=164
03/18/2022 02:13:15 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.8514020362543845 on epoch=164
03/18/2022 02:13:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
03/18/2022 02:13:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/18/2022 02:13:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=166
03/18/2022 02:13:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=167
03/18/2022 02:13:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=167
03/18/2022 02:13:34 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7956654623400354 on epoch=167
03/18/2022 02:13:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
03/18/2022 02:13:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=169
03/18/2022 02:13:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=169
03/18/2022 02:13:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=170
03/18/2022 02:13:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=171
03/18/2022 02:13:54 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.8545315502073011 on epoch=171
03/18/2022 02:13:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
03/18/2022 02:13:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=172
03/18/2022 02:14:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=173
03/18/2022 02:14:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
03/18/2022 02:14:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/18/2022 02:14:13 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.914360540892799 on epoch=174
03/18/2022 02:14:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
03/18/2022 02:14:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=176
03/18/2022 02:14:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=177
03/18/2022 02:14:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=177
03/18/2022 02:14:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=178
03/18/2022 02:14:33 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.9186705767350929 on epoch=178
03/18/2022 02:14:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=179
03/18/2022 02:14:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=179
03/18/2022 02:14:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
03/18/2022 02:14:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=181
03/18/2022 02:14:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
03/18/2022 02:14:52 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.9822527251369758 on epoch=182
03/18/2022 02:14:52 - INFO - __main__ - Saving model with best Classification-F1: 0.9776304656760065 -> 0.9822527251369758 on epoch=182, global_step=2550
03/18/2022 02:14:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/18/2022 02:14:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=183
03/18/2022 02:14:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=184
03/18/2022 02:15:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/18/2022 02:15:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=185
03/18/2022 02:15:12 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.9865940511101802 on epoch=185
03/18/2022 02:15:12 - INFO - __main__ - Saving model with best Classification-F1: 0.9822527251369758 -> 0.9865940511101802 on epoch=185, global_step=2600
03/18/2022 02:15:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/18/2022 02:15:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
03/18/2022 02:15:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
03/18/2022 02:15:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=188
03/18/2022 02:15:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=189
03/18/2022 02:15:31 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9822527251369758 on epoch=189
03/18/2022 02:15:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=189
03/18/2022 02:15:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=190
03/18/2022 02:15:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=191
03/18/2022 02:15:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=192
03/18/2022 02:15:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=192
03/18/2022 02:15:54 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.8461291290913564 on epoch=192
03/18/2022 02:15:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
03/18/2022 02:16:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/18/2022 02:16:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
03/18/2022 02:16:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=195
03/18/2022 02:16:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=196
03/18/2022 02:16:17 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7980345037982609 on epoch=196
03/18/2022 02:16:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
03/18/2022 02:16:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
03/18/2022 02:16:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=198
03/18/2022 02:16:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=199
03/18/2022 02:16:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/18/2022 02:16:40 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.9821297653958945 on epoch=199
03/18/2022 02:16:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
03/18/2022 02:16:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
03/18/2022 02:16:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
03/18/2022 02:16:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=202
03/18/2022 02:16:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
03/18/2022 02:17:05 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.9823428491631258 on epoch=203
03/18/2022 02:17:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/18/2022 02:17:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=204
03/18/2022 02:17:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
03/18/2022 02:17:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=206
03/18/2022 02:17:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/18/2022 02:17:26 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.804190711992922 on epoch=207
03/18/2022 02:17:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
03/18/2022 02:17:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=208
03/18/2022 02:17:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/18/2022 02:17:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/18/2022 02:17:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=210
03/18/2022 02:17:51 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9186460429724188 on epoch=210
03/18/2022 02:17:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
03/18/2022 02:17:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=212
03/18/2022 02:17:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=212
03/18/2022 02:18:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=213
03/18/2022 02:18:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
03/18/2022 02:18:05 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:18:05 - INFO - __main__ - Printing 3 examples
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 02:18:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:18:05 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 02:18:05 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:18:05 - INFO - __main__ - Printing 3 examples
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 02:18:05 - INFO - __main__ - ['Company']
03/18/2022 02:18:05 - INFO - __main__ - Tokenizing Input ...
03/18/2022 02:18:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:18:06 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 02:18:13 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.9058601192187531 on epoch=214
03/18/2022 02:18:13 - INFO - __main__ - save last model!
03/18/2022 02:18:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 02:18:13 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 02:18:13 - INFO - __main__ - Printing 3 examples
03/18/2022 02:18:13 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 02:18:13 - INFO - __main__ - ['Animal']
03/18/2022 02:18:13 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 02:18:13 - INFO - __main__ - ['Animal']
03/18/2022 02:18:13 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 02:18:13 - INFO - __main__ - ['Village']
03/18/2022 02:18:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 02:18:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:18:19 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 02:18:24 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 02:18:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 02:18:25 - INFO - __main__ - Starting training!
03/18/2022 02:21:25 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_21_0.2_8_predictions.txt
03/18/2022 02:21:25 - INFO - __main__ - Classification-F1 on test data: 0.5418
03/18/2022 02:21:25 - INFO - __main__ - prefix=dbpedia_14_16_21, lr=0.2, bsz=8, dev_performance=0.9865940511101802, test_performance=0.5418385389752004
03/18/2022 02:21:25 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.5, bsz=8 ...
03/18/2022 02:21:26 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:21:26 - INFO - __main__ - Printing 3 examples
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 02:21:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:21:26 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 02:21:26 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:21:26 - INFO - __main__ - Printing 3 examples
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 02:21:26 - INFO - __main__ - ['Company']
03/18/2022 02:21:26 - INFO - __main__ - Tokenizing Input ...
03/18/2022 02:21:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:21:27 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 02:21:42 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 02:21:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 02:21:43 - INFO - __main__ - Starting training!
03/18/2022 02:21:47 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=0
03/18/2022 02:21:50 - INFO - __main__ - Step 20 Global step 20 Train loss 6.92 on epoch=1
03/18/2022 02:21:53 - INFO - __main__ - Step 30 Global step 30 Train loss 6.40 on epoch=2
03/18/2022 02:21:55 - INFO - __main__ - Step 40 Global step 40 Train loss 5.92 on epoch=2
03/18/2022 02:21:58 - INFO - __main__ - Step 50 Global step 50 Train loss 6.17 on epoch=3
03/18/2022 02:23:42 - INFO - __main__ - Global step 50 Train loss 6.40 Classification-F1 0.0 on epoch=3
03/18/2022 02:23:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/18/2022 02:23:44 - INFO - __main__ - Step 60 Global step 60 Train loss 5.58 on epoch=4
03/18/2022 02:23:47 - INFO - __main__ - Step 70 Global step 70 Train loss 5.21 on epoch=4
03/18/2022 02:23:50 - INFO - __main__ - Step 80 Global step 80 Train loss 4.11 on epoch=5
03/18/2022 02:23:52 - INFO - __main__ - Step 90 Global step 90 Train loss 3.55 on epoch=6
03/18/2022 02:23:55 - INFO - __main__ - Step 100 Global step 100 Train loss 2.87 on epoch=7
03/18/2022 02:24:05 - INFO - __main__ - Global step 100 Train loss 4.26 Classification-F1 0.04455141957156462 on epoch=7
03/18/2022 02:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.04455141957156462 on epoch=7, global_step=100
03/18/2022 02:24:08 - INFO - __main__ - Step 110 Global step 110 Train loss 2.40 on epoch=7
03/18/2022 02:24:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.98 on epoch=8
03/18/2022 02:24:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.87 on epoch=9
03/18/2022 02:24:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.63 on epoch=9
03/18/2022 02:24:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.43 on epoch=10
03/18/2022 02:24:25 - INFO - __main__ - Global step 150 Train loss 1.86 Classification-F1 0.286760512299386 on epoch=10
03/18/2022 02:24:25 - INFO - __main__ - Saving model with best Classification-F1: 0.04455141957156462 -> 0.286760512299386 on epoch=10, global_step=150
03/18/2022 02:24:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.46 on epoch=11
03/18/2022 02:24:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.33 on epoch=12
03/18/2022 02:24:32 - INFO - __main__ - Step 180 Global step 180 Train loss 1.16 on epoch=12
03/18/2022 02:24:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.15 on epoch=13
03/18/2022 02:24:38 - INFO - __main__ - Step 200 Global step 200 Train loss 1.16 on epoch=14
03/18/2022 02:24:44 - INFO - __main__ - Global step 200 Train loss 1.25 Classification-F1 0.49861477036792556 on epoch=14
03/18/2022 02:24:45 - INFO - __main__ - Saving model with best Classification-F1: 0.286760512299386 -> 0.49861477036792556 on epoch=14, global_step=200
03/18/2022 02:24:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.95 on epoch=14
03/18/2022 02:24:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=15
03/18/2022 02:24:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=16
03/18/2022 02:24:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=17
03/18/2022 02:24:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=17
03/18/2022 02:25:05 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.45473370424966464 on epoch=17
03/18/2022 02:25:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=18
03/18/2022 02:25:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=19
03/18/2022 02:25:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=19
03/18/2022 02:25:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.67 on epoch=20
03/18/2022 02:25:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=21
03/18/2022 02:25:25 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.599878861678669 on epoch=21
03/18/2022 02:25:25 - INFO - __main__ - Saving model with best Classification-F1: 0.49861477036792556 -> 0.599878861678669 on epoch=21, global_step=300
03/18/2022 02:25:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=22
03/18/2022 02:25:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.65 on epoch=22
03/18/2022 02:25:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=23
03/18/2022 02:25:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.63 on epoch=24
03/18/2022 02:25:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=24
03/18/2022 02:25:44 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.6991279533600299 on epoch=24
03/18/2022 02:25:44 - INFO - __main__ - Saving model with best Classification-F1: 0.599878861678669 -> 0.6991279533600299 on epoch=24, global_step=350
03/18/2022 02:25:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=25
03/18/2022 02:25:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=26
03/18/2022 02:25:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=27
03/18/2022 02:25:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=27
03/18/2022 02:25:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=28
03/18/2022 02:26:03 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.622797891699087 on epoch=28
03/18/2022 02:26:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=29
03/18/2022 02:26:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=29
03/18/2022 02:26:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=30
03/18/2022 02:26:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=31
03/18/2022 02:26:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=32
03/18/2022 02:26:22 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.6393466627711702 on epoch=32
03/18/2022 02:26:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=32
03/18/2022 02:26:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=33
03/18/2022 02:26:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=34
03/18/2022 02:26:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=34
03/18/2022 02:26:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=35
03/18/2022 02:26:40 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.5979422017728008 on epoch=35
03/18/2022 02:26:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=36
03/18/2022 02:26:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=37
03/18/2022 02:26:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=37
03/18/2022 02:26:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=38
03/18/2022 02:26:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=39
03/18/2022 02:27:00 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.6716200724792787 on epoch=39
03/18/2022 02:27:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=39
03/18/2022 02:27:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=40
03/18/2022 02:27:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=41
03/18/2022 02:27:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=42
03/18/2022 02:27:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=42
03/18/2022 02:27:18 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.5655255359014237 on epoch=42
03/18/2022 02:27:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=43
03/18/2022 02:27:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=44
03/18/2022 02:27:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=44
03/18/2022 02:27:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=45
03/18/2022 02:27:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=46
03/18/2022 02:27:37 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.4762492580698241 on epoch=46
03/18/2022 02:27:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=47
03/18/2022 02:27:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=47
03/18/2022 02:27:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=48
03/18/2022 02:27:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=49
03/18/2022 02:27:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=49
03/18/2022 02:27:55 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.38389876331357403 on epoch=49
03/18/2022 02:27:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=50
03/18/2022 02:28:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=51
03/18/2022 02:28:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=52
03/18/2022 02:28:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=52
03/18/2022 02:28:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=53
03/18/2022 02:28:14 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.7914091163938167 on epoch=53
03/18/2022 02:28:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6991279533600299 -> 0.7914091163938167 on epoch=53, global_step=750
03/18/2022 02:28:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=54
03/18/2022 02:28:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=54
03/18/2022 02:28:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=55
03/18/2022 02:28:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=56
03/18/2022 02:28:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=57
03/18/2022 02:28:32 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.6497547479402318 on epoch=57
03/18/2022 02:28:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=57
03/18/2022 02:28:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/18/2022 02:28:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=59
03/18/2022 02:28:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=59
03/18/2022 02:28:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=60
03/18/2022 02:28:51 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.6130663777319786 on epoch=60
03/18/2022 02:28:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=61
03/18/2022 02:28:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=62
03/18/2022 02:28:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=62
03/18/2022 02:29:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=63
03/18/2022 02:29:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=64
03/18/2022 02:29:10 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.6299682423524474 on epoch=64
03/18/2022 02:29:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=64
03/18/2022 02:29:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=65
03/18/2022 02:29:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=66
03/18/2022 02:29:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=67
03/18/2022 02:29:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
03/18/2022 02:29:28 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.5875358392163331 on epoch=67
03/18/2022 02:29:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=68
03/18/2022 02:29:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=69
03/18/2022 02:29:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=69
03/18/2022 02:29:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=70
03/18/2022 02:29:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=71
03/18/2022 02:29:47 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.7414818952453361 on epoch=71
03/18/2022 02:29:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=72
03/18/2022 02:29:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=72
03/18/2022 02:29:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=73
03/18/2022 02:29:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=74
03/18/2022 02:29:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=74
03/18/2022 02:30:05 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.8493227202904623 on epoch=74
03/18/2022 02:30:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7914091163938167 -> 0.8493227202904623 on epoch=74, global_step=1050
03/18/2022 02:30:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=75
03/18/2022 02:30:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=76
03/18/2022 02:30:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=77
03/18/2022 02:30:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=77
03/18/2022 02:30:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
03/18/2022 02:30:23 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.96861073886527 on epoch=78
03/18/2022 02:30:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8493227202904623 -> 0.96861073886527 on epoch=78, global_step=1100
03/18/2022 02:30:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
03/18/2022 02:30:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=79
03/18/2022 02:30:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
03/18/2022 02:30:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=81
03/18/2022 02:30:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=82
03/18/2022 02:30:42 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.8196921458010168 on epoch=82
03/18/2022 02:30:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=82
03/18/2022 02:30:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=83
03/18/2022 02:30:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=84
03/18/2022 02:30:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=84
03/18/2022 02:30:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=85
03/18/2022 02:31:00 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.8492476801937785 on epoch=85
03/18/2022 02:31:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=86
03/18/2022 02:31:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=87
03/18/2022 02:31:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=87
03/18/2022 02:31:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=88
03/18/2022 02:31:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=89
03/18/2022 02:31:19 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.9865940511101802 on epoch=89
03/18/2022 02:31:19 - INFO - __main__ - Saving model with best Classification-F1: 0.96861073886527 -> 0.9865940511101802 on epoch=89, global_step=1250
03/18/2022 02:31:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=89
03/18/2022 02:31:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=90
03/18/2022 02:31:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=91
03/18/2022 02:31:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=92
03/18/2022 02:31:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=92
03/18/2022 02:31:37 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.9865940511101802 on epoch=92
03/18/2022 02:31:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=93
03/18/2022 02:31:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=94
03/18/2022 02:31:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
03/18/2022 02:31:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=95
03/18/2022 02:31:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=96
03/18/2022 02:31:56 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.9100594656239817 on epoch=96
03/18/2022 02:31:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=97
03/18/2022 02:32:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=97
03/18/2022 02:32:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=98
03/18/2022 02:32:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=99
03/18/2022 02:32:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=99
03/18/2022 02:32:14 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.9163766699250568 on epoch=99
03/18/2022 02:32:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=100
03/18/2022 02:32:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=101
03/18/2022 02:32:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=102
03/18/2022 02:32:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
03/18/2022 02:32:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=103
03/18/2022 02:32:33 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.7950365517951186 on epoch=103
03/18/2022 02:32:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=104
03/18/2022 02:32:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=104
03/18/2022 02:32:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=105
03/18/2022 02:32:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=106
03/18/2022 02:32:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=107
03/18/2022 02:32:52 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.97780076106689 on epoch=107
03/18/2022 02:32:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
03/18/2022 02:32:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=108
03/18/2022 02:32:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=109
03/18/2022 02:33:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=109
03/18/2022 02:33:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=110
03/18/2022 02:33:10 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.9120625610948191 on epoch=110
03/18/2022 02:33:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=111
03/18/2022 02:33:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
03/18/2022 02:33:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=112
03/18/2022 02:33:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=113
03/18/2022 02:33:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=114
03/18/2022 02:33:29 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.9104635060280221 on epoch=114
03/18/2022 02:33:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=114
03/18/2022 02:33:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
03/18/2022 02:33:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=116
03/18/2022 02:33:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=117
03/18/2022 02:33:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=117
03/18/2022 02:33:48 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7815636600029389 on epoch=117
03/18/2022 02:33:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
03/18/2022 02:33:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=119
03/18/2022 02:33:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=119
03/18/2022 02:33:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=120
03/18/2022 02:34:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=121
03/18/2022 02:34:09 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.9122100032583904 on epoch=121
03/18/2022 02:34:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=122
03/18/2022 02:34:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=122
03/18/2022 02:34:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=123
03/18/2022 02:34:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=124
03/18/2022 02:34:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
03/18/2022 02:34:28 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.9732519400722166 on epoch=124
03/18/2022 02:34:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=125
03/18/2022 02:34:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=126
03/18/2022 02:34:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=127
03/18/2022 02:34:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=127
03/18/2022 02:34:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=128
03/18/2022 02:34:47 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.9865940511101802 on epoch=128
03/18/2022 02:34:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=129
03/18/2022 02:34:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
03/18/2022 02:34:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/18/2022 02:34:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=131
03/18/2022 02:35:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=132
03/18/2022 02:35:08 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.9819717916492109 on epoch=132
03/18/2022 02:35:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=132
03/18/2022 02:35:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=133
03/18/2022 02:35:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=134
03/18/2022 02:35:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=134
03/18/2022 02:35:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
03/18/2022 02:35:32 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.9186705767350927 on epoch=135
03/18/2022 02:35:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=136
03/18/2022 02:35:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
03/18/2022 02:35:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
03/18/2022 02:35:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/18/2022 02:35:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=139
03/18/2022 02:35:52 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.9059945278209035 on epoch=139
03/18/2022 02:35:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=139
03/18/2022 02:35:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
03/18/2022 02:36:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=141
03/18/2022 02:36:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
03/18/2022 02:36:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=142
03/18/2022 02:36:12 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9041613265269179 on epoch=142
03/18/2022 02:36:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=143
03/18/2022 02:36:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=144
03/18/2022 02:36:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=144
03/18/2022 02:36:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=145
03/18/2022 02:36:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=146
03/18/2022 02:36:31 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.9143646138807429 on epoch=146
03/18/2022 02:36:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
03/18/2022 02:36:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
03/18/2022 02:36:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/18/2022 02:36:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=149
03/18/2022 02:36:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
03/18/2022 02:36:50 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7901949603562507 on epoch=149
03/18/2022 02:36:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
03/18/2022 02:36:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=151
03/18/2022 02:36:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=152
03/18/2022 02:36:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=152
03/18/2022 02:37:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
03/18/2022 02:37:09 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8859004794488664 on epoch=153
03/18/2022 02:37:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=154
03/18/2022 02:37:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/18/2022 02:37:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=155
03/18/2022 02:37:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=156
03/18/2022 02:37:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=157
03/18/2022 02:37:28 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.8384787689831488 on epoch=157
03/18/2022 02:37:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/18/2022 02:37:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=158
03/18/2022 02:37:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/18/2022 02:37:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=159
03/18/2022 02:37:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=160
03/18/2022 02:37:46 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7027445623956843 on epoch=160
03/18/2022 02:37:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/18/2022 02:37:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
03/18/2022 02:37:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
03/18/2022 02:37:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/18/2022 02:37:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/18/2022 02:38:05 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6914161471509962 on epoch=164
03/18/2022 02:38:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
03/18/2022 02:38:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
03/18/2022 02:38:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
03/18/2022 02:38:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
03/18/2022 02:38:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=167
03/18/2022 02:38:24 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7942989526802375 on epoch=167
03/18/2022 02:38:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/18/2022 02:38:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/18/2022 02:38:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/18/2022 02:38:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
03/18/2022 02:38:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
03/18/2022 02:38:42 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8507436714194223 on epoch=171
03/18/2022 02:38:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=172
03/18/2022 02:38:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
03/18/2022 02:38:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=173
03/18/2022 02:38:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
03/18/2022 02:38:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=174
03/18/2022 02:39:01 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7391150321321098 on epoch=174
03/18/2022 02:39:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=175
03/18/2022 02:39:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
03/18/2022 02:39:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=177
03/18/2022 02:39:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/18/2022 02:39:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
03/18/2022 02:39:19 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7005951569669712 on epoch=178
03/18/2022 02:39:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
03/18/2022 02:39:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/18/2022 02:39:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/18/2022 02:39:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/18/2022 02:39:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=182
03/18/2022 02:39:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.837558651026393 on epoch=182
03/18/2022 02:39:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/18/2022 02:39:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 02:39:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=184
03/18/2022 02:39:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=184
03/18/2022 02:39:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
03/18/2022 02:39:56 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.8287081898933232 on epoch=185
03/18/2022 02:39:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/18/2022 02:40:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
03/18/2022 02:40:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/18/2022 02:40:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
03/18/2022 02:40:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
03/18/2022 02:40:15 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=189
03/18/2022 02:40:15 - INFO - __main__ - Saving model with best Classification-F1: 0.9865940511101802 -> 0.9910627007401202 on epoch=189, global_step=2650
03/18/2022 02:40:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 02:40:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=190
03/18/2022 02:40:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=191
03/18/2022 02:40:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/18/2022 02:40:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
03/18/2022 02:40:34 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.859241355083089 on epoch=192
03/18/2022 02:40:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/18/2022 02:40:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/18/2022 02:40:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=194
03/18/2022 02:40:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=195
03/18/2022 02:40:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/18/2022 02:40:52 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.9179519331243469 on epoch=196
03/18/2022 02:40:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
03/18/2022 02:40:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 02:41:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=198
03/18/2022 02:41:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
03/18/2022 02:41:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=199
03/18/2022 02:41:12 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9180953022999743 on epoch=199
03/18/2022 02:41:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=200
03/18/2022 02:41:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
03/18/2022 02:41:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
03/18/2022 02:41:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
03/18/2022 02:41:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/18/2022 02:41:33 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9910627007401202 on epoch=203
03/18/2022 02:41:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/18/2022 02:41:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
03/18/2022 02:41:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
03/18/2022 02:41:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/18/2022 02:41:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
03/18/2022 02:41:52 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8570946358748779 on epoch=207
03/18/2022 02:41:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
03/18/2022 02:41:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/18/2022 02:41:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/18/2022 02:42:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/18/2022 02:42:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
03/18/2022 02:42:12 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9185272075594656 on epoch=210
03/18/2022 02:42:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=211
03/18/2022 02:42:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
03/18/2022 02:42:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/18/2022 02:42:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
03/18/2022 02:42:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/18/2022 02:42:25 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:42:25 - INFO - __main__ - Printing 3 examples
03/18/2022 02:42:25 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 02:42:25 - INFO - __main__ - ['Company']
03/18/2022 02:42:25 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 02:42:25 - INFO - __main__ - ['Company']
03/18/2022 02:42:25 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 02:42:25 - INFO - __main__ - ['Company']
03/18/2022 02:42:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 02:42:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:42:26 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 02:42:26 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:42:26 - INFO - __main__ - Printing 3 examples
03/18/2022 02:42:26 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 02:42:26 - INFO - __main__ - ['Company']
03/18/2022 02:42:26 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 02:42:26 - INFO - __main__ - ['Company']
03/18/2022 02:42:26 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 02:42:26 - INFO - __main__ - ['Company']
03/18/2022 02:42:26 - INFO - __main__ - Tokenizing Input ...
03/18/2022 02:42:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:42:26 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 02:42:32 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.9821297653958945 on epoch=214
03/18/2022 02:42:32 - INFO - __main__ - save last model!
03/18/2022 02:42:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 02:42:32 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 02:42:32 - INFO - __main__ - Printing 3 examples
03/18/2022 02:42:32 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 02:42:32 - INFO - __main__ - ['Animal']
03/18/2022 02:42:32 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 02:42:32 - INFO - __main__ - ['Animal']
03/18/2022 02:42:32 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 02:42:32 - INFO - __main__ - ['Village']
03/18/2022 02:42:32 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 02:42:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:42:37 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 02:42:42 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 02:42:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 02:42:42 - INFO - __main__ - Starting training!
03/18/2022 02:45:05 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_42_0.5_8_predictions.txt
03/18/2022 02:45:05 - INFO - __main__ - Classification-F1 on test data: 0.6512
03/18/2022 02:45:05 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.5, bsz=8, dev_performance=0.9910627007401202, test_performance=0.6511790595393706
03/18/2022 02:45:05 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.4, bsz=8 ...
03/18/2022 02:45:06 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:45:06 - INFO - __main__ - Printing 3 examples
03/18/2022 02:45:06 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 02:45:06 - INFO - __main__ - ['Company']
03/18/2022 02:45:06 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 02:45:06 - INFO - __main__ - ['Company']
03/18/2022 02:45:06 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 02:45:06 - INFO - __main__ - ['Company']
03/18/2022 02:45:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 02:45:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:45:07 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 02:45:07 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 02:45:07 - INFO - __main__ - Printing 3 examples
03/18/2022 02:45:07 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 02:45:07 - INFO - __main__ - ['Company']
03/18/2022 02:45:07 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 02:45:07 - INFO - __main__ - ['Company']
03/18/2022 02:45:07 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 02:45:07 - INFO - __main__ - ['Company']
03/18/2022 02:45:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 02:45:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 02:45:07 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 02:45:25 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 02:45:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 02:45:26 - INFO - __main__ - Starting training!
03/18/2022 02:45:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.59 on epoch=0
03/18/2022 02:45:32 - INFO - __main__ - Step 20 Global step 20 Train loss 6.06 on epoch=1
03/18/2022 02:45:35 - INFO - __main__ - Step 30 Global step 30 Train loss 4.26 on epoch=2
03/18/2022 02:45:37 - INFO - __main__ - Step 40 Global step 40 Train loss 3.10 on epoch=2
03/18/2022 02:45:40 - INFO - __main__ - Step 50 Global step 50 Train loss 2.40 on epoch=3
03/18/2022 02:45:52 - INFO - __main__ - Global step 50 Train loss 4.48 Classification-F1 0.12227463980619661 on epoch=3
03/18/2022 02:45:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12227463980619661 on epoch=3, global_step=50
03/18/2022 02:45:55 - INFO - __main__ - Step 60 Global step 60 Train loss 1.91 on epoch=4
03/18/2022 02:45:58 - INFO - __main__ - Step 70 Global step 70 Train loss 1.60 on epoch=4
03/18/2022 02:46:00 - INFO - __main__ - Step 80 Global step 80 Train loss 1.23 on epoch=5
03/18/2022 02:46:03 - INFO - __main__ - Step 90 Global step 90 Train loss 1.18 on epoch=6
03/18/2022 02:46:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=7
03/18/2022 02:46:12 - INFO - __main__ - Global step 100 Train loss 1.40 Classification-F1 0.5141155839440154 on epoch=7
03/18/2022 02:46:12 - INFO - __main__ - Saving model with best Classification-F1: 0.12227463980619661 -> 0.5141155839440154 on epoch=7, global_step=100
03/18/2022 02:46:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=7
03/18/2022 02:46:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=8
03/18/2022 02:46:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=9
03/18/2022 02:46:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=9
03/18/2022 02:46:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=10
03/18/2022 02:46:32 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.5757232804567363 on epoch=10
03/18/2022 02:46:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5141155839440154 -> 0.5757232804567363 on epoch=10, global_step=150
03/18/2022 02:46:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=11
03/18/2022 02:46:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=12
03/18/2022 02:46:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=12
03/18/2022 02:46:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=13
03/18/2022 02:46:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=14
03/18/2022 02:46:52 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.5598177354633322 on epoch=14
03/18/2022 02:46:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=14
03/18/2022 02:46:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=15
03/18/2022 02:47:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=16
03/18/2022 02:47:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=17
03/18/2022 02:47:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=17
03/18/2022 02:47:11 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.6428549242092939 on epoch=17
03/18/2022 02:47:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5757232804567363 -> 0.6428549242092939 on epoch=17, global_step=250
03/18/2022 02:47:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=18
03/18/2022 02:47:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=19
03/18/2022 02:47:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=19
03/18/2022 02:47:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=20
03/18/2022 02:47:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=21
03/18/2022 02:47:29 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.5620670372550277 on epoch=21
03/18/2022 02:47:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=22
03/18/2022 02:47:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=22
03/18/2022 02:47:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=23
03/18/2022 02:47:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=24
03/18/2022 02:47:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=24
03/18/2022 02:47:48 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.6285622892459647 on epoch=24
03/18/2022 02:47:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=25
03/18/2022 02:47:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=26
03/18/2022 02:47:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=27
03/18/2022 02:47:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=27
03/18/2022 02:48:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=28
03/18/2022 02:48:07 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.6791530900401868 on epoch=28
03/18/2022 02:48:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6428549242092939 -> 0.6791530900401868 on epoch=28, global_step=400
03/18/2022 02:48:10 - INFO - __main__ - Step 410 Global step 410 Train loss 1.59 on epoch=29
03/18/2022 02:48:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=29
03/18/2022 02:48:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=30
03/18/2022 02:48:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=31
03/18/2022 02:48:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=32
03/18/2022 02:48:26 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.7577492072574685 on epoch=32
03/18/2022 02:48:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6791530900401868 -> 0.7577492072574685 on epoch=32, global_step=450
03/18/2022 02:48:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=32
03/18/2022 02:48:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=33
03/18/2022 02:48:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=34
03/18/2022 02:48:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=34
03/18/2022 02:48:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=35
03/18/2022 02:48:46 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.7882258593139965 on epoch=35
03/18/2022 02:48:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7577492072574685 -> 0.7882258593139965 on epoch=35, global_step=500
03/18/2022 02:48:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=36
03/18/2022 02:48:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=37
03/18/2022 02:48:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=37
03/18/2022 02:48:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=38
03/18/2022 02:48:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=39
03/18/2022 02:49:06 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.6115684207465926 on epoch=39
03/18/2022 02:49:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=39
03/18/2022 02:49:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=40
03/18/2022 02:49:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=41
03/18/2022 02:49:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=42
03/18/2022 02:49:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=42
03/18/2022 02:49:25 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.765966780104885 on epoch=42
03/18/2022 02:49:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=43
03/18/2022 02:49:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=44
03/18/2022 02:49:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=44
03/18/2022 02:49:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=45
03/18/2022 02:49:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=46
03/18/2022 02:49:45 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.7937886784112472 on epoch=46
03/18/2022 02:49:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7882258593139965 -> 0.7937886784112472 on epoch=46, global_step=650
03/18/2022 02:49:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=47
03/18/2022 02:49:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=47
03/18/2022 02:49:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=48
03/18/2022 02:49:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=49
03/18/2022 02:49:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=49
03/18/2022 02:50:04 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.7598433725687046 on epoch=49
03/18/2022 02:50:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=50
03/18/2022 02:50:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=51
03/18/2022 02:50:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=52
03/18/2022 02:50:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=52
03/18/2022 02:50:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=53
03/18/2022 02:50:24 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.7002538184647276 on epoch=53
03/18/2022 02:50:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=54
03/18/2022 02:50:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=54
03/18/2022 02:50:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=55
03/18/2022 02:50:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=56
03/18/2022 02:50:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=57
03/18/2022 02:50:43 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.5748621987305726 on epoch=57
03/18/2022 02:50:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=57
03/18/2022 02:50:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=58
03/18/2022 02:50:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=59
03/18/2022 02:50:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=59
03/18/2022 02:50:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=60
03/18/2022 02:51:02 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.7342569495642306 on epoch=60
03/18/2022 02:51:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=61
03/18/2022 02:51:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=62
03/18/2022 02:51:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=62
03/18/2022 02:51:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=63
03/18/2022 02:51:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=64
03/18/2022 02:51:21 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.6081041507512095 on epoch=64
03/18/2022 02:51:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=64
03/18/2022 02:51:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=65
03/18/2022 02:51:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=66
03/18/2022 02:51:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
03/18/2022 02:51:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=67
03/18/2022 02:51:40 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.6315263712927784 on epoch=67
03/18/2022 02:51:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=68
03/18/2022 02:51:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=69
03/18/2022 02:51:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=69
03/18/2022 02:51:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=70
03/18/2022 02:51:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=71
03/18/2022 02:51:59 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.7257441420429536 on epoch=71
03/18/2022 02:52:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=72
03/18/2022 02:52:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=72
03/18/2022 02:52:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=73
03/18/2022 02:52:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=74
03/18/2022 02:52:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=74
03/18/2022 02:52:18 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.6284813866348906 on epoch=74
03/18/2022 02:52:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=75
03/18/2022 02:52:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=76
03/18/2022 02:52:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=77
03/18/2022 02:52:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=77
03/18/2022 02:52:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=78
03/18/2022 02:52:37 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.7593594021473807 on epoch=78
03/18/2022 02:52:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=79
03/18/2022 02:52:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=79
03/18/2022 02:52:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=80
03/18/2022 02:52:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=81
03/18/2022 02:52:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=82
03/18/2022 02:52:56 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.7074430945398686 on epoch=82
03/18/2022 02:52:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=82
03/18/2022 02:53:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=83
03/18/2022 02:53:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=84
03/18/2022 02:53:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=84
03/18/2022 02:53:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=85
03/18/2022 02:53:15 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.7605258896628392 on epoch=85
03/18/2022 02:53:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=86
03/18/2022 02:53:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
03/18/2022 02:53:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=87
03/18/2022 02:53:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=88
03/18/2022 02:53:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=89
03/18/2022 02:53:34 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.7111315559133397 on epoch=89
03/18/2022 02:53:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=89
03/18/2022 02:53:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=90
03/18/2022 02:53:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=91
03/18/2022 02:53:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=92
03/18/2022 02:53:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=92
03/18/2022 02:53:54 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.9136970135072602 on epoch=92
03/18/2022 02:53:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7937886784112472 -> 0.9136970135072602 on epoch=92, global_step=1300
03/18/2022 02:53:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=93
03/18/2022 02:53:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=94
03/18/2022 02:54:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=94
03/18/2022 02:54:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=95
03/18/2022 02:54:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=96
03/18/2022 02:54:13 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.7987317102103908 on epoch=96
03/18/2022 02:54:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
03/18/2022 02:54:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=97
03/18/2022 02:54:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=98
03/18/2022 02:54:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=99
03/18/2022 02:54:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=99
03/18/2022 02:54:32 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.9143564679048549 on epoch=99
03/18/2022 02:54:32 - INFO - __main__ - Saving model with best Classification-F1: 0.9136970135072602 -> 0.9143564679048549 on epoch=99, global_step=1400
03/18/2022 02:54:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=100
03/18/2022 02:54:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
03/18/2022 02:54:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=102
03/18/2022 02:54:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=102
03/18/2022 02:54:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=103
03/18/2022 02:54:51 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.9647904741341893 on epoch=103
03/18/2022 02:54:51 - INFO - __main__ - Saving model with best Classification-F1: 0.9143564679048549 -> 0.9647904741341893 on epoch=103, global_step=1450
03/18/2022 02:54:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=104
03/18/2022 02:54:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=104
03/18/2022 02:54:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=105
03/18/2022 02:55:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=106
03/18/2022 02:55:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=107
03/18/2022 02:55:11 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.9046842023439114 on epoch=107
03/18/2022 02:55:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=107
03/18/2022 02:55:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=108
03/18/2022 02:55:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=109
03/18/2022 02:55:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=109
03/18/2022 02:55:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=110
03/18/2022 02:55:30 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.9723279341061938 on epoch=110
03/18/2022 02:55:30 - INFO - __main__ - Saving model with best Classification-F1: 0.9647904741341893 -> 0.9723279341061938 on epoch=110, global_step=1550
03/18/2022 02:55:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=111
03/18/2022 02:55:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=112
03/18/2022 02:55:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=112
03/18/2022 02:55:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=113
03/18/2022 02:55:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=114
03/18/2022 02:55:49 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.9731056746695038 on epoch=114
03/18/2022 02:55:49 - INFO - __main__ - Saving model with best Classification-F1: 0.9723279341061938 -> 0.9731056746695038 on epoch=114, global_step=1600
03/18/2022 02:55:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=114
03/18/2022 02:55:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=115
03/18/2022 02:55:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=116
03/18/2022 02:56:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=117
03/18/2022 02:56:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=117
03/18/2022 02:56:08 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.9730824395690119 on epoch=117
03/18/2022 02:56:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=118
03/18/2022 02:56:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
03/18/2022 02:56:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=119
03/18/2022 02:56:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=120
03/18/2022 02:56:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=121
03/18/2022 02:56:28 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.8427140256762529 on epoch=121
03/18/2022 02:56:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=122
03/18/2022 02:56:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=122
03/18/2022 02:56:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=123
03/18/2022 02:56:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/18/2022 02:56:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=124
03/18/2022 02:56:48 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.8489649586513238 on epoch=124
03/18/2022 02:56:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=125
03/18/2022 02:56:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
03/18/2022 02:56:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=127
03/18/2022 02:56:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=127
03/18/2022 02:57:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=128
03/18/2022 02:57:07 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9679989297771893 on epoch=128
03/18/2022 02:57:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=129
03/18/2022 02:57:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
03/18/2022 02:57:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/18/2022 02:57:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=131
03/18/2022 02:57:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=132
03/18/2022 02:57:27 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.9679989297771895 on epoch=132
03/18/2022 02:57:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=132
03/18/2022 02:57:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=133
03/18/2022 02:57:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=134
03/18/2022 02:57:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=134
03/18/2022 02:57:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=135
03/18/2022 02:57:46 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.9682426636343686 on epoch=135
03/18/2022 02:57:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=136
03/18/2022 02:57:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=137
03/18/2022 02:57:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=137
03/18/2022 02:57:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=138
03/18/2022 02:57:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=139
03/18/2022 02:58:05 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.972571667963373 on epoch=139
03/18/2022 02:58:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=139
03/18/2022 02:58:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
03/18/2022 02:58:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
03/18/2022 02:58:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=142
03/18/2022 02:58:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
03/18/2022 02:58:24 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9015426150910021 on epoch=142
03/18/2022 02:58:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 02:58:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
03/18/2022 02:58:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=144
03/18/2022 02:58:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
03/18/2022 02:58:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=146
03/18/2022 02:58:45 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7869710868860251 on epoch=146
03/18/2022 02:58:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=147
03/18/2022 02:58:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
03/18/2022 02:58:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/18/2022 02:58:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=149
03/18/2022 02:58:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
03/18/2022 02:59:05 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.8203725038402457 on epoch=149
03/18/2022 02:59:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=150
03/18/2022 02:59:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
03/18/2022 02:59:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=152
03/18/2022 02:59:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=152
03/18/2022 02:59:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=153
03/18/2022 02:59:24 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.8470832382212163 on epoch=153
03/18/2022 02:59:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
03/18/2022 02:59:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=154
03/18/2022 02:59:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/18/2022 02:59:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
03/18/2022 02:59:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
03/18/2022 02:59:44 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7371646797050023 on epoch=157
03/18/2022 02:59:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/18/2022 02:59:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
03/18/2022 02:59:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=159
03/18/2022 02:59:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
03/18/2022 02:59:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=160
03/18/2022 03:00:03 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7840538004057926 on epoch=160
03/18/2022 03:00:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/18/2022 03:00:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
03/18/2022 03:00:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=162
03/18/2022 03:00:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=163
03/18/2022 03:00:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/18/2022 03:00:23 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7077741360473808 on epoch=164
03/18/2022 03:00:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=164
03/18/2022 03:00:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/18/2022 03:00:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=166
03/18/2022 03:00:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
03/18/2022 03:00:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/18/2022 03:00:42 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.973086803484666 on epoch=167
03/18/2022 03:00:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/18/2022 03:00:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=169
03/18/2022 03:00:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
03/18/2022 03:00:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=170
03/18/2022 03:00:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=171
03/18/2022 03:01:01 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.8335002094679513 on epoch=171
03/18/2022 03:01:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
03/18/2022 03:01:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
03/18/2022 03:01:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
03/18/2022 03:01:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
03/18/2022 03:01:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/18/2022 03:01:20 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.9035424205412099 on epoch=174
03/18/2022 03:01:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
03/18/2022 03:01:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/18/2022 03:01:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=177
03/18/2022 03:01:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/18/2022 03:01:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
03/18/2022 03:01:39 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.9226979472140762 on epoch=178
03/18/2022 03:01:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=179
03/18/2022 03:01:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/18/2022 03:01:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/18/2022 03:01:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=181
03/18/2022 03:01:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=182
03/18/2022 03:01:59 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.9101333296321191 on epoch=182
03/18/2022 03:02:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
03/18/2022 03:02:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
03/18/2022 03:02:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
03/18/2022 03:02:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/18/2022 03:02:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
03/18/2022 03:02:18 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.901433965570588 on epoch=185
03/18/2022 03:02:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
03/18/2022 03:02:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
03/18/2022 03:02:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/18/2022 03:02:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/18/2022 03:02:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
03/18/2022 03:02:37 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7864716214909241 on epoch=189
03/18/2022 03:02:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 03:02:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=190
03/18/2022 03:02:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=191
03/18/2022 03:02:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=192
03/18/2022 03:02:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
03/18/2022 03:02:56 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.9819717916492109 on epoch=192
03/18/2022 03:02:56 - INFO - __main__ - Saving model with best Classification-F1: 0.9731056746695038 -> 0.9819717916492109 on epoch=192, global_step=2700
03/18/2022 03:02:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/18/2022 03:03:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/18/2022 03:03:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/18/2022 03:03:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/18/2022 03:03:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=196
03/18/2022 03:03:15 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9865940511101802 on epoch=196
03/18/2022 03:03:15 - INFO - __main__ - Saving model with best Classification-F1: 0.9819717916492109 -> 0.9865940511101802 on epoch=196, global_step=2750
03/18/2022 03:03:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=197
03/18/2022 03:03:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=197
03/18/2022 03:03:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=198
03/18/2022 03:03:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
03/18/2022 03:03:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/18/2022 03:03:34 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.9097578959786967 on epoch=199
03/18/2022 03:03:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
03/18/2022 03:03:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
03/18/2022 03:03:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=202
03/18/2022 03:03:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/18/2022 03:03:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=203
03/18/2022 03:03:53 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.8428323968883742 on epoch=203
03/18/2022 03:03:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/18/2022 03:03:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/18/2022 03:04:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
03/18/2022 03:04:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/18/2022 03:04:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/18/2022 03:04:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.9058363273512458 on epoch=207
03/18/2022 03:04:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
03/18/2022 03:04:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/18/2022 03:04:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=209
03/18/2022 03:04:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/18/2022 03:04:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=210
03/18/2022 03:04:32 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9055741227626655 on epoch=210
03/18/2022 03:04:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=211
03/18/2022 03:04:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=212
03/18/2022 03:04:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/18/2022 03:04:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
03/18/2022 03:04:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
03/18/2022 03:04:46 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:04:46 - INFO - __main__ - Printing 3 examples
03/18/2022 03:04:46 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 03:04:46 - INFO - __main__ - ['Company']
03/18/2022 03:04:46 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 03:04:46 - INFO - __main__ - ['Company']
03/18/2022 03:04:46 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 03:04:46 - INFO - __main__ - ['Company']
03/18/2022 03:04:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 03:04:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:04:47 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:04:47 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:04:47 - INFO - __main__ - Printing 3 examples
03/18/2022 03:04:47 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 03:04:47 - INFO - __main__ - ['Company']
03/18/2022 03:04:47 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 03:04:47 - INFO - __main__ - ['Company']
03/18/2022 03:04:47 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 03:04:47 - INFO - __main__ - ['Company']
03/18/2022 03:04:47 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:04:47 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:04:47 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:04:51 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9059945278209035 on epoch=214
03/18/2022 03:04:51 - INFO - __main__ - save last model!
03/18/2022 03:04:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 03:04:51 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 03:04:51 - INFO - __main__ - Printing 3 examples
03/18/2022 03:04:51 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 03:04:51 - INFO - __main__ - ['Animal']
03/18/2022 03:04:51 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 03:04:51 - INFO - __main__ - ['Animal']
03/18/2022 03:04:51 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 03:04:51 - INFO - __main__ - ['Village']
03/18/2022 03:04:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:04:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:04:56 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 03:05:03 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:05:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:05:03 - INFO - __main__ - Starting training!
03/18/2022 03:07:03 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_42_0.4_8_predictions.txt
03/18/2022 03:07:03 - INFO - __main__ - Classification-F1 on test data: 0.6821
03/18/2022 03:07:03 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.4, bsz=8, dev_performance=0.9865940511101802, test_performance=0.6820589608337715
03/18/2022 03:07:03 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.3, bsz=8 ...
03/18/2022 03:07:04 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:07:04 - INFO - __main__ - Printing 3 examples
03/18/2022 03:07:04 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 03:07:04 - INFO - __main__ - ['Company']
03/18/2022 03:07:04 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 03:07:04 - INFO - __main__ - ['Company']
03/18/2022 03:07:04 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 03:07:04 - INFO - __main__ - ['Company']
03/18/2022 03:07:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:07:04 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:07:05 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:07:05 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:07:05 - INFO - __main__ - Printing 3 examples
03/18/2022 03:07:05 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 03:07:05 - INFO - __main__ - ['Company']
03/18/2022 03:07:05 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 03:07:05 - INFO - __main__ - ['Company']
03/18/2022 03:07:05 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 03:07:05 - INFO - __main__ - ['Company']
03/18/2022 03:07:05 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:07:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:07:05 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:07:23 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:07:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:07:24 - INFO - __main__ - Starting training!
03/18/2022 03:07:28 - INFO - __main__ - Step 10 Global step 10 Train loss 6.78 on epoch=0
03/18/2022 03:07:30 - INFO - __main__ - Step 20 Global step 20 Train loss 6.44 on epoch=1
03/18/2022 03:07:33 - INFO - __main__ - Step 30 Global step 30 Train loss 5.29 on epoch=2
03/18/2022 03:07:35 - INFO - __main__ - Step 40 Global step 40 Train loss 4.45 on epoch=2
03/18/2022 03:07:38 - INFO - __main__ - Step 50 Global step 50 Train loss 4.94 on epoch=3
03/18/2022 03:09:15 - INFO - __main__ - Global step 50 Train loss 5.58 Classification-F1 0.0 on epoch=3
03/18/2022 03:09:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/18/2022 03:09:18 - INFO - __main__ - Step 60 Global step 60 Train loss 4.25 on epoch=4
03/18/2022 03:09:20 - INFO - __main__ - Step 70 Global step 70 Train loss 4.07 on epoch=4
03/18/2022 03:09:23 - INFO - __main__ - Step 80 Global step 80 Train loss 3.74 on epoch=5
03/18/2022 03:09:25 - INFO - __main__ - Step 90 Global step 90 Train loss 3.68 on epoch=6
03/18/2022 03:09:28 - INFO - __main__ - Step 100 Global step 100 Train loss 3.37 on epoch=7
03/18/2022 03:10:16 - INFO - __main__ - Global step 100 Train loss 3.82 Classification-F1 0.003315640007731537 on epoch=7
03/18/2022 03:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.003315640007731537 on epoch=7, global_step=100
03/18/2022 03:10:19 - INFO - __main__ - Step 110 Global step 110 Train loss 3.10 on epoch=7
03/18/2022 03:10:21 - INFO - __main__ - Step 120 Global step 120 Train loss 2.88 on epoch=8
03/18/2022 03:10:24 - INFO - __main__ - Step 130 Global step 130 Train loss 2.67 on epoch=9
03/18/2022 03:10:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.49 on epoch=9
03/18/2022 03:10:29 - INFO - __main__ - Step 150 Global step 150 Train loss 2.21 on epoch=10
03/18/2022 03:10:35 - INFO - __main__ - Global step 150 Train loss 2.67 Classification-F1 0.1331649864432732 on epoch=10
03/18/2022 03:10:35 - INFO - __main__ - Saving model with best Classification-F1: 0.003315640007731537 -> 0.1331649864432732 on epoch=10, global_step=150
03/18/2022 03:10:37 - INFO - __main__ - Step 160 Global step 160 Train loss 2.22 on epoch=11
03/18/2022 03:10:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.87 on epoch=12
03/18/2022 03:10:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.57 on epoch=12
03/18/2022 03:10:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.49 on epoch=13
03/18/2022 03:10:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.42 on epoch=14
03/18/2022 03:10:53 - INFO - __main__ - Global step 200 Train loss 1.71 Classification-F1 0.2517885663356252 on epoch=14
03/18/2022 03:10:53 - INFO - __main__ - Saving model with best Classification-F1: 0.1331649864432732 -> 0.2517885663356252 on epoch=14, global_step=200
03/18/2022 03:10:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.42 on epoch=14
03/18/2022 03:10:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.17 on epoch=15
03/18/2022 03:11:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.31 on epoch=16
03/18/2022 03:11:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.19 on epoch=17
03/18/2022 03:11:06 - INFO - __main__ - Step 250 Global step 250 Train loss 1.04 on epoch=17
03/18/2022 03:11:11 - INFO - __main__ - Global step 250 Train loss 1.23 Classification-F1 0.32453458952488845 on epoch=17
03/18/2022 03:11:11 - INFO - __main__ - Saving model with best Classification-F1: 0.2517885663356252 -> 0.32453458952488845 on epoch=17, global_step=250
03/18/2022 03:11:14 - INFO - __main__ - Step 260 Global step 260 Train loss 1.04 on epoch=18
03/18/2022 03:11:17 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=19
03/18/2022 03:11:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.90 on epoch=19
03/18/2022 03:11:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=20
03/18/2022 03:11:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.90 on epoch=21
03/18/2022 03:11:31 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.3156203344951077 on epoch=21
03/18/2022 03:11:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=22
03/18/2022 03:11:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=22
03/18/2022 03:11:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=23
03/18/2022 03:11:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.71 on epoch=24
03/18/2022 03:11:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=24
03/18/2022 03:11:51 - INFO - __main__ - Global step 350 Train loss 0.78 Classification-F1 0.4255827026406294 on epoch=24
03/18/2022 03:11:51 - INFO - __main__ - Saving model with best Classification-F1: 0.32453458952488845 -> 0.4255827026406294 on epoch=24, global_step=350
03/18/2022 03:11:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.68 on epoch=25
03/18/2022 03:11:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=26
03/18/2022 03:11:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=27
03/18/2022 03:12:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=27
03/18/2022 03:12:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=28
03/18/2022 03:12:10 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.4673292554760232 on epoch=28
03/18/2022 03:12:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4255827026406294 -> 0.4673292554760232 on epoch=28, global_step=400
03/18/2022 03:12:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=29
03/18/2022 03:12:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=29
03/18/2022 03:12:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=30
03/18/2022 03:12:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=31
03/18/2022 03:12:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=32
03/18/2022 03:12:30 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.43645375944949005 on epoch=32
03/18/2022 03:12:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=32
03/18/2022 03:12:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=33
03/18/2022 03:12:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=34
03/18/2022 03:12:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=34
03/18/2022 03:12:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=35
03/18/2022 03:12:51 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.5505057502510481 on epoch=35
03/18/2022 03:12:51 - INFO - __main__ - Saving model with best Classification-F1: 0.4673292554760232 -> 0.5505057502510481 on epoch=35, global_step=500
03/18/2022 03:12:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=36
03/18/2022 03:12:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=37
03/18/2022 03:12:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=37
03/18/2022 03:13:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=38
03/18/2022 03:13:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=39
03/18/2022 03:13:11 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.5272010195559015 on epoch=39
03/18/2022 03:13:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=39
03/18/2022 03:13:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=40
03/18/2022 03:13:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=41
03/18/2022 03:13:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=42
03/18/2022 03:13:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=42
03/18/2022 03:13:31 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.5192072110025733 on epoch=42
03/18/2022 03:13:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=43
03/18/2022 03:13:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=44
03/18/2022 03:13:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=44
03/18/2022 03:13:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=45
03/18/2022 03:13:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=46
03/18/2022 03:13:50 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.5457968482839254 on epoch=46
03/18/2022 03:13:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=47
03/18/2022 03:13:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=47
03/18/2022 03:13:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=48
03/18/2022 03:14:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=49
03/18/2022 03:14:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=49
03/18/2022 03:14:10 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.6489429181911024 on epoch=49
03/18/2022 03:14:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5505057502510481 -> 0.6489429181911024 on epoch=49, global_step=700
03/18/2022 03:14:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=50
03/18/2022 03:14:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=51
03/18/2022 03:14:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=52
03/18/2022 03:14:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=52
03/18/2022 03:14:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=53
03/18/2022 03:14:29 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.6641377541567295 on epoch=53
03/18/2022 03:14:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6489429181911024 -> 0.6641377541567295 on epoch=53, global_step=750
03/18/2022 03:14:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=54
03/18/2022 03:14:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=54
03/18/2022 03:14:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=55
03/18/2022 03:14:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=56
03/18/2022 03:14:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=57
03/18/2022 03:14:49 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.717618176553243 on epoch=57
03/18/2022 03:14:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6641377541567295 -> 0.717618176553243 on epoch=57, global_step=800
03/18/2022 03:14:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=57
03/18/2022 03:14:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=58
03/18/2022 03:14:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=59
03/18/2022 03:14:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=59
03/18/2022 03:15:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=60
03/18/2022 03:15:08 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.7054819718095391 on epoch=60
03/18/2022 03:15:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=61
03/18/2022 03:15:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=62
03/18/2022 03:15:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=62
03/18/2022 03:15:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=63
03/18/2022 03:15:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=64
03/18/2022 03:15:28 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.6407358811045203 on epoch=64
03/18/2022 03:15:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=64
03/18/2022 03:15:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=65
03/18/2022 03:15:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=66
03/18/2022 03:15:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=67
03/18/2022 03:15:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
03/18/2022 03:15:47 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.7236555293902964 on epoch=67
03/18/2022 03:15:47 - INFO - __main__ - Saving model with best Classification-F1: 0.717618176553243 -> 0.7236555293902964 on epoch=67, global_step=950
03/18/2022 03:15:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=68
03/18/2022 03:15:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=69
03/18/2022 03:15:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=69
03/18/2022 03:15:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=70
03/18/2022 03:16:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=71
03/18/2022 03:16:07 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.9685132704108036 on epoch=71
03/18/2022 03:16:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7236555293902964 -> 0.9685132704108036 on epoch=71, global_step=1000
03/18/2022 03:16:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=72
03/18/2022 03:16:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=72
03/18/2022 03:16:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=73
03/18/2022 03:16:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=74
03/18/2022 03:16:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=74
03/18/2022 03:16:26 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.859103128054741 on epoch=74
03/18/2022 03:16:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=75
03/18/2022 03:16:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=76
03/18/2022 03:16:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=77
03/18/2022 03:16:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=77
03/18/2022 03:16:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=78
03/18/2022 03:16:45 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.8385327591447136 on epoch=78
03/18/2022 03:16:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=79
03/18/2022 03:16:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=79
03/18/2022 03:16:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=80
03/18/2022 03:16:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=81
03/18/2022 03:16:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=82
03/18/2022 03:17:04 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.7368905817458948 on epoch=82
03/18/2022 03:17:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=82
03/18/2022 03:17:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=83
03/18/2022 03:17:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=84
03/18/2022 03:17:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=84
03/18/2022 03:17:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=85
03/18/2022 03:17:23 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.9061785747096224 on epoch=85
03/18/2022 03:17:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=86
03/18/2022 03:17:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
03/18/2022 03:17:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=87
03/18/2022 03:17:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=88
03/18/2022 03:17:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=89
03/18/2022 03:17:43 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.9186705767350927 on epoch=89
03/18/2022 03:17:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=89
03/18/2022 03:17:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=90
03/18/2022 03:17:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=91
03/18/2022 03:17:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=92
03/18/2022 03:17:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=92
03/18/2022 03:18:02 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.8547651480452632 on epoch=92
03/18/2022 03:18:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=93
03/18/2022 03:18:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=94
03/18/2022 03:18:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
03/18/2022 03:18:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=95
03/18/2022 03:18:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=96
03/18/2022 03:18:21 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7817520320562917 on epoch=96
03/18/2022 03:18:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
03/18/2022 03:18:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=97
03/18/2022 03:18:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=98
03/18/2022 03:18:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
03/18/2022 03:18:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=99
03/18/2022 03:18:40 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.9064306995742797 on epoch=99
03/18/2022 03:18:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=100
03/18/2022 03:18:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
03/18/2022 03:18:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=102
03/18/2022 03:18:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=102
03/18/2022 03:18:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=103
03/18/2022 03:18:59 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7926560101356397 on epoch=103
03/18/2022 03:19:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=104
03/18/2022 03:19:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=104
03/18/2022 03:19:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=105
03/18/2022 03:19:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=106
03/18/2022 03:19:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=107
03/18/2022 03:19:18 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.68180225735404 on epoch=107
03/18/2022 03:19:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=107
03/18/2022 03:19:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=108
03/18/2022 03:19:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=109
03/18/2022 03:19:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=109
03/18/2022 03:19:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=110
03/18/2022 03:19:37 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.9228413163897033 on epoch=110
03/18/2022 03:19:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=111
03/18/2022 03:19:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=112
03/18/2022 03:19:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=112
03/18/2022 03:19:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=113
03/18/2022 03:19:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=114
03/18/2022 03:19:55 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7906224820920023 on epoch=114
03/18/2022 03:19:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=114
03/18/2022 03:20:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=115
03/18/2022 03:20:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=116
03/18/2022 03:20:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
03/18/2022 03:20:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=117
03/18/2022 03:20:14 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.8566668546507257 on epoch=117
03/18/2022 03:20:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=118
03/18/2022 03:20:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=119
03/18/2022 03:20:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=119
03/18/2022 03:20:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=120
03/18/2022 03:20:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=121
03/18/2022 03:20:33 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.8610008858748779 on epoch=121
03/18/2022 03:20:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=122
03/18/2022 03:20:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=122
03/18/2022 03:20:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=123
03/18/2022 03:20:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/18/2022 03:20:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=124
03/18/2022 03:20:52 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.8006970891221366 on epoch=124
03/18/2022 03:20:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=125
03/18/2022 03:20:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
03/18/2022 03:21:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=127
03/18/2022 03:21:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
03/18/2022 03:21:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=128
03/18/2022 03:21:11 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6998865901205903 on epoch=128
03/18/2022 03:21:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=129
03/18/2022 03:21:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=129
03/18/2022 03:21:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=130
03/18/2022 03:21:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
03/18/2022 03:21:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
03/18/2022 03:21:30 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7386435148031758 on epoch=132
03/18/2022 03:21:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=132
03/18/2022 03:21:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=133
03/18/2022 03:21:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=134
03/18/2022 03:21:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=134
03/18/2022 03:21:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=135
03/18/2022 03:21:49 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7613757148796081 on epoch=135
03/18/2022 03:21:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=136
03/18/2022 03:21:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=137
03/18/2022 03:21:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
03/18/2022 03:22:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=138
03/18/2022 03:22:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=139
03/18/2022 03:22:08 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.8524472838988968 on epoch=139
03/18/2022 03:22:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=139
03/18/2022 03:22:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=140
03/18/2022 03:22:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=141
03/18/2022 03:22:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=142
03/18/2022 03:22:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=142
03/18/2022 03:22:28 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.9077417669464386 on epoch=142
03/18/2022 03:22:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=143
03/18/2022 03:22:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=144
03/18/2022 03:22:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=144
03/18/2022 03:22:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
03/18/2022 03:22:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=146
03/18/2022 03:22:47 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.9163766699250571 on epoch=146
03/18/2022 03:22:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=147
03/18/2022 03:22:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
03/18/2022 03:22:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/18/2022 03:22:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=149
03/18/2022 03:23:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=149
03/18/2022 03:23:06 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.9186787227109808 on epoch=149
03/18/2022 03:23:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=150
03/18/2022 03:23:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=151
03/18/2022 03:23:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=152
03/18/2022 03:23:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=152
03/18/2022 03:23:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=153
03/18/2022 03:23:25 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.7816753237626104 on epoch=153
03/18/2022 03:23:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=154
03/18/2022 03:23:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=154
03/18/2022 03:23:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/18/2022 03:23:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=156
03/18/2022 03:23:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
03/18/2022 03:23:44 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.8486594051110181 on epoch=157
03/18/2022 03:23:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/18/2022 03:23:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
03/18/2022 03:23:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=159
03/18/2022 03:23:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=159
03/18/2022 03:23:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=160
03/18/2022 03:24:03 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7561188451066092 on epoch=160
03/18/2022 03:24:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=161
03/18/2022 03:24:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
03/18/2022 03:24:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=162
03/18/2022 03:24:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/18/2022 03:24:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/18/2022 03:24:22 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6785267420751293 on epoch=164
03/18/2022 03:24:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=164
03/18/2022 03:24:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=165
03/18/2022 03:24:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=166
03/18/2022 03:24:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
03/18/2022 03:24:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/18/2022 03:24:40 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.5795654262311658 on epoch=167
03/18/2022 03:24:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/18/2022 03:24:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=169
03/18/2022 03:24:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/18/2022 03:24:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=170
03/18/2022 03:24:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
03/18/2022 03:25:00 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7570628627976298 on epoch=171
03/18/2022 03:25:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
03/18/2022 03:25:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=172
03/18/2022 03:25:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=173
03/18/2022 03:25:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=174
03/18/2022 03:25:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
03/18/2022 03:25:19 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6408132229832267 on epoch=174
03/18/2022 03:25:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
03/18/2022 03:25:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
03/18/2022 03:25:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
03/18/2022 03:25:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/18/2022 03:25:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=178
03/18/2022 03:25:38 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7480950635386119 on epoch=178
03/18/2022 03:25:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=179
03/18/2022 03:25:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/18/2022 03:25:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
03/18/2022 03:25:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/18/2022 03:25:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=182
03/18/2022 03:25:57 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6360601239518289 on epoch=182
03/18/2022 03:25:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/18/2022 03:26:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=183
03/18/2022 03:26:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=184
03/18/2022 03:26:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/18/2022 03:26:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=185
03/18/2022 03:26:16 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7517749282834671 on epoch=185
03/18/2022 03:26:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/18/2022 03:26:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=187
03/18/2022 03:26:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=187
03/18/2022 03:26:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
03/18/2022 03:26:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=189
03/18/2022 03:26:34 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.683375226923614 on epoch=189
03/18/2022 03:26:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=189
03/18/2022 03:26:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
03/18/2022 03:26:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=191
03/18/2022 03:26:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/18/2022 03:26:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/18/2022 03:26:53 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7131237113198063 on epoch=192
03/18/2022 03:26:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=193
03/18/2022 03:26:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=194
03/18/2022 03:27:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=194
03/18/2022 03:27:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/18/2022 03:27:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=196
03/18/2022 03:27:13 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7875773763540861 on epoch=196
03/18/2022 03:27:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=197
03/18/2022 03:27:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 03:27:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=198
03/18/2022 03:27:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
03/18/2022 03:27:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=199
03/18/2022 03:27:31 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7816804874442445 on epoch=199
03/18/2022 03:27:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=200
03/18/2022 03:27:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
03/18/2022 03:27:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
03/18/2022 03:27:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=202
03/18/2022 03:27:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/18/2022 03:27:50 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7534788747691974 on epoch=203
03/18/2022 03:27:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/18/2022 03:27:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=204
03/18/2022 03:27:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
03/18/2022 03:28:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=206
03/18/2022 03:28:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=207
03/18/2022 03:28:09 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6239706238746178 on epoch=207
03/18/2022 03:28:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=207
03/18/2022 03:28:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/18/2022 03:28:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/18/2022 03:28:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=209
03/18/2022 03:28:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
03/18/2022 03:28:28 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5198399761058319 on epoch=210
03/18/2022 03:28:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=211
03/18/2022 03:28:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=212
03/18/2022 03:28:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=212
03/18/2022 03:28:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/18/2022 03:28:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/18/2022 03:28:43 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:28:43 - INFO - __main__ - Printing 3 examples
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 03:28:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:28:43 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:28:43 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:28:43 - INFO - __main__ - Printing 3 examples
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 03:28:43 - INFO - __main__ - ['Company']
03/18/2022 03:28:43 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:28:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:28:43 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:28:47 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.5973215210243452 on epoch=214
03/18/2022 03:28:47 - INFO - __main__ - save last model!
03/18/2022 03:28:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 03:28:47 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 03:28:47 - INFO - __main__ - Printing 3 examples
03/18/2022 03:28:47 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 03:28:47 - INFO - __main__ - ['Animal']
03/18/2022 03:28:47 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 03:28:47 - INFO - __main__ - ['Animal']
03/18/2022 03:28:47 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 03:28:47 - INFO - __main__ - ['Village']
03/18/2022 03:28:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:28:49 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:28:53 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 03:29:02 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:29:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:29:03 - INFO - __main__ - Starting training!
03/18/2022 03:30:52 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_42_0.3_8_predictions.txt
03/18/2022 03:30:52 - INFO - __main__ - Classification-F1 on test data: 0.3976
03/18/2022 03:30:53 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.3, bsz=8, dev_performance=0.9685132704108036, test_performance=0.39763443082313843
03/18/2022 03:30:53 - INFO - __main__ - Running ... prefix=dbpedia_14_16_42, lr=0.2, bsz=8 ...
03/18/2022 03:30:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:30:54 - INFO - __main__ - Printing 3 examples
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] The Sterling Piano Company was a piano manufacturer in Derby Connecticut. The company was founded in 1873 by Charles A. Sterling as the Sterling Organ Company. Sterling had purchased the Birmingham Organ Company in 1871 and had $30000 to fund the company. The Sterling Organ Company began making pianos in 1885.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] UltraVision CLPL is a contact lens manufacturer with headquarters based in Leighton Buzzard Bedfordshire England. UltraVision CLPL also has a Research and Development office based in Cambridge England.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] Databank is a financial services provider and a brokerage ffirm with its headquarters in Accra Ghana. It provides corporate and public finance advisory services.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:30:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:30:54 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:30:54 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:30:54 - INFO - __main__ - Printing 3 examples
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] Speedball is an American company that manufactures art materials and other stationery items. The company first successful with its dip pens expanded its product line to other art areas such as painting sculpture and printing press.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] Newag S.A. is a Polish company based in Nowy Sącz specialising in the production maintenance and modernisation of railway rolling stock. The company's products include the 14WE 19WE 35WE types electric multiple units; it has also developed the Nevelo tram.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ -  [dbpedia_14] McMullens is a regional brewery founded in 1827 in Hertford England.
03/18/2022 03:30:54 - INFO - __main__ - ['Company']
03/18/2022 03:30:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:30:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:30:54 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:31:10 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:31:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:31:11 - INFO - __main__ - Starting training!
03/18/2022 03:31:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.93 on epoch=0
03/18/2022 03:31:17 - INFO - __main__ - Step 20 Global step 20 Train loss 6.85 on epoch=1
03/18/2022 03:31:19 - INFO - __main__ - Step 30 Global step 30 Train loss 6.40 on epoch=2
03/18/2022 03:31:22 - INFO - __main__ - Step 40 Global step 40 Train loss 5.30 on epoch=2
03/18/2022 03:31:24 - INFO - __main__ - Step 50 Global step 50 Train loss 4.63 on epoch=3
03/18/2022 03:32:40 - INFO - __main__ - Global step 50 Train loss 6.02 Classification-F1 0.000585308750365818 on epoch=3
03/18/2022 03:32:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.000585308750365818 on epoch=3, global_step=50
03/18/2022 03:32:42 - INFO - __main__ - Step 60 Global step 60 Train loss 3.77 on epoch=4
03/18/2022 03:32:45 - INFO - __main__ - Step 70 Global step 70 Train loss 3.19 on epoch=4
03/18/2022 03:32:47 - INFO - __main__ - Step 80 Global step 80 Train loss 2.60 on epoch=5
03/18/2022 03:32:50 - INFO - __main__ - Step 90 Global step 90 Train loss 2.44 on epoch=6
03/18/2022 03:32:52 - INFO - __main__ - Step 100 Global step 100 Train loss 2.26 on epoch=7
03/18/2022 03:32:59 - INFO - __main__ - Global step 100 Train loss 2.85 Classification-F1 0.10434549622723562 on epoch=7
03/18/2022 03:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.000585308750365818 -> 0.10434549622723562 on epoch=7, global_step=100
03/18/2022 03:33:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.99 on epoch=7
03/18/2022 03:33:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.90 on epoch=8
03/18/2022 03:33:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.61 on epoch=9
03/18/2022 03:33:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.77 on epoch=9
03/18/2022 03:33:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.33 on epoch=10
03/18/2022 03:33:18 - INFO - __main__ - Global step 150 Train loss 1.72 Classification-F1 0.3206399503736091 on epoch=10
03/18/2022 03:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.10434549622723562 -> 0.3206399503736091 on epoch=10, global_step=150
03/18/2022 03:33:20 - INFO - __main__ - Step 160 Global step 160 Train loss 1.33 on epoch=11
03/18/2022 03:33:23 - INFO - __main__ - Step 170 Global step 170 Train loss 1.23 on epoch=12
03/18/2022 03:33:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.18 on epoch=12
03/18/2022 03:33:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.13 on epoch=13
03/18/2022 03:33:31 - INFO - __main__ - Step 200 Global step 200 Train loss 1.16 on epoch=14
03/18/2022 03:33:37 - INFO - __main__ - Global step 200 Train loss 1.20 Classification-F1 0.3960786568440998 on epoch=14
03/18/2022 03:33:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3206399503736091 -> 0.3960786568440998 on epoch=14, global_step=200
03/18/2022 03:33:40 - INFO - __main__ - Step 210 Global step 210 Train loss 1.07 on epoch=14
03/18/2022 03:33:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=15
03/18/2022 03:33:45 - INFO - __main__ - Step 230 Global step 230 Train loss 1.08 on epoch=16
03/18/2022 03:33:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=17
03/18/2022 03:33:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=17
03/18/2022 03:33:57 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.5069948704346328 on epoch=17
03/18/2022 03:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3960786568440998 -> 0.5069948704346328 on epoch=17, global_step=250
03/18/2022 03:33:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=18
03/18/2022 03:34:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=19
03/18/2022 03:34:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=19
03/18/2022 03:34:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=20
03/18/2022 03:34:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=21
03/18/2022 03:34:17 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.5465116071496858 on epoch=21
03/18/2022 03:34:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5069948704346328 -> 0.5465116071496858 on epoch=21, global_step=300
03/18/2022 03:34:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.79 on epoch=22
03/18/2022 03:34:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.65 on epoch=22
03/18/2022 03:34:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=23
03/18/2022 03:34:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=24
03/18/2022 03:34:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.69 on epoch=24
03/18/2022 03:34:36 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.6193398068151692 on epoch=24
03/18/2022 03:34:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5465116071496858 -> 0.6193398068151692 on epoch=24, global_step=350
03/18/2022 03:34:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=25
03/18/2022 03:34:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.68 on epoch=26
03/18/2022 03:34:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=27
03/18/2022 03:34:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=27
03/18/2022 03:34:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=28
03/18/2022 03:34:57 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.6572297282419675 on epoch=28
03/18/2022 03:34:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6193398068151692 -> 0.6572297282419675 on epoch=28, global_step=400
03/18/2022 03:34:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=29
03/18/2022 03:35:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=29
03/18/2022 03:35:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=30
03/18/2022 03:35:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.71 on epoch=31
03/18/2022 03:35:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=32
03/18/2022 03:35:16 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.5922554276266002 on epoch=32
03/18/2022 03:35:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=32
03/18/2022 03:35:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=33
03/18/2022 03:35:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=34
03/18/2022 03:35:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=34
03/18/2022 03:35:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=35
03/18/2022 03:35:35 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.5996106860917491 on epoch=35
03/18/2022 03:35:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=36
03/18/2022 03:35:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=37
03/18/2022 03:35:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=37
03/18/2022 03:35:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=38
03/18/2022 03:35:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=39
03/18/2022 03:35:55 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.6789607071042658 on epoch=39
03/18/2022 03:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6572297282419675 -> 0.6789607071042658 on epoch=39, global_step=550
03/18/2022 03:35:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.57 on epoch=39
03/18/2022 03:36:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=40
03/18/2022 03:36:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=41
03/18/2022 03:36:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=42
03/18/2022 03:36:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=42
03/18/2022 03:36:14 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.6734130702095876 on epoch=42
03/18/2022 03:36:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=43
03/18/2022 03:36:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=44
03/18/2022 03:36:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=44
03/18/2022 03:36:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=45
03/18/2022 03:36:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=46
03/18/2022 03:36:34 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.7295960647736897 on epoch=46
03/18/2022 03:36:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6789607071042658 -> 0.7295960647736897 on epoch=46, global_step=650
03/18/2022 03:36:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=47
03/18/2022 03:36:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=47
03/18/2022 03:36:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=48
03/18/2022 03:36:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=49
03/18/2022 03:36:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=49
03/18/2022 03:36:54 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.7539287175268025 on epoch=49
03/18/2022 03:36:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7295960647736897 -> 0.7539287175268025 on epoch=49, global_step=700
03/18/2022 03:36:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=50
03/18/2022 03:36:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=51
03/18/2022 03:37:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=52
03/18/2022 03:37:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=52
03/18/2022 03:37:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=53
03/18/2022 03:37:13 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.5361530147691463 on epoch=53
03/18/2022 03:37:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=54
03/18/2022 03:37:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=54
03/18/2022 03:37:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=55
03/18/2022 03:37:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=56
03/18/2022 03:37:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=57
03/18/2022 03:37:32 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.5285024437400835 on epoch=57
03/18/2022 03:37:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=57
03/18/2022 03:37:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=58
03/18/2022 03:37:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=59
03/18/2022 03:37:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=59
03/18/2022 03:37:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=60
03/18/2022 03:37:51 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.5153932926928972 on epoch=60
03/18/2022 03:37:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=61
03/18/2022 03:37:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=62
03/18/2022 03:37:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=62
03/18/2022 03:38:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=63
03/18/2022 03:38:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=64
03/18/2022 03:38:10 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.49240650339853165 on epoch=64
03/18/2022 03:38:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=64
03/18/2022 03:38:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=65
03/18/2022 03:38:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=66
03/18/2022 03:38:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=67
03/18/2022 03:38:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=67
03/18/2022 03:38:29 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.6541023158118865 on epoch=67
03/18/2022 03:38:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=68
03/18/2022 03:38:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=69
03/18/2022 03:38:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=69
03/18/2022 03:38:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=70
03/18/2022 03:38:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=71
03/18/2022 03:38:47 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.7700110913681548 on epoch=71
03/18/2022 03:38:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7539287175268025 -> 0.7700110913681548 on epoch=71, global_step=1000
03/18/2022 03:38:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=72
03/18/2022 03:38:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=72
03/18/2022 03:38:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=73
03/18/2022 03:38:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=74
03/18/2022 03:39:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=74
03/18/2022 03:39:06 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.6305191416366531 on epoch=74
03/18/2022 03:39:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=75
03/18/2022 03:39:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=76
03/18/2022 03:39:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=77
03/18/2022 03:39:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=77
03/18/2022 03:39:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=78
03/18/2022 03:39:25 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.7423368344383979 on epoch=78
03/18/2022 03:39:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=79
03/18/2022 03:39:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=79
03/18/2022 03:39:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=80
03/18/2022 03:39:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=81
03/18/2022 03:39:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=82
03/18/2022 03:39:44 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.5757997414454679 on epoch=82
03/18/2022 03:39:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=82
03/18/2022 03:39:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=83
03/18/2022 03:39:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=84
03/18/2022 03:39:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=84
03/18/2022 03:39:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=85
03/18/2022 03:40:03 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.6373730326233107 on epoch=85
03/18/2022 03:40:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=86
03/18/2022 03:40:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=87
03/18/2022 03:40:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=87
03/18/2022 03:40:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=88
03/18/2022 03:40:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=89
03/18/2022 03:40:21 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.5770610424435315 on epoch=89
03/18/2022 03:40:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=89
03/18/2022 03:40:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=90
03/18/2022 03:40:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=91
03/18/2022 03:40:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=92
03/18/2022 03:40:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=92
03/18/2022 03:40:40 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.6046263129332653 on epoch=92
03/18/2022 03:40:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=93
03/18/2022 03:40:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=94
03/18/2022 03:40:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=94
03/18/2022 03:40:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=95
03/18/2022 03:40:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=96
03/18/2022 03:40:59 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.9061092902459128 on epoch=96
03/18/2022 03:40:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7700110913681548 -> 0.9061092902459128 on epoch=96, global_step=1350
03/18/2022 03:41:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=97
03/18/2022 03:41:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=97
03/18/2022 03:41:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=98
03/18/2022 03:41:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=99
03/18/2022 03:41:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=99
03/18/2022 03:41:18 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.8008395146915072 on epoch=99
03/18/2022 03:41:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=100
03/18/2022 03:41:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=101
03/18/2022 03:41:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=102
03/18/2022 03:41:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=102
03/18/2022 03:41:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=103
03/18/2022 03:41:37 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.9731661799617205 on epoch=103
03/18/2022 03:41:37 - INFO - __main__ - Saving model with best Classification-F1: 0.9061092902459128 -> 0.9731661799617205 on epoch=103, global_step=1450
03/18/2022 03:41:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=104
03/18/2022 03:41:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=104
03/18/2022 03:41:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=105
03/18/2022 03:41:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=106
03/18/2022 03:41:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=107
03/18/2022 03:41:56 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.9730082062150373 on epoch=107
03/18/2022 03:41:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=107
03/18/2022 03:42:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=108
03/18/2022 03:42:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=109
03/18/2022 03:42:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=109
03/18/2022 03:42:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=110
03/18/2022 03:42:14 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.9776304656760065 on epoch=110
03/18/2022 03:42:14 - INFO - __main__ - Saving model with best Classification-F1: 0.9731661799617205 -> 0.9776304656760065 on epoch=110, global_step=1550
03/18/2022 03:42:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=111
03/18/2022 03:42:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=112
03/18/2022 03:42:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=112
03/18/2022 03:42:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=113
03/18/2022 03:42:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=114
03/18/2022 03:42:33 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.9775031420192709 on epoch=114
03/18/2022 03:42:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=114
03/18/2022 03:42:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=115
03/18/2022 03:42:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=116
03/18/2022 03:42:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=117
03/18/2022 03:42:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=117
03/18/2022 03:42:51 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.8375837081956627 on epoch=117
03/18/2022 03:42:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=118
03/18/2022 03:42:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=119
03/18/2022 03:42:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=119
03/18/2022 03:43:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=120
03/18/2022 03:43:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=121
03/18/2022 03:43:10 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8006999260418093 on epoch=121
03/18/2022 03:43:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=122
03/18/2022 03:43:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=122
03/18/2022 03:43:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=123
03/18/2022 03:43:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=124
03/18/2022 03:43:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=124
03/18/2022 03:43:28 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.98212540148024 on epoch=124
03/18/2022 03:43:28 - INFO - __main__ - Saving model with best Classification-F1: 0.9776304656760065 -> 0.98212540148024 on epoch=124, global_step=1750
03/18/2022 03:43:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=125
03/18/2022 03:43:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=126
03/18/2022 03:43:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=127
03/18/2022 03:43:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=127
03/18/2022 03:43:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=128
03/18/2022 03:43:47 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.9101857282502444 on epoch=128
03/18/2022 03:43:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=129
03/18/2022 03:43:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=129
03/18/2022 03:43:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=130
03/18/2022 03:43:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=131
03/18/2022 03:44:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=132
03/18/2022 03:44:06 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7096117238731839 on epoch=132
03/18/2022 03:44:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=132
03/18/2022 03:44:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=133
03/18/2022 03:44:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=134
03/18/2022 03:44:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=134
03/18/2022 03:44:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=135
03/18/2022 03:44:24 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.8487045419846572 on epoch=135
03/18/2022 03:44:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=136
03/18/2022 03:44:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=137
03/18/2022 03:44:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=137
03/18/2022 03:44:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=138
03/18/2022 03:44:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=139
03/18/2022 03:44:42 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.6959801608558225 on epoch=139
03/18/2022 03:44:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=139
03/18/2022 03:44:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=140
03/18/2022 03:44:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=141
03/18/2022 03:44:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=142
03/18/2022 03:44:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=142
03/18/2022 03:45:00 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7204373483943376 on epoch=142
03/18/2022 03:45:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 03:45:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=144
03/18/2022 03:45:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=144
03/18/2022 03:45:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=145
03/18/2022 03:45:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=146
03/18/2022 03:45:19 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.773273532286815 on epoch=146
03/18/2022 03:45:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=147
03/18/2022 03:45:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
03/18/2022 03:45:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=148
03/18/2022 03:45:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=149
03/18/2022 03:45:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=149
03/18/2022 03:45:37 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7979159079728338 on epoch=149
03/18/2022 03:45:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=150
03/18/2022 03:45:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
03/18/2022 03:45:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=152
03/18/2022 03:45:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=152
03/18/2022 03:45:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=153
03/18/2022 03:45:55 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.9685841712051388 on epoch=153
03/18/2022 03:45:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=154
03/18/2022 03:46:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=154
03/18/2022 03:46:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/18/2022 03:46:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=156
03/18/2022 03:46:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
03/18/2022 03:46:14 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.855327468230694 on epoch=157
03/18/2022 03:46:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
03/18/2022 03:46:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=158
03/18/2022 03:46:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=159
03/18/2022 03:46:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=159
03/18/2022 03:46:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=160
03/18/2022 03:46:32 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.9078918214402084 on epoch=160
03/18/2022 03:46:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=161
03/18/2022 03:46:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=162
03/18/2022 03:46:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
03/18/2022 03:46:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=163
03/18/2022 03:46:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=164
03/18/2022 03:46:51 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.8513943994019896 on epoch=164
03/18/2022 03:46:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=164
03/18/2022 03:46:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=165
03/18/2022 03:46:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=166
03/18/2022 03:47:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
03/18/2022 03:47:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=167
03/18/2022 03:47:09 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.855327468230694 on epoch=167
03/18/2022 03:47:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=168
03/18/2022 03:47:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=169
03/18/2022 03:47:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=169
03/18/2022 03:47:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=170
03/18/2022 03:47:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=171
03/18/2022 03:47:27 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.9101857282502441 on epoch=171
03/18/2022 03:47:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=172
03/18/2022 03:47:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=172
03/18/2022 03:47:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
03/18/2022 03:47:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=174
03/18/2022 03:47:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=174
03/18/2022 03:47:46 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.9775031420192709 on epoch=174
03/18/2022 03:47:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
03/18/2022 03:47:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=176
03/18/2022 03:47:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=177
03/18/2022 03:47:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
03/18/2022 03:47:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=178
03/18/2022 03:48:06 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.8449166631967784 on epoch=178
03/18/2022 03:48:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=179
03/18/2022 03:48:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=179
03/18/2022 03:48:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=180
03/18/2022 03:48:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=181
03/18/2022 03:48:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=182
03/18/2022 03:48:24 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.9097538229907528 on epoch=182
03/18/2022 03:48:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
03/18/2022 03:48:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=183
03/18/2022 03:48:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=184
03/18/2022 03:48:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=184
03/18/2022 03:48:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=185
03/18/2022 03:48:42 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.914503910068426 on epoch=185
03/18/2022 03:48:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=186
03/18/2022 03:48:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=187
03/18/2022 03:48:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=187
03/18/2022 03:48:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
03/18/2022 03:48:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
03/18/2022 03:49:01 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.9122059302704462 on epoch=189
03/18/2022 03:49:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 03:49:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
03/18/2022 03:49:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=191
03/18/2022 03:49:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=192
03/18/2022 03:49:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/18/2022 03:49:19 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.9144998370804821 on epoch=192
03/18/2022 03:49:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=193
03/18/2022 03:49:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/18/2022 03:49:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
03/18/2022 03:49:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=195
03/18/2022 03:49:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=196
03/18/2022 03:49:38 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.8592375366568915 on epoch=196
03/18/2022 03:49:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
03/18/2022 03:49:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=197
03/18/2022 03:49:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=198
03/18/2022 03:49:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
03/18/2022 03:49:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
03/18/2022 03:49:56 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.9163766699250571 on epoch=199
03/18/2022 03:49:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=200
03/18/2022 03:50:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=201
03/18/2022 03:50:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=202
03/18/2022 03:50:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/18/2022 03:50:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=203
03/18/2022 03:50:17 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.9910627007401202 on epoch=203
03/18/2022 03:50:17 - INFO - __main__ - Saving model with best Classification-F1: 0.98212540148024 -> 0.9910627007401202 on epoch=203, global_step=2850
03/18/2022 03:50:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=204
03/18/2022 03:50:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/18/2022 03:50:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/18/2022 03:50:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
03/18/2022 03:50:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=207
03/18/2022 03:50:38 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9865940511101802 on epoch=207
03/18/2022 03:50:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=207
03/18/2022 03:50:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=208
03/18/2022 03:50:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=209
03/18/2022 03:50:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=209
03/18/2022 03:50:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=210
03/18/2022 03:50:59 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.9865940511101802 on epoch=210
03/18/2022 03:51:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=211
03/18/2022 03:51:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/18/2022 03:51:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=212
03/18/2022 03:51:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/18/2022 03:51:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
03/18/2022 03:51:13 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:51:13 - INFO - __main__ - Printing 3 examples
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 03:51:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:51:13 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:51:13 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:51:13 - INFO - __main__ - Printing 3 examples
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 03:51:13 - INFO - __main__ - ['Film']
03/18/2022 03:51:13 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:51:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:51:14 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:51:19 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.9100423590746171 on epoch=214
03/18/2022 03:51:19 - INFO - __main__ - save last model!
03/18/2022 03:51:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 03:51:19 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 03:51:19 - INFO - __main__ - Printing 3 examples
03/18/2022 03:51:19 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 03:51:19 - INFO - __main__ - ['Animal']
03/18/2022 03:51:19 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 03:51:19 - INFO - __main__ - ['Animal']
03/18/2022 03:51:19 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 03:51:19 - INFO - __main__ - ['Village']
03/18/2022 03:51:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:51:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:51:25 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 03:51:30 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:51:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:51:30 - INFO - __main__ - Starting training!
03/18/2022 03:54:22 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_42_0.2_8_predictions.txt
03/18/2022 03:54:22 - INFO - __main__ - Classification-F1 on test data: 0.5932
03/18/2022 03:54:23 - INFO - __main__ - prefix=dbpedia_14_16_42, lr=0.2, bsz=8, dev_performance=0.9910627007401202, test_performance=0.5931599358313603
03/18/2022 03:54:23 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.5, bsz=8 ...
03/18/2022 03:54:23 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:54:23 - INFO - __main__ - Printing 3 examples
03/18/2022 03:54:23 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 03:54:23 - INFO - __main__ - ['Film']
03/18/2022 03:54:23 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 03:54:23 - INFO - __main__ - ['Film']
03/18/2022 03:54:23 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 03:54:23 - INFO - __main__ - ['Film']
03/18/2022 03:54:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 03:54:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:54:24 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 03:54:24 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 03:54:24 - INFO - __main__ - Printing 3 examples
03/18/2022 03:54:24 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 03:54:24 - INFO - __main__ - ['Film']
03/18/2022 03:54:24 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 03:54:24 - INFO - __main__ - ['Film']
03/18/2022 03:54:24 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 03:54:24 - INFO - __main__ - ['Film']
03/18/2022 03:54:24 - INFO - __main__ - Tokenizing Input ...
03/18/2022 03:54:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 03:54:24 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 03:54:39 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 03:54:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 03:54:40 - INFO - __main__ - Starting training!
03/18/2022 03:54:43 - INFO - __main__ - Step 10 Global step 10 Train loss 6.62 on epoch=0
03/18/2022 03:54:46 - INFO - __main__ - Step 20 Global step 20 Train loss 5.46 on epoch=1
03/18/2022 03:54:49 - INFO - __main__ - Step 30 Global step 30 Train loss 3.55 on epoch=2
03/18/2022 03:54:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.61 on epoch=2
03/18/2022 03:54:54 - INFO - __main__ - Step 50 Global step 50 Train loss 2.11 on epoch=3
03/18/2022 03:55:00 - INFO - __main__ - Global step 50 Train loss 4.07 Classification-F1 0.1994654174609289 on epoch=3
03/18/2022 03:55:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1994654174609289 on epoch=3, global_step=50
03/18/2022 03:55:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.74 on epoch=4
03/18/2022 03:55:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.34 on epoch=4
03/18/2022 03:55:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.23 on epoch=5
03/18/2022 03:55:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=6
03/18/2022 03:55:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=7
03/18/2022 03:55:20 - INFO - __main__ - Global step 100 Train loss 1.24 Classification-F1 0.34276150001509914 on epoch=7
03/18/2022 03:55:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1994654174609289 -> 0.34276150001509914 on epoch=7, global_step=100
03/18/2022 03:55:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.85 on epoch=7
03/18/2022 03:55:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=8
03/18/2022 03:55:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=9
03/18/2022 03:55:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=9
03/18/2022 03:55:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=10
03/18/2022 03:55:40 - INFO - __main__ - Global step 150 Train loss 0.78 Classification-F1 0.5138265983642425 on epoch=10
03/18/2022 03:55:40 - INFO - __main__ - Saving model with best Classification-F1: 0.34276150001509914 -> 0.5138265983642425 on epoch=10, global_step=150
03/18/2022 03:55:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=11
03/18/2022 03:55:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=12
03/18/2022 03:55:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=12
03/18/2022 03:55:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=13
03/18/2022 03:55:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=14
03/18/2022 03:56:00 - INFO - __main__ - Global step 200 Train loss 0.64 Classification-F1 0.7755152106972419 on epoch=14
03/18/2022 03:56:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5138265983642425 -> 0.7755152106972419 on epoch=14, global_step=200
03/18/2022 03:56:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=14
03/18/2022 03:56:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=15
03/18/2022 03:56:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=16
03/18/2022 03:56:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=17
03/18/2022 03:56:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=17
03/18/2022 03:56:19 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.7501404893244857 on epoch=17
03/18/2022 03:56:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=18
03/18/2022 03:56:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=19
03/18/2022 03:56:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=19
03/18/2022 03:56:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=20
03/18/2022 03:56:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=21
03/18/2022 03:56:39 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.6739440131001689 on epoch=21
03/18/2022 03:56:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=22
03/18/2022 03:56:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=22
03/18/2022 03:56:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=23
03/18/2022 03:56:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=24
03/18/2022 03:56:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=24
03/18/2022 03:56:59 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.7303534368967806 on epoch=24
03/18/2022 03:57:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=25
03/18/2022 03:57:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=26
03/18/2022 03:57:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=27
03/18/2022 03:57:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=27
03/18/2022 03:57:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=28
03/18/2022 03:57:19 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.7545834103984801 on epoch=28
03/18/2022 03:57:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=29
03/18/2022 03:57:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=29
03/18/2022 03:57:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=30
03/18/2022 03:57:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=31
03/18/2022 03:57:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=32
03/18/2022 03:57:39 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.7201147357944991 on epoch=32
03/18/2022 03:57:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=32
03/18/2022 03:57:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=33
03/18/2022 03:57:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=34
03/18/2022 03:57:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=34
03/18/2022 03:57:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=35
03/18/2022 03:57:59 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.7145416005703642 on epoch=35
03/18/2022 03:58:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=36
03/18/2022 03:58:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=37
03/18/2022 03:58:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=37
03/18/2022 03:58:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=38
03/18/2022 03:58:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=39
03/18/2022 03:58:18 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.7745839345839346 on epoch=39
03/18/2022 03:58:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=39
03/18/2022 03:58:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=40
03/18/2022 03:58:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=41
03/18/2022 03:58:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=42
03/18/2022 03:58:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=42
03/18/2022 03:58:37 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.6537506843946188 on epoch=42
03/18/2022 03:58:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=43
03/18/2022 03:58:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=44
03/18/2022 03:58:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=44
03/18/2022 03:58:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=45
03/18/2022 03:58:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=46
03/18/2022 03:58:57 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.6451283410837738 on epoch=46
03/18/2022 03:58:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=47
03/18/2022 03:59:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=47
03/18/2022 03:59:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=48
03/18/2022 03:59:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=49
03/18/2022 03:59:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=49
03/18/2022 03:59:16 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.8107125401480241 on epoch=49
03/18/2022 03:59:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7755152106972419 -> 0.8107125401480241 on epoch=49, global_step=700
03/18/2022 03:59:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=50
03/18/2022 03:59:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=51
03/18/2022 03:59:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=52
03/18/2022 03:59:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=52
03/18/2022 03:59:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=53
03/18/2022 03:59:37 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.8340516017624059 on epoch=53
03/18/2022 03:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.8107125401480241 -> 0.8340516017624059 on epoch=53, global_step=750
03/18/2022 03:59:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=54
03/18/2022 03:59:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=54
03/18/2022 03:59:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=55
03/18/2022 03:59:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=56
03/18/2022 03:59:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=57
03/18/2022 03:59:57 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.7003224961155514 on epoch=57
03/18/2022 03:59:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=57
03/18/2022 04:00:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/18/2022 04:00:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=59
03/18/2022 04:00:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=59
03/18/2022 04:00:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=60
03/18/2022 04:00:17 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.7598798262225716 on epoch=60
03/18/2022 04:00:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=61
03/18/2022 04:00:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=62
03/18/2022 04:00:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=62
03/18/2022 04:00:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=63
03/18/2022 04:00:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=64
03/18/2022 04:00:37 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.8697067754920045 on epoch=64
03/18/2022 04:00:37 - INFO - __main__ - Saving model with best Classification-F1: 0.8340516017624059 -> 0.8697067754920045 on epoch=64, global_step=900
03/18/2022 04:00:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=64
03/18/2022 04:00:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=65
03/18/2022 04:00:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=66
03/18/2022 04:00:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=67
03/18/2022 04:00:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=67
03/18/2022 04:00:57 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.7682734191395776 on epoch=67
03/18/2022 04:00:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=68
03/18/2022 04:01:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=69
03/18/2022 04:01:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=69
03/18/2022 04:01:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=70
03/18/2022 04:01:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=71
03/18/2022 04:01:16 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.8388221544685222 on epoch=71
03/18/2022 04:01:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=72
03/18/2022 04:01:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=72
03/18/2022 04:01:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=73
03/18/2022 04:01:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=74
03/18/2022 04:01:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=74
03/18/2022 04:01:35 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.7942284602699022 on epoch=74
03/18/2022 04:01:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=75
03/18/2022 04:01:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=76
03/18/2022 04:01:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=77
03/18/2022 04:01:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=77
03/18/2022 04:01:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=78
03/18/2022 04:01:55 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.9644542861132723 on epoch=78
03/18/2022 04:01:55 - INFO - __main__ - Saving model with best Classification-F1: 0.8697067754920045 -> 0.9644542861132723 on epoch=78, global_step=1100
03/18/2022 04:01:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=79
03/18/2022 04:02:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=79
03/18/2022 04:02:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=80
03/18/2022 04:02:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=81
03/18/2022 04:02:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=82
03/18/2022 04:02:14 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.7461842816681526 on epoch=82
03/18/2022 04:02:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=82
03/18/2022 04:02:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=83
03/18/2022 04:02:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=84
03/18/2022 04:02:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=84
03/18/2022 04:02:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=85
03/18/2022 04:02:34 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.9822527251369758 on epoch=85
03/18/2022 04:02:34 - INFO - __main__ - Saving model with best Classification-F1: 0.9644542861132723 -> 0.9822527251369758 on epoch=85, global_step=1200
03/18/2022 04:02:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=86
03/18/2022 04:02:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=87
03/18/2022 04:02:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=87
03/18/2022 04:02:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=88
03/18/2022 04:02:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=89
03/18/2022 04:02:53 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.9163521361623829 on epoch=89
03/18/2022 04:02:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=89
03/18/2022 04:02:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=90
03/18/2022 04:03:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=91
03/18/2022 04:03:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=92
03/18/2022 04:03:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=92
03/18/2022 04:03:12 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.9103331704138157 on epoch=92
03/18/2022 04:03:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=93
03/18/2022 04:03:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=94
03/18/2022 04:03:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=94
03/18/2022 04:03:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=95
03/18/2022 04:03:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
03/18/2022 04:03:32 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.8938017412670622 on epoch=96
03/18/2022 04:03:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=97
03/18/2022 04:03:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=97
03/18/2022 04:03:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
03/18/2022 04:03:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=99
03/18/2022 04:03:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=99
03/18/2022 04:03:51 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.7858322756790779 on epoch=99
03/18/2022 04:03:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=100
03/18/2022 04:03:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
03/18/2022 04:03:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=102
03/18/2022 04:04:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=102
03/18/2022 04:04:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=103
03/18/2022 04:04:11 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.8824081441761217 on epoch=103
03/18/2022 04:04:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=104
03/18/2022 04:04:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=104
03/18/2022 04:04:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=105
03/18/2022 04:04:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=106
03/18/2022 04:04:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=107
03/18/2022 04:04:31 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.9144753033178081 on epoch=107
03/18/2022 04:04:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=107
03/18/2022 04:04:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
03/18/2022 04:04:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=109
03/18/2022 04:04:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=109
03/18/2022 04:04:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
03/18/2022 04:04:50 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.855304467828187 on epoch=110
03/18/2022 04:04:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=111
03/18/2022 04:04:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=112
03/18/2022 04:04:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=112
03/18/2022 04:05:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
03/18/2022 04:05:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=114
03/18/2022 04:05:09 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.863147605083089 on epoch=114
03/18/2022 04:05:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=114
03/18/2022 04:05:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=115
03/18/2022 04:05:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=116
03/18/2022 04:05:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=117
03/18/2022 04:05:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=117
03/18/2022 04:05:29 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.9101857282502446 on epoch=117
03/18/2022 04:05:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=118
03/18/2022 04:05:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=119
03/18/2022 04:05:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=119
03/18/2022 04:05:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=120
03/18/2022 04:05:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=121
03/18/2022 04:05:48 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8416609083044011 on epoch=121
03/18/2022 04:05:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=122
03/18/2022 04:05:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=122
03/18/2022 04:05:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=123
03/18/2022 04:05:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=124
03/18/2022 04:06:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
03/18/2022 04:06:08 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8322376433972652 on epoch=124
03/18/2022 04:06:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=125
03/18/2022 04:06:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=126
03/18/2022 04:06:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=127
03/18/2022 04:06:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=127
03/18/2022 04:06:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=128
03/18/2022 04:06:28 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.9147027882511752 on epoch=128
03/18/2022 04:06:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=129
03/18/2022 04:06:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=129
03/18/2022 04:06:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=130
03/18/2022 04:06:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=131
03/18/2022 04:06:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
03/18/2022 04:06:47 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.9100308588733638 on epoch=132
03/18/2022 04:06:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=132
03/18/2022 04:06:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=133
03/18/2022 04:06:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=134
03/18/2022 04:06:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=134
03/18/2022 04:07:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=135
03/18/2022 04:07:07 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.968374419997547 on epoch=135
03/18/2022 04:07:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=136
03/18/2022 04:07:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=137
03/18/2022 04:07:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=137
03/18/2022 04:07:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/18/2022 04:07:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=139
03/18/2022 04:07:27 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.9058716194200066 on epoch=139
03/18/2022 04:07:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=139
03/18/2022 04:07:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=140
03/18/2022 04:07:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=141
03/18/2022 04:07:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=142
03/18/2022 04:07:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=142
03/18/2022 04:07:50 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.9775118698505797 on epoch=142
03/18/2022 04:07:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=143
03/18/2022 04:07:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=144
03/18/2022 04:07:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=144
03/18/2022 04:08:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=145
03/18/2022 04:08:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=146
03/18/2022 04:08:10 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.9228413163897036 on epoch=146
03/18/2022 04:08:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
03/18/2022 04:08:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=147
03/18/2022 04:08:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=148
03/18/2022 04:08:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
03/18/2022 04:08:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=149
03/18/2022 04:08:30 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.9222660419545848 on epoch=149
03/18/2022 04:08:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=150
03/18/2022 04:08:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=151
03/18/2022 04:08:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=152
03/18/2022 04:08:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
03/18/2022 04:08:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
03/18/2022 04:08:51 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.9865940511101802 on epoch=153
03/18/2022 04:08:52 - INFO - __main__ - Saving model with best Classification-F1: 0.9822527251369758 -> 0.9865940511101802 on epoch=153, global_step=2150
03/18/2022 04:08:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=154
03/18/2022 04:08:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=154
03/18/2022 04:08:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=155
03/18/2022 04:09:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=156
03/18/2022 04:09:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=157
03/18/2022 04:09:13 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.9228413163897036 on epoch=157
03/18/2022 04:09:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=157
03/18/2022 04:09:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=158
03/18/2022 04:09:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/18/2022 04:09:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=159
03/18/2022 04:09:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=160
03/18/2022 04:09:32 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.9821297653958947 on epoch=160
03/18/2022 04:09:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=161
03/18/2022 04:09:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=162
03/18/2022 04:09:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=162
03/18/2022 04:09:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=163
03/18/2022 04:09:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=164
03/18/2022 04:09:52 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.9186705767350929 on epoch=164
03/18/2022 04:09:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
03/18/2022 04:09:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
03/18/2022 04:09:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=166
03/18/2022 04:10:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=167
03/18/2022 04:10:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=167
03/18/2022 04:10:11 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.9102304789512801 on epoch=167
03/18/2022 04:10:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/18/2022 04:10:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=169
03/18/2022 04:10:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=169
03/18/2022 04:10:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=170
03/18/2022 04:10:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
03/18/2022 04:10:32 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.8889820022189322 on epoch=171
03/18/2022 04:10:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=172
03/18/2022 04:10:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
03/18/2022 04:10:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=173
03/18/2022 04:10:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=174
03/18/2022 04:10:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=174
03/18/2022 04:10:51 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8483225710144509 on epoch=174
03/18/2022 04:10:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=175
03/18/2022 04:10:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=176
03/18/2022 04:10:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=177
03/18/2022 04:11:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/18/2022 04:11:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=178
03/18/2022 04:11:13 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8948001693061994 on epoch=178
03/18/2022 04:11:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=179
03/18/2022 04:11:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=179
03/18/2022 04:11:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=180
03/18/2022 04:11:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/18/2022 04:11:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
03/18/2022 04:11:35 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.9734328827765982 on epoch=182
03/18/2022 04:11:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=182
03/18/2022 04:11:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 04:11:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=184
03/18/2022 04:11:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/18/2022 04:11:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=185
03/18/2022 04:11:58 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.9775031420192712 on epoch=185
03/18/2022 04:12:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=186
03/18/2022 04:12:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=187
03/18/2022 04:12:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=187
03/18/2022 04:12:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/18/2022 04:12:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=189
03/18/2022 04:12:17 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.905832254363302 on epoch=189
03/18/2022 04:12:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 04:12:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=190
03/18/2022 04:12:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=191
03/18/2022 04:12:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=192
03/18/2022 04:12:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=192
03/18/2022 04:12:36 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.8513249449845668 on epoch=192
03/18/2022 04:12:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=193
03/18/2022 04:12:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=194
03/18/2022 04:12:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
03/18/2022 04:12:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=195
03/18/2022 04:12:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/18/2022 04:12:57 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.8322376433972652 on epoch=196
03/18/2022 04:12:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=197
03/18/2022 04:13:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 04:13:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/18/2022 04:13:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=199
03/18/2022 04:13:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/18/2022 04:13:17 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.8289345063538611 on epoch=199
03/18/2022 04:13:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/18/2022 04:13:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=201
03/18/2022 04:13:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=202
03/18/2022 04:13:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/18/2022 04:13:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=203
03/18/2022 04:13:39 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7797793604245217 on epoch=203
03/18/2022 04:13:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/18/2022 04:13:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
03/18/2022 04:13:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=205
03/18/2022 04:13:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=206
03/18/2022 04:13:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=207
03/18/2022 04:14:02 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7983817573950401 on epoch=207
03/18/2022 04:14:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
03/18/2022 04:14:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=208
03/18/2022 04:14:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/18/2022 04:14:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=209
03/18/2022 04:14:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
03/18/2022 04:14:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9078918214402087 on epoch=210
03/18/2022 04:14:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=211
03/18/2022 04:14:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=212
03/18/2022 04:14:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
03/18/2022 04:14:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=213
03/18/2022 04:14:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=214
03/18/2022 04:14:40 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:14:40 - INFO - __main__ - Printing 3 examples
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 04:14:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:14:40 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 04:14:40 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:14:40 - INFO - __main__ - Printing 3 examples
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 04:14:40 - INFO - __main__ - ['Film']
03/18/2022 04:14:40 - INFO - __main__ - Tokenizing Input ...
03/18/2022 04:14:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:14:40 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 04:14:46 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9819717916492111 on epoch=214
03/18/2022 04:14:46 - INFO - __main__ - save last model!
03/18/2022 04:14:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 04:14:46 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 04:14:46 - INFO - __main__ - Printing 3 examples
03/18/2022 04:14:46 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 04:14:46 - INFO - __main__ - ['Animal']
03/18/2022 04:14:46 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 04:14:46 - INFO - __main__ - ['Animal']
03/18/2022 04:14:46 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 04:14:46 - INFO - __main__ - ['Village']
03/18/2022 04:14:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 04:14:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:14:51 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 04:14:56 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 04:14:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 04:14:56 - INFO - __main__ - Starting training!
03/18/2022 04:17:13 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_87_0.5_8_predictions.txt
03/18/2022 04:17:13 - INFO - __main__ - Classification-F1 on test data: 0.7542
03/18/2022 04:17:13 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.5, bsz=8, dev_performance=0.9865940511101802, test_performance=0.7542197126481419
03/18/2022 04:17:13 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.4, bsz=8 ...
03/18/2022 04:17:14 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:17:14 - INFO - __main__ - Printing 3 examples
03/18/2022 04:17:14 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 04:17:14 - INFO - __main__ - ['Film']
03/18/2022 04:17:14 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 04:17:14 - INFO - __main__ - ['Film']
03/18/2022 04:17:14 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 04:17:14 - INFO - __main__ - ['Film']
03/18/2022 04:17:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 04:17:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:17:15 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 04:17:15 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:17:15 - INFO - __main__ - Printing 3 examples
03/18/2022 04:17:15 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 04:17:15 - INFO - __main__ - ['Film']
03/18/2022 04:17:15 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 04:17:15 - INFO - __main__ - ['Film']
03/18/2022 04:17:15 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 04:17:15 - INFO - __main__ - ['Film']
03/18/2022 04:17:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 04:17:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:17:15 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 04:17:33 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 04:17:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 04:17:34 - INFO - __main__ - Starting training!
03/18/2022 04:17:38 - INFO - __main__ - Step 10 Global step 10 Train loss 6.85 on epoch=0
03/18/2022 04:17:40 - INFO - __main__ - Step 20 Global step 20 Train loss 6.34 on epoch=1
03/18/2022 04:17:43 - INFO - __main__ - Step 30 Global step 30 Train loss 5.02 on epoch=2
03/18/2022 04:17:45 - INFO - __main__ - Step 40 Global step 40 Train loss 3.40 on epoch=2
03/18/2022 04:17:48 - INFO - __main__ - Step 50 Global step 50 Train loss 2.64 on epoch=3
03/18/2022 04:17:56 - INFO - __main__ - Global step 50 Train loss 4.85 Classification-F1 0.138206153521943 on epoch=3
03/18/2022 04:17:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.138206153521943 on epoch=3, global_step=50
03/18/2022 04:17:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.14 on epoch=4
03/18/2022 04:18:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.88 on epoch=4
03/18/2022 04:18:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.38 on epoch=5
03/18/2022 04:18:06 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=6
03/18/2022 04:18:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.17 on epoch=7
03/18/2022 04:18:16 - INFO - __main__ - Global step 100 Train loss 1.57 Classification-F1 0.31524787500784035 on epoch=7
03/18/2022 04:18:16 - INFO - __main__ - Saving model with best Classification-F1: 0.138206153521943 -> 0.31524787500784035 on epoch=7, global_step=100
03/18/2022 04:18:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.13 on epoch=7
03/18/2022 04:18:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=8
03/18/2022 04:18:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=9
03/18/2022 04:18:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=9
03/18/2022 04:18:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=10
03/18/2022 04:18:36 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.5414251659919667 on epoch=10
03/18/2022 04:18:36 - INFO - __main__ - Saving model with best Classification-F1: 0.31524787500784035 -> 0.5414251659919667 on epoch=10, global_step=150
03/18/2022 04:18:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=11
03/18/2022 04:18:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=12
03/18/2022 04:18:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=12
03/18/2022 04:18:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=13
03/18/2022 04:18:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=14
03/18/2022 04:18:55 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.5722614543305675 on epoch=14
03/18/2022 04:18:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5414251659919667 -> 0.5722614543305675 on epoch=14, global_step=200
03/18/2022 04:18:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=14
03/18/2022 04:19:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=15
03/18/2022 04:19:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=16
03/18/2022 04:19:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=17
03/18/2022 04:19:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=17
03/18/2022 04:19:16 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.6854053434239193 on epoch=17
03/18/2022 04:19:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5722614543305675 -> 0.6854053434239193 on epoch=17, global_step=250
03/18/2022 04:19:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=18
03/18/2022 04:19:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=19
03/18/2022 04:19:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=19
03/18/2022 04:19:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=20
03/18/2022 04:19:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=21
03/18/2022 04:19:36 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.681632472613324 on epoch=21
03/18/2022 04:19:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=22
03/18/2022 04:19:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=22
03/18/2022 04:19:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=23
03/18/2022 04:19:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.63 on epoch=24
03/18/2022 04:19:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=24
03/18/2022 04:19:56 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.5805829915107399 on epoch=24
03/18/2022 04:19:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=25
03/18/2022 04:20:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=26
03/18/2022 04:20:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=27
03/18/2022 04:20:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=27
03/18/2022 04:20:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=28
03/18/2022 04:20:15 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.763076923954533 on epoch=28
03/18/2022 04:20:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6854053434239193 -> 0.763076923954533 on epoch=28, global_step=400
03/18/2022 04:20:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=29
03/18/2022 04:20:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=29
03/18/2022 04:20:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=30
03/18/2022 04:20:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=31
03/18/2022 04:20:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=32
03/18/2022 04:20:34 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.7568472578500137 on epoch=32
03/18/2022 04:20:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=32
03/18/2022 04:20:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=33
03/18/2022 04:20:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=34
03/18/2022 04:20:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=34
03/18/2022 04:20:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=35
03/18/2022 04:20:53 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.6822840360953648 on epoch=35
03/18/2022 04:20:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=36
03/18/2022 04:20:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=37
03/18/2022 04:21:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=37
03/18/2022 04:21:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=38
03/18/2022 04:21:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=39
03/18/2022 04:21:12 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.5854369968277062 on epoch=39
03/18/2022 04:21:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=39
03/18/2022 04:21:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=40
03/18/2022 04:21:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=41
03/18/2022 04:21:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=42
03/18/2022 04:21:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=42
03/18/2022 04:21:31 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.631687319930959 on epoch=42
03/18/2022 04:21:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=43
03/18/2022 04:21:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=44
03/18/2022 04:21:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=44
03/18/2022 04:21:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=45
03/18/2022 04:21:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=46
03/18/2022 04:21:51 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.6517676054410065 on epoch=46
03/18/2022 04:21:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=47
03/18/2022 04:21:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=47
03/18/2022 04:21:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=48
03/18/2022 04:22:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=49
03/18/2022 04:22:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=49
03/18/2022 04:22:10 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.6953056968451086 on epoch=49
03/18/2022 04:22:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=50
03/18/2022 04:22:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=51
03/18/2022 04:22:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=52
03/18/2022 04:22:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=52
03/18/2022 04:22:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=53
03/18/2022 04:22:29 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6413398712210256 on epoch=53
03/18/2022 04:22:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=54
03/18/2022 04:22:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=54
03/18/2022 04:22:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=55
03/18/2022 04:22:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=56
03/18/2022 04:22:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=57
03/18/2022 04:22:48 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.9093245627334193 on epoch=57
03/18/2022 04:22:48 - INFO - __main__ - Saving model with best Classification-F1: 0.763076923954533 -> 0.9093245627334193 on epoch=57, global_step=800
03/18/2022 04:22:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=57
03/18/2022 04:22:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=58
03/18/2022 04:22:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=59
03/18/2022 04:22:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=59
03/18/2022 04:23:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=60
03/18/2022 04:23:08 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.635992254446669 on epoch=60
03/18/2022 04:23:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=61
03/18/2022 04:23:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=62
03/18/2022 04:23:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=62
03/18/2022 04:23:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=63
03/18/2022 04:23:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=64
03/18/2022 04:23:27 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.7499937804378061 on epoch=64
03/18/2022 04:23:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=64
03/18/2022 04:23:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=65
03/18/2022 04:23:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=66
03/18/2022 04:23:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=67
03/18/2022 04:23:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=67
03/18/2022 04:23:46 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.6447313604147219 on epoch=67
03/18/2022 04:23:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=68
03/18/2022 04:23:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=69
03/18/2022 04:23:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=69
03/18/2022 04:23:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=70
03/18/2022 04:23:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=71
03/18/2022 04:24:05 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.9097292892280787 on epoch=71
03/18/2022 04:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.9093245627334193 -> 0.9097292892280787 on epoch=71, global_step=1000
03/18/2022 04:24:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=72
03/18/2022 04:24:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=72
03/18/2022 04:24:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=73
03/18/2022 04:24:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=74
03/18/2022 04:24:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=74
03/18/2022 04:24:25 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.9820991153059465 on epoch=74
03/18/2022 04:24:25 - INFO - __main__ - Saving model with best Classification-F1: 0.9097292892280787 -> 0.9820991153059465 on epoch=74, global_step=1050
03/18/2022 04:24:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=75
03/18/2022 04:24:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=76
03/18/2022 04:24:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=77
03/18/2022 04:24:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=77
03/18/2022 04:24:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=78
03/18/2022 04:24:44 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.8589687194525905 on epoch=78
03/18/2022 04:24:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=79
03/18/2022 04:24:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=79
03/18/2022 04:24:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=80
03/18/2022 04:24:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=81
03/18/2022 04:24:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=82
03/18/2022 04:25:03 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.8630131964809384 on epoch=82
03/18/2022 04:25:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=82
03/18/2022 04:25:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=83
03/18/2022 04:25:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=84
03/18/2022 04:25:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=84
03/18/2022 04:25:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=85
03/18/2022 04:25:22 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.8267584616324536 on epoch=85
03/18/2022 04:25:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=86
03/18/2022 04:25:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=87
03/18/2022 04:25:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=87
03/18/2022 04:25:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=88
03/18/2022 04:25:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=89
03/18/2022 04:25:41 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.7001623018424094 on epoch=89
03/18/2022 04:25:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=89
03/18/2022 04:25:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=90
03/18/2022 04:25:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=91
03/18/2022 04:25:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=92
03/18/2022 04:25:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=92
03/18/2022 04:26:01 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.798855664561575 on epoch=92
03/18/2022 04:26:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=93
03/18/2022 04:26:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=94
03/18/2022 04:26:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=94
03/18/2022 04:26:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=95
03/18/2022 04:26:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=96
03/18/2022 04:26:20 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.9060231345715216 on epoch=96
03/18/2022 04:26:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=97
03/18/2022 04:26:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=97
03/18/2022 04:26:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=98
03/18/2022 04:26:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=99
03/18/2022 04:26:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=99
03/18/2022 04:26:39 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.9140589712475141 on epoch=99
03/18/2022 04:26:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=100
03/18/2022 04:26:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=101
03/18/2022 04:26:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=102
03/18/2022 04:26:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=102
03/18/2022 04:26:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=103
03/18/2022 04:26:58 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.9140589712475141 on epoch=103
03/18/2022 04:27:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=104
03/18/2022 04:27:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=104
03/18/2022 04:27:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=105
03/18/2022 04:27:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=106
03/18/2022 04:27:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=107
03/18/2022 04:27:17 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.8932593158475512 on epoch=107
03/18/2022 04:27:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=107
03/18/2022 04:27:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=108
03/18/2022 04:27:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=109
03/18/2022 04:27:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=109
03/18/2022 04:27:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=110
03/18/2022 04:27:36 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.9100070670058565 on epoch=110
03/18/2022 04:27:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=111
03/18/2022 04:27:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=112
03/18/2022 04:27:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=112
03/18/2022 04:27:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=113
03/18/2022 04:27:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=114
03/18/2022 04:27:56 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.9864404412791511 on epoch=114
03/18/2022 04:27:56 - INFO - __main__ - Saving model with best Classification-F1: 0.9820991153059465 -> 0.9864404412791511 on epoch=114, global_step=1600
03/18/2022 04:27:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=114
03/18/2022 04:28:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=115
03/18/2022 04:28:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=116
03/18/2022 04:28:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=117
03/18/2022 04:28:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=117
03/18/2022 04:28:15 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.9910627007401202 on epoch=117
03/18/2022 04:28:15 - INFO - __main__ - Saving model with best Classification-F1: 0.9864404412791511 -> 0.9910627007401202 on epoch=117, global_step=1650
03/18/2022 04:28:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=118
03/18/2022 04:28:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=119
03/18/2022 04:28:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=119
03/18/2022 04:28:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=120
03/18/2022 04:28:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=121
03/18/2022 04:28:34 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.9186746497230369 on epoch=121
03/18/2022 04:28:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=122
03/18/2022 04:28:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=122
03/18/2022 04:28:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
03/18/2022 04:28:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=124
03/18/2022 04:28:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=124
03/18/2022 04:28:52 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.9910627007401202 on epoch=124
03/18/2022 04:28:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
03/18/2022 04:28:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=126
03/18/2022 04:29:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=127
03/18/2022 04:29:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=127
03/18/2022 04:29:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=128
03/18/2022 04:29:11 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.9773073553417726 on epoch=128
03/18/2022 04:29:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=129
03/18/2022 04:29:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=129
03/18/2022 04:29:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=130
03/18/2022 04:29:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=131
03/18/2022 04:29:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=132
03/18/2022 04:29:30 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.9821034792216007 on epoch=132
03/18/2022 04:29:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=132
03/18/2022 04:29:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=133
03/18/2022 04:29:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=134
03/18/2022 04:29:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=134
03/18/2022 04:29:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=135
03/18/2022 04:29:49 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.9247181492342783 on epoch=135
03/18/2022 04:29:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=136
03/18/2022 04:29:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=137
03/18/2022 04:29:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=137
03/18/2022 04:29:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=138
03/18/2022 04:30:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=139
03/18/2022 04:30:08 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.9777205897021565 on epoch=139
03/18/2022 04:30:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=139
03/18/2022 04:30:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=140
03/18/2022 04:30:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=141
03/18/2022 04:30:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=142
03/18/2022 04:30:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=142
03/18/2022 04:30:26 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.9821892393320965 on epoch=142
03/18/2022 04:30:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=143
03/18/2022 04:30:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=144
03/18/2022 04:30:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=144
03/18/2022 04:30:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=145
03/18/2022 04:30:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=146
03/18/2022 04:30:45 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.990909090909091 on epoch=146
03/18/2022 04:30:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=147
03/18/2022 04:30:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=147
03/18/2022 04:30:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=148
03/18/2022 04:30:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=149
03/18/2022 04:30:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=149
03/18/2022 04:31:03 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.9864448051948053 on epoch=149
03/18/2022 04:31:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=150
03/18/2022 04:31:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=151
03/18/2022 04:31:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=152
03/18/2022 04:31:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=152
03/18/2022 04:31:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=153
03/18/2022 04:31:22 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.9226979472140762 on epoch=153
03/18/2022 04:31:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=154
03/18/2022 04:31:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=154
03/18/2022 04:31:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=155
03/18/2022 04:31:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=156
03/18/2022 04:31:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=157
03/18/2022 04:31:40 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.9776304656760065 on epoch=157
03/18/2022 04:31:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
03/18/2022 04:31:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
03/18/2022 04:31:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=159
03/18/2022 04:31:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=159
03/18/2022 04:31:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=160
03/18/2022 04:31:58 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.9861173309449172 on epoch=160
03/18/2022 04:32:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=161
03/18/2022 04:32:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=162
03/18/2022 04:32:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=162
03/18/2022 04:32:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=163
03/18/2022 04:32:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=164
03/18/2022 04:32:17 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.8406094508013306 on epoch=164
03/18/2022 04:32:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=164
03/18/2022 04:32:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=165
03/18/2022 04:32:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=166
03/18/2022 04:32:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=167
03/18/2022 04:32:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/18/2022 04:32:35 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.9816530452306315 on epoch=167
03/18/2022 04:32:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=168
03/18/2022 04:32:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
03/18/2022 04:32:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=169
03/18/2022 04:32:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=170
03/18/2022 04:32:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=171
03/18/2022 04:32:54 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.9117609914495344 on epoch=171
03/18/2022 04:32:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=172
03/18/2022 04:32:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=172
03/18/2022 04:33:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=173
03/18/2022 04:33:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=174
03/18/2022 04:33:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.33 on epoch=174
03/18/2022 04:33:12 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.860145231112973 on epoch=174
03/18/2022 04:33:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=175
03/18/2022 04:33:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=176
03/18/2022 04:33:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=177
03/18/2022 04:33:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=177
03/18/2022 04:33:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=178
03/18/2022 04:33:31 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.9817760049717126 on epoch=178
03/18/2022 04:33:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=179
03/18/2022 04:33:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=179
03/18/2022 04:33:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=180
03/18/2022 04:33:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=181
03/18/2022 04:33:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=182
03/18/2022 04:33:50 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.9861173309449172 on epoch=182
03/18/2022 04:33:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=182
03/18/2022 04:33:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 04:33:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=184
03/18/2022 04:34:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=184
03/18/2022 04:34:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=185
03/18/2022 04:34:08 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7955429072225513 on epoch=185
03/18/2022 04:34:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
03/18/2022 04:34:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
03/18/2022 04:34:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=187
03/18/2022 04:34:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=188
03/18/2022 04:34:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=189
03/18/2022 04:34:27 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.9159317311041448 on epoch=189
03/18/2022 04:34:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=189
03/18/2022 04:34:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=190
03/18/2022 04:34:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=191
03/18/2022 04:34:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=192
03/18/2022 04:34:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=192
03/18/2022 04:34:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.9117609914495344 on epoch=192
03/18/2022 04:34:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=193
03/18/2022 04:34:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=194
03/18/2022 04:34:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=194
03/18/2022 04:34:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=195
03/18/2022 04:34:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=196
03/18/2022 04:35:04 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.9861173309449172 on epoch=196
03/18/2022 04:35:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=197
03/18/2022 04:35:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=197
03/18/2022 04:35:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/18/2022 04:35:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=199
03/18/2022 04:35:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=199
03/18/2022 04:35:22 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.9816530452306315 on epoch=199
03/18/2022 04:35:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=200
03/18/2022 04:35:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=201
03/18/2022 04:35:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=202
03/18/2022 04:35:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=202
03/18/2022 04:35:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=203
03/18/2022 04:35:41 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.9771843956006915 on epoch=203
03/18/2022 04:35:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=204
03/18/2022 04:35:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=204
03/18/2022 04:35:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=205
03/18/2022 04:35:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=206
03/18/2022 04:35:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=207
03/18/2022 04:35:59 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9775075059349254 on epoch=207
03/18/2022 04:36:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=207
03/18/2022 04:36:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=208
03/18/2022 04:36:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=209
03/18/2022 04:36:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=209
03/18/2022 04:36:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=210
03/18/2022 04:36:18 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.9819761555648654 on epoch=210
03/18/2022 04:36:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=211
03/18/2022 04:36:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=212
03/18/2022 04:36:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=212
03/18/2022 04:36:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=213
03/18/2022 04:36:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/18/2022 04:36:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:36:33 - INFO - __main__ - Printing 3 examples
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 04:36:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:36:33 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 04:36:33 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:36:33 - INFO - __main__ - Printing 3 examples
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 04:36:33 - INFO - __main__ - ['Film']
03/18/2022 04:36:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 04:36:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:36:33 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 04:36:37 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.9143605408927992 on epoch=214
03/18/2022 04:36:37 - INFO - __main__ - save last model!
03/18/2022 04:36:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 04:36:37 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 04:36:37 - INFO - __main__ - Printing 3 examples
03/18/2022 04:36:37 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 04:36:37 - INFO - __main__ - ['Animal']
03/18/2022 04:36:37 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 04:36:37 - INFO - __main__ - ['Animal']
03/18/2022 04:36:37 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 04:36:37 - INFO - __main__ - ['Village']
03/18/2022 04:36:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 04:36:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:36:42 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 04:36:48 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 04:36:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 04:36:49 - INFO - __main__ - Starting training!
03/18/2022 04:38:50 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_87_0.4_8_predictions.txt
03/18/2022 04:38:50 - INFO - __main__ - Classification-F1 on test data: 0.6556
03/18/2022 04:38:51 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.4, bsz=8, dev_performance=0.9910627007401202, test_performance=0.6555805211717504
03/18/2022 04:38:51 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.3, bsz=8 ...
03/18/2022 04:38:52 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:38:52 - INFO - __main__ - Printing 3 examples
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 04:38:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:38:52 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 04:38:52 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 04:38:52 - INFO - __main__ - Printing 3 examples
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 04:38:52 - INFO - __main__ - ['Film']
03/18/2022 04:38:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 04:38:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 04:38:52 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 04:39:11 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 04:39:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 04:39:12 - INFO - __main__ - Starting training!
03/18/2022 04:39:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=0
03/18/2022 04:39:19 - INFO - __main__ - Step 20 Global step 20 Train loss 6.60 on epoch=1
03/18/2022 04:39:21 - INFO - __main__ - Step 30 Global step 30 Train loss 6.08 on epoch=2
03/18/2022 04:39:24 - INFO - __main__ - Step 40 Global step 40 Train loss 5.95 on epoch=2
03/18/2022 04:39:26 - INFO - __main__ - Step 50 Global step 50 Train loss 4.99 on epoch=3
03/18/2022 04:40:47 - INFO - __main__ - Global step 50 Train loss 6.10 Classification-F1 0.0 on epoch=3
03/18/2022 04:40:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
03/18/2022 04:40:49 - INFO - __main__ - Step 60 Global step 60 Train loss 3.89 on epoch=4
03/18/2022 04:40:52 - INFO - __main__ - Step 70 Global step 70 Train loss 3.18 on epoch=4
03/18/2022 04:40:54 - INFO - __main__ - Step 80 Global step 80 Train loss 2.57 on epoch=5
03/18/2022 04:40:57 - INFO - __main__ - Step 90 Global step 90 Train loss 2.18 on epoch=6
03/18/2022 04:40:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.89 on epoch=7
03/18/2022 04:41:06 - INFO - __main__ - Global step 100 Train loss 2.74 Classification-F1 0.22887879024109728 on epoch=7
03/18/2022 04:41:06 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.22887879024109728 on epoch=7, global_step=100
03/18/2022 04:41:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.63 on epoch=7
03/18/2022 04:41:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.50 on epoch=8
03/18/2022 04:41:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.24 on epoch=9
03/18/2022 04:41:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.24 on epoch=9
03/18/2022 04:41:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.07 on epoch=10
03/18/2022 04:41:25 - INFO - __main__ - Global step 150 Train loss 1.34 Classification-F1 0.3660288354358087 on epoch=10
03/18/2022 04:41:25 - INFO - __main__ - Saving model with best Classification-F1: 0.22887879024109728 -> 0.3660288354358087 on epoch=10, global_step=150
03/18/2022 04:41:27 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=11
03/18/2022 04:41:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.10 on epoch=12
03/18/2022 04:41:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=12
03/18/2022 04:41:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=13
03/18/2022 04:41:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=14
03/18/2022 04:41:44 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.5056113541701142 on epoch=14
03/18/2022 04:41:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3660288354358087 -> 0.5056113541701142 on epoch=14, global_step=200
03/18/2022 04:41:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=14
03/18/2022 04:41:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=15
03/18/2022 04:41:51 - INFO - __main__ - Step 230 Global step 230 Train loss 1.39 on epoch=16
03/18/2022 04:41:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=17
03/18/2022 04:41:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=17
03/18/2022 04:42:04 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.4426629581711179 on epoch=17
03/18/2022 04:42:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=18
03/18/2022 04:42:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=19
03/18/2022 04:42:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=19
03/18/2022 04:42:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.64 on epoch=20
03/18/2022 04:42:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.73 on epoch=21
03/18/2022 04:42:25 - INFO - __main__ - Global step 300 Train loss 0.77 Classification-F1 0.5228426560335969 on epoch=21
03/18/2022 04:42:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5056113541701142 -> 0.5228426560335969 on epoch=21, global_step=300
03/18/2022 04:42:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=22
03/18/2022 04:42:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=22
03/18/2022 04:42:33 - INFO - __main__ - Step 330 Global step 330 Train loss 1.07 on epoch=23
03/18/2022 04:42:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.99 on epoch=24
03/18/2022 04:42:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.78 on epoch=24
03/18/2022 04:42:45 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.49397055150731617 on epoch=24
03/18/2022 04:42:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=25
03/18/2022 04:42:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.77 on epoch=26
03/18/2022 04:42:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=27
03/18/2022 04:42:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=27
03/18/2022 04:42:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=28
03/18/2022 04:43:06 - INFO - __main__ - Global step 400 Train loss 0.73 Classification-F1 0.49800327807204575 on epoch=28
03/18/2022 04:43:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=29
03/18/2022 04:43:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=29
03/18/2022 04:43:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=30
03/18/2022 04:43:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=31
03/18/2022 04:43:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.67 on epoch=32
03/18/2022 04:43:26 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.6589270505380318 on epoch=32
03/18/2022 04:43:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5228426560335969 -> 0.6589270505380318 on epoch=32, global_step=450
03/18/2022 04:43:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=32
03/18/2022 04:43:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=33
03/18/2022 04:43:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=34
03/18/2022 04:43:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=34
03/18/2022 04:43:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=35
03/18/2022 04:43:48 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.7797878252391437 on epoch=35
03/18/2022 04:43:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6589270505380318 -> 0.7797878252391437 on epoch=35, global_step=500
03/18/2022 04:43:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=36
03/18/2022 04:43:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=37
03/18/2022 04:43:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=37
03/18/2022 04:43:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=38
03/18/2022 04:44:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=39
03/18/2022 04:44:08 - INFO - __main__ - Global step 550 Train loss 0.59 Classification-F1 0.6462292387119812 on epoch=39
03/18/2022 04:44:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=39
03/18/2022 04:44:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=40
03/18/2022 04:44:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=41
03/18/2022 04:44:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.66 on epoch=42
03/18/2022 04:44:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=42
03/18/2022 04:44:29 - INFO - __main__ - Global step 600 Train loss 0.57 Classification-F1 0.7216449215917096 on epoch=42
03/18/2022 04:44:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.59 on epoch=43
03/18/2022 04:44:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=44
03/18/2022 04:44:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=44
03/18/2022 04:44:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=45
03/18/2022 04:44:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=46
03/18/2022 04:44:49 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.5331185301767751 on epoch=46
03/18/2022 04:44:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=47
03/18/2022 04:44:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=47
03/18/2022 04:44:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=48
03/18/2022 04:44:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=49
03/18/2022 04:45:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=49
03/18/2022 04:45:09 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.6537265501348455 on epoch=49
03/18/2022 04:45:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=50
03/18/2022 04:45:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=51
03/18/2022 04:45:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=52
03/18/2022 04:45:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=52
03/18/2022 04:45:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=53
03/18/2022 04:45:30 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.7123304387896449 on epoch=53
03/18/2022 04:45:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=54
03/18/2022 04:45:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=54
03/18/2022 04:45:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=55
03/18/2022 04:45:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=56
03/18/2022 04:45:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=57
03/18/2022 04:45:51 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.8951565221663876 on epoch=57
03/18/2022 04:45:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7797878252391437 -> 0.8951565221663876 on epoch=57, global_step=800
03/18/2022 04:45:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=57
03/18/2022 04:45:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=58
03/18/2022 04:45:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=59
03/18/2022 04:46:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=59
03/18/2022 04:46:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=60
03/18/2022 04:46:12 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.7874449218225903 on epoch=60
03/18/2022 04:46:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=61
03/18/2022 04:46:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=62
03/18/2022 04:46:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=62
03/18/2022 04:46:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=63
03/18/2022 04:46:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=64
03/18/2022 04:46:32 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.7496661072103352 on epoch=64
03/18/2022 04:46:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=64
03/18/2022 04:46:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=65
03/18/2022 04:46:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=66
03/18/2022 04:46:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=67
03/18/2022 04:46:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=67
03/18/2022 04:46:52 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.7903460884915974 on epoch=67
03/18/2022 04:46:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=68
03/18/2022 04:46:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=69
03/18/2022 04:47:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=69
03/18/2022 04:47:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=70
03/18/2022 04:47:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=71
03/18/2022 04:47:12 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.7571689244718213 on epoch=71
03/18/2022 04:47:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=72
03/18/2022 04:47:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=72
03/18/2022 04:47:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=73
03/18/2022 04:47:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=74
03/18/2022 04:47:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=74
03/18/2022 04:47:32 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.7670619895403458 on epoch=74
03/18/2022 04:47:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=75
03/18/2022 04:47:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=76
03/18/2022 04:47:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=77
03/18/2022 04:47:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=77
03/18/2022 04:47:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=78
03/18/2022 04:47:52 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.7576770185390008 on epoch=78
03/18/2022 04:47:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=79
03/18/2022 04:47:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=79
03/18/2022 04:47:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=80
03/18/2022 04:48:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=81
03/18/2022 04:48:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=82
03/18/2022 04:48:11 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.6690922907123097 on epoch=82
03/18/2022 04:48:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=82
03/18/2022 04:48:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=83
03/18/2022 04:48:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=84
03/18/2022 04:48:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=84
03/18/2022 04:48:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=85
03/18/2022 04:48:31 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7057558672264554 on epoch=85
03/18/2022 04:48:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=86
03/18/2022 04:48:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=87
03/18/2022 04:48:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=87
03/18/2022 04:48:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=88
03/18/2022 04:48:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=89
03/18/2022 04:48:51 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.6768241594425596 on epoch=89
03/18/2022 04:48:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=89
03/18/2022 04:48:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=90
03/18/2022 04:48:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=91
03/18/2022 04:49:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=92
03/18/2022 04:49:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=92
03/18/2022 04:49:10 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.6189156681863553 on epoch=92
03/18/2022 04:49:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=93
03/18/2022 04:49:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=94
03/18/2022 04:49:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=94
03/18/2022 04:49:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=95
03/18/2022 04:49:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=96
03/18/2022 04:49:30 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.7284986852349282 on epoch=96
03/18/2022 04:49:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=97
03/18/2022 04:49:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=97
03/18/2022 04:49:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=98
03/18/2022 04:49:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=99
03/18/2022 04:49:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=99
03/18/2022 04:49:49 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.8627499856247485 on epoch=99
03/18/2022 04:49:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=100
03/18/2022 04:49:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=101
03/18/2022 04:49:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=102
03/18/2022 04:50:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=102
03/18/2022 04:50:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=103
03/18/2022 04:50:09 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.968434925289764 on epoch=103
03/18/2022 04:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8951565221663876 -> 0.968434925289764 on epoch=103, global_step=1450
03/18/2022 04:50:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=104
03/18/2022 04:50:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=104
03/18/2022 04:50:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=105
03/18/2022 04:50:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=106
03/18/2022 04:50:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=107
03/18/2022 04:50:28 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.8725529693793452 on epoch=107
03/18/2022 04:50:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=107
03/18/2022 04:50:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=108
03/18/2022 04:50:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=109
03/18/2022 04:50:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=109
03/18/2022 04:50:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=110
03/18/2022 04:50:48 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.7600093869990063 on epoch=110
03/18/2022 04:50:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=111
03/18/2022 04:50:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=112
03/18/2022 04:50:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=112
03/18/2022 04:50:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=113
03/18/2022 04:51:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=114
03/18/2022 04:51:07 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.5456419541981039 on epoch=114
03/18/2022 04:51:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=114
03/18/2022 04:51:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=115
03/18/2022 04:51:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=116
03/18/2022 04:51:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=117
03/18/2022 04:51:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=117
03/18/2022 04:51:26 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7846343962805064 on epoch=117
03/18/2022 04:51:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=118
03/18/2022 04:51:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=119
03/18/2022 04:51:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=119
03/18/2022 04:51:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=120
03/18/2022 04:51:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=121
03/18/2022 04:51:46 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.6748823282978882 on epoch=121
03/18/2022 04:51:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=122
03/18/2022 04:51:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=122
03/18/2022 04:51:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=123
03/18/2022 04:51:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=124
03/18/2022 04:51:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=124
03/18/2022 04:52:05 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7955864783189261 on epoch=124
03/18/2022 04:52:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=125
03/18/2022 04:52:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=126
03/18/2022 04:52:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=127
03/18/2022 04:52:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=127
03/18/2022 04:52:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=128
03/18/2022 04:52:25 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.7044069050263296 on epoch=128
03/18/2022 04:52:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=129
03/18/2022 04:52:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=129
03/18/2022 04:52:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=130
03/18/2022 04:52:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=131
03/18/2022 04:52:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=132
03/18/2022 04:52:46 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.8336865626521617 on epoch=132
03/18/2022 04:52:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=132
03/18/2022 04:52:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=133
03/18/2022 04:52:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=134
03/18/2022 04:52:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=134
03/18/2022 04:52:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=135
03/18/2022 04:53:06 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.8494089076246334 on epoch=135
03/18/2022 04:53:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=136
03/18/2022 04:53:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=137
03/18/2022 04:53:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=137
03/18/2022 04:53:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=138
03/18/2022 04:53:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=139
03/18/2022 04:53:27 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.8845857720510929 on epoch=139
03/18/2022 04:53:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=139
03/18/2022 04:53:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=140
03/18/2022 04:53:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=141
03/18/2022 04:53:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=142
03/18/2022 04:53:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=142
03/18/2022 04:53:47 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.8407276916543238 on epoch=142
03/18/2022 04:53:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=143
03/18/2022 04:53:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=144
03/18/2022 04:53:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=144
03/18/2022 04:53:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=145
03/18/2022 04:54:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=146
03/18/2022 04:54:08 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.8669941238031599 on epoch=146
03/18/2022 04:54:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=147
03/18/2022 04:54:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=147
03/18/2022 04:54:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=148
03/18/2022 04:54:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=149
03/18/2022 04:54:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
03/18/2022 04:54:28 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.909753822990753 on epoch=149
03/18/2022 04:54:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=150
03/18/2022 04:54:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=151
03/18/2022 04:54:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=152
03/18/2022 04:54:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=152
03/18/2022 04:54:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=153
03/18/2022 04:54:49 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.905951089776255 on epoch=153
03/18/2022 04:54:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
03/18/2022 04:54:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=154
03/18/2022 04:54:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=155
03/18/2022 04:54:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
03/18/2022 04:55:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=157
03/18/2022 04:55:10 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.9104479328388246 on epoch=157
03/18/2022 04:55:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=157
03/18/2022 04:55:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=158
03/18/2022 04:55:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=159
03/18/2022 04:55:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=159
03/18/2022 04:55:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=160
03/18/2022 04:55:29 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.8310007729822773 on epoch=160
03/18/2022 04:55:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=161
03/18/2022 04:55:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=162
03/18/2022 04:55:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=162
03/18/2022 04:55:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=163
03/18/2022 04:55:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=164
03/18/2022 04:55:49 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.8863514173998046 on epoch=164
03/18/2022 04:55:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=164
03/18/2022 04:55:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=165
03/18/2022 04:55:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=166
03/18/2022 04:56:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=167
03/18/2022 04:56:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=167
03/18/2022 04:56:10 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.9026346803877395 on epoch=167
03/18/2022 04:56:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=168
03/18/2022 04:56:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=169
03/18/2022 04:56:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=169
03/18/2022 04:56:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=170
03/18/2022 04:56:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=171
03/18/2022 04:56:29 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.8514173998044966 on epoch=171
03/18/2022 04:56:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=172
03/18/2022 04:56:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=172
03/18/2022 04:56:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=173
03/18/2022 04:56:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=174
03/18/2022 04:56:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=174
03/18/2022 04:56:48 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.8932254899128224 on epoch=174
03/18/2022 04:56:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=175
03/18/2022 04:56:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=176
03/18/2022 04:56:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=177
03/18/2022 04:56:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=177
03/18/2022 04:57:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=178
03/18/2022 04:57:08 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.9144998370804823 on epoch=178
03/18/2022 04:57:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=179
03/18/2022 04:57:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=179
03/18/2022 04:57:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=180
03/18/2022 04:57:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=181
03/18/2022 04:57:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=182
03/18/2022 04:57:27 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.9122059302704464 on epoch=182
03/18/2022 04:57:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=182
03/18/2022 04:57:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=183
03/18/2022 04:57:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=184
03/18/2022 04:57:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=184
03/18/2022 04:57:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=185
03/18/2022 04:57:46 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.901762737015796 on epoch=185
03/18/2022 04:57:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=186
03/18/2022 04:57:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=187
03/18/2022 04:57:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
03/18/2022 04:57:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=188
03/18/2022 04:57:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=189
03/18/2022 04:58:05 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9776611157659547 on epoch=189
03/18/2022 04:58:05 - INFO - __main__ - Saving model with best Classification-F1: 0.968434925289764 -> 0.9776611157659547 on epoch=189, global_step=2650
03/18/2022 04:58:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=189
03/18/2022 04:58:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=190
03/18/2022 04:58:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=191
03/18/2022 04:58:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=192
03/18/2022 04:58:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
03/18/2022 04:58:24 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.9144998370804823 on epoch=192
03/18/2022 04:58:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=193
03/18/2022 04:58:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=194
03/18/2022 04:58:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=194
03/18/2022 04:58:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=195
03/18/2022 04:58:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=196
03/18/2022 04:58:44 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.8454574205048588 on epoch=196
03/18/2022 04:58:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=197
03/18/2022 04:58:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=197
03/18/2022 04:58:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=198
03/18/2022 04:58:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=199
03/18/2022 04:58:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=199
03/18/2022 04:59:03 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.9080277634025262 on epoch=199
03/18/2022 04:59:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=200
03/18/2022 04:59:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=201
03/18/2022 04:59:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=202
03/18/2022 04:59:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=202
03/18/2022 04:59:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=203
03/18/2022 04:59:22 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.9186705767350929 on epoch=203
03/18/2022 04:59:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=204
03/18/2022 04:59:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=204
03/18/2022 04:59:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=205
03/18/2022 04:59:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=206
03/18/2022 04:59:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=207
03/18/2022 04:59:42 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.9732097632257474 on epoch=207
03/18/2022 04:59:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=207
03/18/2022 04:59:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=208
03/18/2022 04:59:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=209
03/18/2022 04:59:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=209
03/18/2022 04:59:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=210
03/18/2022 05:00:01 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.9683744199975469 on epoch=210
03/18/2022 05:00:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=211
03/18/2022 05:00:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=212
03/18/2022 05:00:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=212
03/18/2022 05:00:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/18/2022 05:00:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=214
03/18/2022 05:00:15 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 05:00:15 - INFO - __main__ - Printing 3 examples
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 05:00:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:00:15 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 05:00:15 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 05:00:15 - INFO - __main__ - Printing 3 examples
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 05:00:15 - INFO - __main__ - ['Film']
03/18/2022 05:00:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:00:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:00:16 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 05:00:20 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8452032031654304 on epoch=214
03/18/2022 05:00:20 - INFO - __main__ - save last model!
03/18/2022 05:00:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 05:00:20 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 05:00:20 - INFO - __main__ - Printing 3 examples
03/18/2022 05:00:20 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 05:00:20 - INFO - __main__ - ['Animal']
03/18/2022 05:00:20 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 05:00:20 - INFO - __main__ - ['Animal']
03/18/2022 05:00:20 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 05:00:20 - INFO - __main__ - ['Village']
03/18/2022 05:00:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:00:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:00:26 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 05:00:31 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:00:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:00:32 - INFO - __main__ - Starting training!
03/18/2022 05:02:33 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_87_0.3_8_predictions.txt
03/18/2022 05:02:33 - INFO - __main__ - Classification-F1 on test data: 0.6499
03/18/2022 05:02:33 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.3, bsz=8, dev_performance=0.9776611157659547, test_performance=0.6499093096198377
03/18/2022 05:02:33 - INFO - __main__ - Running ... prefix=dbpedia_14_16_87, lr=0.2, bsz=8 ...
03/18/2022 05:02:34 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 05:02:34 - INFO - __main__ - Printing 3 examples
03/18/2022 05:02:34 - INFO - __main__ -  [dbpedia_14] Aibō The Movie (相棒 -劇場版- 絶体絶命! 42.195km 東京ビッグシティマラソン) is a 2008 Japanese film directed by Seiji Izumi and based on the television series Aibō.
03/18/2022 05:02:34 - INFO - __main__ - ['Film']
03/18/2022 05:02:34 - INFO - __main__ -  [dbpedia_14] Time Traveller: The Girl Who Leapt Through Time originally released as Toki o Kakeru Shōjo (時をかける少女 lit. The Girl Who Runs Through Time) is a 2010 Japanese science fiction film directed by Masaaki Taniguchi and written by Tomoe Kanno. It is the fourth film based on the novel The Girl Who Leapt Through Time and is a sequel to the original 1983 film adaptation. The film stars Riisa Naka as the protagonist Akari Yoshiyama daughter of the original story's protagonist Kazuko Yoshiyama.
03/18/2022 05:02:34 - INFO - __main__ - ['Film']
03/18/2022 05:02:34 - INFO - __main__ -  [dbpedia_14] Judy of Rogue's Harbor was a 1920 silent drama film directed by William Desmond Taylor and starring Mary Miles Minter. The film is based on the novel of the same name by Grace Miller White. It was produced by Famous Players-Lasky and distributed through Realart and Paramount Pictures.As with many of Minter's films Judy of Rogue's Harbor is considered lost.
03/18/2022 05:02:34 - INFO - __main__ - ['Film']
03/18/2022 05:02:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:02:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:02:35 - INFO - __main__ - Loaded 224 examples from train data
use DistributedSampler
03/18/2022 05:02:35 - INFO - __main__ - Start tokenizing ... 224 instances
03/18/2022 05:02:35 - INFO - __main__ - Printing 3 examples
03/18/2022 05:02:35 - INFO - __main__ -  [dbpedia_14] Spartacus is a 1960 American epic historical drama film directed by Stanley Kubrick and starring Kirk Douglas as the rebellious slave of the title. The screenplay by Dalton Trumbo was based on the novel Spartacus by Howard Fast.
03/18/2022 05:02:35 - INFO - __main__ - ['Film']
03/18/2022 05:02:35 - INFO - __main__ -  [dbpedia_14] Three Rooms in Manhattan (French: Trois chambres à Manhattan) is a 1965 French drama film filmed in New York City. It is based on the 1946 novel Trois Chambres à Manhattan (which has been translated into English as Three Bedrooms in Manhattan) by Belgian writer Georges Simenon about a romance between François a French actor and Kay an American woman.
03/18/2022 05:02:35 - INFO - __main__ - ['Film']
03/18/2022 05:02:35 - INFO - __main__ -  [dbpedia_14] Return Home is a 1990 Australian drama film directed by Ray Argall. Argall won the AFI Award for Best Director in 1990 and Frankie J. Holden was nominated for Best Actor in a Lead Role.
03/18/2022 05:02:35 - INFO - __main__ - ['Film']
03/18/2022 05:02:35 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:02:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:02:35 - INFO - __main__ - Loaded 224 examples from dev data
03/18/2022 05:02:54 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:02:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:02:54 - INFO - __main__ - Starting training!
03/18/2022 05:02:58 - INFO - __main__ - Step 10 Global step 10 Train loss 7.07 on epoch=0
03/18/2022 05:03:01 - INFO - __main__ - Step 20 Global step 20 Train loss 6.59 on epoch=1
03/18/2022 05:03:03 - INFO - __main__ - Step 30 Global step 30 Train loss 6.24 on epoch=2
03/18/2022 05:03:06 - INFO - __main__ - Step 40 Global step 40 Train loss 5.70 on epoch=2
03/18/2022 05:03:08 - INFO - __main__ - Step 50 Global step 50 Train loss 4.50 on epoch=3
03/18/2022 05:04:28 - INFO - __main__ - Global step 50 Train loss 6.02 Classification-F1 0.000639386189258312 on epoch=3
03/18/2022 05:04:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.000639386189258312 on epoch=3, global_step=50
03/18/2022 05:04:30 - INFO - __main__ - Step 60 Global step 60 Train loss 3.57 on epoch=4
03/18/2022 05:04:33 - INFO - __main__ - Step 70 Global step 70 Train loss 3.03 on epoch=4
03/18/2022 05:04:36 - INFO - __main__ - Step 80 Global step 80 Train loss 2.60 on epoch=5
03/18/2022 05:04:38 - INFO - __main__ - Step 90 Global step 90 Train loss 2.15 on epoch=6
03/18/2022 05:04:41 - INFO - __main__ - Step 100 Global step 100 Train loss 2.02 on epoch=7
03/18/2022 05:04:47 - INFO - __main__ - Global step 100 Train loss 2.67 Classification-F1 0.22928905496699162 on epoch=7
03/18/2022 05:04:47 - INFO - __main__ - Saving model with best Classification-F1: 0.000639386189258312 -> 0.22928905496699162 on epoch=7, global_step=100
03/18/2022 05:04:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.82 on epoch=7
03/18/2022 05:04:52 - INFO - __main__ - Step 120 Global step 120 Train loss 1.64 on epoch=8
03/18/2022 05:04:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.41 on epoch=9
03/18/2022 05:04:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.43 on epoch=9
03/18/2022 05:05:00 - INFO - __main__ - Step 150 Global step 150 Train loss 1.23 on epoch=10
03/18/2022 05:05:06 - INFO - __main__ - Global step 150 Train loss 1.51 Classification-F1 0.36629987655249013 on epoch=10
03/18/2022 05:05:06 - INFO - __main__ - Saving model with best Classification-F1: 0.22928905496699162 -> 0.36629987655249013 on epoch=10, global_step=150
03/18/2022 05:05:08 - INFO - __main__ - Step 160 Global step 160 Train loss 1.17 on epoch=11
03/18/2022 05:05:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.23 on epoch=12
03/18/2022 05:05:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=12
03/18/2022 05:05:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=13
03/18/2022 05:05:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=14
03/18/2022 05:05:25 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.493600398317122 on epoch=14
03/18/2022 05:05:25 - INFO - __main__ - Saving model with best Classification-F1: 0.36629987655249013 -> 0.493600398317122 on epoch=14, global_step=200
03/18/2022 05:05:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=14
03/18/2022 05:05:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=15
03/18/2022 05:05:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=16
03/18/2022 05:05:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=17
03/18/2022 05:05:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=17
03/18/2022 05:05:47 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.44222355197351987 on epoch=17
03/18/2022 05:05:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=18
03/18/2022 05:05:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=19
03/18/2022 05:05:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=19
03/18/2022 05:05:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=20
03/18/2022 05:06:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.67 on epoch=21
03/18/2022 05:06:06 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.5600429483244007 on epoch=21
03/18/2022 05:06:06 - INFO - __main__ - Saving model with best Classification-F1: 0.493600398317122 -> 0.5600429483244007 on epoch=21, global_step=300
03/18/2022 05:06:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=22
03/18/2022 05:06:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=22
03/18/2022 05:06:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=23
03/18/2022 05:06:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=24
03/18/2022 05:06:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=24
03/18/2022 05:06:26 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.5952223994871053 on epoch=24
03/18/2022 05:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5600429483244007 -> 0.5952223994871053 on epoch=24, global_step=350
03/18/2022 05:06:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=25
03/18/2022 05:06:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.66 on epoch=26
03/18/2022 05:06:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=27
03/18/2022 05:06:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=27
03/18/2022 05:06:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.64 on epoch=28
03/18/2022 05:06:47 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.6891753062422928 on epoch=28
03/18/2022 05:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5952223994871053 -> 0.6891753062422928 on epoch=28, global_step=400
03/18/2022 05:06:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=29
03/18/2022 05:06:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=29
03/18/2022 05:06:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=30
03/18/2022 05:06:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.63 on epoch=31
03/18/2022 05:07:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.63 on epoch=32
03/18/2022 05:07:07 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.8049888836653543 on epoch=32
03/18/2022 05:07:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6891753062422928 -> 0.8049888836653543 on epoch=32, global_step=450
03/18/2022 05:07:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=32
03/18/2022 05:07:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=33
03/18/2022 05:07:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=34
03/18/2022 05:07:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=34
03/18/2022 05:07:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=35
03/18/2022 05:07:27 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.6070643940722052 on epoch=35
03/18/2022 05:07:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=36
03/18/2022 05:07:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=37
03/18/2022 05:07:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=37
03/18/2022 05:07:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=38
03/18/2022 05:07:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=39
03/18/2022 05:07:47 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.6779862585638594 on epoch=39
03/18/2022 05:07:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=39
03/18/2022 05:07:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=40
03/18/2022 05:07:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=41
03/18/2022 05:07:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=42
03/18/2022 05:08:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=42
03/18/2022 05:08:06 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.6381991564775844 on epoch=42
03/18/2022 05:08:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=43
03/18/2022 05:08:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=44
03/18/2022 05:08:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=44
03/18/2022 05:08:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=45
03/18/2022 05:08:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=46
03/18/2022 05:08:26 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.7261978203948144 on epoch=46
03/18/2022 05:08:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=47
03/18/2022 05:08:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=47
03/18/2022 05:08:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=48
03/18/2022 05:08:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=49
03/18/2022 05:08:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=49
03/18/2022 05:08:45 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6236768950004243 on epoch=49
03/18/2022 05:08:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=50
03/18/2022 05:08:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=51
03/18/2022 05:08:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=52
03/18/2022 05:08:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=52
03/18/2022 05:08:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=53
03/18/2022 05:09:05 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.7443657807884791 on epoch=53
03/18/2022 05:09:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=54
03/18/2022 05:09:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=54
03/18/2022 05:09:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=55
03/18/2022 05:09:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=56
03/18/2022 05:09:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=57
03/18/2022 05:09:24 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.6196830993132438 on epoch=57
03/18/2022 05:09:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=57
03/18/2022 05:09:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=58
03/18/2022 05:09:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=59
03/18/2022 05:09:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=59
03/18/2022 05:09:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=60
03/18/2022 05:09:44 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.6908177624293729 on epoch=60
03/18/2022 05:09:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=61
03/18/2022 05:09:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=62
03/18/2022 05:09:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=62
03/18/2022 05:09:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=63
03/18/2022 05:09:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=64
03/18/2022 05:10:04 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.7225898772672967 on epoch=64
03/18/2022 05:10:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=64
03/18/2022 05:10:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=65
03/18/2022 05:10:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=66
03/18/2022 05:10:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=67
03/18/2022 05:10:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=67
03/18/2022 05:10:23 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.8853810631341221 on epoch=67
03/18/2022 05:10:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8049888836653543 -> 0.8853810631341221 on epoch=67, global_step=950
03/18/2022 05:10:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=68
03/18/2022 05:10:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=69
03/18/2022 05:10:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=69
03/18/2022 05:10:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=70
03/18/2022 05:10:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=71
03/18/2022 05:10:43 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.8204906117126292 on epoch=71
03/18/2022 05:10:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=72
03/18/2022 05:10:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=72
03/18/2022 05:10:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=73
03/18/2022 05:10:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=74
03/18/2022 05:10:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=74
03/18/2022 05:11:02 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.7179518509416013 on epoch=74
03/18/2022 05:11:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=75
03/18/2022 05:11:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=76
03/18/2022 05:11:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=77
03/18/2022 05:11:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=77
03/18/2022 05:11:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=78
03/18/2022 05:11:21 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.7304084427200846 on epoch=78
03/18/2022 05:11:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=79
03/18/2022 05:11:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=79
03/18/2022 05:11:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=80
03/18/2022 05:11:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=81
03/18/2022 05:11:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=82
03/18/2022 05:11:41 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.7245850235637041 on epoch=82
03/18/2022 05:11:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=82
03/18/2022 05:11:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=83
03/18/2022 05:11:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=84
03/18/2022 05:11:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=84
03/18/2022 05:11:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=85
03/18/2022 05:12:00 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.6812750300911525 on epoch=85
03/18/2022 05:12:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=86
03/18/2022 05:12:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=87
03/18/2022 05:12:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=87
03/18/2022 05:12:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=88
03/18/2022 05:12:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=89
03/18/2022 05:12:19 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7543439111024779 on epoch=89
03/18/2022 05:12:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=89
03/18/2022 05:12:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=90
03/18/2022 05:12:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=91
03/18/2022 05:12:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=92
03/18/2022 05:12:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=92
03/18/2022 05:12:38 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.9143735744542195 on epoch=92
03/18/2022 05:12:38 - INFO - __main__ - Saving model with best Classification-F1: 0.8853810631341221 -> 0.9143735744542195 on epoch=92, global_step=1300
03/18/2022 05:12:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=93
03/18/2022 05:12:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=94
03/18/2022 05:12:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=94
03/18/2022 05:12:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=95
03/18/2022 05:12:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=96
03/18/2022 05:12:57 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.8472016094333374 on epoch=96
03/18/2022 05:13:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=97
03/18/2022 05:13:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=97
03/18/2022 05:13:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=98
03/18/2022 05:13:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=99
03/18/2022 05:13:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=99
03/18/2022 05:13:16 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.8469366106552338 on epoch=99
03/18/2022 05:13:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=100
03/18/2022 05:13:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=101
03/18/2022 05:13:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=102
03/18/2022 05:13:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=102
03/18/2022 05:13:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=103
03/18/2022 05:13:36 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.8451931172534743 on epoch=103
03/18/2022 05:13:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=104
03/18/2022 05:13:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=104
03/18/2022 05:13:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=105
03/18/2022 05:13:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=106
03/18/2022 05:13:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=107
03/18/2022 05:13:55 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.8468374214194223 on epoch=107
03/18/2022 05:13:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=107
03/18/2022 05:14:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=108
03/18/2022 05:14:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=109
03/18/2022 05:14:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=109
03/18/2022 05:14:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=110
03/18/2022 05:14:14 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.9183444733271338 on epoch=110
03/18/2022 05:14:14 - INFO - __main__ - Saving model with best Classification-F1: 0.9143735744542195 -> 0.9183444733271338 on epoch=110, global_step=1550
03/18/2022 05:14:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=111
03/18/2022 05:14:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=112
03/18/2022 05:14:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=112
03/18/2022 05:14:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=113
03/18/2022 05:14:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=114
03/18/2022 05:14:33 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.8890668900991483 on epoch=114
03/18/2022 05:14:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=114
03/18/2022 05:14:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=115
03/18/2022 05:14:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=116
03/18/2022 05:14:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=117
03/18/2022 05:14:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=117
03/18/2022 05:14:51 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.9145079830563703 on epoch=117
03/18/2022 05:14:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=118
03/18/2022 05:14:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=119
03/18/2022 05:14:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=119
03/18/2022 05:15:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=120
03/18/2022 05:15:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=121
03/18/2022 05:15:11 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.8472016094333376 on epoch=121
03/18/2022 05:15:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=122
03/18/2022 05:15:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=122
03/18/2022 05:15:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=123
03/18/2022 05:15:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=124
03/18/2022 05:15:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=124
03/18/2022 05:15:30 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7918447891966622 on epoch=124
03/18/2022 05:15:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=125
03/18/2022 05:15:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=126
03/18/2022 05:15:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=127
03/18/2022 05:15:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=127
03/18/2022 05:15:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=128
03/18/2022 05:15:49 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.8942923158072346 on epoch=128
03/18/2022 05:15:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=129
03/18/2022 05:15:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=129
03/18/2022 05:15:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=130
03/18/2022 05:16:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=131
03/18/2022 05:16:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=132
03/18/2022 05:16:09 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.9058077206006279 on epoch=132
03/18/2022 05:16:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=132
03/18/2022 05:16:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=133
03/18/2022 05:16:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=134
03/18/2022 05:16:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=134
03/18/2022 05:16:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=135
03/18/2022 05:16:28 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.851129400623886 on epoch=135
03/18/2022 05:16:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=136
03/18/2022 05:16:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=137
03/18/2022 05:16:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=137
03/18/2022 05:16:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=138
03/18/2022 05:16:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=139
03/18/2022 05:16:47 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.8509772692573844 on epoch=139
03/18/2022 05:16:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=139
03/18/2022 05:16:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=140
03/18/2022 05:16:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=141
03/18/2022 05:16:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=142
03/18/2022 05:17:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=142
03/18/2022 05:17:06 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.8609970674486804 on epoch=142
03/18/2022 05:17:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=143
03/18/2022 05:17:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=144
03/18/2022 05:17:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=144
03/18/2022 05:17:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=145
03/18/2022 05:17:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=146
03/18/2022 05:17:25 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.9021145995054916 on epoch=146
03/18/2022 05:17:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=147
03/18/2022 05:17:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=147
03/18/2022 05:17:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=148
03/18/2022 05:17:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=149
03/18/2022 05:17:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=149
03/18/2022 05:17:44 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.7968586255533305 on epoch=149
03/18/2022 05:17:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=150
03/18/2022 05:17:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=151
03/18/2022 05:17:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=152
03/18/2022 05:17:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=152
03/18/2022 05:17:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=153
03/18/2022 05:18:03 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.8528942090538308 on epoch=153
03/18/2022 05:18:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=154
03/18/2022 05:18:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=154
03/18/2022 05:18:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=155
03/18/2022 05:18:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=156
03/18/2022 05:18:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=157
03/18/2022 05:18:22 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.9103045636631976 on epoch=157
03/18/2022 05:18:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=157
03/18/2022 05:18:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=158
03/18/2022 05:18:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=159
03/18/2022 05:18:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=159
03/18/2022 05:18:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=160
03/18/2022 05:18:41 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.9776348295916607 on epoch=160
03/18/2022 05:18:41 - INFO - __main__ - Saving model with best Classification-F1: 0.9183444733271338 -> 0.9776348295916607 on epoch=160, global_step=2250
03/18/2022 05:18:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=161
03/18/2022 05:18:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=162
03/18/2022 05:18:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=162
03/18/2022 05:18:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=163
03/18/2022 05:18:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=164
03/18/2022 05:19:00 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.9821892393320965 on epoch=164
03/18/2022 05:19:00 - INFO - __main__ - Saving model with best Classification-F1: 0.9776348295916607 -> 0.9821892393320965 on epoch=164, global_step=2300
03/18/2022 05:19:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=164
03/18/2022 05:19:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=165
03/18/2022 05:19:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=166
03/18/2022 05:19:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=167
03/18/2022 05:19:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=167
03/18/2022 05:19:19 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.9773974793679228 on epoch=167
03/18/2022 05:19:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=168
03/18/2022 05:19:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=169
03/18/2022 05:19:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=169
03/18/2022 05:19:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=170
03/18/2022 05:19:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=171
03/18/2022 05:19:39 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.9186460429724188 on epoch=171
03/18/2022 05:19:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=172
03/18/2022 05:19:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=172
03/18/2022 05:19:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=173
03/18/2022 05:19:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=174
03/18/2022 05:19:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=174
03/18/2022 05:19:58 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.9101611944875704 on epoch=174
03/18/2022 05:20:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=175
03/18/2022 05:20:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=176
03/18/2022 05:20:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=177
03/18/2022 05:20:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=177
03/18/2022 05:20:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=178
03/18/2022 05:20:16 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.847067200831187 on epoch=178
03/18/2022 05:20:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=179
03/18/2022 05:20:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=179
03/18/2022 05:20:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=180
03/18/2022 05:20:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=181
03/18/2022 05:20:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=182
03/18/2022 05:20:35 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.8469679514034353 on epoch=182
03/18/2022 05:20:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=182
03/18/2022 05:20:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=183
03/18/2022 05:20:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=184
03/18/2022 05:20:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=184
03/18/2022 05:20:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=185
03/18/2022 05:20:54 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.9186746497230369 on epoch=185
03/18/2022 05:20:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=186
03/18/2022 05:21:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=187
03/18/2022 05:21:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=187
03/18/2022 05:21:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=188
03/18/2022 05:21:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=189
03/18/2022 05:21:14 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.9077131601958206 on epoch=189
03/18/2022 05:21:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=189
03/18/2022 05:21:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=190
03/18/2022 05:21:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=191
03/18/2022 05:21:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=192
03/18/2022 05:21:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=192
03/18/2022 05:21:33 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.9776348295916607 on epoch=192
03/18/2022 05:21:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=193
03/18/2022 05:21:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=194
03/18/2022 05:21:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=194
03/18/2022 05:21:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=195
03/18/2022 05:21:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=196
03/18/2022 05:21:52 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7895087462673132 on epoch=196
03/18/2022 05:21:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=197
03/18/2022 05:21:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=197
03/18/2022 05:22:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=198
03/18/2022 05:22:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=199
03/18/2022 05:22:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=199
03/18/2022 05:22:11 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7919154151700118 on epoch=199
03/18/2022 05:22:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=200
03/18/2022 05:22:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=201
03/18/2022 05:22:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=202
03/18/2022 05:22:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=202
03/18/2022 05:22:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=203
03/18/2022 05:22:30 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.8468106025907178 on epoch=203
03/18/2022 05:22:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=204
03/18/2022 05:22:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=204
03/18/2022 05:22:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=205
03/18/2022 05:22:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=206
03/18/2022 05:22:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=207
03/18/2022 05:22:49 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.8550217462857325 on epoch=207
03/18/2022 05:22:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=207
03/18/2022 05:22:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=208
03/18/2022 05:22:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=209
03/18/2022 05:23:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=209
03/18/2022 05:23:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=210
03/18/2022 05:23:09 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.8490018633919845 on epoch=210
03/18/2022 05:23:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=211
03/18/2022 05:23:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=212
03/18/2022 05:23:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=212
03/18/2022 05:23:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=213
03/18/2022 05:23:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=214
03/18/2022 05:23:28 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.9143564679048549 on epoch=214
03/18/2022 05:23:28 - INFO - __main__ - save last model!
03/18/2022 05:23:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 05:23:28 - INFO - __main__ - Start tokenizing ... 3500 instances
03/18/2022 05:23:28 - INFO - __main__ - Printing 3 examples
03/18/2022 05:23:28 - INFO - __main__ -  [dbpedia_14] Platymetopus is a genus of beetles in the family Carabidae containing the following species: Platymetopus brevilabris Laferte-Senectere 1853 Platymetopus colpophilus Alluaud 1918 Platymetopus congestulus Basilewsky 1948 Platymetopus crenulatus Chaudoir 1878 Platymetopus cribricollis Facchini 2004 Platymetopus curtulus (Peringuey 1908) Platymetopus cyaneus Facchini 2004 Platymetopus diversepunctatus Facchini 2004 Platymetopus figuratus Boheman 1848 Platymetopus flavilabris (Fabricius 1798) Platymetopus guineensis Dejean 1831 Platymetopus indicus Jedlicka 1969 Platymetopus interpunctatus Dejean 1829 Platymetopus keiseri Louwerens 1956 Platymetopus laevigatus Kuntzen 1919 Platymetopus laticeps Dejean 1829 Platymetopus lepidus Dejean 1829 Platymetopus ludificus (H.Kolbe 1883) Platymetopus majusculus Lorenz 1998 Platymetopus obscuripes Chaudoir 1878 Platymetopus pictus Andrewes 1923 Platymetopus platythorax Basilewsky 1948 Platymetopus quadrimaculatus Dejean 1829 Platymetopus quadrinotatus Burgeon 1936 Platymetopus rectangularis Burgeon 1936 Platymetopus rugosus (Nietner 1857) Platymetopus sakalava Jeannel 1948 Platymetopus schoenherri Dejean 1831 Platymetopus seriatus Chaudoir 1878 Platymetopus straeleni Basilewsky 1947 Platymetopus subrugosus Schauberger 1938 Platymetopus sudanicus Basilewsky 1967 Platymetopus tessellatus Dejean 1829 Platymetopus tibialis (H.Kolbe 1883) Platymetopus tritus Bates 1889 Platymetopus vestitus Dejean 1829 Platymetopus xanthographus (Alluaud 1916)↑
03/18/2022 05:23:28 - INFO - __main__ - ['Animal']
03/18/2022 05:23:28 - INFO - __main__ -  [dbpedia_14] Sicera is a genus of moth in the family Gelechiidae.
03/18/2022 05:23:28 - INFO - __main__ - ['Animal']
03/18/2022 05:23:28 - INFO - __main__ -  [dbpedia_14] Strzeczonka [stʂɛˈt͡ʂɔnka] is a village in the administrative district of Gmina Debrzno within Człuchów County Pomeranian Voivodeship in northern Poland. It lies approximately 7 kilometres (4 mi) north-west of Debrzno 16 km (10 mi) south-west of Człuchów and 130 km (81 mi) south-west of the regional capital Gdańsk.For details of the history of the region see History of Pomerania.
03/18/2022 05:23:28 - INFO - __main__ - ['Village']
03/18/2022 05:23:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:23:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:23:33 - INFO - __main__ - Loaded 3500 examples from test data
03/18/2022 05:25:39 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-dbpedia_14/dbpedia_14_16_87_0.2_8_predictions.txt
03/18/2022 05:25:39 - INFO - __main__ - Classification-F1 on test data: 0.7514
03/18/2022 05:25:40 - INFO - __main__ - prefix=dbpedia_14_16_87, lr=0.2, bsz=8, dev_performance=0.9821892393320965, test_performance=0.7513917371926772
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (21095): No such process
Task: wiki_qa, Checkpoint: models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10
03/18/2022 05:25:45 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 05:25:45 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa
Output directory () already exists and is not empty.
03/18/2022 05:25:45 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 05:25:45 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa
03/18/2022 05:25:47 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 05:25:47 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 05:25:47 - INFO - __main__ - args.device: cuda:0
03/18/2022 05:25:47 - INFO - __main__ - Using 2 gpus
03/18/2022 05:25:47 - INFO - __main__ - args.device: cuda:1
03/18/2022 05:25:47 - INFO - __main__ - Using 2 gpus
03/18/2022 05:25:47 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/18/2022 05:25:47 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/18/2022 05:25:54 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.5, bsz=8 ...
03/18/2022 05:25:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:25:55 - INFO - __main__ - Printing 3 examples
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:25:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:25:55 - INFO - __main__ - Printing 3 examples
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:25:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:25:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:25:55 - INFO - __main__ - Printing 3 examples
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:25:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:25:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:25:55 - INFO - __main__ - Printing 3 examples
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:25:55 - INFO - __main__ - ['false']
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:25:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:25:55 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:25:55 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:26:12 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:26:13 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:26:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:26:13 - INFO - __main__ - Starting training!
03/18/2022 05:26:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:26:21 - INFO - __main__ - Starting training!
03/18/2022 05:26:25 - INFO - __main__ - Step 10 Global step 10 Train loss 5.09 on epoch=4
03/18/2022 05:26:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.97 on epoch=9
03/18/2022 05:26:30 - INFO - __main__ - Step 30 Global step 30 Train loss 3.41 on epoch=14
03/18/2022 05:26:32 - INFO - __main__ - Step 40 Global step 40 Train loss 4.32 on epoch=19
03/18/2022 05:26:35 - INFO - __main__ - Step 50 Global step 50 Train loss 4.58 on epoch=24
03/18/2022 05:26:35 - INFO - __main__ - Global step 50 Train loss 3.67 Classification-F1 0.5465587044534412 on epoch=24
03/18/2022 05:26:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5465587044534412 on epoch=24, global_step=50
03/18/2022 05:26:38 - INFO - __main__ - Step 60 Global step 60 Train loss 2.95 on epoch=29
03/18/2022 05:26:40 - INFO - __main__ - Step 70 Global step 70 Train loss 2.46 on epoch=34
03/18/2022 05:26:42 - INFO - __main__ - Step 80 Global step 80 Train loss 2.15 on epoch=39
03/18/2022 05:26:45 - INFO - __main__ - Step 90 Global step 90 Train loss 1.65 on epoch=44
03/18/2022 05:26:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.49 on epoch=49
03/18/2022 05:26:49 - INFO - __main__ - Global step 100 Train loss 2.14 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 05:26:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.34 on epoch=54
03/18/2022 05:26:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.37 on epoch=59
03/18/2022 05:26:56 - INFO - __main__ - Step 130 Global step 130 Train loss 1.16 on epoch=64
03/18/2022 05:26:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=69
03/18/2022 05:27:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=74
03/18/2022 05:27:01 - INFO - __main__ - Global step 150 Train loss 1.15 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 05:27:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=79
03/18/2022 05:27:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=84
03/18/2022 05:27:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=89
03/18/2022 05:27:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=94
03/18/2022 05:27:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=99
03/18/2022 05:27:14 - INFO - __main__ - Global step 200 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 05:27:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=104
03/18/2022 05:27:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=109
03/18/2022 05:27:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=114
03/18/2022 05:27:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=119
03/18/2022 05:27:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=124
03/18/2022 05:27:26 - INFO - __main__ - Global step 250 Train loss 0.77 Classification-F1 0.3816425120772947 on epoch=124
03/18/2022 05:27:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=129
03/18/2022 05:27:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=134
03/18/2022 05:27:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=139
03/18/2022 05:27:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=144
03/18/2022 05:27:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=149
03/18/2022 05:27:39 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 05:27:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.56 on epoch=154
03/18/2022 05:27:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=159
03/18/2022 05:27:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=164
03/18/2022 05:27:48 - INFO - __main__ - Step 340 Global step 340 Train loss 2.80 on epoch=169
03/18/2022 05:27:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.73 on epoch=174
03/18/2022 05:27:51 - INFO - __main__ - Global step 350 Train loss 1.04 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 05:27:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=179
03/18/2022 05:27:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=184
03/18/2022 05:27:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=189
03/18/2022 05:28:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.08 on epoch=194
03/18/2022 05:28:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=199
03/18/2022 05:28:04 - INFO - __main__ - Global step 400 Train loss 0.79 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 05:28:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=204
03/18/2022 05:28:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=209
03/18/2022 05:28:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=214
03/18/2022 05:28:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=219
03/18/2022 05:28:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=224
03/18/2022 05:28:16 - INFO - __main__ - Global step 450 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=224
03/18/2022 05:28:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=229
03/18/2022 05:28:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=234
03/18/2022 05:28:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=239
03/18/2022 05:28:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=244
03/18/2022 05:28:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=249
03/18/2022 05:28:29 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.3992490613266583 on epoch=249
03/18/2022 05:28:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.55 on epoch=254
03/18/2022 05:28:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=259
03/18/2022 05:28:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=264
03/18/2022 05:28:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=269
03/18/2022 05:28:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=274
03/18/2022 05:28:41 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 05:28:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=279
03/18/2022 05:28:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=284
03/18/2022 05:28:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=289
03/18/2022 05:28:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=294
03/18/2022 05:28:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=299
03/18/2022 05:28:53 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 05:28:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
03/18/2022 05:28:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=309
03/18/2022 05:29:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=314
03/18/2022 05:29:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=319
03/18/2022 05:29:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=324
03/18/2022 05:29:05 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 05:29:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=329
03/18/2022 05:29:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=334
03/18/2022 05:29:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=339
03/18/2022 05:29:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=344
03/18/2022 05:29:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=349
03/18/2022 05:29:18 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 05:29:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
03/18/2022 05:29:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=359
03/18/2022 05:29:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
03/18/2022 05:29:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=369
03/18/2022 05:29:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=374
03/18/2022 05:29:30 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=374
03/18/2022 05:29:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=379
03/18/2022 05:29:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
03/18/2022 05:29:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
03/18/2022 05:29:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
03/18/2022 05:29:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=399
03/18/2022 05:29:43 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=399
03/18/2022 05:29:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
03/18/2022 05:29:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=409
03/18/2022 05:29:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=414
03/18/2022 05:29:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=419
03/18/2022 05:29:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
03/18/2022 05:29:55 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=424
03/18/2022 05:29:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
03/18/2022 05:30:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=434
03/18/2022 05:30:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=439
03/18/2022 05:30:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
03/18/2022 05:30:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=449
03/18/2022 05:30:08 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.46843853820598 on epoch=449
03/18/2022 05:30:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
03/18/2022 05:30:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
03/18/2022 05:30:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
03/18/2022 05:30:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=469
03/18/2022 05:30:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=474
03/18/2022 05:30:20 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.4385964912280702 on epoch=474
03/18/2022 05:30:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=479
03/18/2022 05:30:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=484
03/18/2022 05:30:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=489
03/18/2022 05:30:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=494
03/18/2022 05:30:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=499
03/18/2022 05:30:32 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.4909862142099682 on epoch=499
03/18/2022 05:30:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=504
03/18/2022 05:30:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=509
03/18/2022 05:30:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=514
03/18/2022 05:30:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=519
03/18/2022 05:30:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
03/18/2022 05:30:45 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.43529411764705883 on epoch=524
03/18/2022 05:30:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
03/18/2022 05:30:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=534
03/18/2022 05:30:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
03/18/2022 05:30:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=544
03/18/2022 05:30:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=549
03/18/2022 05:30:57 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.37254901960784315 on epoch=549
03/18/2022 05:30:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=554
03/18/2022 05:31:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=559
03/18/2022 05:31:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=564
03/18/2022 05:31:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=569
03/18/2022 05:31:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
03/18/2022 05:31:09 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=574
03/18/2022 05:31:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
03/18/2022 05:31:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=584
03/18/2022 05:31:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=589
03/18/2022 05:31:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
03/18/2022 05:31:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
03/18/2022 05:31:22 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=599
03/18/2022 05:31:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=604
03/18/2022 05:31:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/18/2022 05:31:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=614
03/18/2022 05:31:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=619
03/18/2022 05:31:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=624
03/18/2022 05:31:34 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=624
03/18/2022 05:31:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=629
03/18/2022 05:31:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
03/18/2022 05:31:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=639
03/18/2022 05:31:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=644
03/18/2022 05:31:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
03/18/2022 05:31:47 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=649
03/18/2022 05:31:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=654
03/18/2022 05:31:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=659
03/18/2022 05:31:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=664
03/18/2022 05:31:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=669
03/18/2022 05:32:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
03/18/2022 05:32:00 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=674
03/18/2022 05:32:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=679
03/18/2022 05:32:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
03/18/2022 05:32:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=689
03/18/2022 05:32:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=694
03/18/2022 05:32:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=699
03/18/2022 05:32:13 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=699
03/18/2022 05:32:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
03/18/2022 05:32:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
03/18/2022 05:32:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
03/18/2022 05:32:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
03/18/2022 05:32:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
03/18/2022 05:32:26 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.3191489361702127 on epoch=724
03/18/2022 05:32:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=729
03/18/2022 05:32:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
03/18/2022 05:32:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
03/18/2022 05:32:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
03/18/2022 05:32:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
03/18/2022 05:32:39 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3552492046659597 on epoch=749
03/18/2022 05:32:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
03/18/2022 05:32:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=759
03/18/2022 05:32:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
03/18/2022 05:32:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
03/18/2022 05:32:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=774
03/18/2022 05:32:52 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=774
03/18/2022 05:32:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=779
03/18/2022 05:32:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=784
03/18/2022 05:33:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=789
03/18/2022 05:33:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=794
03/18/2022 05:33:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
03/18/2022 05:33:05 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3816425120772947 on epoch=799
03/18/2022 05:33:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=804
03/18/2022 05:33:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=809
03/18/2022 05:33:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=814
03/18/2022 05:33:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=819
03/18/2022 05:33:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
03/18/2022 05:33:18 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=824
03/18/2022 05:33:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=829
03/18/2022 05:33:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
03/18/2022 05:33:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
03/18/2022 05:33:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=844
03/18/2022 05:33:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
03/18/2022 05:33:31 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3992490613266583 on epoch=849
03/18/2022 05:33:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
03/18/2022 05:33:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
03/18/2022 05:33:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=864
03/18/2022 05:33:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=869
03/18/2022 05:33:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
03/18/2022 05:33:44 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=874
03/18/2022 05:33:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.43 on epoch=879
03/18/2022 05:33:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=884
03/18/2022 05:33:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=889
03/18/2022 05:33:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
03/18/2022 05:33:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
03/18/2022 05:33:58 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=899
03/18/2022 05:34:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=904
03/18/2022 05:34:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
03/18/2022 05:34:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
03/18/2022 05:34:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
03/18/2022 05:34:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=924
03/18/2022 05:34:11 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=924
03/18/2022 05:34:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=929
03/18/2022 05:34:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=934
03/18/2022 05:34:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
03/18/2022 05:34:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
03/18/2022 05:34:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=949
03/18/2022 05:34:24 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=949
03/18/2022 05:34:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
03/18/2022 05:34:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/18/2022 05:34:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=964
03/18/2022 05:34:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
03/18/2022 05:34:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
03/18/2022 05:34:37 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.3454545454545454 on epoch=974
03/18/2022 05:34:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
03/18/2022 05:34:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=984
03/18/2022 05:34:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
03/18/2022 05:34:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
03/18/2022 05:34:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=999
03/18/2022 05:34:50 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.3522267206477733 on epoch=999
03/18/2022 05:34:50 - INFO - __main__ - save last model!
03/18/2022 05:34:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 05:34:50 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 05:34:50 - INFO - __main__ - Printing 3 examples
03/18/2022 05:34:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 05:34:50 - INFO - __main__ - ['false']
03/18/2022 05:34:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 05:34:50 - INFO - __main__ - ['false']
03/18/2022 05:34:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 05:34:50 - INFO - __main__ - ['false']
03/18/2022 05:34:50 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:34:52 - INFO - __main__ - Printing 3 examples
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 05:34:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:34:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:34:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:34:52 - INFO - __main__ - Printing 3 examples
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:34:52 - INFO - __main__ - ['false']
03/18/2022 05:34:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:34:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:34:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:34:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:34:54 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 05:35:06 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:35:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:35:07 - INFO - __main__ - Starting training!
03/18/2022 05:35:46 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_100_0.5_8_predictions.txt
03/18/2022 05:35:46 - INFO - __main__ - Classification-F1 on test data: 0.3143
03/18/2022 05:35:47 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.5, bsz=8, dev_performance=0.5465587044534412, test_performance=0.3143278051687682
03/18/2022 05:35:47 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.4, bsz=8 ...
03/18/2022 05:35:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:35:48 - INFO - __main__ - Printing 3 examples
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:35:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:35:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:35:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:35:48 - INFO - __main__ - Printing 3 examples
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:35:48 - INFO - __main__ - ['false']
03/18/2022 05:35:48 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:35:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:35:48 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:36:07 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:36:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:36:07 - INFO - __main__ - Starting training!
03/18/2022 05:36:11 - INFO - __main__ - Step 10 Global step 10 Train loss 6.35 on epoch=4
03/18/2022 05:36:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.67 on epoch=9
03/18/2022 05:36:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.77 on epoch=14
03/18/2022 05:36:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=19
03/18/2022 05:36:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=24
03/18/2022 05:36:21 - INFO - __main__ - Global step 50 Train loss 1.96 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 05:36:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 05:36:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=29
03/18/2022 05:36:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=34
03/18/2022 05:36:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
03/18/2022 05:36:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
03/18/2022 05:36:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=49
03/18/2022 05:36:35 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 05:36:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
03/18/2022 05:36:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=59
03/18/2022 05:36:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
03/18/2022 05:36:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
03/18/2022 05:36:47 - INFO - __main__ - Step 150 Global step 150 Train loss 2.05 on epoch=74
03/18/2022 05:36:51 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.007246376811594203 on epoch=74
03/18/2022 05:36:54 - INFO - __main__ - Step 160 Global step 160 Train loss 4.70 on epoch=79
03/18/2022 05:36:56 - INFO - __main__ - Step 170 Global step 170 Train loss 3.05 on epoch=84
03/18/2022 05:36:59 - INFO - __main__ - Step 180 Global step 180 Train loss 1.67 on epoch=89
03/18/2022 05:37:01 - INFO - __main__ - Step 190 Global step 190 Train loss 2.63 on epoch=94
03/18/2022 05:37:04 - INFO - __main__ - Step 200 Global step 200 Train loss 2.34 on epoch=99
03/18/2022 05:37:05 - INFO - __main__ - Global step 200 Train loss 2.88 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 05:37:07 - INFO - __main__ - Step 210 Global step 210 Train loss 1.59 on epoch=104
03/18/2022 05:37:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.65 on epoch=109
03/18/2022 05:37:12 - INFO - __main__ - Step 230 Global step 230 Train loss 3.20 on epoch=114
03/18/2022 05:37:15 - INFO - __main__ - Step 240 Global step 240 Train loss 2.75 on epoch=119
03/18/2022 05:37:17 - INFO - __main__ - Step 250 Global step 250 Train loss 1.99 on epoch=124
03/18/2022 05:37:18 - INFO - __main__ - Global step 250 Train loss 2.24 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 05:37:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.45 on epoch=129
03/18/2022 05:37:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.46 on epoch=134
03/18/2022 05:37:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.35 on epoch=139
03/18/2022 05:37:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.25 on epoch=144
03/18/2022 05:37:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=149
03/18/2022 05:37:31 - INFO - __main__ - Global step 300 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 05:37:34 - INFO - __main__ - Step 310 Global step 310 Train loss 1.24 on epoch=154
03/18/2022 05:37:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.06 on epoch=159
03/18/2022 05:37:39 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=164
03/18/2022 05:37:41 - INFO - __main__ - Step 340 Global step 340 Train loss 1.06 on epoch=169
03/18/2022 05:37:44 - INFO - __main__ - Step 350 Global step 350 Train loss 1.09 on epoch=174
03/18/2022 05:37:44 - INFO - __main__ - Global step 350 Train loss 1.10 Classification-F1 0.3992490613266583 on epoch=174
03/18/2022 05:37:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=174, global_step=350
03/18/2022 05:37:47 - INFO - __main__ - Step 360 Global step 360 Train loss 1.01 on epoch=179
03/18/2022 05:37:49 - INFO - __main__ - Step 370 Global step 370 Train loss 1.09 on epoch=184
03/18/2022 05:37:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=189
03/18/2022 05:37:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.85 on epoch=194
03/18/2022 05:37:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.77 on epoch=199
03/18/2022 05:37:57 - INFO - __main__ - Global step 400 Train loss 0.94 Classification-F1 0.5195195195195195 on epoch=199
03/18/2022 05:37:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5195195195195195 on epoch=199, global_step=400
03/18/2022 05:38:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=204
03/18/2022 05:38:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.72 on epoch=209
03/18/2022 05:38:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=214
03/18/2022 05:38:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.71 on epoch=219
03/18/2022 05:38:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=224
03/18/2022 05:38:11 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.3107692307692308 on epoch=224
03/18/2022 05:38:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=229
03/18/2022 05:38:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.65 on epoch=234
03/18/2022 05:38:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=239
03/18/2022 05:38:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=244
03/18/2022 05:38:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=249
03/18/2022 05:38:24 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.33793103448275863 on epoch=249
03/18/2022 05:38:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=254
03/18/2022 05:38:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=259
03/18/2022 05:38:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=264
03/18/2022 05:38:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=269
03/18/2022 05:38:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=274
03/18/2022 05:38:37 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.33793103448275863 on epoch=274
03/18/2022 05:38:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=279
03/18/2022 05:38:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=284
03/18/2022 05:38:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=289
03/18/2022 05:38:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=294
03/18/2022 05:38:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=299
03/18/2022 05:38:50 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.33793103448275863 on epoch=299
03/18/2022 05:38:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
03/18/2022 05:38:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=309
03/18/2022 05:38:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=314
03/18/2022 05:39:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=319
03/18/2022 05:39:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=324
03/18/2022 05:39:03 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.26666666666666666 on epoch=324
03/18/2022 05:39:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=329
03/18/2022 05:39:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=334
03/18/2022 05:39:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
03/18/2022 05:39:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=344
03/18/2022 05:39:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=349
03/18/2022 05:39:16 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.24705882352941178 on epoch=349
03/18/2022 05:39:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=354
03/18/2022 05:39:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=359
03/18/2022 05:39:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
03/18/2022 05:39:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=369
03/18/2022 05:39:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=374
03/18/2022 05:39:29 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.3273273273273273 on epoch=374
03/18/2022 05:39:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=379
03/18/2022 05:39:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=384
03/18/2022 05:39:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=389
03/18/2022 05:39:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=394
03/18/2022 05:39:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
03/18/2022 05:39:43 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3992490613266583 on epoch=399
03/18/2022 05:39:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=404
03/18/2022 05:39:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=409
03/18/2022 05:39:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=414
03/18/2022 05:39:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=419
03/18/2022 05:39:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
03/18/2022 05:39:56 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3992490613266583 on epoch=424
03/18/2022 05:39:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=429
03/18/2022 05:40:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
03/18/2022 05:40:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=439
03/18/2022 05:40:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=444
03/18/2022 05:40:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=449
03/18/2022 05:40:09 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.3992490613266583 on epoch=449
03/18/2022 05:40:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
03/18/2022 05:40:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
03/18/2022 05:40:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/18/2022 05:40:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=469
03/18/2022 05:40:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=474
03/18/2022 05:40:22 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.49090909090909085 on epoch=474
03/18/2022 05:40:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=479
03/18/2022 05:40:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
03/18/2022 05:40:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=489
03/18/2022 05:40:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=494
03/18/2022 05:40:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=499
03/18/2022 05:40:35 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.3816425120772947 on epoch=499
03/18/2022 05:40:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=504
03/18/2022 05:40:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/18/2022 05:40:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=514
03/18/2022 05:40:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=519
03/18/2022 05:40:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=524
03/18/2022 05:40:48 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.4231177094379639 on epoch=524
03/18/2022 05:40:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=529
03/18/2022 05:40:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=534
03/18/2022 05:40:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
03/18/2022 05:40:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=544
03/18/2022 05:41:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
03/18/2022 05:41:01 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.25 on epoch=549
03/18/2022 05:41:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=554
03/18/2022 05:41:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
03/18/2022 05:41:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=564
03/18/2022 05:41:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
03/18/2022 05:41:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=574
03/18/2022 05:41:14 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.3552492046659597 on epoch=574
03/18/2022 05:41:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=579
03/18/2022 05:41:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/18/2022 05:41:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=589
03/18/2022 05:41:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=594
03/18/2022 05:41:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=599
03/18/2022 05:41:27 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.33793103448275863 on epoch=599
03/18/2022 05:41:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=604
03/18/2022 05:41:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
03/18/2022 05:41:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=614
03/18/2022 05:41:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
03/18/2022 05:41:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=624
03/18/2022 05:41:40 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.36374269005847953 on epoch=624
03/18/2022 05:41:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
03/18/2022 05:41:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=634
03/18/2022 05:41:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
03/18/2022 05:41:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
03/18/2022 05:41:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=649
03/18/2022 05:41:53 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3454545454545454 on epoch=649
03/18/2022 05:41:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=654
03/18/2022 05:41:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=659
03/18/2022 05:42:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=664
03/18/2022 05:42:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
03/18/2022 05:42:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=674
03/18/2022 05:42:07 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=674
03/18/2022 05:42:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=679
03/18/2022 05:42:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
03/18/2022 05:42:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=689
03/18/2022 05:42:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
03/18/2022 05:42:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=699
03/18/2022 05:42:19 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.3454545454545454 on epoch=699
03/18/2022 05:42:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=704
03/18/2022 05:42:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=709
03/18/2022 05:42:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=714
03/18/2022 05:42:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
03/18/2022 05:42:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
03/18/2022 05:42:32 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=724
03/18/2022 05:42:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
03/18/2022 05:42:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=734
03/18/2022 05:42:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=739
03/18/2022 05:42:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=744
03/18/2022 05:42:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
03/18/2022 05:42:44 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.24705882352941178 on epoch=749
03/18/2022 05:42:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
03/18/2022 05:42:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=759
03/18/2022 05:42:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
03/18/2022 05:42:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=769
03/18/2022 05:42:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
03/18/2022 05:42:57 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3273273273273273 on epoch=774
03/18/2022 05:43:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=779
03/18/2022 05:43:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
03/18/2022 05:43:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
03/18/2022 05:43:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=794
03/18/2022 05:43:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
03/18/2022 05:43:10 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3454545454545454 on epoch=799
03/18/2022 05:43:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
03/18/2022 05:43:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/18/2022 05:43:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
03/18/2022 05:43:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
03/18/2022 05:43:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
03/18/2022 05:43:22 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.24705882352941178 on epoch=824
03/18/2022 05:43:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
03/18/2022 05:43:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=834
03/18/2022 05:43:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=839
03/18/2022 05:43:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=844
03/18/2022 05:43:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
03/18/2022 05:43:35 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.24705882352941178 on epoch=849
03/18/2022 05:43:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=854
03/18/2022 05:43:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
03/18/2022 05:43:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=864
03/18/2022 05:43:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=869
03/18/2022 05:43:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
03/18/2022 05:43:48 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.37662337662337664 on epoch=874
03/18/2022 05:43:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
03/18/2022 05:43:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
03/18/2022 05:43:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
03/18/2022 05:43:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=894
03/18/2022 05:44:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=899
03/18/2022 05:44:00 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3266888150609081 on epoch=899
03/18/2022 05:44:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
03/18/2022 05:44:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
03/18/2022 05:44:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
03/18/2022 05:44:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=919
03/18/2022 05:44:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
03/18/2022 05:44:13 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.24705882352941178 on epoch=924
03/18/2022 05:44:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=929
03/18/2022 05:44:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
03/18/2022 05:44:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
03/18/2022 05:44:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
03/18/2022 05:44:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=949
03/18/2022 05:44:26 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.3107692307692308 on epoch=949
03/18/2022 05:44:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
03/18/2022 05:44:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/18/2022 05:44:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
03/18/2022 05:44:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
03/18/2022 05:44:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
03/18/2022 05:44:38 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.2632632632632632 on epoch=974
03/18/2022 05:44:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
03/18/2022 05:44:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=984
03/18/2022 05:44:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=989
03/18/2022 05:44:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
03/18/2022 05:44:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
03/18/2022 05:44:51 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.37662337662337664 on epoch=999
03/18/2022 05:44:51 - INFO - __main__ - save last model!
03/18/2022 05:44:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 05:44:51 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 05:44:51 - INFO - __main__ - Printing 3 examples
03/18/2022 05:44:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 05:44:51 - INFO - __main__ - ['false']
03/18/2022 05:44:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 05:44:51 - INFO - __main__ - ['false']
03/18/2022 05:44:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 05:44:51 - INFO - __main__ - ['false']
03/18/2022 05:44:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:44:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:44:52 - INFO - __main__ - Printing 3 examples
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 05:44:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:44:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:44:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:44:52 - INFO - __main__ - Printing 3 examples
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:44:52 - INFO - __main__ - ['false']
03/18/2022 05:44:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:44:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:44:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:44:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:44:55 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 05:45:11 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:45:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:45:12 - INFO - __main__ - Starting training!
03/18/2022 05:45:45 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_100_0.4_8_predictions.txt
03/18/2022 05:45:45 - INFO - __main__ - Classification-F1 on test data: 0.3224
03/18/2022 05:45:46 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.4, bsz=8, dev_performance=0.5195195195195195, test_performance=0.32242385242385246
03/18/2022 05:45:46 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.3, bsz=8 ...
03/18/2022 05:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:45:47 - INFO - __main__ - Printing 3 examples
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:45:47 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:45:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:45:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:45:47 - INFO - __main__ - Printing 3 examples
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:45:47 - INFO - __main__ - ['false']
03/18/2022 05:45:47 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:45:47 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:45:47 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:46:04 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:46:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:46:04 - INFO - __main__ - Starting training!
03/18/2022 05:46:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.74 on epoch=4
03/18/2022 05:46:10 - INFO - __main__ - Step 20 Global step 20 Train loss 5.18 on epoch=9
03/18/2022 05:46:12 - INFO - __main__ - Step 30 Global step 30 Train loss 3.82 on epoch=14
03/18/2022 05:46:15 - INFO - __main__ - Step 40 Global step 40 Train loss 2.59 on epoch=19
03/18/2022 05:46:17 - INFO - __main__ - Step 50 Global step 50 Train loss 1.70 on epoch=24
03/18/2022 05:46:19 - INFO - __main__ - Global step 50 Train loss 4.00 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 05:46:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 05:46:22 - INFO - __main__ - Step 60 Global step 60 Train loss 1.24 on epoch=29
03/18/2022 05:46:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.83 on epoch=34
03/18/2022 05:46:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=39
03/18/2022 05:46:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=44
03/18/2022 05:46:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
03/18/2022 05:46:32 - INFO - __main__ - Global step 100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 05:46:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
03/18/2022 05:46:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
03/18/2022 05:46:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
03/18/2022 05:46:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=69
03/18/2022 05:46:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=74
03/18/2022 05:46:44 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 05:46:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=79
03/18/2022 05:46:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=84
03/18/2022 05:46:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=89
03/18/2022 05:46:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=94
03/18/2022 05:46:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=99
03/18/2022 05:46:56 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 05:46:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=104
03/18/2022 05:47:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
03/18/2022 05:47:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=114
03/18/2022 05:47:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
03/18/2022 05:47:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
03/18/2022 05:47:09 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3992490613266583 on epoch=124
03/18/2022 05:47:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=124, global_step=250
03/18/2022 05:47:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
03/18/2022 05:47:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
03/18/2022 05:47:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=139
03/18/2022 05:47:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
03/18/2022 05:47:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=149
03/18/2022 05:47:21 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=149
03/18/2022 05:47:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=154
03/18/2022 05:47:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
03/18/2022 05:47:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=164
03/18/2022 05:47:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=169
03/18/2022 05:47:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/18/2022 05:47:34 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.2748768472906404 on epoch=174
03/18/2022 05:47:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
03/18/2022 05:47:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=184
03/18/2022 05:47:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
03/18/2022 05:47:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=194
03/18/2022 05:47:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
03/18/2022 05:47:46 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=199
03/18/2022 05:47:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
03/18/2022 05:47:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=209
03/18/2022 05:47:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
03/18/2022 05:47:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/18/2022 05:47:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=224
03/18/2022 05:47:59 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3266888150609081 on epoch=224
03/18/2022 05:48:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=229
03/18/2022 05:48:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=234
03/18/2022 05:48:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=239
03/18/2022 05:48:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=244
03/18/2022 05:48:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=249
03/18/2022 05:48:11 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.3454545454545454 on epoch=249
03/18/2022 05:48:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
03/18/2022 05:48:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
03/18/2022 05:48:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/18/2022 05:48:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=269
03/18/2022 05:48:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=274
03/18/2022 05:48:24 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 05:48:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=279
03/18/2022 05:48:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
03/18/2022 05:48:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=289
03/18/2022 05:48:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=294
03/18/2022 05:48:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=299
03/18/2022 05:48:36 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 05:48:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=304
03/18/2022 05:48:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=309
03/18/2022 05:48:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=314
03/18/2022 05:48:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=319
03/18/2022 05:48:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=324
03/18/2022 05:48:49 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.2632632632632632 on epoch=324
03/18/2022 05:48:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=329
03/18/2022 05:48:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=334
03/18/2022 05:48:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/18/2022 05:48:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
03/18/2022 05:49:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=349
03/18/2022 05:49:01 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.3273273273273273 on epoch=349
03/18/2022 05:49:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=354
03/18/2022 05:49:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=359
03/18/2022 05:49:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/18/2022 05:49:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=369
03/18/2022 05:49:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=374
03/18/2022 05:49:14 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.28744939271255066 on epoch=374
03/18/2022 05:49:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=379
03/18/2022 05:49:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
03/18/2022 05:49:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=389
03/18/2022 05:49:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=394
03/18/2022 05:49:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=399
03/18/2022 05:49:26 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.24512820512820513 on epoch=399
03/18/2022 05:49:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
03/18/2022 05:49:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=409
03/18/2022 05:49:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
03/18/2022 05:49:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
03/18/2022 05:49:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
03/18/2022 05:49:39 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.21951219512195122 on epoch=424
03/18/2022 05:49:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=429
03/18/2022 05:49:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
03/18/2022 05:49:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
03/18/2022 05:49:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
03/18/2022 05:49:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=449
03/18/2022 05:49:52 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.28744939271255066 on epoch=449
03/18/2022 05:49:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/18/2022 05:49:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/18/2022 05:49:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
03/18/2022 05:50:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
03/18/2022 05:50:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/18/2022 05:50:04 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.24512820512820513 on epoch=474
03/18/2022 05:50:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
03/18/2022 05:50:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/18/2022 05:50:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
03/18/2022 05:50:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/18/2022 05:50:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/18/2022 05:50:17 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.39139139139139134 on epoch=499
03/18/2022 05:50:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/18/2022 05:50:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
03/18/2022 05:50:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/18/2022 05:50:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
03/18/2022 05:50:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/18/2022 05:50:29 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.3107692307692308 on epoch=524
03/18/2022 05:50:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/18/2022 05:50:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/18/2022 05:50:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
03/18/2022 05:50:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/18/2022 05:50:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/18/2022 05:50:42 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.22267206477732793 on epoch=549
03/18/2022 05:50:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
03/18/2022 05:50:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/18/2022 05:50:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
03/18/2022 05:50:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/18/2022 05:50:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/18/2022 05:50:54 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.30158730158730157 on epoch=574
03/18/2022 05:50:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
03/18/2022 05:50:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
03/18/2022 05:51:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
03/18/2022 05:51:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.66 on epoch=594
03/18/2022 05:51:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
03/18/2022 05:51:07 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.3107692307692308 on epoch=599
03/18/2022 05:51:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
03/18/2022 05:51:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/18/2022 05:51:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/18/2022 05:51:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/18/2022 05:51:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
03/18/2022 05:51:19 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.22267206477732793 on epoch=624
03/18/2022 05:51:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/18/2022 05:51:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/18/2022 05:51:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/18/2022 05:51:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/18/2022 05:51:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/18/2022 05:51:32 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.26666666666666666 on epoch=649
03/18/2022 05:51:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/18/2022 05:51:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/18/2022 05:51:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/18/2022 05:51:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/18/2022 05:51:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/18/2022 05:51:44 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.3107692307692308 on epoch=674
03/18/2022 05:51:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/18/2022 05:51:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/18/2022 05:51:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/18/2022 05:51:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/18/2022 05:51:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/18/2022 05:51:56 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.22267206477732793 on epoch=699
03/18/2022 05:51:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
03/18/2022 05:52:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/18/2022 05:52:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 05:52:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/18/2022 05:52:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/18/2022 05:52:09 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.22267206477732793 on epoch=724
03/18/2022 05:52:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 05:52:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/18/2022 05:52:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 05:52:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/18/2022 05:52:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/18/2022 05:52:22 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.21951219512195122 on epoch=749
03/18/2022 05:52:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 05:52:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/18/2022 05:52:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/18/2022 05:52:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 05:52:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 05:52:36 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=774
03/18/2022 05:52:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 05:52:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 05:52:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 05:52:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/18/2022 05:52:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/18/2022 05:52:49 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=799
03/18/2022 05:52:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 05:52:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
03/18/2022 05:52:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 05:52:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/18/2022 05:53:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/18/2022 05:53:02 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.26666666666666666 on epoch=824
03/18/2022 05:53:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/18/2022 05:53:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 05:53:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 05:53:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 05:53:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 05:53:15 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=849
03/18/2022 05:53:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 05:53:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 05:53:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 05:53:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 05:53:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/18/2022 05:53:28 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.28744939271255066 on epoch=874
03/18/2022 05:53:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 05:53:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 05:53:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 05:53:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 05:53:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 05:53:41 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=899
03/18/2022 05:53:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 05:53:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 05:53:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 05:53:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 05:53:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 05:53:55 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=924
03/18/2022 05:53:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 05:54:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 05:54:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 05:54:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 05:54:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 05:54:08 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.24512820512820513 on epoch=949
03/18/2022 05:54:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 05:54:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/18/2022 05:54:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 05:54:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/18/2022 05:54:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 05:54:21 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.21951219512195122 on epoch=974
03/18/2022 05:54:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
03/18/2022 05:54:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
03/18/2022 05:54:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 05:54:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
03/18/2022 05:54:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 05:54:34 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.28744939271255066 on epoch=999
03/18/2022 05:54:34 - INFO - __main__ - save last model!
03/18/2022 05:54:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 05:54:34 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 05:54:34 - INFO - __main__ - Printing 3 examples
03/18/2022 05:54:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 05:54:34 - INFO - __main__ - ['false']
03/18/2022 05:54:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 05:54:34 - INFO - __main__ - ['false']
03/18/2022 05:54:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 05:54:34 - INFO - __main__ - ['false']
03/18/2022 05:54:34 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:54:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:54:35 - INFO - __main__ - Printing 3 examples
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 05:54:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:54:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:54:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:54:35 - INFO - __main__ - Printing 3 examples
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:54:35 - INFO - __main__ - ['false']
03/18/2022 05:54:35 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:54:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:54:35 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:54:35 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:54:38 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 05:54:53 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:54:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:54:54 - INFO - __main__ - Starting training!
03/18/2022 05:55:29 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_100_0.3_8_predictions.txt
03/18/2022 05:55:29 - INFO - __main__ - Classification-F1 on test data: 0.3054
03/18/2022 05:55:30 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.3, bsz=8, dev_performance=0.3992490613266583, test_performance=0.30540412448088533
03/18/2022 05:55:30 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.2, bsz=8 ...
03/18/2022 05:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:55:31 - INFO - __main__ - Printing 3 examples
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 05:55:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:55:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 05:55:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 05:55:31 - INFO - __main__ - Printing 3 examples
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: when is susan smith eligible for parole [SEP] answer: She is incarcerated at South Carolina's Leath Correctional Institution , near Greenwood .
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: when did thomson make the plum-pudding model [SEP] answer: Although gold has an atomic number of 79, immediately after Rutherford's paper appeared in 1911 Antonius Van den Broek made the intuitive suggestion that atomic number is nuclear charge.
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ -  [wiki_qa] question: what year was mario popular [SEP] answer: Mario , who serves as Nintendo 's mascot, is a fictional character created by game designer Shigeru Miyamoto and voiced by Charles Martinet .
03/18/2022 05:55:31 - INFO - __main__ - ['false']
03/18/2022 05:55:31 - INFO - __main__ - Tokenizing Input ...
03/18/2022 05:55:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 05:55:31 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 05:55:49 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 05:55:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 05:55:50 - INFO - __main__ - Starting training!
03/18/2022 05:55:53 - INFO - __main__ - Step 10 Global step 10 Train loss 7.46 on epoch=4
03/18/2022 05:55:56 - INFO - __main__ - Step 20 Global step 20 Train loss 5.31 on epoch=9
03/18/2022 05:55:59 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=14
03/18/2022 05:56:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.42 on epoch=19
03/18/2022 05:56:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=24
03/18/2022 05:56:04 - INFO - __main__ - Global step 50 Train loss 3.47 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 05:56:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 05:56:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=29
03/18/2022 05:56:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.61 on epoch=34
03/18/2022 05:56:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=39
03/18/2022 05:56:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=44
03/18/2022 05:56:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=49
03/18/2022 05:56:17 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 05:56:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=54
03/18/2022 05:56:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=59
03/18/2022 05:56:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=64
03/18/2022 05:56:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
03/18/2022 05:56:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
03/18/2022 05:56:30 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 05:56:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/18/2022 05:56:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
03/18/2022 05:56:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=89
03/18/2022 05:56:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=94
03/18/2022 05:56:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=99
03/18/2022 05:56:43 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.23809523809523808 on epoch=99
03/18/2022 05:56:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
03/18/2022 05:56:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
03/18/2022 05:56:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=114
03/18/2022 05:56:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
03/18/2022 05:56:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=124
03/18/2022 05:56:57 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=124
03/18/2022 05:56:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=124, global_step=250
03/18/2022 05:56:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
03/18/2022 05:57:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
03/18/2022 05:57:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=139
03/18/2022 05:57:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=144
03/18/2022 05:57:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
03/18/2022 05:57:10 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.21798631476050834 on epoch=149
03/18/2022 05:57:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=154
03/18/2022 05:57:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
03/18/2022 05:57:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=164
03/18/2022 05:57:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=169
03/18/2022 05:57:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=174
03/18/2022 05:57:23 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.21798631476050834 on epoch=174
03/18/2022 05:57:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=179
03/18/2022 05:57:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/18/2022 05:57:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=189
03/18/2022 05:57:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
03/18/2022 05:57:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=199
03/18/2022 05:57:36 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.25 on epoch=199
03/18/2022 05:57:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
03/18/2022 05:57:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
03/18/2022 05:57:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=214
03/18/2022 05:57:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=219
03/18/2022 05:57:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=224
03/18/2022 05:57:49 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.36374269005847953 on epoch=224
03/18/2022 05:57:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.18 on epoch=229
03/18/2022 05:57:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=234
03/18/2022 05:57:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=239
03/18/2022 05:57:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=244
03/18/2022 05:58:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=249
03/18/2022 05:58:02 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3552492046659597 on epoch=249
03/18/2022 05:58:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
03/18/2022 05:58:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=259
03/18/2022 05:58:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=264
03/18/2022 05:58:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
03/18/2022 05:58:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=274
03/18/2022 05:58:15 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.24705882352941178 on epoch=274
03/18/2022 05:58:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=279
03/18/2022 05:58:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=284
03/18/2022 05:58:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
03/18/2022 05:58:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=294
03/18/2022 05:58:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=299
03/18/2022 05:58:28 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.3107692307692308 on epoch=299
03/18/2022 05:58:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=304
03/18/2022 05:58:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=309
03/18/2022 05:58:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=314
03/18/2022 05:58:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=319
03/18/2022 05:58:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=324
03/18/2022 05:58:41 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 05:58:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/18/2022 05:58:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=334
03/18/2022 05:58:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/18/2022 05:58:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=344
03/18/2022 05:58:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=349
03/18/2022 05:58:54 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.3454545454545454 on epoch=349
03/18/2022 05:58:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=354
03/18/2022 05:58:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=359
03/18/2022 05:59:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=364
03/18/2022 05:59:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=369
03/18/2022 05:59:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=374
03/18/2022 05:59:07 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.2632632632632632 on epoch=374
03/18/2022 05:59:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=379
03/18/2022 05:59:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=384
03/18/2022 05:59:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=389
03/18/2022 05:59:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=394
03/18/2022 05:59:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=399
03/18/2022 05:59:20 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.24705882352941178 on epoch=399
03/18/2022 05:59:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
03/18/2022 05:59:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
03/18/2022 05:59:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=414
03/18/2022 05:59:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/18/2022 05:59:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
03/18/2022 05:59:33 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.37662337662337664 on epoch=424
03/18/2022 05:59:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=429
03/18/2022 05:59:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=434
03/18/2022 05:59:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=439
03/18/2022 05:59:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
03/18/2022 05:59:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=449
03/18/2022 05:59:46 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.3266888150609081 on epoch=449
03/18/2022 05:59:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=454
03/18/2022 05:59:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/18/2022 05:59:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=464
03/18/2022 05:59:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/18/2022 05:59:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/18/2022 05:59:59 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.25 on epoch=474
03/18/2022 06:00:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=479
03/18/2022 06:00:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
03/18/2022 06:00:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
03/18/2022 06:00:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
03/18/2022 06:00:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=499
03/18/2022 06:00:12 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.3266888150609081 on epoch=499
03/18/2022 06:00:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=504
03/18/2022 06:00:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=509
03/18/2022 06:00:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=514
03/18/2022 06:00:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=519
03/18/2022 06:00:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/18/2022 06:00:25 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.39999999999999997 on epoch=524
03/18/2022 06:00:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.39999999999999997 on epoch=524, global_step=1050
03/18/2022 06:00:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
03/18/2022 06:00:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=534
03/18/2022 06:00:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
03/18/2022 06:00:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=544
03/18/2022 06:00:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=549
03/18/2022 06:00:38 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.24512820512820513 on epoch=549
03/18/2022 06:00:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=554
03/18/2022 06:00:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=559
03/18/2022 06:00:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
03/18/2022 06:00:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=569
03/18/2022 06:00:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=574
03/18/2022 06:00:51 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.39999999999999997 on epoch=574
03/18/2022 06:00:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=579
03/18/2022 06:00:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=584
03/18/2022 06:00:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/18/2022 06:01:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
03/18/2022 06:01:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=599
03/18/2022 06:01:04 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.24705882352941178 on epoch=599
03/18/2022 06:01:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
03/18/2022 06:01:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=609
03/18/2022 06:01:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=614
03/18/2022 06:01:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=619
03/18/2022 06:01:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
03/18/2022 06:01:17 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.3266888150609081 on epoch=624
03/18/2022 06:01:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
03/18/2022 06:01:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=634
03/18/2022 06:01:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
03/18/2022 06:01:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
03/18/2022 06:01:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=649
03/18/2022 06:01:30 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.37662337662337664 on epoch=649
03/18/2022 06:01:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
03/18/2022 06:01:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=659
03/18/2022 06:01:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
03/18/2022 06:01:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
03/18/2022 06:01:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
03/18/2022 06:01:43 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.37662337662337664 on epoch=674
03/18/2022 06:01:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=679
03/18/2022 06:01:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
03/18/2022 06:01:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
03/18/2022 06:01:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
03/18/2022 06:01:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=699
03/18/2022 06:01:56 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.3454545454545454 on epoch=699
03/18/2022 06:01:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
03/18/2022 06:02:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=709
03/18/2022 06:02:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
03/18/2022 06:02:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
03/18/2022 06:02:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
03/18/2022 06:02:09 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.2748768472906404 on epoch=724
03/18/2022 06:02:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=729
03/18/2022 06:02:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
03/18/2022 06:02:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=739
03/18/2022 06:02:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
03/18/2022 06:02:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
03/18/2022 06:02:22 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=749
03/18/2022 06:02:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=754
03/18/2022 06:02:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=759
03/18/2022 06:02:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
03/18/2022 06:02:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=769
03/18/2022 06:02:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=774
03/18/2022 06:02:35 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=774
03/18/2022 06:02:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=779
03/18/2022 06:02:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=784
03/18/2022 06:02:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
03/18/2022 06:02:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
03/18/2022 06:02:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=799
03/18/2022 06:02:49 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3816425120772947 on epoch=799
03/18/2022 06:02:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
03/18/2022 06:02:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
03/18/2022 06:02:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
03/18/2022 06:02:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
03/18/2022 06:03:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=824
03/18/2022 06:03:02 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=824
03/18/2022 06:03:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=829
03/18/2022 06:03:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
03/18/2022 06:03:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=839
03/18/2022 06:03:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
03/18/2022 06:03:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
03/18/2022 06:03:15 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=849
03/18/2022 06:03:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
03/18/2022 06:03:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
03/18/2022 06:03:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=864
03/18/2022 06:03:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
03/18/2022 06:03:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
03/18/2022 06:03:28 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=874
03/18/2022 06:03:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
03/18/2022 06:03:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
03/18/2022 06:03:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
03/18/2022 06:03:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
03/18/2022 06:03:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
03/18/2022 06:03:41 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.3266888150609081 on epoch=899
03/18/2022 06:03:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
03/18/2022 06:03:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
03/18/2022 06:03:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
03/18/2022 06:03:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=919
03/18/2022 06:03:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
03/18/2022 06:03:54 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=924
03/18/2022 06:03:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=929
03/18/2022 06:03:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=934
03/18/2022 06:04:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
03/18/2022 06:04:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
03/18/2022 06:04:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
03/18/2022 06:04:07 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.3266888150609081 on epoch=949
03/18/2022 06:04:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=954
03/18/2022 06:04:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/18/2022 06:04:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
03/18/2022 06:04:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
03/18/2022 06:04:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
03/18/2022 06:04:20 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.3816425120772947 on epoch=974
03/18/2022 06:04:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
03/18/2022 06:04:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
03/18/2022 06:04:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=989
03/18/2022 06:04:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
03/18/2022 06:04:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
03/18/2022 06:04:33 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.2805474095796677 on epoch=999
03/18/2022 06:04:33 - INFO - __main__ - save last model!
03/18/2022 06:04:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 06:04:33 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 06:04:33 - INFO - __main__ - Printing 3 examples
03/18/2022 06:04:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 06:04:33 - INFO - __main__ - ['false']
03/18/2022 06:04:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 06:04:33 - INFO - __main__ - ['false']
03/18/2022 06:04:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 06:04:33 - INFO - __main__ - ['false']
03/18/2022 06:04:33 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:04:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:04:34 - INFO - __main__ - Printing 3 examples
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 06:04:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:04:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:04:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:04:34 - INFO - __main__ - Printing 3 examples
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:04:34 - INFO - __main__ - ['false']
03/18/2022 06:04:34 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:04:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:04:34 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:04:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:04:37 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 06:04:49 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:04:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:04:50 - INFO - __main__ - Starting training!
03/18/2022 06:05:20 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_100_0.2_8_predictions.txt
03/18/2022 06:05:20 - INFO - __main__ - Classification-F1 on test data: 0.4145
03/18/2022 06:05:21 - INFO - __main__ - prefix=wiki_qa_16_100, lr=0.2, bsz=8, dev_performance=0.39999999999999997, test_performance=0.414507269198374
03/18/2022 06:05:21 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.5, bsz=8 ...
03/18/2022 06:05:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:05:22 - INFO - __main__ - Printing 3 examples
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:05:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:05:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:05:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:05:22 - INFO - __main__ - Printing 3 examples
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:05:22 - INFO - __main__ - ['false']
03/18/2022 06:05:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:05:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:05:22 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:05:38 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:05:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:05:39 - INFO - __main__ - Starting training!
03/18/2022 06:05:43 - INFO - __main__ - Step 10 Global step 10 Train loss 7.07 on epoch=4
03/18/2022 06:05:46 - INFO - __main__ - Step 20 Global step 20 Train loss 2.75 on epoch=9
03/18/2022 06:05:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.86 on epoch=14
03/18/2022 06:05:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=19
03/18/2022 06:05:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=24
03/18/2022 06:05:57 - INFO - __main__ - Global step 50 Train loss 2.36 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 06:05:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 06:06:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=29
03/18/2022 06:06:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
03/18/2022 06:06:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
03/18/2022 06:06:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=44
03/18/2022 06:06:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
03/18/2022 06:06:14 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 06:06:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=54
03/18/2022 06:06:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
03/18/2022 06:06:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
03/18/2022 06:06:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
03/18/2022 06:06:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
03/18/2022 06:06:31 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 06:06:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=79
03/18/2022 06:06:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
03/18/2022 06:06:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
03/18/2022 06:06:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=94
03/18/2022 06:06:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=99
03/18/2022 06:06:48 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 06:06:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=104
03/18/2022 06:06:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
03/18/2022 06:06:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
03/18/2022 06:07:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=119
03/18/2022 06:07:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
03/18/2022 06:07:06 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 06:07:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
03/18/2022 06:07:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
03/18/2022 06:07:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=139
03/18/2022 06:07:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
03/18/2022 06:07:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/18/2022 06:07:23 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 06:07:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
03/18/2022 06:07:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
03/18/2022 06:07:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
03/18/2022 06:07:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=169
03/18/2022 06:07:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
03/18/2022 06:07:40 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3992490613266583 on epoch=174
03/18/2022 06:07:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=174, global_step=350
03/18/2022 06:07:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
03/18/2022 06:07:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=184
03/18/2022 06:07:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=189
03/18/2022 06:07:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
03/18/2022 06:07:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
03/18/2022 06:07:57 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 06:08:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
03/18/2022 06:08:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=209
03/18/2022 06:08:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
03/18/2022 06:08:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=219
03/18/2022 06:08:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=224
03/18/2022 06:08:15 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.5151515151515151 on epoch=224
03/18/2022 06:08:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5151515151515151 on epoch=224, global_step=450
03/18/2022 06:08:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=229
03/18/2022 06:08:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
03/18/2022 06:08:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=239
03/18/2022 06:08:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
03/18/2022 06:08:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=249
03/18/2022 06:08:32 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 06:08:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=254
03/18/2022 06:08:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=259
03/18/2022 06:08:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/18/2022 06:08:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=269
03/18/2022 06:08:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=274
03/18/2022 06:08:49 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=274
03/18/2022 06:08:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
03/18/2022 06:08:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
03/18/2022 06:08:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=289
03/18/2022 06:09:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=294
03/18/2022 06:09:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=299
03/18/2022 06:09:06 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 06:09:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=304
03/18/2022 06:09:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=309
03/18/2022 06:09:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/18/2022 06:09:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=319
03/18/2022 06:09:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=324
03/18/2022 06:09:23 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=324
03/18/2022 06:09:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=329
03/18/2022 06:09:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=334
03/18/2022 06:09:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=339
03/18/2022 06:09:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=344
03/18/2022 06:09:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=349
03/18/2022 06:09:40 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=349
03/18/2022 06:09:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=354
03/18/2022 06:09:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=359
03/18/2022 06:09:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/18/2022 06:09:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/18/2022 06:09:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=374
03/18/2022 06:09:58 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.37254901960784315 on epoch=374
03/18/2022 06:10:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=379
03/18/2022 06:10:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=384
03/18/2022 06:10:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
03/18/2022 06:10:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=394
03/18/2022 06:10:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=399
03/18/2022 06:10:15 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.39139139139139134 on epoch=399
03/18/2022 06:10:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=404
03/18/2022 06:10:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
03/18/2022 06:10:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/18/2022 06:10:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=419
03/18/2022 06:10:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/18/2022 06:10:32 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.4181818181818182 on epoch=424
03/18/2022 06:10:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=429
03/18/2022 06:10:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=434
03/18/2022 06:10:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/18/2022 06:10:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/18/2022 06:10:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=449
03/18/2022 06:10:50 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=449
03/18/2022 06:10:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=454
03/18/2022 06:10:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
03/18/2022 06:11:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=464
03/18/2022 06:11:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/18/2022 06:11:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/18/2022 06:11:07 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3816425120772947 on epoch=474
03/18/2022 06:11:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/18/2022 06:11:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
03/18/2022 06:11:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=489
03/18/2022 06:11:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=494
03/18/2022 06:11:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
03/18/2022 06:11:24 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.3816425120772947 on epoch=499
03/18/2022 06:11:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=504
03/18/2022 06:11:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=509
03/18/2022 06:11:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=514
03/18/2022 06:11:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
03/18/2022 06:11:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
03/18/2022 06:11:41 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=524
03/18/2022 06:11:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
03/18/2022 06:11:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
03/18/2022 06:11:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=539
03/18/2022 06:11:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=544
03/18/2022 06:11:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/18/2022 06:11:58 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=549
03/18/2022 06:12:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/18/2022 06:12:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
03/18/2022 06:12:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
03/18/2022 06:12:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=569
03/18/2022 06:12:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
03/18/2022 06:12:15 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=574
03/18/2022 06:12:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/18/2022 06:12:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=584
03/18/2022 06:12:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=589
03/18/2022 06:12:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
03/18/2022 06:12:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=599
03/18/2022 06:12:32 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.3552492046659597 on epoch=599
03/18/2022 06:12:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=604
03/18/2022 06:12:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/18/2022 06:12:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=614
03/18/2022 06:12:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=619
03/18/2022 06:12:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=624
03/18/2022 06:12:50 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.3454545454545454 on epoch=624
03/18/2022 06:12:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/18/2022 06:12:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=634
03/18/2022 06:12:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=639
03/18/2022 06:13:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=644
03/18/2022 06:13:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
03/18/2022 06:13:07 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.2748768472906404 on epoch=649
03/18/2022 06:13:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
03/18/2022 06:13:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
03/18/2022 06:13:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
03/18/2022 06:13:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=669
03/18/2022 06:13:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=674
03/18/2022 06:13:24 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.4285714285714286 on epoch=674
03/18/2022 06:13:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
03/18/2022 06:13:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
03/18/2022 06:13:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/18/2022 06:13:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/18/2022 06:13:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=699
03/18/2022 06:13:41 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.30158730158730157 on epoch=699
03/18/2022 06:13:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/18/2022 06:13:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
03/18/2022 06:13:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/18/2022 06:13:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/18/2022 06:13:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/18/2022 06:13:58 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3522267206477733 on epoch=724
03/18/2022 06:14:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/18/2022 06:14:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/18/2022 06:14:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/18/2022 06:14:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/18/2022 06:14:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/18/2022 06:14:15 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=749
03/18/2022 06:14:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 06:14:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/18/2022 06:14:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/18/2022 06:14:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 06:14:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 06:14:33 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.37254901960784315 on epoch=774
03/18/2022 06:14:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 06:14:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 06:14:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=789
03/18/2022 06:14:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/18/2022 06:14:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
03/18/2022 06:14:50 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.40566959921798634 on epoch=799
03/18/2022 06:14:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
03/18/2022 06:14:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 06:14:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/18/2022 06:15:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/18/2022 06:15:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/18/2022 06:15:07 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.375 on epoch=824
03/18/2022 06:15:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 06:15:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/18/2022 06:15:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=839
03/18/2022 06:15:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/18/2022 06:15:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/18/2022 06:15:24 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.3522267206477733 on epoch=849
03/18/2022 06:15:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/18/2022 06:15:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/18/2022 06:15:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 06:15:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=869
03/18/2022 06:15:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 06:15:41 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4285714285714286 on epoch=874
03/18/2022 06:15:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/18/2022 06:15:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/18/2022 06:15:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 06:15:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 06:15:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/18/2022 06:15:58 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=899
03/18/2022 06:16:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/18/2022 06:16:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 06:16:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 06:16:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 06:16:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 06:16:15 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=924
03/18/2022 06:16:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 06:16:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 06:16:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/18/2022 06:16:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 06:16:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/18/2022 06:16:32 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=949
03/18/2022 06:16:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 06:16:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 06:16:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 06:16:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 06:16:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 06:16:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=974
03/18/2022 06:16:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 06:16:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 06:16:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 06:17:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
03/18/2022 06:17:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 06:17:06 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.375 on epoch=999
03/18/2022 06:17:06 - INFO - __main__ - save last model!
03/18/2022 06:17:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 06:17:06 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 06:17:06 - INFO - __main__ - Printing 3 examples
03/18/2022 06:17:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 06:17:06 - INFO - __main__ - ['false']
03/18/2022 06:17:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 06:17:06 - INFO - __main__ - ['false']
03/18/2022 06:17:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 06:17:06 - INFO - __main__ - ['false']
03/18/2022 06:17:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:17:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:17:07 - INFO - __main__ - Printing 3 examples
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 06:17:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:17:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:17:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:17:07 - INFO - __main__ - Printing 3 examples
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:17:07 - INFO - __main__ - ['false']
03/18/2022 06:17:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:17:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:17:07 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:17:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:17:10 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 06:17:26 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:17:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:17:27 - INFO - __main__ - Starting training!
03/18/2022 06:17:58 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_13_0.5_8_predictions.txt
03/18/2022 06:17:58 - INFO - __main__ - Classification-F1 on test data: 0.3699
03/18/2022 06:17:58 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.5, bsz=8, dev_performance=0.5151515151515151, test_performance=0.36993417821004027
03/18/2022 06:17:58 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.4, bsz=8 ...
03/18/2022 06:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:17:59 - INFO - __main__ - Printing 3 examples
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:17:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:17:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:17:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:17:59 - INFO - __main__ - Printing 3 examples
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:17:59 - INFO - __main__ - ['false']
03/18/2022 06:17:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:17:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:17:59 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:18:17 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:18:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:18:18 - INFO - __main__ - Starting training!
03/18/2022 06:18:25 - INFO - __main__ - Step 10 Global step 10 Train loss 7.84 on epoch=4
03/18/2022 06:18:28 - INFO - __main__ - Step 20 Global step 20 Train loss 7.83 on epoch=9
03/18/2022 06:18:31 - INFO - __main__ - Step 30 Global step 30 Train loss 7.79 on epoch=14
03/18/2022 06:18:35 - INFO - __main__ - Step 40 Global step 40 Train loss 7.86 on epoch=19
03/18/2022 06:18:38 - INFO - __main__ - Step 50 Global step 50 Train loss 8.10 on epoch=24
03/18/2022 06:18:50 - INFO - __main__ - Global step 50 Train loss 7.88 Classification-F1 0.0 on epoch=24
03/18/2022 06:18:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/18/2022 06:18:54 - INFO - __main__ - Step 60 Global step 60 Train loss 7.48 on epoch=29
03/18/2022 06:18:57 - INFO - __main__ - Step 70 Global step 70 Train loss 7.27 on epoch=34
03/18/2022 06:19:00 - INFO - __main__ - Step 80 Global step 80 Train loss 7.11 on epoch=39
03/18/2022 06:19:04 - INFO - __main__ - Step 90 Global step 90 Train loss 6.77 on epoch=44
03/18/2022 06:19:07 - INFO - __main__ - Step 100 Global step 100 Train loss 6.11 on epoch=49
03/18/2022 06:19:21 - INFO - __main__ - Global step 100 Train loss 6.95 Classification-F1 0.0 on epoch=49
03/18/2022 06:19:24 - INFO - __main__ - Step 110 Global step 110 Train loss 5.16 on epoch=54
03/18/2022 06:19:28 - INFO - __main__ - Step 120 Global step 120 Train loss 4.33 on epoch=59
03/18/2022 06:19:31 - INFO - __main__ - Step 130 Global step 130 Train loss 3.72 on epoch=64
03/18/2022 06:19:35 - INFO - __main__ - Step 140 Global step 140 Train loss 3.16 on epoch=69
03/18/2022 06:19:38 - INFO - __main__ - Step 150 Global step 150 Train loss 2.84 on epoch=74
03/18/2022 06:19:44 - INFO - __main__ - Global step 150 Train loss 3.84 Classification-F1 0.021978021978021976 on epoch=74
03/18/2022 06:19:44 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.021978021978021976 on epoch=74, global_step=150
03/18/2022 06:19:47 - INFO - __main__ - Step 160 Global step 160 Train loss 2.49 on epoch=79
03/18/2022 06:19:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.91 on epoch=84
03/18/2022 06:19:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.72 on epoch=89
03/18/2022 06:19:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.45 on epoch=94
03/18/2022 06:20:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=99
03/18/2022 06:20:03 - INFO - __main__ - Global step 200 Train loss 1.71 Classification-F1 0.3454545454545454 on epoch=99
03/18/2022 06:20:04 - INFO - __main__ - Saving model with best Classification-F1: 0.021978021978021976 -> 0.3454545454545454 on epoch=99, global_step=200
03/18/2022 06:20:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=104
03/18/2022 06:20:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=109
03/18/2022 06:20:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=114
03/18/2022 06:20:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.72 on epoch=119
03/18/2022 06:20:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=124
03/18/2022 06:20:21 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 06:20:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=129
03/18/2022 06:20:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=134
03/18/2022 06:20:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=139
03/18/2022 06:20:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=144
03/18/2022 06:20:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=149
03/18/2022 06:20:39 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.3191489361702127 on epoch=149
03/18/2022 06:20:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=154
03/18/2022 06:20:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=159
03/18/2022 06:20:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=164
03/18/2022 06:20:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=169
03/18/2022 06:20:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=174
03/18/2022 06:20:56 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.3191489361702127 on epoch=174
03/18/2022 06:21:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=179
03/18/2022 06:21:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=184
03/18/2022 06:21:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=189
03/18/2022 06:21:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=194
03/18/2022 06:21:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=199
03/18/2022 06:21:14 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.3650793650793651 on epoch=199
03/18/2022 06:21:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3454545454545454 -> 0.3650793650793651 on epoch=199, global_step=400
03/18/2022 06:21:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=204
03/18/2022 06:21:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=209
03/18/2022 06:21:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=214
03/18/2022 06:21:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=219
03/18/2022 06:21:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=224
03/18/2022 06:21:31 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 06:21:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=229
03/18/2022 06:21:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=234
03/18/2022 06:21:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=239
03/18/2022 06:21:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=244
03/18/2022 06:21:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=249
03/18/2022 06:21:49 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 06:21:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=254
03/18/2022 06:21:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=259
03/18/2022 06:21:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=264
03/18/2022 06:22:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=269
03/18/2022 06:22:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=274
03/18/2022 06:22:06 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 06:22:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=279
03/18/2022 06:22:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=284
03/18/2022 06:22:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=289
03/18/2022 06:22:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=294
03/18/2022 06:22:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=299
03/18/2022 06:22:24 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.3191489361702127 on epoch=299
03/18/2022 06:22:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=304
03/18/2022 06:22:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=309
03/18/2022 06:22:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/18/2022 06:22:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=319
03/18/2022 06:22:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=324
03/18/2022 06:22:41 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.3191489361702127 on epoch=324
03/18/2022 06:22:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/18/2022 06:22:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=334
03/18/2022 06:22:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/18/2022 06:22:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=344
03/18/2022 06:22:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=349
03/18/2022 06:22:59 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 06:23:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=354
03/18/2022 06:23:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/18/2022 06:23:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=364
03/18/2022 06:23:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
03/18/2022 06:23:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=374
03/18/2022 06:23:16 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=374
03/18/2022 06:23:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=379
03/18/2022 06:23:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=384
03/18/2022 06:23:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
03/18/2022 06:23:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=394
03/18/2022 06:23:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=399
03/18/2022 06:23:33 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=399
03/18/2022 06:23:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=404
03/18/2022 06:23:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
03/18/2022 06:23:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=414
03/18/2022 06:23:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
03/18/2022 06:23:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=424
03/18/2022 06:23:51 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3191489361702127 on epoch=424
03/18/2022 06:23:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=429
03/18/2022 06:23:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
03/18/2022 06:24:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/18/2022 06:24:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
03/18/2022 06:24:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
03/18/2022 06:24:08 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=449
03/18/2022 06:24:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3650793650793651 -> 0.3816425120772947 on epoch=449, global_step=900
03/18/2022 06:24:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=454
03/18/2022 06:24:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=459
03/18/2022 06:24:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
03/18/2022 06:24:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/18/2022 06:24:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/18/2022 06:24:26 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=474
03/18/2022 06:24:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
03/18/2022 06:24:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=484
03/18/2022 06:24:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=489
03/18/2022 06:24:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/18/2022 06:24:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
03/18/2022 06:24:43 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=499
03/18/2022 06:24:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=504
03/18/2022 06:24:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=509
03/18/2022 06:24:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/18/2022 06:24:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=519
03/18/2022 06:25:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/18/2022 06:25:00 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.4009852216748768 on epoch=524
03/18/2022 06:25:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4009852216748768 on epoch=524, global_step=1050
03/18/2022 06:25:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=529
03/18/2022 06:25:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
03/18/2022 06:25:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=539
03/18/2022 06:25:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=544
03/18/2022 06:25:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
03/18/2022 06:25:18 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.36374269005847953 on epoch=549
03/18/2022 06:25:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/18/2022 06:25:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
03/18/2022 06:25:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=564
03/18/2022 06:25:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=569
03/18/2022 06:25:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
03/18/2022 06:25:35 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.24705882352941178 on epoch=574
03/18/2022 06:25:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
03/18/2022 06:25:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
03/18/2022 06:25:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=589
03/18/2022 06:25:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=594
03/18/2022 06:25:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=599
03/18/2022 06:25:53 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.3522267206477733 on epoch=599
03/18/2022 06:25:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/18/2022 06:26:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=609
03/18/2022 06:26:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=614
03/18/2022 06:26:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=619
03/18/2022 06:26:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=624
03/18/2022 06:26:10 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=624
03/18/2022 06:26:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4009852216748768 -> 0.4231177094379639 on epoch=624, global_step=1250
03/18/2022 06:26:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/18/2022 06:26:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=634
03/18/2022 06:26:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=639
03/18/2022 06:26:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=644
03/18/2022 06:26:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=649
03/18/2022 06:26:28 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.3650793650793651 on epoch=649
03/18/2022 06:26:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/18/2022 06:26:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/18/2022 06:26:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
03/18/2022 06:26:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=669
03/18/2022 06:26:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
03/18/2022 06:26:45 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.3273273273273273 on epoch=674
03/18/2022 06:26:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=679
03/18/2022 06:26:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/18/2022 06:26:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=689
03/18/2022 06:26:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=694
03/18/2022 06:27:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
03/18/2022 06:27:03 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.3650793650793651 on epoch=699
03/18/2022 06:27:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
03/18/2022 06:27:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=709
03/18/2022 06:27:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
03/18/2022 06:27:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
03/18/2022 06:27:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/18/2022 06:27:21 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.375 on epoch=724
03/18/2022 06:27:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
03/18/2022 06:27:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
03/18/2022 06:27:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=739
03/18/2022 06:27:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
03/18/2022 06:27:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=749
03/18/2022 06:27:38 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.40566959921798634 on epoch=749
03/18/2022 06:27:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=754
03/18/2022 06:27:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
03/18/2022 06:27:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/18/2022 06:27:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/18/2022 06:27:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/18/2022 06:27:56 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.37254901960784315 on epoch=774
03/18/2022 06:27:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
03/18/2022 06:28:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/18/2022 06:28:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/18/2022 06:28:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/18/2022 06:28:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/18/2022 06:28:13 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.375 on epoch=799
03/18/2022 06:28:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 06:28:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/18/2022 06:28:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/18/2022 06:28:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=819
03/18/2022 06:28:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/18/2022 06:28:31 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.30980392156862746 on epoch=824
03/18/2022 06:28:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/18/2022 06:28:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/18/2022 06:28:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 06:28:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 06:28:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/18/2022 06:28:49 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.375 on epoch=849
03/18/2022 06:28:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/18/2022 06:28:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/18/2022 06:28:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/18/2022 06:29:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 06:29:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 06:29:06 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.375 on epoch=874
03/18/2022 06:29:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/18/2022 06:29:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 06:29:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 06:29:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 06:29:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 06:29:25 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.375 on epoch=899
03/18/2022 06:29:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/18/2022 06:29:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 06:29:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/18/2022 06:29:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 06:29:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/18/2022 06:29:42 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=924
03/18/2022 06:29:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 06:29:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/18/2022 06:29:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/18/2022 06:29:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 06:29:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/18/2022 06:30:00 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.375 on epoch=949
03/18/2022 06:30:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 06:30:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 06:30:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=964
03/18/2022 06:30:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/18/2022 06:30:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 06:30:18 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=974
03/18/2022 06:30:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 06:30:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 06:30:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 06:30:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/18/2022 06:30:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 06:30:36 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=999
03/18/2022 06:30:36 - INFO - __main__ - save last model!
03/18/2022 06:30:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 06:30:36 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 06:30:36 - INFO - __main__ - Printing 3 examples
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:30:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:30:36 - INFO - __main__ - Printing 3 examples
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 06:30:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:30:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:30:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:30:36 - INFO - __main__ - Printing 3 examples
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:30:36 - INFO - __main__ - ['false']
03/18/2022 06:30:36 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:30:36 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:30:36 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:30:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:30:40 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 06:30:55 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:30:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:30:56 - INFO - __main__ - Starting training!
03/18/2022 06:31:37 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_13_0.4_8_predictions.txt
03/18/2022 06:31:37 - INFO - __main__ - Classification-F1 on test data: 0.2718
03/18/2022 06:31:37 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.4, bsz=8, dev_performance=0.4231177094379639, test_performance=0.271784969665626
03/18/2022 06:31:37 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.3, bsz=8 ...
03/18/2022 06:31:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:31:38 - INFO - __main__ - Printing 3 examples
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:31:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:31:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:31:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:31:38 - INFO - __main__ - Printing 3 examples
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:31:38 - INFO - __main__ - ['false']
03/18/2022 06:31:38 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:31:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:31:38 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:31:55 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:31:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:31:55 - INFO - __main__ - Starting training!
03/18/2022 06:31:59 - INFO - __main__ - Step 10 Global step 10 Train loss 7.55 on epoch=4
03/18/2022 06:32:03 - INFO - __main__ - Step 20 Global step 20 Train loss 6.78 on epoch=9
03/18/2022 06:32:06 - INFO - __main__ - Step 30 Global step 30 Train loss 5.17 on epoch=14
03/18/2022 06:32:09 - INFO - __main__ - Step 40 Global step 40 Train loss 3.00 on epoch=19
03/18/2022 06:32:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.98 on epoch=24
03/18/2022 06:32:14 - INFO - __main__ - Global step 50 Train loss 4.90 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 06:32:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 06:32:18 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=29
03/18/2022 06:32:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=34
03/18/2022 06:32:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=39
03/18/2022 06:32:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=44
03/18/2022 06:32:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=49
03/18/2022 06:32:32 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 06:32:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
03/18/2022 06:32:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=59
03/18/2022 06:32:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
03/18/2022 06:32:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=69
03/18/2022 06:32:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=74
03/18/2022 06:32:49 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.3043478260869565 on epoch=74
03/18/2022 06:32:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=79
03/18/2022 06:32:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=84
03/18/2022 06:32:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
03/18/2022 06:33:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=94
03/18/2022 06:33:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
03/18/2022 06:33:06 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 06:33:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=104
03/18/2022 06:33:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=109
03/18/2022 06:33:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=114
03/18/2022 06:33:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=119
03/18/2022 06:33:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
03/18/2022 06:33:23 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.3191489361702127 on epoch=124
03/18/2022 06:33:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
03/18/2022 06:33:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
03/18/2022 06:33:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
03/18/2022 06:33:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=144
03/18/2022 06:33:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=149
03/18/2022 06:33:40 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.3191489361702127 on epoch=149
03/18/2022 06:33:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=154
03/18/2022 06:33:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=159
03/18/2022 06:33:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
03/18/2022 06:33:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=169
03/18/2022 06:33:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=174
03/18/2022 06:33:58 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 06:34:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
03/18/2022 06:34:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=184
03/18/2022 06:34:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=189
03/18/2022 06:34:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=194
03/18/2022 06:34:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=199
03/18/2022 06:34:15 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.3191489361702127 on epoch=199
03/18/2022 06:34:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=204
03/18/2022 06:34:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=209
03/18/2022 06:34:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=214
03/18/2022 06:34:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=219
03/18/2022 06:34:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=224
03/18/2022 06:34:32 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 06:34:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=229
03/18/2022 06:34:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=234
03/18/2022 06:34:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=239
03/18/2022 06:34:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=244
03/18/2022 06:34:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=249
03/18/2022 06:34:49 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 06:34:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=254
03/18/2022 06:34:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=259
03/18/2022 06:34:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=264
03/18/2022 06:35:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
03/18/2022 06:35:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=274
03/18/2022 06:35:06 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=274
03/18/2022 06:35:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=274, global_step=550
03/18/2022 06:35:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=279
03/18/2022 06:35:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=284
03/18/2022 06:35:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=289
03/18/2022 06:35:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/18/2022 06:35:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
03/18/2022 06:35:23 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=299
03/18/2022 06:35:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=304
03/18/2022 06:35:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=309
03/18/2022 06:35:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=314
03/18/2022 06:35:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=319
03/18/2022 06:35:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=324
03/18/2022 06:35:41 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 06:35:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=329
03/18/2022 06:35:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=334
03/18/2022 06:35:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=339
03/18/2022 06:35:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=344
03/18/2022 06:35:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=349
03/18/2022 06:35:58 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 06:36:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
03/18/2022 06:36:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=359
03/18/2022 06:36:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=364
03/18/2022 06:36:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=369
03/18/2022 06:36:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=374
03/18/2022 06:36:15 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=374
03/18/2022 06:36:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=379
03/18/2022 06:36:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=384
03/18/2022 06:36:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
03/18/2022 06:36:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=394
03/18/2022 06:36:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=399
03/18/2022 06:36:32 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=399
03/18/2022 06:36:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=404
03/18/2022 06:36:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=409
03/18/2022 06:36:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/18/2022 06:36:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/18/2022 06:36:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=424
03/18/2022 06:36:49 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=424
03/18/2022 06:36:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4181818181818182 on epoch=424, global_step=850
03/18/2022 06:36:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=429
03/18/2022 06:36:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=434
03/18/2022 06:37:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/18/2022 06:37:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=444
03/18/2022 06:37:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=449
03/18/2022 06:37:07 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.37662337662337664 on epoch=449
03/18/2022 06:37:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/18/2022 06:37:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
03/18/2022 06:37:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=464
03/18/2022 06:37:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=469
03/18/2022 06:37:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
03/18/2022 06:37:24 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.4285714285714286 on epoch=474
03/18/2022 06:37:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4285714285714286 on epoch=474, global_step=950
03/18/2022 06:37:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/18/2022 06:37:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=484
03/18/2022 06:37:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=489
03/18/2022 06:37:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/18/2022 06:37:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=499
03/18/2022 06:37:41 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.37662337662337664 on epoch=499
03/18/2022 06:37:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=504
03/18/2022 06:37:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/18/2022 06:37:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=514
03/18/2022 06:37:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=519
03/18/2022 06:37:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/18/2022 06:37:59 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.3650793650793651 on epoch=524
03/18/2022 06:38:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
03/18/2022 06:38:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=534
03/18/2022 06:38:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=539
03/18/2022 06:38:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=544
03/18/2022 06:38:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
03/18/2022 06:38:16 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.34310850439882695 on epoch=549
03/18/2022 06:38:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
03/18/2022 06:38:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=559
03/18/2022 06:38:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/18/2022 06:38:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/18/2022 06:38:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
03/18/2022 06:38:34 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.34310850439882695 on epoch=574
03/18/2022 06:38:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=579
03/18/2022 06:38:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
03/18/2022 06:38:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/18/2022 06:38:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
03/18/2022 06:38:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/18/2022 06:38:51 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.4009852216748768 on epoch=599
03/18/2022 06:38:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
03/18/2022 06:38:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/18/2022 06:39:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/18/2022 06:39:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
03/18/2022 06:39:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/18/2022 06:39:08 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=624
03/18/2022 06:39:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/18/2022 06:39:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/18/2022 06:39:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/18/2022 06:39:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/18/2022 06:39:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/18/2022 06:39:25 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=649
03/18/2022 06:39:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
03/18/2022 06:39:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=659
03/18/2022 06:39:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/18/2022 06:39:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
03/18/2022 06:39:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/18/2022 06:39:43 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.30980392156862746 on epoch=674
03/18/2022 06:39:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
03/18/2022 06:39:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
03/18/2022 06:39:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
03/18/2022 06:39:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/18/2022 06:40:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/18/2022 06:40:00 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.30980392156862746 on epoch=699
03/18/2022 06:40:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/18/2022 06:40:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
03/18/2022 06:40:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 06:40:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/18/2022 06:40:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/18/2022 06:40:18 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.30980392156862746 on epoch=724
03/18/2022 06:40:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 06:40:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/18/2022 06:40:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 06:40:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=744
03/18/2022 06:40:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 06:40:35 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.30980392156862746 on epoch=749
03/18/2022 06:40:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/18/2022 06:40:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
03/18/2022 06:40:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/18/2022 06:40:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 06:40:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 06:40:52 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=774
03/18/2022 06:40:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 06:40:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 06:41:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 06:41:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/18/2022 06:41:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/18/2022 06:41:10 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=799
03/18/2022 06:41:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 06:41:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 06:41:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 06:41:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/18/2022 06:41:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/18/2022 06:41:27 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=824
03/18/2022 06:41:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 06:41:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 06:41:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/18/2022 06:41:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 06:41:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 06:41:45 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=849
03/18/2022 06:41:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 06:41:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 06:41:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 06:41:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 06:42:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 06:42:02 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=874
03/18/2022 06:42:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=879
03/18/2022 06:42:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 06:42:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 06:42:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/18/2022 06:42:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 06:42:20 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.37254901960784315 on epoch=899
03/18/2022 06:42:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 06:42:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 06:42:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 06:42:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 06:42:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 06:42:37 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.34310850439882695 on epoch=924
03/18/2022 06:42:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 06:42:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 06:42:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 06:42:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 06:42:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
03/18/2022 06:42:54 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.37254901960784315 on epoch=949
03/18/2022 06:42:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 06:43:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 06:43:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/18/2022 06:43:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 06:43:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
03/18/2022 06:43:12 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=974
03/18/2022 06:43:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 06:43:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 06:43:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 06:43:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/18/2022 06:43:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 06:43:29 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=999
03/18/2022 06:43:29 - INFO - __main__ - save last model!
03/18/2022 06:43:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 06:43:29 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 06:43:29 - INFO - __main__ - Printing 3 examples
03/18/2022 06:43:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 06:43:29 - INFO - __main__ - ['false']
03/18/2022 06:43:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 06:43:29 - INFO - __main__ - ['false']
03/18/2022 06:43:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 06:43:29 - INFO - __main__ - ['false']
03/18/2022 06:43:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:43:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:43:30 - INFO - __main__ - Printing 3 examples
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 06:43:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:43:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:43:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:43:30 - INFO - __main__ - Printing 3 examples
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:43:30 - INFO - __main__ - ['false']
03/18/2022 06:43:30 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:43:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:43:30 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:43:30 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:43:33 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 06:43:46 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:43:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:43:46 - INFO - __main__ - Starting training!
03/18/2022 06:44:23 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_13_0.3_8_predictions.txt
03/18/2022 06:44:23 - INFO - __main__ - Classification-F1 on test data: 0.3817
03/18/2022 06:44:23 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.3, bsz=8, dev_performance=0.4285714285714286, test_performance=0.38167677403130723
03/18/2022 06:44:24 - INFO - __main__ - Running ... prefix=wiki_qa_16_13, lr=0.2, bsz=8 ...
03/18/2022 06:44:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:44:24 - INFO - __main__ - Printing 3 examples
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:44:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:44:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:44:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:44:24 - INFO - __main__ - Printing 3 examples
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ -  [wiki_qa] question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.
03/18/2022 06:44:24 - INFO - __main__ - ['false']
03/18/2022 06:44:24 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:44:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:44:24 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:44:43 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:44:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:44:43 - INFO - __main__ - Starting training!
03/18/2022 06:44:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.71 on epoch=4
03/18/2022 06:44:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.35 on epoch=9
03/18/2022 06:44:54 - INFO - __main__ - Step 30 Global step 30 Train loss 4.16 on epoch=14
03/18/2022 06:44:58 - INFO - __main__ - Step 40 Global step 40 Train loss 2.49 on epoch=19
03/18/2022 06:45:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=24
03/18/2022 06:45:03 - INFO - __main__ - Global step 50 Train loss 4.44 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 06:45:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 06:45:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.03 on epoch=29
03/18/2022 06:45:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=34
03/18/2022 06:45:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.68 on epoch=39
03/18/2022 06:45:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=44
03/18/2022 06:45:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
03/18/2022 06:45:21 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.3191489361702127 on epoch=49
03/18/2022 06:45:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=54
03/18/2022 06:45:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=59
03/18/2022 06:45:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=64
03/18/2022 06:45:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=69
03/18/2022 06:45:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=74
03/18/2022 06:45:40 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 06:45:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=79
03/18/2022 06:45:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
03/18/2022 06:45:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=89
03/18/2022 06:45:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=94
03/18/2022 06:45:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=99
03/18/2022 06:45:58 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 06:46:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=104
03/18/2022 06:46:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=109
03/18/2022 06:46:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
03/18/2022 06:46:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
03/18/2022 06:46:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=124
03/18/2022 06:46:16 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 06:46:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=129
03/18/2022 06:46:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=134
03/18/2022 06:46:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=139
03/18/2022 06:46:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
03/18/2022 06:46:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=149
03/18/2022 06:46:33 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 06:46:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=154
03/18/2022 06:46:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=159
03/18/2022 06:46:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=164
03/18/2022 06:46:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=169
03/18/2022 06:46:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=174
03/18/2022 06:46:50 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 06:46:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
03/18/2022 06:46:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=184
03/18/2022 06:47:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=189
03/18/2022 06:47:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
03/18/2022 06:47:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=199
03/18/2022 06:47:08 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 06:47:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=204
03/18/2022 06:47:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
03/18/2022 06:47:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
03/18/2022 06:47:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=219
03/18/2022 06:47:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=224
03/18/2022 06:47:25 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 06:47:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=229
03/18/2022 06:47:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=234
03/18/2022 06:47:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=239
03/18/2022 06:47:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=244
03/18/2022 06:47:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=249
03/18/2022 06:47:42 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 06:47:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
03/18/2022 06:47:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=259
03/18/2022 06:47:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=264
03/18/2022 06:47:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=269
03/18/2022 06:47:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
03/18/2022 06:48:00 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 06:48:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=279
03/18/2022 06:48:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=284
03/18/2022 06:48:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=289
03/18/2022 06:48:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.80 on epoch=294
03/18/2022 06:48:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=299
03/18/2022 06:48:17 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 06:48:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=304
03/18/2022 06:48:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=309
03/18/2022 06:48:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=314
03/18/2022 06:48:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=319
03/18/2022 06:48:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=324
03/18/2022 06:48:35 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 06:48:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/18/2022 06:48:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=334
03/18/2022 06:48:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=339
03/18/2022 06:48:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=344
03/18/2022 06:48:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=349
03/18/2022 06:48:52 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 06:48:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=354
03/18/2022 06:48:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=359
03/18/2022 06:49:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/18/2022 06:49:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
03/18/2022 06:49:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=374
03/18/2022 06:49:10 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=374
03/18/2022 06:49:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=374, global_step=750
03/18/2022 06:49:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=379
03/18/2022 06:49:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=384
03/18/2022 06:49:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
03/18/2022 06:49:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
03/18/2022 06:49:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=399
03/18/2022 06:49:27 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=399
03/18/2022 06:49:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=404
03/18/2022 06:49:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=409
03/18/2022 06:49:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=414
03/18/2022 06:49:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=419
03/18/2022 06:49:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=424
03/18/2022 06:49:45 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=424
03/18/2022 06:49:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=429
03/18/2022 06:49:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
03/18/2022 06:49:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=439
03/18/2022 06:49:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
03/18/2022 06:50:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=449
03/18/2022 06:50:02 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=449
03/18/2022 06:50:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/18/2022 06:50:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/18/2022 06:50:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=464
03/18/2022 06:50:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=469
03/18/2022 06:50:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=474
03/18/2022 06:50:20 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=474
03/18/2022 06:50:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=479
03/18/2022 06:50:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=484
03/18/2022 06:50:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=489
03/18/2022 06:50:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=494
03/18/2022 06:50:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/18/2022 06:50:37 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=499
03/18/2022 06:50:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
03/18/2022 06:50:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=509
03/18/2022 06:50:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=514
03/18/2022 06:50:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=519
03/18/2022 06:50:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
03/18/2022 06:50:55 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=524
03/18/2022 06:50:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=529
03/18/2022 06:51:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
03/18/2022 06:51:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=539
03/18/2022 06:51:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/18/2022 06:51:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=549
03/18/2022 06:51:12 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.3266888150609081 on epoch=549
03/18/2022 06:51:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=554
03/18/2022 06:51:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
03/18/2022 06:51:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.31 on epoch=564
03/18/2022 06:51:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=569
03/18/2022 06:51:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=574
03/18/2022 06:51:30 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.4554554554554554 on epoch=574
03/18/2022 06:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4554554554554554 on epoch=574, global_step=1150
03/18/2022 06:51:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=579
03/18/2022 06:51:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=584
03/18/2022 06:51:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=589
03/18/2022 06:51:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=594
03/18/2022 06:51:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=599
03/18/2022 06:51:47 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.3073593073593074 on epoch=599
03/18/2022 06:51:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=604
03/18/2022 06:51:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/18/2022 06:51:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=614
03/18/2022 06:52:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=619
03/18/2022 06:52:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/18/2022 06:52:05 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.40566959921798634 on epoch=624
03/18/2022 06:52:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=629
03/18/2022 06:52:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=634
03/18/2022 06:52:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=639
03/18/2022 06:52:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
03/18/2022 06:52:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=649
03/18/2022 06:52:23 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.40566959921798634 on epoch=649
03/18/2022 06:52:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=654
03/18/2022 06:52:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=659
03/18/2022 06:52:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
03/18/2022 06:52:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=669
03/18/2022 06:52:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=674
03/18/2022 06:52:41 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.37254901960784315 on epoch=674
03/18/2022 06:52:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
03/18/2022 06:52:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
03/18/2022 06:52:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
03/18/2022 06:52:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=694
03/18/2022 06:52:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=699
03/18/2022 06:52:58 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.4009852216748768 on epoch=699
03/18/2022 06:53:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=704
03/18/2022 06:53:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
03/18/2022 06:53:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
03/18/2022 06:53:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=719
03/18/2022 06:53:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=724
03/18/2022 06:53:16 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.4920634920634921 on epoch=724
03/18/2022 06:53:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4920634920634921 on epoch=724, global_step=1450
03/18/2022 06:53:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
03/18/2022 06:53:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/18/2022 06:53:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=739
03/18/2022 06:53:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=744
03/18/2022 06:53:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/18/2022 06:53:34 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.4285714285714286 on epoch=749
03/18/2022 06:53:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=754
03/18/2022 06:53:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=759
03/18/2022 06:53:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
03/18/2022 06:53:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
03/18/2022 06:53:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
03/18/2022 06:53:51 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.3764102564102564 on epoch=774
03/18/2022 06:53:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=779
03/18/2022 06:53:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
03/18/2022 06:54:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
03/18/2022 06:54:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/18/2022 06:54:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
03/18/2022 06:54:09 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.464039408866995 on epoch=799
03/18/2022 06:54:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
03/18/2022 06:54:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/18/2022 06:54:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/18/2022 06:54:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
03/18/2022 06:54:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/18/2022 06:54:27 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=824
03/18/2022 06:54:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/18/2022 06:54:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
03/18/2022 06:54:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/18/2022 06:54:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/18/2022 06:54:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
03/18/2022 06:54:45 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.4375 on epoch=849
03/18/2022 06:54:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
03/18/2022 06:54:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/18/2022 06:54:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/18/2022 06:54:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/18/2022 06:55:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
03/18/2022 06:55:03 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.40566959921798634 on epoch=874
03/18/2022 06:55:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/18/2022 06:55:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/18/2022 06:55:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
03/18/2022 06:55:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=894
03/18/2022 06:55:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/18/2022 06:55:20 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=899
03/18/2022 06:55:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 06:55:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/18/2022 06:55:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 06:55:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/18/2022 06:55:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/18/2022 06:55:38 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=924
03/18/2022 06:55:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/18/2022 06:55:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/18/2022 06:55:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/18/2022 06:55:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
03/18/2022 06:55:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/18/2022 06:55:56 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.4375 on epoch=949
03/18/2022 06:55:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/18/2022 06:56:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 06:56:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
03/18/2022 06:56:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/18/2022 06:56:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 06:56:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=974
03/18/2022 06:56:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=979
03/18/2022 06:56:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
03/18/2022 06:56:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
03/18/2022 06:56:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/18/2022 06:56:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
03/18/2022 06:56:31 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.4682306940371457 on epoch=999
03/18/2022 06:56:31 - INFO - __main__ - save last model!
03/18/2022 06:56:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 06:56:31 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 06:56:31 - INFO - __main__ - Printing 3 examples
03/18/2022 06:56:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 06:56:31 - INFO - __main__ - ['false']
03/18/2022 06:56:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 06:56:31 - INFO - __main__ - ['false']
03/18/2022 06:56:31 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 06:56:31 - INFO - __main__ - ['false']
03/18/2022 06:56:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:56:32 - INFO - __main__ - Printing 3 examples
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 06:56:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:56:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:56:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:56:32 - INFO - __main__ - Printing 3 examples
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 06:56:32 - INFO - __main__ - ['false']
03/18/2022 06:56:32 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:56:32 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:56:32 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:56:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:56:35 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 06:56:47 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:56:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:56:48 - INFO - __main__ - Starting training!
03/18/2022 06:57:50 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_13_0.2_8_predictions.txt
03/18/2022 06:57:50 - INFO - __main__ - Classification-F1 on test data: 0.3915
03/18/2022 06:57:51 - INFO - __main__ - prefix=wiki_qa_16_13, lr=0.2, bsz=8, dev_performance=0.4920634920634921, test_performance=0.3914743163763604
03/18/2022 06:57:51 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.5, bsz=8 ...
03/18/2022 06:57:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:57:52 - INFO - __main__ - Printing 3 examples
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 06:57:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:57:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 06:57:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 06:57:52 - INFO - __main__ - Printing 3 examples
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 06:57:52 - INFO - __main__ - ['false']
03/18/2022 06:57:52 - INFO - __main__ - Tokenizing Input ...
03/18/2022 06:57:52 - INFO - __main__ - Tokenizing Output ...
03/18/2022 06:57:52 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 06:58:08 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 06:58:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 06:58:09 - INFO - __main__ - Starting training!
03/18/2022 06:58:12 - INFO - __main__ - Step 10 Global step 10 Train loss 5.11 on epoch=4
03/18/2022 06:58:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
03/18/2022 06:58:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=14
03/18/2022 06:58:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=19
03/18/2022 06:58:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=24
03/18/2022 06:58:23 - INFO - __main__ - Global step 50 Train loss 1.54 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 06:58:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 06:58:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
03/18/2022 06:58:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
03/18/2022 06:58:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
03/18/2022 06:58:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=44
03/18/2022 06:58:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
03/18/2022 06:58:36 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 06:58:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=54
03/18/2022 06:58:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=59
03/18/2022 06:58:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=64
03/18/2022 06:58:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
03/18/2022 06:58:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
03/18/2022 06:58:49 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=74
03/18/2022 06:58:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=74, global_step=150
03/18/2022 06:58:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/18/2022 06:58:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
03/18/2022 06:58:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
03/18/2022 06:58:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
03/18/2022 06:59:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
03/18/2022 06:59:02 - INFO - __main__ - Global step 200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 06:59:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/18/2022 06:59:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
03/18/2022 06:59:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
03/18/2022 06:59:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
03/18/2022 06:59:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
03/18/2022 06:59:15 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 06:59:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
03/18/2022 06:59:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
03/18/2022 06:59:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
03/18/2022 06:59:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
03/18/2022 06:59:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/18/2022 06:59:27 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 06:59:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=154
03/18/2022 06:59:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=159
03/18/2022 06:59:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
03/18/2022 06:59:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=169
03/18/2022 06:59:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=174
03/18/2022 06:59:40 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.625 on epoch=174
03/18/2022 06:59:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.625 on epoch=174, global_step=350
03/18/2022 06:59:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
03/18/2022 06:59:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/18/2022 06:59:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=189
03/18/2022 06:59:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
03/18/2022 06:59:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
03/18/2022 06:59:53 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6945917285259808 on epoch=199
03/18/2022 06:59:53 - INFO - __main__ - Saving model with best Classification-F1: 0.625 -> 0.6945917285259808 on epoch=199, global_step=400
03/18/2022 06:59:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=204
03/18/2022 06:59:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=209
03/18/2022 07:00:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=214
03/18/2022 07:00:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=219
03/18/2022 07:00:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=224
03/18/2022 07:00:06 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.3191489361702127 on epoch=224
03/18/2022 07:00:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
03/18/2022 07:00:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
03/18/2022 07:00:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=239
03/18/2022 07:00:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=244
03/18/2022 07:00:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=249
03/18/2022 07:00:18 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.7702564102564102 on epoch=249
03/18/2022 07:00:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6945917285259808 -> 0.7702564102564102 on epoch=249, global_step=500
03/18/2022 07:00:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=254
03/18/2022 07:00:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=259
03/18/2022 07:00:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
03/18/2022 07:00:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=269
03/18/2022 07:00:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=274
03/18/2022 07:00:31 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.5607843137254902 on epoch=274
03/18/2022 07:00:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
03/18/2022 07:00:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/18/2022 07:00:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/18/2022 07:00:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
03/18/2022 07:00:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/18/2022 07:00:44 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.6113360323886641 on epoch=299
03/18/2022 07:00:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=304
03/18/2022 07:00:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
03/18/2022 07:00:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=314
03/18/2022 07:00:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
03/18/2022 07:00:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
03/18/2022 07:00:58 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.5588547189819725 on epoch=324
03/18/2022 07:01:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=329
03/18/2022 07:01:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
03/18/2022 07:01:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
03/18/2022 07:01:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/18/2022 07:01:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/18/2022 07:01:10 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.5151515151515151 on epoch=349
03/18/2022 07:01:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=354
03/18/2022 07:01:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
03/18/2022 07:01:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/18/2022 07:01:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/18/2022 07:01:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
03/18/2022 07:01:25 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.5151515151515151 on epoch=374
03/18/2022 07:01:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/18/2022 07:01:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
03/18/2022 07:01:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/18/2022 07:01:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/18/2022 07:01:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/18/2022 07:01:37 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.5151515151515151 on epoch=399
03/18/2022 07:01:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
03/18/2022 07:01:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/18/2022 07:01:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/18/2022 07:01:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/18/2022 07:01:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/18/2022 07:01:50 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.5835835835835835 on epoch=424
03/18/2022 07:01:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/18/2022 07:01:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/18/2022 07:01:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/18/2022 07:02:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
03/18/2022 07:02:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/18/2022 07:02:03 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5733333333333335 on epoch=449
03/18/2022 07:02:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/18/2022 07:02:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/18/2022 07:02:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/18/2022 07:02:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/18/2022 07:02:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/18/2022 07:02:16 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.5333333333333333 on epoch=474
03/18/2022 07:02:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/18/2022 07:02:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/18/2022 07:02:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/18/2022 07:02:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/18/2022 07:02:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/18/2022 07:02:29 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.4181818181818182 on epoch=499
03/18/2022 07:02:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/18/2022 07:02:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/18/2022 07:02:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/18/2022 07:02:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/18/2022 07:02:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
03/18/2022 07:02:41 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.6389743589743591 on epoch=524
03/18/2022 07:02:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/18/2022 07:02:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
03/18/2022 07:02:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/18/2022 07:02:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/18/2022 07:02:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/18/2022 07:02:54 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.5588547189819725 on epoch=549
03/18/2022 07:02:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/18/2022 07:02:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/18/2022 07:03:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/18/2022 07:03:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/18/2022 07:03:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/18/2022 07:03:07 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5733333333333335 on epoch=574
03/18/2022 07:03:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/18/2022 07:03:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/18/2022 07:03:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/18/2022 07:03:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/18/2022 07:03:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/18/2022 07:03:19 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=599
03/18/2022 07:03:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/18/2022 07:03:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/18/2022 07:03:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/18/2022 07:03:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/18/2022 07:03:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/18/2022 07:03:32 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=624
03/18/2022 07:03:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/18/2022 07:03:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
03/18/2022 07:03:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/18/2022 07:03:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/18/2022 07:03:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/18/2022 07:03:45 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=649
03/18/2022 07:03:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/18/2022 07:03:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/18/2022 07:03:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/18/2022 07:03:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/18/2022 07:03:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/18/2022 07:03:58 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=674
03/18/2022 07:04:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/18/2022 07:04:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/18/2022 07:04:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/18/2022 07:04:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/18/2022 07:04:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/18/2022 07:04:10 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4458874458874459 on epoch=699
03/18/2022 07:04:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/18/2022 07:04:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/18/2022 07:04:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 07:04:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/18/2022 07:04:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/18/2022 07:04:23 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=724
03/18/2022 07:04:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 07:04:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/18/2022 07:04:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 07:04:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/18/2022 07:04:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
03/18/2022 07:04:36 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5333333333333333 on epoch=749
03/18/2022 07:04:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/18/2022 07:04:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 07:04:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/18/2022 07:04:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/18/2022 07:04:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 07:04:49 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=774
03/18/2022 07:04:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 07:04:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 07:04:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 07:04:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/18/2022 07:05:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/18/2022 07:05:02 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=799
03/18/2022 07:05:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 07:05:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 07:05:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 07:05:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/18/2022 07:05:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/18/2022 07:05:15 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=824
03/18/2022 07:05:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 07:05:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 07:05:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
03/18/2022 07:05:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 07:05:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 07:05:27 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=849
03/18/2022 07:05:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
03/18/2022 07:05:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/18/2022 07:05:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 07:05:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 07:05:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 07:05:40 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=874
03/18/2022 07:05:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 07:05:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 07:05:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 07:05:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 07:05:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 07:05:53 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=899
03/18/2022 07:05:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 07:05:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 07:06:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 07:06:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 07:06:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 07:06:06 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=924
03/18/2022 07:06:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 07:06:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 07:06:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 07:06:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 07:06:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 07:06:19 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=949
03/18/2022 07:06:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 07:06:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 07:06:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 07:06:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 07:06:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 07:06:31 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=974
03/18/2022 07:06:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 07:06:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 07:06:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 07:06:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/18/2022 07:06:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 07:06:44 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=999
03/18/2022 07:06:44 - INFO - __main__ - save last model!
03/18/2022 07:06:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:06:44 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:06:44 - INFO - __main__ - Printing 3 examples
03/18/2022 07:06:44 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:06:44 - INFO - __main__ - ['false']
03/18/2022 07:06:44 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:06:44 - INFO - __main__ - ['false']
03/18/2022 07:06:44 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:06:44 - INFO - __main__ - ['false']
03/18/2022 07:06:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:06:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:06:45 - INFO - __main__ - Printing 3 examples
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:06:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:06:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:06:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:06:45 - INFO - __main__ - Printing 3 examples
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:06:45 - INFO - __main__ - ['false']
03/18/2022 07:06:45 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:06:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:06:45 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:06:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:06:48 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:07:00 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:07:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:07:01 - INFO - __main__ - Starting training!
03/18/2022 07:07:43 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_21_0.5_8_predictions.txt
03/18/2022 07:07:43 - INFO - __main__ - Classification-F1 on test data: 0.2011
03/18/2022 07:07:44 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.5, bsz=8, dev_performance=0.7702564102564102, test_performance=0.20111998996303695
03/18/2022 07:07:44 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.4, bsz=8 ...
03/18/2022 07:07:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:07:45 - INFO - __main__ - Printing 3 examples
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:07:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:07:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:07:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:07:45 - INFO - __main__ - Printing 3 examples
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:07:45 - INFO - __main__ - ['false']
03/18/2022 07:07:45 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:07:45 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:07:45 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:08:03 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:08:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:08:04 - INFO - __main__ - Starting training!
03/18/2022 07:08:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.14 on epoch=4
03/18/2022 07:08:09 - INFO - __main__ - Step 20 Global step 20 Train loss 4.22 on epoch=9
03/18/2022 07:08:11 - INFO - __main__ - Step 30 Global step 30 Train loss 2.15 on epoch=14
03/18/2022 07:08:14 - INFO - __main__ - Step 40 Global step 40 Train loss 1.39 on epoch=19
03/18/2022 07:08:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=24
03/18/2022 07:08:18 - INFO - __main__ - Global step 50 Train loss 3.01 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 07:08:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 07:08:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=29
03/18/2022 07:08:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.67 on epoch=34
03/18/2022 07:08:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=39
03/18/2022 07:08:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
03/18/2022 07:08:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=49
03/18/2022 07:08:32 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.3992490613266583 on epoch=49
03/18/2022 07:08:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=49, global_step=100
03/18/2022 07:08:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
03/18/2022 07:08:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
03/18/2022 07:08:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=64
03/18/2022 07:08:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
03/18/2022 07:08:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
03/18/2022 07:08:44 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 07:08:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/18/2022 07:08:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
03/18/2022 07:08:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
03/18/2022 07:08:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=94
03/18/2022 07:08:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
03/18/2022 07:08:57 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 07:09:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
03/18/2022 07:09:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
03/18/2022 07:09:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=114
03/18/2022 07:09:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
03/18/2022 07:09:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=124
03/18/2022 07:09:10 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.4231177094379639 on epoch=124
03/18/2022 07:09:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4231177094379639 on epoch=124, global_step=250
03/18/2022 07:09:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
03/18/2022 07:09:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
03/18/2022 07:09:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/18/2022 07:09:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=144
03/18/2022 07:09:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=149
03/18/2022 07:09:23 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=149
03/18/2022 07:09:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
03/18/2022 07:09:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=159
03/18/2022 07:09:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=164
03/18/2022 07:09:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
03/18/2022 07:09:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/18/2022 07:09:35 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=174
03/18/2022 07:09:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
03/18/2022 07:09:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
03/18/2022 07:09:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=189
03/18/2022 07:09:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=194
03/18/2022 07:09:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
03/18/2022 07:09:48 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.6267232237539766 on epoch=199
03/18/2022 07:09:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.6267232237539766 on epoch=199, global_step=400
03/18/2022 07:09:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=204
03/18/2022 07:09:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
03/18/2022 07:09:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
03/18/2022 07:09:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/18/2022 07:10:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
03/18/2022 07:10:01 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.5195195195195195 on epoch=224
03/18/2022 07:10:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=229
03/18/2022 07:10:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=234
03/18/2022 07:10:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=239
03/18/2022 07:10:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/18/2022 07:10:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=249
03/18/2022 07:10:14 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.5607843137254902 on epoch=249
03/18/2022 07:10:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/18/2022 07:10:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
03/18/2022 07:10:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=264
03/18/2022 07:10:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=269
03/18/2022 07:10:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=274
03/18/2022 07:10:27 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.6267232237539766 on epoch=274
03/18/2022 07:10:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
03/18/2022 07:10:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=284
03/18/2022 07:10:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=289
03/18/2022 07:10:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/18/2022 07:10:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=299
03/18/2022 07:10:39 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.5933528836754642 on epoch=299
03/18/2022 07:10:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=304
03/18/2022 07:10:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=309
03/18/2022 07:10:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=314
03/18/2022 07:10:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=319
03/18/2022 07:10:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=324
03/18/2022 07:10:52 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.5901477832512315 on epoch=324
03/18/2022 07:10:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=329
03/18/2022 07:10:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=334
03/18/2022 07:11:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=339
03/18/2022 07:11:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=344
03/18/2022 07:11:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=349
03/18/2022 07:11:05 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6476476476476476 on epoch=349
03/18/2022 07:11:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6267232237539766 -> 0.6476476476476476 on epoch=349, global_step=700
03/18/2022 07:11:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=354
03/18/2022 07:11:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/18/2022 07:11:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
03/18/2022 07:11:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=369
03/18/2022 07:11:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=374
03/18/2022 07:11:18 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6875 on epoch=374
03/18/2022 07:11:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6476476476476476 -> 0.6875 on epoch=374, global_step=750
03/18/2022 07:11:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=379
03/18/2022 07:11:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=384
03/18/2022 07:11:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/18/2022 07:11:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=394
03/18/2022 07:11:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=399
03/18/2022 07:11:31 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.5151515151515151 on epoch=399
03/18/2022 07:11:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=404
03/18/2022 07:11:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=409
03/18/2022 07:11:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=414
03/18/2022 07:11:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=419
03/18/2022 07:11:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=424
03/18/2022 07:11:43 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.5901477832512315 on epoch=424
03/18/2022 07:11:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=429
03/18/2022 07:11:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/18/2022 07:11:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=439
03/18/2022 07:11:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/18/2022 07:11:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
03/18/2022 07:11:56 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.5555555555555556 on epoch=449
03/18/2022 07:11:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
03/18/2022 07:12:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=459
03/18/2022 07:12:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/18/2022 07:12:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=469
03/18/2022 07:12:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=474
03/18/2022 07:12:09 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.6862745098039216 on epoch=474
03/18/2022 07:12:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=479
03/18/2022 07:12:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=484
03/18/2022 07:12:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
03/18/2022 07:12:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=494
03/18/2022 07:12:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=499
03/18/2022 07:12:23 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.5333333333333333 on epoch=499
03/18/2022 07:12:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
03/18/2022 07:12:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
03/18/2022 07:12:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=514
03/18/2022 07:12:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
03/18/2022 07:12:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
03/18/2022 07:12:36 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.5901477832512315 on epoch=524
03/18/2022 07:12:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=529
03/18/2022 07:12:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
03/18/2022 07:12:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
03/18/2022 07:12:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
03/18/2022 07:12:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
03/18/2022 07:12:49 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.5933528836754642 on epoch=549
03/18/2022 07:12:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=554
03/18/2022 07:12:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
03/18/2022 07:12:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
03/18/2022 07:12:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
03/18/2022 07:13:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
03/18/2022 07:13:02 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5933528836754642 on epoch=574
03/18/2022 07:13:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
03/18/2022 07:13:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
03/18/2022 07:13:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
03/18/2022 07:13:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
03/18/2022 07:13:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/18/2022 07:13:15 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.5465587044534412 on epoch=599
03/18/2022 07:13:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/18/2022 07:13:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
03/18/2022 07:13:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
03/18/2022 07:13:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/18/2022 07:13:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/18/2022 07:13:29 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.5151515151515151 on epoch=624
03/18/2022 07:13:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/18/2022 07:13:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/18/2022 07:13:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
03/18/2022 07:13:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
03/18/2022 07:13:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/18/2022 07:13:44 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.5588547189819725 on epoch=649
03/18/2022 07:13:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/18/2022 07:13:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/18/2022 07:13:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/18/2022 07:13:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/18/2022 07:13:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/18/2022 07:13:57 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5933528836754642 on epoch=674
03/18/2022 07:14:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/18/2022 07:14:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
03/18/2022 07:14:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/18/2022 07:14:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/18/2022 07:14:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
03/18/2022 07:14:10 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6235294117647059 on epoch=699
03/18/2022 07:14:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/18/2022 07:14:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
03/18/2022 07:14:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/18/2022 07:14:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/18/2022 07:14:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/18/2022 07:14:24 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6113360323886641 on epoch=724
03/18/2022 07:14:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
03/18/2022 07:14:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/18/2022 07:14:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 07:14:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/18/2022 07:14:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 07:14:37 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=749
03/18/2022 07:14:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 07:14:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 07:14:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
03/18/2022 07:14:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/18/2022 07:14:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 07:14:54 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=774
03/18/2022 07:14:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/18/2022 07:14:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/18/2022 07:15:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/18/2022 07:15:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/18/2022 07:15:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/18/2022 07:15:08 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=799
03/18/2022 07:15:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/18/2022 07:15:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
03/18/2022 07:15:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 07:15:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/18/2022 07:15:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
03/18/2022 07:15:21 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=824
03/18/2022 07:15:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/18/2022 07:15:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 07:15:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 07:15:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 07:15:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 07:15:34 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=849
03/18/2022 07:15:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 07:15:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 07:15:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 07:15:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 07:15:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 07:15:47 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=874
03/18/2022 07:15:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=879
03/18/2022 07:15:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/18/2022 07:15:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 07:15:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 07:15:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/18/2022 07:16:00 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=899
03/18/2022 07:16:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/18/2022 07:16:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 07:16:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 07:16:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 07:16:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 07:16:14 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5835835835835835 on epoch=924
03/18/2022 07:16:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/18/2022 07:16:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 07:16:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 07:16:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 07:16:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 07:16:28 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=949
03/18/2022 07:16:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/18/2022 07:16:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 07:16:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 07:16:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/18/2022 07:16:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 07:16:41 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=974
03/18/2022 07:16:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 07:16:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 07:16:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 07:16:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/18/2022 07:16:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 07:16:54 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=999
03/18/2022 07:16:54 - INFO - __main__ - save last model!
03/18/2022 07:16:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:16:54 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:16:54 - INFO - __main__ - Printing 3 examples
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:16:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:16:54 - INFO - __main__ - Printing 3 examples
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:16:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:16:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:16:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:16:54 - INFO - __main__ - Printing 3 examples
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:16:54 - INFO - __main__ - ['false']
03/18/2022 07:16:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:16:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:16:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:16:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:16:58 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:17:13 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:17:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:17:14 - INFO - __main__ - Starting training!
03/18/2022 07:18:20 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_21_0.4_8_predictions.txt
03/18/2022 07:18:20 - INFO - __main__ - Classification-F1 on test data: 0.3557
03/18/2022 07:18:21 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.4, bsz=8, dev_performance=0.6875, test_performance=0.35565929482535114
03/18/2022 07:18:21 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.3, bsz=8 ...
03/18/2022 07:18:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:18:22 - INFO - __main__ - Printing 3 examples
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:18:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:18:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:18:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:18:22 - INFO - __main__ - Printing 3 examples
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:18:22 - INFO - __main__ - ['false']
03/18/2022 07:18:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:18:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:18:22 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:18:38 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:18:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:18:39 - INFO - __main__ - Starting training!
03/18/2022 07:18:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.50 on epoch=4
03/18/2022 07:18:45 - INFO - __main__ - Step 20 Global step 20 Train loss 2.86 on epoch=9
03/18/2022 07:18:47 - INFO - __main__ - Step 30 Global step 30 Train loss 2.16 on epoch=14
03/18/2022 07:18:50 - INFO - __main__ - Step 40 Global step 40 Train loss 3.57 on epoch=19
03/18/2022 07:18:52 - INFO - __main__ - Step 50 Global step 50 Train loss 3.18 on epoch=24
03/18/2022 07:18:53 - INFO - __main__ - Global step 50 Train loss 3.65 Classification-F1 0.4385964912280702 on epoch=24
03/18/2022 07:18:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4385964912280702 on epoch=24, global_step=50
03/18/2022 07:18:55 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=29
03/18/2022 07:18:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=34
03/18/2022 07:19:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=39
03/18/2022 07:19:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=44
03/18/2022 07:19:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.37 on epoch=49
03/18/2022 07:19:06 - INFO - __main__ - Global step 100 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 07:19:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=54
03/18/2022 07:19:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.28 on epoch=59
03/18/2022 07:19:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=64
03/18/2022 07:19:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=69
03/18/2022 07:19:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=74
03/18/2022 07:19:21 - INFO - __main__ - Global step 150 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 07:19:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=79
03/18/2022 07:19:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=84
03/18/2022 07:19:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.67 on epoch=89
03/18/2022 07:19:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.68 on epoch=94
03/18/2022 07:19:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=99
03/18/2022 07:19:35 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 07:19:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=104
03/18/2022 07:19:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=109
03/18/2022 07:19:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=114
03/18/2022 07:19:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=119
03/18/2022 07:19:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=124
03/18/2022 07:19:48 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.3816425120772947 on epoch=124
03/18/2022 07:19:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=129
03/18/2022 07:19:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=134
03/18/2022 07:19:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=139
03/18/2022 07:19:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=144
03/18/2022 07:20:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=149
03/18/2022 07:20:01 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=149
03/18/2022 07:20:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=154
03/18/2022 07:20:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=159
03/18/2022 07:20:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=164
03/18/2022 07:20:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=169
03/18/2022 07:20:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=174
03/18/2022 07:20:14 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=174
03/18/2022 07:20:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=179
03/18/2022 07:20:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=184
03/18/2022 07:20:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=189
03/18/2022 07:20:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=194
03/18/2022 07:20:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=199
03/18/2022 07:20:27 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.4909862142099682 on epoch=199
03/18/2022 07:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4909862142099682 on epoch=199, global_step=400
03/18/2022 07:20:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=204
03/18/2022 07:20:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=209
03/18/2022 07:20:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=214
03/18/2022 07:20:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=219
03/18/2022 07:20:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=224
03/18/2022 07:20:40 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 07:20:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=229
03/18/2022 07:20:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=234
03/18/2022 07:20:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=239
03/18/2022 07:20:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=244
03/18/2022 07:20:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=249
03/18/2022 07:20:52 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.49090909090909085 on epoch=249
03/18/2022 07:20:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=254
03/18/2022 07:20:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=259
03/18/2022 07:21:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=264
03/18/2022 07:21:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=269
03/18/2022 07:21:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=274
03/18/2022 07:21:05 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 07:21:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=279
03/18/2022 07:21:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=284
03/18/2022 07:21:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=289
03/18/2022 07:21:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=294
03/18/2022 07:21:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=299
03/18/2022 07:21:18 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.5333333333333333 on epoch=299
03/18/2022 07:21:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4909862142099682 -> 0.5333333333333333 on epoch=299, global_step=600
03/18/2022 07:21:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=304
03/18/2022 07:21:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=309
03/18/2022 07:21:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=314
03/18/2022 07:21:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=319
03/18/2022 07:21:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=324
03/18/2022 07:21:31 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.6267232237539766 on epoch=324
03/18/2022 07:21:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.6267232237539766 on epoch=324, global_step=650
03/18/2022 07:21:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=329
03/18/2022 07:21:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=334
03/18/2022 07:21:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=339
03/18/2022 07:21:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=344
03/18/2022 07:21:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/18/2022 07:21:44 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.6267232237539766 on epoch=349
03/18/2022 07:21:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=354
03/18/2022 07:21:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=359
03/18/2022 07:21:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=364
03/18/2022 07:21:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=369
03/18/2022 07:21:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=374
03/18/2022 07:21:57 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.6267232237539766 on epoch=374
03/18/2022 07:21:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=379
03/18/2022 07:22:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=384
03/18/2022 07:22:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/18/2022 07:22:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=394
03/18/2022 07:22:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/18/2022 07:22:10 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.4909862142099682 on epoch=399
03/18/2022 07:22:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=404
03/18/2022 07:22:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
03/18/2022 07:22:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=414
03/18/2022 07:22:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=419
03/18/2022 07:22:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/18/2022 07:22:23 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.4181818181818182 on epoch=424
03/18/2022 07:22:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=429
03/18/2022 07:22:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=434
03/18/2022 07:22:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/18/2022 07:22:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=444
03/18/2022 07:22:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
03/18/2022 07:22:36 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.39756367663344405 on epoch=449
03/18/2022 07:22:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=454
03/18/2022 07:22:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=459
03/18/2022 07:22:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
03/18/2022 07:22:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=469
03/18/2022 07:22:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=474
03/18/2022 07:22:49 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.6476476476476476 on epoch=474
03/18/2022 07:22:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6267232237539766 -> 0.6476476476476476 on epoch=474, global_step=950
03/18/2022 07:22:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
03/18/2022 07:22:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=484
03/18/2022 07:22:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/18/2022 07:22:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/18/2022 07:23:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=499
03/18/2022 07:23:02 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.6532019704433498 on epoch=499
03/18/2022 07:23:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6476476476476476 -> 0.6532019704433498 on epoch=499, global_step=1000
03/18/2022 07:23:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=504
03/18/2022 07:23:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=509
03/18/2022 07:23:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
03/18/2022 07:23:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=519
03/18/2022 07:23:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=524
03/18/2022 07:23:14 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5333333333333333 on epoch=524
03/18/2022 07:23:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
03/18/2022 07:23:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=534
03/18/2022 07:23:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=539
03/18/2022 07:23:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=544
03/18/2022 07:23:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
03/18/2022 07:23:27 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.7810361681329424 on epoch=549
03/18/2022 07:23:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6532019704433498 -> 0.7810361681329424 on epoch=549, global_step=1100
03/18/2022 07:23:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=554
03/18/2022 07:23:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
03/18/2022 07:23:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=564
03/18/2022 07:23:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=569
03/18/2022 07:23:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
03/18/2022 07:23:40 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.4458874458874459 on epoch=574
03/18/2022 07:23:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=579
03/18/2022 07:23:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=584
03/18/2022 07:23:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
03/18/2022 07:23:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=594
03/18/2022 07:23:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=599
03/18/2022 07:23:53 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.75 on epoch=599
03/18/2022 07:23:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=604
03/18/2022 07:23:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
03/18/2022 07:24:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=614
03/18/2022 07:24:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=619
03/18/2022 07:24:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=624
03/18/2022 07:24:06 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.7184750733137829 on epoch=624
03/18/2022 07:24:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=629
03/18/2022 07:24:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=634
03/18/2022 07:24:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
03/18/2022 07:24:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=644
03/18/2022 07:24:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=649
03/18/2022 07:24:19 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.5134502923976608 on epoch=649
03/18/2022 07:24:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
03/18/2022 07:24:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=659
03/18/2022 07:24:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
03/18/2022 07:24:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
03/18/2022 07:24:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=674
03/18/2022 07:24:32 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.6113360323886641 on epoch=674
03/18/2022 07:24:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
03/18/2022 07:24:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
03/18/2022 07:24:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=689
03/18/2022 07:24:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=694
03/18/2022 07:24:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/18/2022 07:24:45 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.5195195195195195 on epoch=699
03/18/2022 07:24:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
03/18/2022 07:24:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
03/18/2022 07:24:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/18/2022 07:24:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=719
03/18/2022 07:24:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=724
03/18/2022 07:24:58 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.5195195195195195 on epoch=724
03/18/2022 07:25:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/18/2022 07:25:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
03/18/2022 07:25:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=739
03/18/2022 07:25:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=744
03/18/2022 07:25:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=749
03/18/2022 07:25:11 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.36374269005847953 on epoch=749
03/18/2022 07:25:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/18/2022 07:25:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=759
03/18/2022 07:25:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/18/2022 07:25:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=769
03/18/2022 07:25:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
03/18/2022 07:25:24 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.5307917888563051 on epoch=774
03/18/2022 07:25:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/18/2022 07:25:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=784
03/18/2022 07:25:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=789
03/18/2022 07:25:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
03/18/2022 07:25:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=799
03/18/2022 07:25:37 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.4817813765182186 on epoch=799
03/18/2022 07:25:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=804
03/18/2022 07:25:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/18/2022 07:25:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=814
03/18/2022 07:25:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=819
03/18/2022 07:25:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/18/2022 07:25:50 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.4385964912280702 on epoch=824
03/18/2022 07:25:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
03/18/2022 07:25:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/18/2022 07:25:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
03/18/2022 07:26:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/18/2022 07:26:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/18/2022 07:26:03 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.4385964912280702 on epoch=849
03/18/2022 07:26:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
03/18/2022 07:26:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
03/18/2022 07:26:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
03/18/2022 07:26:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=869
03/18/2022 07:26:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=874
03/18/2022 07:26:16 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.49090909090909085 on epoch=874
03/18/2022 07:26:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/18/2022 07:26:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/18/2022 07:26:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
03/18/2022 07:26:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
03/18/2022 07:26:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/18/2022 07:26:29 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=899
03/18/2022 07:26:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/18/2022 07:26:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/18/2022 07:26:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/18/2022 07:26:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=919
03/18/2022 07:26:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/18/2022 07:26:42 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.3816425120772947 on epoch=924
03/18/2022 07:26:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
03/18/2022 07:26:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=934
03/18/2022 07:26:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/18/2022 07:26:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
03/18/2022 07:26:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
03/18/2022 07:26:56 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.39756367663344405 on epoch=949
03/18/2022 07:26:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
03/18/2022 07:27:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/18/2022 07:27:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 07:27:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/18/2022 07:27:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
03/18/2022 07:27:09 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=974
03/18/2022 07:27:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/18/2022 07:27:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
03/18/2022 07:27:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/18/2022 07:27:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
03/18/2022 07:27:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/18/2022 07:27:22 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.4009852216748768 on epoch=999
03/18/2022 07:27:22 - INFO - __main__ - save last model!
03/18/2022 07:27:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:27:22 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:27:22 - INFO - __main__ - Printing 3 examples
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:27:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:27:22 - INFO - __main__ - Printing 3 examples
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:27:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:27:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:27:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:27:22 - INFO - __main__ - Printing 3 examples
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:27:22 - INFO - __main__ - ['false']
03/18/2022 07:27:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:27:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:27:22 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:27:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:27:26 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:27:41 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:27:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:27:42 - INFO - __main__ - Starting training!
03/18/2022 07:28:31 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_21_0.3_8_predictions.txt
03/18/2022 07:28:31 - INFO - __main__ - Classification-F1 on test data: 0.4023
03/18/2022 07:28:32 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.3, bsz=8, dev_performance=0.7810361681329424, test_performance=0.4022903950533829
03/18/2022 07:28:32 - INFO - __main__ - Running ... prefix=wiki_qa_16_21, lr=0.2, bsz=8 ...
03/18/2022 07:28:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:28:33 - INFO - __main__ - Printing 3 examples
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:28:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:28:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:28:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:28:33 - INFO - __main__ - Printing 3 examples
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: who shot abraham lincoln [SEP] answer: By simultaneously eliminating the top three people in the administration, Booth and his co-conspirators hoped to sever the continuity of the United States government.
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: what does s in ulysses s grant stand for [SEP] answer: In terms of foreign policy , Grant revealed an "unexpected capacity for deliberation and consultation" that promoted the national interest.
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ -  [wiki_qa] question: how many black people live in green bay [SEP] answer: It is home to the National Railroad Museum ; the Neville Public Museum, with exhibitions of art, history, and science; and the University of Wisconsin–Green Bay .
03/18/2022 07:28:33 - INFO - __main__ - ['false']
03/18/2022 07:28:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:28:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:28:33 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:28:51 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:28:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:28:51 - INFO - __main__ - Starting training!
03/18/2022 07:28:54 - INFO - __main__ - Step 10 Global step 10 Train loss 7.45 on epoch=4
03/18/2022 07:28:57 - INFO - __main__ - Step 20 Global step 20 Train loss 6.86 on epoch=9
03/18/2022 07:28:59 - INFO - __main__ - Step 30 Global step 30 Train loss 7.54 on epoch=14
03/18/2022 07:29:02 - INFO - __main__ - Step 40 Global step 40 Train loss 6.96 on epoch=19
03/18/2022 07:29:04 - INFO - __main__ - Step 50 Global step 50 Train loss 6.36 on epoch=24
03/18/2022 07:29:17 - INFO - __main__ - Global step 50 Train loss 7.03 Classification-F1 0.0 on epoch=24
03/18/2022 07:29:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/18/2022 07:29:19 - INFO - __main__ - Step 60 Global step 60 Train loss 7.13 on epoch=29
03/18/2022 07:29:22 - INFO - __main__ - Step 70 Global step 70 Train loss 6.95 on epoch=34
03/18/2022 07:29:24 - INFO - __main__ - Step 80 Global step 80 Train loss 6.22 on epoch=39
03/18/2022 07:29:27 - INFO - __main__ - Step 90 Global step 90 Train loss 5.95 on epoch=44
03/18/2022 07:29:29 - INFO - __main__ - Step 100 Global step 100 Train loss 5.84 on epoch=49
03/18/2022 07:29:48 - INFO - __main__ - Global step 100 Train loss 6.42 Classification-F1 0.0 on epoch=49
03/18/2022 07:29:51 - INFO - __main__ - Step 110 Global step 110 Train loss 5.62 on epoch=54
03/18/2022 07:29:53 - INFO - __main__ - Step 120 Global step 120 Train loss 5.53 on epoch=59
03/18/2022 07:29:55 - INFO - __main__ - Step 130 Global step 130 Train loss 5.18 on epoch=64
03/18/2022 07:29:58 - INFO - __main__ - Step 140 Global step 140 Train loss 4.80 on epoch=69
03/18/2022 07:30:00 - INFO - __main__ - Step 150 Global step 150 Train loss 4.44 on epoch=74
03/18/2022 07:30:19 - INFO - __main__ - Global step 150 Train loss 5.11 Classification-F1 0.028409090909090908 on epoch=74
03/18/2022 07:30:19 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.028409090909090908 on epoch=74, global_step=150
03/18/2022 07:30:21 - INFO - __main__ - Step 160 Global step 160 Train loss 3.95 on epoch=79
03/18/2022 07:30:24 - INFO - __main__ - Step 170 Global step 170 Train loss 3.60 on epoch=84
03/18/2022 07:30:26 - INFO - __main__ - Step 180 Global step 180 Train loss 3.16 on epoch=89
03/18/2022 07:30:29 - INFO - __main__ - Step 190 Global step 190 Train loss 2.68 on epoch=94
03/18/2022 07:30:31 - INFO - __main__ - Step 200 Global step 200 Train loss 2.56 on epoch=99
03/18/2022 07:30:32 - INFO - __main__ - Global step 200 Train loss 3.19 Classification-F1 0.22695035460992907 on epoch=99
03/18/2022 07:30:32 - INFO - __main__ - Saving model with best Classification-F1: 0.028409090909090908 -> 0.22695035460992907 on epoch=99, global_step=200
03/18/2022 07:30:35 - INFO - __main__ - Step 210 Global step 210 Train loss 2.36 on epoch=104
03/18/2022 07:30:37 - INFO - __main__ - Step 220 Global step 220 Train loss 2.01 on epoch=109
03/18/2022 07:30:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.97 on epoch=114
03/18/2022 07:30:42 - INFO - __main__ - Step 240 Global step 240 Train loss 1.77 on epoch=119
03/18/2022 07:30:44 - INFO - __main__ - Step 250 Global step 250 Train loss 1.50 on epoch=124
03/18/2022 07:30:47 - INFO - __main__ - Global step 250 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 07:30:47 - INFO - __main__ - Saving model with best Classification-F1: 0.22695035460992907 -> 0.3333333333333333 on epoch=124, global_step=250
03/18/2022 07:30:50 - INFO - __main__ - Step 260 Global step 260 Train loss 1.45 on epoch=129
03/18/2022 07:30:52 - INFO - __main__ - Step 270 Global step 270 Train loss 1.16 on epoch=134
03/18/2022 07:30:55 - INFO - __main__ - Step 280 Global step 280 Train loss 1.20 on epoch=139
03/18/2022 07:30:57 - INFO - __main__ - Step 290 Global step 290 Train loss 1.03 on epoch=144
03/18/2022 07:30:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.96 on epoch=149
03/18/2022 07:31:01 - INFO - __main__ - Global step 300 Train loss 1.16 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 07:31:04 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=154
03/18/2022 07:31:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=159
03/18/2022 07:31:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.77 on epoch=164
03/18/2022 07:31:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=169
03/18/2022 07:31:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=174
03/18/2022 07:31:14 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 07:31:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.68 on epoch=179
03/18/2022 07:31:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=184
03/18/2022 07:31:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.62 on epoch=189
03/18/2022 07:31:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=194
03/18/2022 07:31:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=199
03/18/2022 07:31:27 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 07:31:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=204
03/18/2022 07:31:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=209
03/18/2022 07:31:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=214
03/18/2022 07:31:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=219
03/18/2022 07:31:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=224
03/18/2022 07:31:40 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 07:31:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=229
03/18/2022 07:31:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=234
03/18/2022 07:31:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=239
03/18/2022 07:31:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=244
03/18/2022 07:31:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=249
03/18/2022 07:31:53 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 07:31:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=254
03/18/2022 07:31:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=259
03/18/2022 07:32:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=264
03/18/2022 07:32:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=269
03/18/2022 07:32:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=274
03/18/2022 07:32:06 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 07:32:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=279
03/18/2022 07:32:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=284
03/18/2022 07:32:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=289
03/18/2022 07:32:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/18/2022 07:32:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=299
03/18/2022 07:32:19 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 07:32:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=304
03/18/2022 07:32:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=309
03/18/2022 07:32:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=314
03/18/2022 07:32:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=319
03/18/2022 07:32:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=324
03/18/2022 07:32:31 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 07:32:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=329
03/18/2022 07:32:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=334
03/18/2022 07:32:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=339
03/18/2022 07:32:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=344
03/18/2022 07:32:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=349
03/18/2022 07:32:44 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.5835835835835835 on epoch=349
03/18/2022 07:32:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5835835835835835 on epoch=349, global_step=700
03/18/2022 07:32:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=354
03/18/2022 07:32:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=359
03/18/2022 07:32:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=364
03/18/2022 07:32:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=369
03/18/2022 07:32:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=374
03/18/2022 07:32:57 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.539313399778516 on epoch=374
03/18/2022 07:32:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=379
03/18/2022 07:33:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=384
03/18/2022 07:33:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=389
03/18/2022 07:33:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
03/18/2022 07:33:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=399
03/18/2022 07:33:10 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=399
03/18/2022 07:33:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=404
03/18/2022 07:33:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=409
03/18/2022 07:33:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=414
03/18/2022 07:33:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=419
03/18/2022 07:33:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/18/2022 07:33:23 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.6559139784946237 on epoch=424
03/18/2022 07:33:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5835835835835835 -> 0.6559139784946237 on epoch=424, global_step=850
03/18/2022 07:33:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/18/2022 07:33:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=434
03/18/2022 07:33:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
03/18/2022 07:33:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
03/18/2022 07:33:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=449
03/18/2022 07:33:36 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.539313399778516 on epoch=449
03/18/2022 07:33:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
03/18/2022 07:33:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=459
03/18/2022 07:33:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=464
03/18/2022 07:33:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
03/18/2022 07:33:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
03/18/2022 07:33:48 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.49090909090909085 on epoch=474
03/18/2022 07:33:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/18/2022 07:33:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
03/18/2022 07:33:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=489
03/18/2022 07:33:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/18/2022 07:34:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=499
03/18/2022 07:34:01 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=499
03/18/2022 07:34:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=504
03/18/2022 07:34:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=509
03/18/2022 07:34:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=514
03/18/2022 07:34:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=519
03/18/2022 07:34:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
03/18/2022 07:34:14 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.5270935960591133 on epoch=524
03/18/2022 07:34:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=529
03/18/2022 07:34:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=534
03/18/2022 07:34:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=539
03/18/2022 07:34:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
03/18/2022 07:34:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=549
03/18/2022 07:34:27 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.6235294117647059 on epoch=549
03/18/2022 07:34:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=554
03/18/2022 07:34:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=559
03/18/2022 07:34:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
03/18/2022 07:34:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=569
03/18/2022 07:34:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=574
03/18/2022 07:34:40 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.5195195195195195 on epoch=574
03/18/2022 07:34:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
03/18/2022 07:34:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=584
03/18/2022 07:34:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=589
03/18/2022 07:34:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
03/18/2022 07:34:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=599
03/18/2022 07:34:52 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.6532019704433498 on epoch=599
03/18/2022 07:34:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
03/18/2022 07:34:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=609
03/18/2022 07:35:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
03/18/2022 07:35:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=619
03/18/2022 07:35:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
03/18/2022 07:35:05 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.6532019704433498 on epoch=624
03/18/2022 07:35:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
03/18/2022 07:35:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=634
03/18/2022 07:35:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=639
03/18/2022 07:35:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=644
03/18/2022 07:35:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
03/18/2022 07:35:18 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.6190476190476191 on epoch=649
03/18/2022 07:35:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=654
03/18/2022 07:35:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=659
03/18/2022 07:35:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=664
03/18/2022 07:35:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
03/18/2022 07:35:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=674
03/18/2022 07:35:31 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.4909862142099682 on epoch=674
03/18/2022 07:35:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
03/18/2022 07:35:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=684
03/18/2022 07:35:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=689
03/18/2022 07:35:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=694
03/18/2022 07:35:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=699
03/18/2022 07:35:44 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.5733333333333335 on epoch=699
03/18/2022 07:35:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=704
03/18/2022 07:35:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=709
03/18/2022 07:35:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=714
03/18/2022 07:35:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=719
03/18/2022 07:35:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
03/18/2022 07:35:57 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.5933528836754642 on epoch=724
03/18/2022 07:35:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=729
03/18/2022 07:36:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
03/18/2022 07:36:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=739
03/18/2022 07:36:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=744
03/18/2022 07:36:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=749
03/18/2022 07:36:10 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.5835835835835835 on epoch=749
03/18/2022 07:36:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=754
03/18/2022 07:36:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=759
03/18/2022 07:36:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=764
03/18/2022 07:36:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
03/18/2022 07:36:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
03/18/2022 07:36:23 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.6190476190476191 on epoch=774
03/18/2022 07:36:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
03/18/2022 07:36:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=784
03/18/2022 07:36:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
03/18/2022 07:36:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=794
03/18/2022 07:36:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
03/18/2022 07:36:36 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.6476476476476476 on epoch=799
03/18/2022 07:36:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
03/18/2022 07:36:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
03/18/2022 07:36:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
03/18/2022 07:36:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=819
03/18/2022 07:36:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=824
03/18/2022 07:36:49 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.4458874458874459 on epoch=824
03/18/2022 07:36:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=829
03/18/2022 07:36:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=834
03/18/2022 07:36:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
03/18/2022 07:36:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
03/18/2022 07:37:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=849
03/18/2022 07:37:02 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.6113360323886641 on epoch=849
03/18/2022 07:37:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=854
03/18/2022 07:37:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=859
03/18/2022 07:37:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/18/2022 07:37:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=869
03/18/2022 07:37:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=874
03/18/2022 07:37:15 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.5333333333333333 on epoch=874
03/18/2022 07:37:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=879
03/18/2022 07:37:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=884
03/18/2022 07:37:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
03/18/2022 07:37:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=894
03/18/2022 07:37:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=899
03/18/2022 07:37:28 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.5333333333333333 on epoch=899
03/18/2022 07:37:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
03/18/2022 07:37:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=909
03/18/2022 07:37:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=914
03/18/2022 07:37:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=919
03/18/2022 07:37:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=924
03/18/2022 07:37:41 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.5333333333333333 on epoch=924
03/18/2022 07:37:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=929
03/18/2022 07:37:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=934
03/18/2022 07:37:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=939
03/18/2022 07:37:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
03/18/2022 07:37:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=949
03/18/2022 07:37:54 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.5333333333333333 on epoch=949
03/18/2022 07:37:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=954
03/18/2022 07:37:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=959
03/18/2022 07:38:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=964
03/18/2022 07:38:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=969
03/18/2022 07:38:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=974
03/18/2022 07:38:06 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.5555555555555556 on epoch=974
03/18/2022 07:38:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=979
03/18/2022 07:38:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
03/18/2022 07:38:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=989
03/18/2022 07:38:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/18/2022 07:38:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=999
03/18/2022 07:38:19 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.4909862142099682 on epoch=999
03/18/2022 07:38:19 - INFO - __main__ - save last model!
03/18/2022 07:38:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:38:19 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:38:19 - INFO - __main__ - Printing 3 examples
03/18/2022 07:38:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:38:19 - INFO - __main__ - ['false']
03/18/2022 07:38:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:38:19 - INFO - __main__ - ['false']
03/18/2022 07:38:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:38:19 - INFO - __main__ - ['false']
03/18/2022 07:38:19 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:38:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:38:21 - INFO - __main__ - Printing 3 examples
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:38:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:38:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:38:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:38:21 - INFO - __main__ - Printing 3 examples
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 07:38:21 - INFO - __main__ - ['false']
03/18/2022 07:38:21 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:38:21 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:38:21 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:38:23 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:38:39 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:38:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:38:40 - INFO - __main__ - Starting training!
03/18/2022 07:39:16 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_21_0.2_8_predictions.txt
03/18/2022 07:39:16 - INFO - __main__ - Classification-F1 on test data: 0.1281
03/18/2022 07:39:19 - INFO - __main__ - prefix=wiki_qa_16_21, lr=0.2, bsz=8, dev_performance=0.6559139784946237, test_performance=0.12807971083289416
03/18/2022 07:39:19 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.5, bsz=8 ...
03/18/2022 07:39:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:39:20 - INFO - __main__ - Printing 3 examples
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:39:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:39:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:39:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:39:20 - INFO - __main__ - Printing 3 examples
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 07:39:20 - INFO - __main__ - ['false']
03/18/2022 07:39:20 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:39:20 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:39:20 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:39:36 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:39:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:39:37 - INFO - __main__ - Starting training!
03/18/2022 07:39:40 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=4
03/18/2022 07:39:43 - INFO - __main__ - Step 20 Global step 20 Train loss 1.28 on epoch=9
03/18/2022 07:39:45 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=14
03/18/2022 07:39:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/18/2022 07:39:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=24
03/18/2022 07:39:51 - INFO - __main__ - Global step 50 Train loss 1.75 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 07:39:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 07:39:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
03/18/2022 07:39:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=34
03/18/2022 07:39:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
03/18/2022 07:40:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/18/2022 07:40:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
03/18/2022 07:40:04 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.36374269005847953 on epoch=49
03/18/2022 07:40:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=49, global_step=100
03/18/2022 07:40:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
03/18/2022 07:40:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=59
03/18/2022 07:40:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=64
03/18/2022 07:40:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
03/18/2022 07:40:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
03/18/2022 07:40:17 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.375 on epoch=74
03/18/2022 07:40:17 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.375 on epoch=74, global_step=150
03/18/2022 07:40:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
03/18/2022 07:40:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
03/18/2022 07:40:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
03/18/2022 07:40:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=94
03/18/2022 07:40:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
03/18/2022 07:40:30 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=99
03/18/2022 07:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.375 -> 0.3816425120772947 on epoch=99, global_step=200
03/18/2022 07:40:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
03/18/2022 07:40:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=109
03/18/2022 07:40:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=114
03/18/2022 07:40:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
03/18/2022 07:40:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
03/18/2022 07:40:43 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.46843853820598 on epoch=124
03/18/2022 07:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.46843853820598 on epoch=124, global_step=250
03/18/2022 07:40:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
03/18/2022 07:40:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=134
03/18/2022 07:40:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
03/18/2022 07:40:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
03/18/2022 07:40:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/18/2022 07:40:56 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.4285714285714286 on epoch=149
03/18/2022 07:40:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/18/2022 07:41:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/18/2022 07:41:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
03/18/2022 07:41:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=169
03/18/2022 07:41:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/18/2022 07:41:09 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3073593073593074 on epoch=174
03/18/2022 07:41:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
03/18/2022 07:41:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
03/18/2022 07:41:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=189
03/18/2022 07:41:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
03/18/2022 07:41:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
03/18/2022 07:41:22 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.36374269005847953 on epoch=199
03/18/2022 07:41:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=204
03/18/2022 07:41:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
03/18/2022 07:41:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=214
03/18/2022 07:41:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
03/18/2022 07:41:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
03/18/2022 07:41:35 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.36374269005847953 on epoch=224
03/18/2022 07:41:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=229
03/18/2022 07:41:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=234
03/18/2022 07:41:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=239
03/18/2022 07:41:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/18/2022 07:41:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=249
03/18/2022 07:41:49 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=249
03/18/2022 07:41:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
03/18/2022 07:41:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
03/18/2022 07:41:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=264
03/18/2022 07:41:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=269
03/18/2022 07:42:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=274
03/18/2022 07:42:02 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 07:42:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=279
03/18/2022 07:42:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=284
03/18/2022 07:42:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
03/18/2022 07:42:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=294
03/18/2022 07:42:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=299
03/18/2022 07:42:15 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.3191489361702127 on epoch=299
03/18/2022 07:42:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=304
03/18/2022 07:42:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=309
03/18/2022 07:42:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=314
03/18/2022 07:42:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=319
03/18/2022 07:42:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=324
03/18/2022 07:42:28 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.3191489361702127 on epoch=324
03/18/2022 07:42:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=329
03/18/2022 07:42:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=334
03/18/2022 07:42:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
03/18/2022 07:42:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=344
03/18/2022 07:42:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=349
03/18/2022 07:42:41 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.4817813765182186 on epoch=349
03/18/2022 07:42:41 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.4817813765182186 on epoch=349, global_step=700
03/18/2022 07:42:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
03/18/2022 07:42:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=359
03/18/2022 07:42:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=364
03/18/2022 07:42:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=369
03/18/2022 07:42:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
03/18/2022 07:42:54 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.3816425120772947 on epoch=374
03/18/2022 07:42:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=379
03/18/2022 07:42:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=384
03/18/2022 07:43:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/18/2022 07:43:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/18/2022 07:43:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
03/18/2022 07:43:08 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.4980392156862745 on epoch=399
03/18/2022 07:43:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.4980392156862745 on epoch=399, global_step=800
03/18/2022 07:43:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=404
03/18/2022 07:43:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
03/18/2022 07:43:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
03/18/2022 07:43:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=419
03/18/2022 07:43:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
03/18/2022 07:43:21 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.4009852216748768 on epoch=424
03/18/2022 07:43:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/18/2022 07:43:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=434
03/18/2022 07:43:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=439
03/18/2022 07:43:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/18/2022 07:43:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
03/18/2022 07:43:34 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.5270935960591133 on epoch=449
03/18/2022 07:43:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5270935960591133 on epoch=449, global_step=900
03/18/2022 07:43:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=454
03/18/2022 07:43:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=459
03/18/2022 07:43:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
03/18/2022 07:43:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=469
03/18/2022 07:43:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=474
03/18/2022 07:43:47 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.5195195195195195 on epoch=474
03/18/2022 07:43:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/18/2022 07:43:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
03/18/2022 07:43:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
03/18/2022 07:43:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
03/18/2022 07:43:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=499
03/18/2022 07:44:00 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.4817813765182186 on epoch=499
03/18/2022 07:44:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
03/18/2022 07:44:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/18/2022 07:44:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/18/2022 07:44:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/18/2022 07:44:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
03/18/2022 07:44:13 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.4980392156862745 on epoch=524
03/18/2022 07:44:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/18/2022 07:44:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/18/2022 07:44:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
03/18/2022 07:44:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/18/2022 07:44:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/18/2022 07:44:27 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.49090909090909085 on epoch=549
03/18/2022 07:44:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
03/18/2022 07:44:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/18/2022 07:44:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/18/2022 07:44:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
03/18/2022 07:44:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/18/2022 07:44:40 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.4682306940371457 on epoch=574
03/18/2022 07:44:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
03/18/2022 07:44:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/18/2022 07:44:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/18/2022 07:44:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/18/2022 07:44:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=599
03/18/2022 07:44:53 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.5307917888563051 on epoch=599
03/18/2022 07:44:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5307917888563051 on epoch=599, global_step=1200
03/18/2022 07:44:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
03/18/2022 07:44:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/18/2022 07:45:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/18/2022 07:45:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/18/2022 07:45:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/18/2022 07:45:06 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.5307917888563051 on epoch=624
03/18/2022 07:45:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/18/2022 07:45:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
03/18/2022 07:45:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/18/2022 07:45:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/18/2022 07:45:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/18/2022 07:45:19 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.4666666666666667 on epoch=649
03/18/2022 07:45:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/18/2022 07:45:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/18/2022 07:45:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
03/18/2022 07:45:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/18/2022 07:45:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/18/2022 07:45:32 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5 on epoch=674
03/18/2022 07:45:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/18/2022 07:45:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/18/2022 07:45:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/18/2022 07:45:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/18/2022 07:45:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/18/2022 07:45:45 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=699
03/18/2022 07:45:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/18/2022 07:45:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/18/2022 07:45:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 07:45:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/18/2022 07:45:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/18/2022 07:46:00 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=724
03/18/2022 07:46:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 07:46:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/18/2022 07:46:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 07:46:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/18/2022 07:46:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 07:46:14 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=749
03/18/2022 07:46:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 07:46:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 07:46:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/18/2022 07:46:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 07:46:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 07:46:27 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=774
03/18/2022 07:46:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 07:46:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 07:46:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 07:46:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/18/2022 07:46:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
03/18/2022 07:46:40 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=799
03/18/2022 07:46:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 07:46:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
03/18/2022 07:46:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 07:46:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/18/2022 07:46:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/18/2022 07:46:54 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5 on epoch=824
03/18/2022 07:46:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 07:46:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 07:47:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/18/2022 07:47:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 07:47:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 07:47:07 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=849
03/18/2022 07:47:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5607843137254902 on epoch=849, global_step=1700
03/18/2022 07:47:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 07:47:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 07:47:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 07:47:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 07:47:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 07:47:20 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=874
03/18/2022 07:47:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 07:47:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 07:47:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 07:47:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/18/2022 07:47:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 07:47:33 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5 on epoch=899
03/18/2022 07:47:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 07:47:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 07:47:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 07:47:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 07:47:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 07:47:46 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=924
03/18/2022 07:47:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/18/2022 07:47:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/18/2022 07:47:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 07:47:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 07:47:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 07:48:00 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=949
03/18/2022 07:48:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 07:48:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/18/2022 07:48:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 07:48:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 07:48:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 07:48:14 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=974
03/18/2022 07:48:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 07:48:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
03/18/2022 07:48:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 07:48:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/18/2022 07:48:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 07:48:27 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5 on epoch=999
03/18/2022 07:48:27 - INFO - __main__ - save last model!
03/18/2022 07:48:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:48:27 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:48:27 - INFO - __main__ - Printing 3 examples
03/18/2022 07:48:27 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:48:27 - INFO - __main__ - ['false']
03/18/2022 07:48:27 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:48:27 - INFO - __main__ - ['false']
03/18/2022 07:48:27 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:48:27 - INFO - __main__ - ['false']
03/18/2022 07:48:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:48:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:48:28 - INFO - __main__ - Printing 3 examples
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:48:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:48:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:48:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:48:28 - INFO - __main__ - Printing 3 examples
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 07:48:28 - INFO - __main__ - ['false']
03/18/2022 07:48:28 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:48:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:48:28 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:48:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:48:31 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:48:47 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:48:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:48:48 - INFO - __main__ - Starting training!
03/18/2022 07:49:53 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_42_0.5_8_predictions.txt
03/18/2022 07:49:53 - INFO - __main__ - Classification-F1 on test data: 0.3926
03/18/2022 07:49:54 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.5, bsz=8, dev_performance=0.5607843137254902, test_performance=0.3926248967324789
03/18/2022 07:49:54 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.4, bsz=8 ...
03/18/2022 07:49:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:49:55 - INFO - __main__ - Printing 3 examples
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:49:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:49:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:49:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:49:55 - INFO - __main__ - Printing 3 examples
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 07:49:55 - INFO - __main__ - ['false']
03/18/2022 07:49:55 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:49:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:49:55 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:50:11 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:50:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:50:12 - INFO - __main__ - Starting training!
03/18/2022 07:50:15 - INFO - __main__ - Step 10 Global step 10 Train loss 7.91 on epoch=4
03/18/2022 07:50:17 - INFO - __main__ - Step 20 Global step 20 Train loss 7.30 on epoch=9
03/18/2022 07:50:20 - INFO - __main__ - Step 30 Global step 30 Train loss 3.86 on epoch=14
03/18/2022 07:50:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.29 on epoch=19
03/18/2022 07:50:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.37 on epoch=24
03/18/2022 07:50:32 - INFO - __main__ - Global step 50 Train loss 4.55 Classification-F1 0.16304347826086957 on epoch=24
03/18/2022 07:50:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16304347826086957 on epoch=24, global_step=50
03/18/2022 07:50:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=29
03/18/2022 07:50:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.82 on epoch=34
03/18/2022 07:50:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=39
03/18/2022 07:50:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=44
03/18/2022 07:50:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=49
03/18/2022 07:50:47 - INFO - __main__ - Global step 100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 07:50:47 - INFO - __main__ - Saving model with best Classification-F1: 0.16304347826086957 -> 0.3333333333333333 on epoch=49, global_step=100
03/18/2022 07:50:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=54
03/18/2022 07:50:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=59
03/18/2022 07:50:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
03/18/2022 07:50:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
03/18/2022 07:50:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=74
03/18/2022 07:51:00 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 07:51:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/18/2022 07:51:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=84
03/18/2022 07:51:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=89
03/18/2022 07:51:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
03/18/2022 07:51:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=99
03/18/2022 07:51:13 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 07:51:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
03/18/2022 07:51:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
03/18/2022 07:51:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
03/18/2022 07:51:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=119
03/18/2022 07:51:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
03/18/2022 07:51:26 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 07:51:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
03/18/2022 07:51:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
03/18/2022 07:51:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
03/18/2022 07:51:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
03/18/2022 07:51:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
03/18/2022 07:51:39 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=149
03/18/2022 07:51:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.09 on epoch=154
03/18/2022 07:51:44 - INFO - __main__ - Step 320 Global step 320 Train loss 4.62 on epoch=159
03/18/2022 07:51:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=164
03/18/2022 07:51:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.96 on epoch=169
03/18/2022 07:51:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=174
03/18/2022 07:51:52 - INFO - __main__ - Global step 350 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 07:51:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.74 on epoch=179
03/18/2022 07:51:57 - INFO - __main__ - Step 370 Global step 370 Train loss 7.26 on epoch=184
03/18/2022 07:52:00 - INFO - __main__ - Step 380 Global step 380 Train loss 7.03 on epoch=189
03/18/2022 07:52:02 - INFO - __main__ - Step 390 Global step 390 Train loss 5.38 on epoch=194
03/18/2022 07:52:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.71 on epoch=199
03/18/2022 07:52:05 - INFO - __main__ - Global step 400 Train loss 4.42 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 07:52:08 - INFO - __main__ - Step 410 Global step 410 Train loss 3.05 on epoch=204
03/18/2022 07:52:10 - INFO - __main__ - Step 420 Global step 420 Train loss 3.45 on epoch=209
03/18/2022 07:52:13 - INFO - __main__ - Step 430 Global step 430 Train loss 6.19 on epoch=214
03/18/2022 07:52:15 - INFO - __main__ - Step 440 Global step 440 Train loss 2.52 on epoch=219
03/18/2022 07:52:18 - INFO - __main__ - Step 450 Global step 450 Train loss 1.95 on epoch=224
03/18/2022 07:52:18 - INFO - __main__ - Global step 450 Train loss 3.43 Classification-F1 0.2873806998939555 on epoch=224
03/18/2022 07:52:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.59 on epoch=229
03/18/2022 07:52:23 - INFO - __main__ - Step 470 Global step 470 Train loss 1.47 on epoch=234
03/18/2022 07:52:26 - INFO - __main__ - Step 480 Global step 480 Train loss 1.73 on epoch=239
03/18/2022 07:52:28 - INFO - __main__ - Step 490 Global step 490 Train loss 1.75 on epoch=244
03/18/2022 07:52:31 - INFO - __main__ - Step 500 Global step 500 Train loss 1.71 on epoch=249
03/18/2022 07:52:31 - INFO - __main__ - Global step 500 Train loss 1.65 Classification-F1 0.37254901960784315 on epoch=249
03/18/2022 07:52:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37254901960784315 on epoch=249, global_step=500
03/18/2022 07:52:34 - INFO - __main__ - Step 510 Global step 510 Train loss 1.89 on epoch=254
03/18/2022 07:52:36 - INFO - __main__ - Step 520 Global step 520 Train loss 1.82 on epoch=259
03/18/2022 07:52:39 - INFO - __main__ - Step 530 Global step 530 Train loss 1.55 on epoch=264
03/18/2022 07:52:41 - INFO - __main__ - Step 540 Global step 540 Train loss 1.52 on epoch=269
03/18/2022 07:52:44 - INFO - __main__ - Step 550 Global step 550 Train loss 1.62 on epoch=274
03/18/2022 07:52:44 - INFO - __main__ - Global step 550 Train loss 1.68 Classification-F1 0.34310850439882695 on epoch=274
03/18/2022 07:52:47 - INFO - __main__ - Step 560 Global step 560 Train loss 1.31 on epoch=279
03/18/2022 07:52:49 - INFO - __main__ - Step 570 Global step 570 Train loss 1.31 on epoch=284
03/18/2022 07:52:52 - INFO - __main__ - Step 580 Global step 580 Train loss 1.20 on epoch=289
03/18/2022 07:52:54 - INFO - __main__ - Step 590 Global step 590 Train loss 1.30 on epoch=294
03/18/2022 07:52:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.41 on epoch=299
03/18/2022 07:52:58 - INFO - __main__ - Global step 600 Train loss 1.31 Classification-F1 0.41700404858299595 on epoch=299
03/18/2022 07:52:58 - INFO - __main__ - Saving model with best Classification-F1: 0.37254901960784315 -> 0.41700404858299595 on epoch=299, global_step=600
03/18/2022 07:53:00 - INFO - __main__ - Step 610 Global step 610 Train loss 1.74 on epoch=304
03/18/2022 07:53:03 - INFO - __main__ - Step 620 Global step 620 Train loss 1.76 on epoch=309
03/18/2022 07:53:05 - INFO - __main__ - Step 630 Global step 630 Train loss 1.44 on epoch=314
03/18/2022 07:53:08 - INFO - __main__ - Step 640 Global step 640 Train loss 1.62 on epoch=319
03/18/2022 07:53:10 - INFO - __main__ - Step 650 Global step 650 Train loss 1.24 on epoch=324
03/18/2022 07:53:11 - INFO - __main__ - Global step 650 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 07:53:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.73 on epoch=329
03/18/2022 07:53:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=334
03/18/2022 07:53:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.69 on epoch=339
03/18/2022 07:53:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.83 on epoch=344
03/18/2022 07:53:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=349
03/18/2022 07:53:24 - INFO - __main__ - Global step 700 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 07:53:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.67 on epoch=354
03/18/2022 07:53:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.67 on epoch=359
03/18/2022 07:53:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=364
03/18/2022 07:53:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=369
03/18/2022 07:53:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
03/18/2022 07:53:37 - INFO - __main__ - Global step 750 Train loss 0.60 Classification-F1 0.3992490613266583 on epoch=374
03/18/2022 07:53:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.59 on epoch=379
03/18/2022 07:53:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=384
03/18/2022 07:53:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=389
03/18/2022 07:53:47 - INFO - __main__ - Step 790 Global step 790 Train loss 1.08 on epoch=394
03/18/2022 07:53:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=399
03/18/2022 07:53:50 - INFO - __main__ - Global step 800 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=399
03/18/2022 07:53:53 - INFO - __main__ - Step 810 Global step 810 Train loss 1.27 on epoch=404
03/18/2022 07:53:55 - INFO - __main__ - Step 820 Global step 820 Train loss 1.52 on epoch=409
03/18/2022 07:53:58 - INFO - __main__ - Step 830 Global step 830 Train loss 1.07 on epoch=414
03/18/2022 07:54:00 - INFO - __main__ - Step 840 Global step 840 Train loss 1.62 on epoch=419
03/18/2022 07:54:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.93 on epoch=424
03/18/2022 07:54:03 - INFO - __main__ - Global step 850 Train loss 1.29 Classification-F1 0.3333333333333333 on epoch=424
03/18/2022 07:54:06 - INFO - __main__ - Step 860 Global step 860 Train loss 1.04 on epoch=429
03/18/2022 07:54:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.92 on epoch=434
03/18/2022 07:54:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.74 on epoch=439
03/18/2022 07:54:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.70 on epoch=444
03/18/2022 07:54:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.94 on epoch=449
03/18/2022 07:54:16 - INFO - __main__ - Global step 900 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=449
03/18/2022 07:54:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.71 on epoch=454
03/18/2022 07:54:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.69 on epoch=459
03/18/2022 07:54:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.66 on epoch=464
03/18/2022 07:54:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.80 on epoch=469
03/18/2022 07:54:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=474
03/18/2022 07:54:29 - INFO - __main__ - Global step 950 Train loss 0.72 Classification-F1 0.3333333333333333 on epoch=474
03/18/2022 07:54:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.63 on epoch=479
03/18/2022 07:54:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.62 on epoch=484
03/18/2022 07:54:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.64 on epoch=489
03/18/2022 07:54:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=494
03/18/2022 07:54:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
03/18/2022 07:54:43 - INFO - __main__ - Global step 1000 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=499
03/18/2022 07:54:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=504
03/18/2022 07:54:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.60 on epoch=509
03/18/2022 07:54:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.60 on epoch=514
03/18/2022 07:54:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.61 on epoch=519
03/18/2022 07:54:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.65 on epoch=524
03/18/2022 07:54:56 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.3333333333333333 on epoch=524
03/18/2022 07:54:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.64 on epoch=529
03/18/2022 07:55:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
03/18/2022 07:55:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=539
03/18/2022 07:55:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.57 on epoch=544
03/18/2022 07:55:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=549
03/18/2022 07:55:09 - INFO - __main__ - Global step 1100 Train loss 0.56 Classification-F1 0.3992490613266583 on epoch=549
03/18/2022 07:55:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=554
03/18/2022 07:55:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
03/18/2022 07:55:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=564
03/18/2022 07:55:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=569
03/18/2022 07:55:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.57 on epoch=574
03/18/2022 07:55:22 - INFO - __main__ - Global step 1150 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=574
03/18/2022 07:55:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.51 on epoch=579
03/18/2022 07:55:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
03/18/2022 07:55:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=589
03/18/2022 07:55:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.54 on epoch=594
03/18/2022 07:55:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=599
03/18/2022 07:55:35 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=599
03/18/2022 07:55:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
03/18/2022 07:55:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=609
03/18/2022 07:55:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.50 on epoch=614
03/18/2022 07:55:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=619
03/18/2022 07:55:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=624
03/18/2022 07:55:48 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=624
03/18/2022 07:55:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.53 on epoch=629
03/18/2022 07:55:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=634
03/18/2022 07:55:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=639
03/18/2022 07:55:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=644
03/18/2022 07:56:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/18/2022 07:56:02 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=649
03/18/2022 07:56:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=654
03/18/2022 07:56:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=659
03/18/2022 07:56:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=664
03/18/2022 07:56:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=669
03/18/2022 07:56:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.54 on epoch=674
03/18/2022 07:56:15 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=674
03/18/2022 07:56:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.58 on epoch=679
03/18/2022 07:56:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=684
03/18/2022 07:56:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
03/18/2022 07:56:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=694
03/18/2022 07:56:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=699
03/18/2022 07:56:28 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=699
03/18/2022 07:56:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
03/18/2022 07:56:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=709
03/18/2022 07:56:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=714
03/18/2022 07:56:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
03/18/2022 07:56:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=724
03/18/2022 07:56:41 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=724
03/18/2022 07:56:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=729
03/18/2022 07:56:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=734
03/18/2022 07:56:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=739
03/18/2022 07:56:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
03/18/2022 07:56:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=749
03/18/2022 07:56:54 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=749
03/18/2022 07:56:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=754
03/18/2022 07:56:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=759
03/18/2022 07:57:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
03/18/2022 07:57:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=769
03/18/2022 07:57:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=774
03/18/2022 07:57:07 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=774
03/18/2022 07:57:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=779
03/18/2022 07:57:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
03/18/2022 07:57:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
03/18/2022 07:57:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=794
03/18/2022 07:57:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=799
03/18/2022 07:57:20 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=799
03/18/2022 07:57:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
03/18/2022 07:57:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
03/18/2022 07:57:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=814
03/18/2022 07:57:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=819
03/18/2022 07:57:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=824
03/18/2022 07:57:34 - INFO - __main__ - Global step 1650 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=824
03/18/2022 07:57:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
03/18/2022 07:57:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=834
03/18/2022 07:57:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=839
03/18/2022 07:57:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
03/18/2022 07:57:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
03/18/2022 07:57:47 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=849
03/18/2022 07:57:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
03/18/2022 07:57:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=859
03/18/2022 07:57:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=864
03/18/2022 07:57:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
03/18/2022 07:57:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=874
03/18/2022 07:58:00 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=874
03/18/2022 07:58:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
03/18/2022 07:58:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
03/18/2022 07:58:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
03/18/2022 07:58:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.50 on epoch=894
03/18/2022 07:58:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
03/18/2022 07:58:13 - INFO - __main__ - Global step 1800 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=899
03/18/2022 07:58:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
03/18/2022 07:58:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=909
03/18/2022 07:58:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=914
03/18/2022 07:58:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
03/18/2022 07:58:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
03/18/2022 07:58:26 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=924
03/18/2022 07:58:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
03/18/2022 07:58:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
03/18/2022 07:58:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
03/18/2022 07:58:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
03/18/2022 07:58:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
03/18/2022 07:58:39 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=949
03/18/2022 07:58:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=954
03/18/2022 07:58:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
03/18/2022 07:58:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
03/18/2022 07:58:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.52 on epoch=969
03/18/2022 07:58:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=974
03/18/2022 07:58:52 - INFO - __main__ - Global step 1950 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=974
03/18/2022 07:58:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
03/18/2022 07:58:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
03/18/2022 07:59:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=989
03/18/2022 07:59:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=994
03/18/2022 07:59:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
03/18/2022 07:59:05 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=999
03/18/2022 07:59:05 - INFO - __main__ - save last model!
03/18/2022 07:59:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 07:59:06 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 07:59:06 - INFO - __main__ - Printing 3 examples
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 07:59:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:59:06 - INFO - __main__ - Printing 3 examples
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 07:59:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:59:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 07:59:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 07:59:06 - INFO - __main__ - Printing 3 examples
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 07:59:06 - INFO - __main__ - ['false']
03/18/2022 07:59:06 - INFO - __main__ - Tokenizing Input ...
03/18/2022 07:59:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:59:06 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 07:59:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 07:59:09 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 07:59:25 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 07:59:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 07:59:26 - INFO - __main__ - Starting training!
03/18/2022 08:00:06 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_42_0.4_8_predictions.txt
03/18/2022 08:00:06 - INFO - __main__ - Classification-F1 on test data: 0.3364
03/18/2022 08:00:07 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.4, bsz=8, dev_performance=0.41700404858299595, test_performance=0.33643844744294515
03/18/2022 08:00:07 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.3, bsz=8 ...
03/18/2022 08:00:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:00:08 - INFO - __main__ - Printing 3 examples
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:00:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:00:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:00:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:00:08 - INFO - __main__ - Printing 3 examples
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 08:00:08 - INFO - __main__ - ['false']
03/18/2022 08:00:08 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:00:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:00:08 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:00:26 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:00:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:00:26 - INFO - __main__ - Starting training!
03/18/2022 08:00:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.61 on epoch=4
03/18/2022 08:00:32 - INFO - __main__ - Step 20 Global step 20 Train loss 2.54 on epoch=9
03/18/2022 08:00:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.88 on epoch=14
03/18/2022 08:00:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=19
03/18/2022 08:00:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.12 on epoch=24
03/18/2022 08:00:45 - INFO - __main__ - Global step 50 Train loss 2.33 Classification-F1 0.3264033264033264 on epoch=24
03/18/2022 08:00:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3264033264033264 on epoch=24, global_step=50
03/18/2022 08:00:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.34 on epoch=29
03/18/2022 08:00:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=34
03/18/2022 08:00:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=39
03/18/2022 08:00:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=44
03/18/2022 08:00:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.60 on epoch=49
03/18/2022 08:00:58 - INFO - __main__ - Global step 100 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 08:00:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3264033264033264 -> 0.3333333333333333 on epoch=49, global_step=100
03/18/2022 08:01:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
03/18/2022 08:01:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=59
03/18/2022 08:01:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
03/18/2022 08:01:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=69
03/18/2022 08:01:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
03/18/2022 08:01:11 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 08:01:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=79
03/18/2022 08:01:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=84
03/18/2022 08:01:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
03/18/2022 08:01:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
03/18/2022 08:01:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=99
03/18/2022 08:01:24 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 08:01:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
03/18/2022 08:01:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
03/18/2022 08:01:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=114
03/18/2022 08:01:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=119
03/18/2022 08:01:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
03/18/2022 08:01:36 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 08:01:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=129
03/18/2022 08:01:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
03/18/2022 08:01:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=139
03/18/2022 08:01:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
03/18/2022 08:01:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=149
03/18/2022 08:01:49 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.33793103448275863 on epoch=149
03/18/2022 08:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.33793103448275863 on epoch=149, global_step=300
03/18/2022 08:01:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=154
03/18/2022 08:01:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=159
03/18/2022 08:01:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
03/18/2022 08:01:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=169
03/18/2022 08:02:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
03/18/2022 08:02:02 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 08:02:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=179
03/18/2022 08:02:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=184
03/18/2022 08:02:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=189
03/18/2022 08:02:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=194
03/18/2022 08:02:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
03/18/2022 08:02:15 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.4231177094379639 on epoch=199
03/18/2022 08:02:15 - INFO - __main__ - Saving model with best Classification-F1: 0.33793103448275863 -> 0.4231177094379639 on epoch=199, global_step=400
03/18/2022 08:02:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=204
03/18/2022 08:02:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
03/18/2022 08:02:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=214
03/18/2022 08:02:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=219
03/18/2022 08:02:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=224
03/18/2022 08:02:28 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=224
03/18/2022 08:02:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=229
03/18/2022 08:02:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=234
03/18/2022 08:02:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
03/18/2022 08:02:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=244
03/18/2022 08:02:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=249
03/18/2022 08:02:41 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 08:02:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
03/18/2022 08:02:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=259
03/18/2022 08:02:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
03/18/2022 08:02:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
03/18/2022 08:02:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=274
03/18/2022 08:02:54 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 08:02:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=279
03/18/2022 08:02:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=284
03/18/2022 08:03:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
03/18/2022 08:03:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=294
03/18/2022 08:03:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=299
03/18/2022 08:03:07 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 08:03:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=304
03/18/2022 08:03:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=309
03/18/2022 08:03:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/18/2022 08:03:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=319
03/18/2022 08:03:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=324
03/18/2022 08:03:20 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 08:03:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=329
03/18/2022 08:03:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=334
03/18/2022 08:03:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=339
03/18/2022 08:03:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=344
03/18/2022 08:03:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=349
03/18/2022 08:03:32 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.3816425120772947 on epoch=349
03/18/2022 08:03:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
03/18/2022 08:03:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=359
03/18/2022 08:03:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=364
03/18/2022 08:03:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/18/2022 08:03:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/18/2022 08:03:45 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=374
03/18/2022 08:03:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=379
03/18/2022 08:03:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=384
03/18/2022 08:03:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=389
03/18/2022 08:03:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
03/18/2022 08:03:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=399
03/18/2022 08:03:58 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.4181818181818182 on epoch=399
03/18/2022 08:04:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=404
03/18/2022 08:04:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=409
03/18/2022 08:04:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=414
03/18/2022 08:04:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/18/2022 08:04:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/18/2022 08:04:11 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=424
03/18/2022 08:04:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=429
03/18/2022 08:04:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=434
03/18/2022 08:04:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/18/2022 08:04:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/18/2022 08:04:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.93 on epoch=449
03/18/2022 08:04:24 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.375 on epoch=449
03/18/2022 08:04:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=454
03/18/2022 08:04:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=459
03/18/2022 08:04:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.92 on epoch=464
03/18/2022 08:04:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=469
03/18/2022 08:04:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=474
03/18/2022 08:04:37 - INFO - __main__ - Global step 950 Train loss 0.53 Classification-F1 0.3816425120772947 on epoch=474
03/18/2022 08:04:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/18/2022 08:04:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=484
03/18/2022 08:04:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
03/18/2022 08:04:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/18/2022 08:04:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
03/18/2022 08:04:50 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=499
03/18/2022 08:04:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/18/2022 08:04:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
03/18/2022 08:04:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/18/2022 08:05:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=519
03/18/2022 08:05:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.74 on epoch=524
03/18/2022 08:05:04 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.3992490613266583 on epoch=524
03/18/2022 08:05:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.54 on epoch=529
03/18/2022 08:05:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
03/18/2022 08:05:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=539
03/18/2022 08:05:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=544
03/18/2022 08:05:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
03/18/2022 08:05:17 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.46843853820598 on epoch=549
03/18/2022 08:05:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.46843853820598 on epoch=549, global_step=1100
03/18/2022 08:05:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
03/18/2022 08:05:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=559
03/18/2022 08:05:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=564
03/18/2022 08:05:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=569
03/18/2022 08:05:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=574
03/18/2022 08:05:29 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3650793650793651 on epoch=574
03/18/2022 08:05:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
03/18/2022 08:05:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
03/18/2022 08:05:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/18/2022 08:05:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=594
03/18/2022 08:05:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
03/18/2022 08:05:42 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.34310850439882695 on epoch=599
03/18/2022 08:05:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=604
03/18/2022 08:05:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=609
03/18/2022 08:05:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=614
03/18/2022 08:05:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=619
03/18/2022 08:05:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=624
03/18/2022 08:05:55 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.33793103448275863 on epoch=624
03/18/2022 08:05:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=629
03/18/2022 08:06:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=634
03/18/2022 08:06:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=639
03/18/2022 08:06:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=644
03/18/2022 08:06:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
03/18/2022 08:06:08 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.375 on epoch=649
03/18/2022 08:06:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
03/18/2022 08:06:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
03/18/2022 08:06:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
03/18/2022 08:06:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
03/18/2022 08:06:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/18/2022 08:06:21 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.36374269005847953 on epoch=674
03/18/2022 08:06:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
03/18/2022 08:06:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
03/18/2022 08:06:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=689
03/18/2022 08:06:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
03/18/2022 08:06:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
03/18/2022 08:06:34 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.36374269005847953 on epoch=699
03/18/2022 08:06:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=704
03/18/2022 08:06:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=709
03/18/2022 08:06:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
03/18/2022 08:06:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=719
03/18/2022 08:06:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=724
03/18/2022 08:06:47 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.36374269005847953 on epoch=724
03/18/2022 08:06:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=729
03/18/2022 08:06:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=734
03/18/2022 08:06:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/18/2022 08:06:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
03/18/2022 08:07:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
03/18/2022 08:07:00 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.36374269005847953 on epoch=749
03/18/2022 08:07:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=754
03/18/2022 08:07:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=759
03/18/2022 08:07:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=764
03/18/2022 08:07:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=769
03/18/2022 08:07:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=774
03/18/2022 08:07:14 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.3191489361702127 on epoch=774
03/18/2022 08:07:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=779
03/18/2022 08:07:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=784
03/18/2022 08:07:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
03/18/2022 08:07:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
03/18/2022 08:07:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
03/18/2022 08:07:27 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.4181818181818182 on epoch=799
03/18/2022 08:07:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=804
03/18/2022 08:07:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
03/18/2022 08:07:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=814
03/18/2022 08:07:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=819
03/18/2022 08:07:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=824
03/18/2022 08:07:40 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.37662337662337664 on epoch=824
03/18/2022 08:07:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=829
03/18/2022 08:07:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=834
03/18/2022 08:07:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
03/18/2022 08:07:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=844
03/18/2022 08:07:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
03/18/2022 08:07:53 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.37662337662337664 on epoch=849
03/18/2022 08:07:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=854
03/18/2022 08:07:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=859
03/18/2022 08:08:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=864
03/18/2022 08:08:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
03/18/2022 08:08:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=874
03/18/2022 08:08:06 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.4231177094379639 on epoch=874
03/18/2022 08:08:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=879
03/18/2022 08:08:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=884
03/18/2022 08:08:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=889
03/18/2022 08:08:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=894
03/18/2022 08:08:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
03/18/2022 08:08:19 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.4231177094379639 on epoch=899
03/18/2022 08:08:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
03/18/2022 08:08:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
03/18/2022 08:08:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=914
03/18/2022 08:08:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
03/18/2022 08:08:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=924
03/18/2022 08:08:32 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.4231177094379639 on epoch=924
03/18/2022 08:08:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=929
03/18/2022 08:08:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=934
03/18/2022 08:08:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=939
03/18/2022 08:08:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
03/18/2022 08:08:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=949
03/18/2022 08:08:45 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.4920634920634921 on epoch=949
03/18/2022 08:08:45 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.4920634920634921 on epoch=949, global_step=1900
03/18/2022 08:08:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=954
03/18/2022 08:08:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=959
03/18/2022 08:08:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
03/18/2022 08:08:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=969
03/18/2022 08:08:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
03/18/2022 08:08:58 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.4920634920634921 on epoch=974
03/18/2022 08:09:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=979
03/18/2022 08:09:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=984
03/18/2022 08:09:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=989
03/18/2022 08:09:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=994
03/18/2022 08:09:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=999
03/18/2022 08:09:11 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.4920634920634921 on epoch=999
03/18/2022 08:09:11 - INFO - __main__ - save last model!
03/18/2022 08:09:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 08:09:11 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 08:09:11 - INFO - __main__ - Printing 3 examples
03/18/2022 08:09:11 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 08:09:11 - INFO - __main__ - ['false']
03/18/2022 08:09:11 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 08:09:11 - INFO - __main__ - ['false']
03/18/2022 08:09:11 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 08:09:11 - INFO - __main__ - ['false']
03/18/2022 08:09:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:09:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:09:12 - INFO - __main__ - Printing 3 examples
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 08:09:12 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:09:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:09:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:09:12 - INFO - __main__ - Printing 3 examples
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 08:09:12 - INFO - __main__ - ['false']
03/18/2022 08:09:12 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:09:12 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:09:12 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:09:13 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:09:15 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 08:09:31 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:09:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:09:32 - INFO - __main__ - Starting training!
03/18/2022 08:10:09 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_42_0.3_8_predictions.txt
03/18/2022 08:10:09 - INFO - __main__ - Classification-F1 on test data: 0.4110
03/18/2022 08:10:10 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.3, bsz=8, dev_performance=0.4920634920634921, test_performance=0.4110470821880218
03/18/2022 08:10:10 - INFO - __main__ - Running ... prefix=wiki_qa_16_42, lr=0.2, bsz=8 ...
03/18/2022 08:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:10:11 - INFO - __main__ - Printing 3 examples
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:10:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:10:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:10:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:10:11 - INFO - __main__ - Printing 3 examples
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: how much does united states spend on health care [SEP] answer: Health care in the United States is provided by many distinct organizations.
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: how many men does the american military have in it [SEP] answer: Leadership is provided by the Chairman of the Joint Chiefs of Staff and the Vice Chairman of the Joint Chiefs of Staff .
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ -  [wiki_qa] question: when was the battle at tombstone fought [SEP] answer: Ike Clanton and Billy Claiborne ran from the fight unharmed, but Ike's brother Billy Clanton was killed, along with both McLaurys.
03/18/2022 08:10:11 - INFO - __main__ - ['false']
03/18/2022 08:10:11 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:10:11 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:10:11 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:10:29 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:10:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:10:29 - INFO - __main__ - Starting training!
03/18/2022 08:10:33 - INFO - __main__ - Step 10 Global step 10 Train loss 7.39 on epoch=4
03/18/2022 08:10:35 - INFO - __main__ - Step 20 Global step 20 Train loss 4.21 on epoch=9
03/18/2022 08:10:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.81 on epoch=14
03/18/2022 08:10:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=19
03/18/2022 08:10:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=24
03/18/2022 08:10:43 - INFO - __main__ - Global step 50 Train loss 3.00 Classification-F1 0.3816425120772947 on epoch=24
03/18/2022 08:10:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3816425120772947 on epoch=24, global_step=50
03/18/2022 08:10:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=29
03/18/2022 08:10:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=34
03/18/2022 08:10:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
03/18/2022 08:10:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
03/18/2022 08:10:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
03/18/2022 08:10:56 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 08:10:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
03/18/2022 08:11:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
03/18/2022 08:11:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
03/18/2022 08:11:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
03/18/2022 08:11:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
03/18/2022 08:11:09 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 08:11:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
03/18/2022 08:11:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
03/18/2022 08:11:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=89
03/18/2022 08:11:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=94
03/18/2022 08:11:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
03/18/2022 08:11:22 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=99
03/18/2022 08:11:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.3992490613266583 on epoch=99, global_step=200
03/18/2022 08:11:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=104
03/18/2022 08:11:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
03/18/2022 08:11:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=114
03/18/2022 08:11:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
03/18/2022 08:11:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
03/18/2022 08:11:35 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3816425120772947 on epoch=124
03/18/2022 08:11:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=129
03/18/2022 08:11:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
03/18/2022 08:11:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=139
03/18/2022 08:11:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=144
03/18/2022 08:11:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=149
03/18/2022 08:11:49 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3191489361702127 on epoch=149
03/18/2022 08:11:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=154
03/18/2022 08:11:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/18/2022 08:11:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
03/18/2022 08:11:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
03/18/2022 08:12:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=174
03/18/2022 08:12:02 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3191489361702127 on epoch=174
03/18/2022 08:12:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=179
03/18/2022 08:12:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/18/2022 08:12:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
03/18/2022 08:12:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=194
03/18/2022 08:12:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=199
03/18/2022 08:12:15 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=199
03/18/2022 08:12:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
03/18/2022 08:12:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
03/18/2022 08:12:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=214
03/18/2022 08:12:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=219
03/18/2022 08:12:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=224
03/18/2022 08:12:27 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 08:12:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=229
03/18/2022 08:12:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=234
03/18/2022 08:12:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
03/18/2022 08:12:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=244
03/18/2022 08:12:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=249
03/18/2022 08:12:40 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3522267206477733 on epoch=249
03/18/2022 08:12:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
03/18/2022 08:12:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=259
03/18/2022 08:12:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=264
03/18/2022 08:12:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
03/18/2022 08:12:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
03/18/2022 08:12:53 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.4420512820512821 on epoch=274
03/18/2022 08:12:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4420512820512821 on epoch=274, global_step=550
03/18/2022 08:12:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=279
03/18/2022 08:12:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=284
03/18/2022 08:13:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=289
03/18/2022 08:13:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=294
03/18/2022 08:13:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=299
03/18/2022 08:13:07 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.3191489361702127 on epoch=299
03/18/2022 08:13:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/18/2022 08:13:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=309
03/18/2022 08:13:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=314
03/18/2022 08:13:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=319
03/18/2022 08:13:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=324
03/18/2022 08:13:20 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 08:13:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=329
03/18/2022 08:13:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=334
03/18/2022 08:13:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=339
03/18/2022 08:13:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=344
03/18/2022 08:13:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=349
03/18/2022 08:13:33 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.4589371980676329 on epoch=349
03/18/2022 08:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.4589371980676329 on epoch=349, global_step=700
03/18/2022 08:13:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/18/2022 08:13:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/18/2022 08:13:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=364
03/18/2022 08:13:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=369
03/18/2022 08:13:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
03/18/2022 08:13:46 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.5076923076923077 on epoch=374
03/18/2022 08:13:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5076923076923077 on epoch=374, global_step=750
03/18/2022 08:13:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/18/2022 08:13:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
03/18/2022 08:13:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=389
03/18/2022 08:13:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/18/2022 08:13:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/18/2022 08:13:59 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.5151515151515151 on epoch=399
03/18/2022 08:13:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5151515151515151 on epoch=399, global_step=800
03/18/2022 08:14:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
03/18/2022 08:14:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=409
03/18/2022 08:14:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=414
03/18/2022 08:14:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=419
03/18/2022 08:14:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=424
03/18/2022 08:14:12 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.3992490613266583 on epoch=424
03/18/2022 08:14:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=429
03/18/2022 08:14:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
03/18/2022 08:14:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
03/18/2022 08:14:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
03/18/2022 08:14:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
03/18/2022 08:14:25 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.4920634920634921 on epoch=449
03/18/2022 08:14:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=454
03/18/2022 08:14:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=459
03/18/2022 08:14:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
03/18/2022 08:14:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
03/18/2022 08:14:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
03/18/2022 08:14:38 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.4458874458874459 on epoch=474
03/18/2022 08:14:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/18/2022 08:14:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/18/2022 08:14:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
03/18/2022 08:14:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/18/2022 08:14:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/18/2022 08:14:51 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.41700404858299595 on epoch=499
03/18/2022 08:14:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/18/2022 08:14:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/18/2022 08:14:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/18/2022 08:15:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=519
03/18/2022 08:15:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
03/18/2022 08:15:04 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.4285714285714286 on epoch=524
03/18/2022 08:15:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/18/2022 08:15:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/18/2022 08:15:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/18/2022 08:15:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/18/2022 08:15:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
03/18/2022 08:15:17 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.4682306940371457 on epoch=549
03/18/2022 08:15:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/18/2022 08:15:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=559
03/18/2022 08:15:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/18/2022 08:15:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/18/2022 08:15:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/18/2022 08:15:30 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=574
03/18/2022 08:15:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/18/2022 08:15:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/18/2022 08:15:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/18/2022 08:15:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/18/2022 08:15:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/18/2022 08:15:43 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=599
03/18/2022 08:15:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
03/18/2022 08:15:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/18/2022 08:15:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
03/18/2022 08:15:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/18/2022 08:15:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/18/2022 08:15:56 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.464039408866995 on epoch=624
03/18/2022 08:15:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/18/2022 08:16:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/18/2022 08:16:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/18/2022 08:16:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/18/2022 08:16:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/18/2022 08:16:09 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=649
03/18/2022 08:16:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/18/2022 08:16:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/18/2022 08:16:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/18/2022 08:16:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/18/2022 08:16:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/18/2022 08:16:22 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=674
03/18/2022 08:16:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/18/2022 08:16:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/18/2022 08:16:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/18/2022 08:16:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/18/2022 08:16:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/18/2022 08:16:35 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=699
03/18/2022 08:16:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/18/2022 08:16:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/18/2022 08:16:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 08:16:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/18/2022 08:16:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
03/18/2022 08:16:49 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=724
03/18/2022 08:16:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 08:16:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/18/2022 08:16:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 08:16:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/18/2022 08:17:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 08:17:02 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=749
03/18/2022 08:17:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 08:17:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 08:17:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/18/2022 08:17:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 08:17:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 08:17:15 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=774
03/18/2022 08:17:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 08:17:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 08:17:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
03/18/2022 08:17:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/18/2022 08:17:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/18/2022 08:17:28 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=799
03/18/2022 08:17:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 08:17:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 08:17:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 08:17:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/18/2022 08:17:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/18/2022 08:17:41 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=824
03/18/2022 08:17:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 08:17:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 08:17:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 08:17:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 08:17:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 08:17:55 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=849
03/18/2022 08:17:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 08:18:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 08:18:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 08:18:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 08:18:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 08:18:08 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=874
03/18/2022 08:18:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 08:18:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 08:18:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 08:18:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 08:18:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 08:18:22 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
03/18/2022 08:18:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 08:18:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 08:18:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 08:18:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 08:18:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 08:18:36 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=924
03/18/2022 08:18:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 08:18:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 08:18:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 08:18:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 08:18:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 08:18:49 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=949
03/18/2022 08:18:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 08:18:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 08:18:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 08:18:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 08:19:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/18/2022 08:19:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=974
03/18/2022 08:19:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 08:19:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 08:19:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 08:19:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/18/2022 08:19:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 08:19:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=999
03/18/2022 08:19:16 - INFO - __main__ - save last model!
03/18/2022 08:19:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 08:19:16 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 08:19:16 - INFO - __main__ - Printing 3 examples
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:19:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:19:16 - INFO - __main__ - Printing 3 examples
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 08:19:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:19:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:19:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:19:16 - INFO - __main__ - Printing 3 examples
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:19:16 - INFO - __main__ - ['false']
03/18/2022 08:19:16 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:19:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:19:16 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:19:18 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:19:20 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 08:19:32 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:19:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:19:33 - INFO - __main__ - Starting training!
03/18/2022 08:20:32 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_42_0.2_8_predictions.txt
03/18/2022 08:20:32 - INFO - __main__ - Classification-F1 on test data: 0.4361
03/18/2022 08:20:33 - INFO - __main__ - prefix=wiki_qa_16_42, lr=0.2, bsz=8, dev_performance=0.5151515151515151, test_performance=0.43606308495212526
03/18/2022 08:20:33 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.5, bsz=8 ...
03/18/2022 08:20:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:20:34 - INFO - __main__ - Printing 3 examples
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:20:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:20:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:20:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:20:34 - INFO - __main__ - Printing 3 examples
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:20:34 - INFO - __main__ - ['false']
03/18/2022 08:20:34 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:20:34 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:20:34 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:20:52 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:20:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:20:53 - INFO - __main__ - Starting training!
03/18/2022 08:20:56 - INFO - __main__ - Step 10 Global step 10 Train loss 7.66 on epoch=4
03/18/2022 08:20:58 - INFO - __main__ - Step 20 Global step 20 Train loss 8.04 on epoch=9
03/18/2022 08:21:00 - INFO - __main__ - Step 30 Global step 30 Train loss 8.16 on epoch=14
03/18/2022 08:21:03 - INFO - __main__ - Step 40 Global step 40 Train loss 7.98 on epoch=19
03/18/2022 08:21:05 - INFO - __main__ - Step 50 Global step 50 Train loss 7.99 on epoch=24
03/18/2022 08:21:21 - INFO - __main__ - Global step 50 Train loss 7.97 Classification-F1 0.0 on epoch=24
03/18/2022 08:21:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/18/2022 08:21:24 - INFO - __main__ - Step 60 Global step 60 Train loss 8.03 on epoch=29
03/18/2022 08:21:26 - INFO - __main__ - Step 70 Global step 70 Train loss 7.99 on epoch=34
03/18/2022 08:21:29 - INFO - __main__ - Step 80 Global step 80 Train loss 7.95 on epoch=39
03/18/2022 08:21:31 - INFO - __main__ - Step 90 Global step 90 Train loss 7.90 on epoch=44
03/18/2022 08:21:34 - INFO - __main__ - Step 100 Global step 100 Train loss 7.91 on epoch=49
03/18/2022 08:21:52 - INFO - __main__ - Global step 100 Train loss 7.96 Classification-F1 0.0 on epoch=49
03/18/2022 08:21:55 - INFO - __main__ - Step 110 Global step 110 Train loss 7.75 on epoch=54
03/18/2022 08:21:57 - INFO - __main__ - Step 120 Global step 120 Train loss 7.66 on epoch=59
03/18/2022 08:22:00 - INFO - __main__ - Step 130 Global step 130 Train loss 7.64 on epoch=64
03/18/2022 08:22:02 - INFO - __main__ - Step 140 Global step 140 Train loss 7.69 on epoch=69
03/18/2022 08:22:05 - INFO - __main__ - Step 150 Global step 150 Train loss 7.45 on epoch=74
03/18/2022 08:22:23 - INFO - __main__ - Global step 150 Train loss 7.64 Classification-F1 0.0 on epoch=74
03/18/2022 08:22:26 - INFO - __main__ - Step 160 Global step 160 Train loss 7.43 on epoch=79
03/18/2022 08:22:28 - INFO - __main__ - Step 170 Global step 170 Train loss 7.39 on epoch=84
03/18/2022 08:22:31 - INFO - __main__ - Step 180 Global step 180 Train loss 7.22 on epoch=89
03/18/2022 08:22:33 - INFO - __main__ - Step 190 Global step 190 Train loss 7.30 on epoch=94
03/18/2022 08:22:36 - INFO - __main__ - Step 200 Global step 200 Train loss 7.31 on epoch=99
03/18/2022 08:22:54 - INFO - __main__ - Global step 200 Train loss 7.33 Classification-F1 0.0 on epoch=99
03/18/2022 08:22:57 - INFO - __main__ - Step 210 Global step 210 Train loss 7.21 on epoch=104
03/18/2022 08:22:59 - INFO - __main__ - Step 220 Global step 220 Train loss 7.05 on epoch=109
03/18/2022 08:23:02 - INFO - __main__ - Step 230 Global step 230 Train loss 7.09 on epoch=114
03/18/2022 08:23:04 - INFO - __main__ - Step 240 Global step 240 Train loss 6.91 on epoch=119
03/18/2022 08:23:06 - INFO - __main__ - Step 250 Global step 250 Train loss 6.64 on epoch=124
03/18/2022 08:23:25 - INFO - __main__ - Global step 250 Train loss 6.98 Classification-F1 0.0 on epoch=124
03/18/2022 08:23:27 - INFO - __main__ - Step 260 Global step 260 Train loss 6.48 on epoch=129
03/18/2022 08:23:30 - INFO - __main__ - Step 270 Global step 270 Train loss 6.29 on epoch=134
03/18/2022 08:23:32 - INFO - __main__ - Step 280 Global step 280 Train loss 6.16 on epoch=139
03/18/2022 08:23:35 - INFO - __main__ - Step 290 Global step 290 Train loss 5.78 on epoch=144
03/18/2022 08:23:37 - INFO - __main__ - Step 300 Global step 300 Train loss 6.14 on epoch=149
03/18/2022 08:23:56 - INFO - __main__ - Global step 300 Train loss 6.17 Classification-F1 0.0 on epoch=149
03/18/2022 08:23:58 - INFO - __main__ - Step 310 Global step 310 Train loss 5.76 on epoch=154
03/18/2022 08:24:01 - INFO - __main__ - Step 320 Global step 320 Train loss 5.46 on epoch=159
03/18/2022 08:24:03 - INFO - __main__ - Step 330 Global step 330 Train loss 4.94 on epoch=164
03/18/2022 08:24:05 - INFO - __main__ - Step 340 Global step 340 Train loss 4.54 on epoch=169
03/18/2022 08:24:08 - INFO - __main__ - Step 350 Global step 350 Train loss 4.23 on epoch=174
03/18/2022 08:24:10 - INFO - __main__ - Global step 350 Train loss 4.99 Classification-F1 0.11111111111111112 on epoch=174
03/18/2022 08:24:10 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.11111111111111112 on epoch=174, global_step=350
03/18/2022 08:24:13 - INFO - __main__ - Step 360 Global step 360 Train loss 3.89 on epoch=179
03/18/2022 08:24:15 - INFO - __main__ - Step 370 Global step 370 Train loss 3.36 on epoch=184
03/18/2022 08:24:17 - INFO - __main__ - Step 380 Global step 380 Train loss 2.85 on epoch=189
03/18/2022 08:24:20 - INFO - __main__ - Step 390 Global step 390 Train loss 2.53 on epoch=194
03/18/2022 08:24:22 - INFO - __main__ - Step 400 Global step 400 Train loss 2.28 on epoch=199
03/18/2022 08:24:23 - INFO - __main__ - Global step 400 Train loss 2.98 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 08:24:23 - INFO - __main__ - Saving model with best Classification-F1: 0.11111111111111112 -> 0.3333333333333333 on epoch=199, global_step=400
03/18/2022 08:24:26 - INFO - __main__ - Step 410 Global step 410 Train loss 2.06 on epoch=204
03/18/2022 08:24:28 - INFO - __main__ - Step 420 Global step 420 Train loss 1.79 on epoch=209
03/18/2022 08:24:30 - INFO - __main__ - Step 430 Global step 430 Train loss 1.54 on epoch=214
03/18/2022 08:24:33 - INFO - __main__ - Step 440 Global step 440 Train loss 1.37 on epoch=219
03/18/2022 08:24:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.31 on epoch=224
03/18/2022 08:24:37 - INFO - __main__ - Global step 450 Train loss 1.62 Classification-F1 0.4420512820512821 on epoch=224
03/18/2022 08:24:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4420512820512821 on epoch=224, global_step=450
03/18/2022 08:24:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.12 on epoch=229
03/18/2022 08:24:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.98 on epoch=234
03/18/2022 08:24:44 - INFO - __main__ - Step 480 Global step 480 Train loss 1.28 on epoch=239
03/18/2022 08:24:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.61 on epoch=244
03/18/2022 08:24:49 - INFO - __main__ - Step 500 Global step 500 Train loss 1.06 on epoch=249
03/18/2022 08:24:50 - INFO - __main__ - Global step 500 Train loss 1.21 Classification-F1 0.4666666666666667 on epoch=249
03/18/2022 08:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.4666666666666667 on epoch=249, global_step=500
03/18/2022 08:24:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=254
03/18/2022 08:24:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
03/18/2022 08:24:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=264
03/18/2022 08:25:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=269
03/18/2022 08:25:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
03/18/2022 08:25:03 - INFO - __main__ - Global step 550 Train loss 0.68 Classification-F1 0.4817813765182186 on epoch=274
03/18/2022 08:25:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4817813765182186 on epoch=274, global_step=550
03/18/2022 08:25:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=279
03/18/2022 08:25:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=284
03/18/2022 08:25:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=289
03/18/2022 08:25:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=294
03/18/2022 08:25:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=299
03/18/2022 08:25:16 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.4458874458874459 on epoch=299
03/18/2022 08:25:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=304
03/18/2022 08:25:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=309
03/18/2022 08:25:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=314
03/18/2022 08:25:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=319
03/18/2022 08:25:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=324
03/18/2022 08:25:29 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.4420512820512821 on epoch=324
03/18/2022 08:25:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=329
03/18/2022 08:25:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=334
03/18/2022 08:25:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=339
03/18/2022 08:25:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=344
03/18/2022 08:25:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=349
03/18/2022 08:25:42 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.3816425120772947 on epoch=349
03/18/2022 08:25:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=354
03/18/2022 08:25:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=359
03/18/2022 08:25:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=364
03/18/2022 08:25:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=369
03/18/2022 08:25:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=374
03/18/2022 08:25:54 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.4009852216748768 on epoch=374
03/18/2022 08:25:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
03/18/2022 08:25:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=384
03/18/2022 08:26:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
03/18/2022 08:26:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=394
03/18/2022 08:26:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=399
03/18/2022 08:26:07 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.3816425120772947 on epoch=399
03/18/2022 08:26:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
03/18/2022 08:26:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=409
03/18/2022 08:26:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=414
03/18/2022 08:26:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=419
03/18/2022 08:26:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=424
03/18/2022 08:26:20 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.3816425120772947 on epoch=424
03/18/2022 08:26:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=429
03/18/2022 08:26:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
03/18/2022 08:26:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=439
03/18/2022 08:26:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
03/18/2022 08:26:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=449
03/18/2022 08:26:32 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=449
03/18/2022 08:26:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=454
03/18/2022 08:26:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=459
03/18/2022 08:26:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=464
03/18/2022 08:26:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=469
03/18/2022 08:26:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/18/2022 08:26:45 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.3816425120772947 on epoch=474
03/18/2022 08:26:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=479
03/18/2022 08:26:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=484
03/18/2022 08:26:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=489
03/18/2022 08:26:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=494
03/18/2022 08:26:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=499
03/18/2022 08:26:58 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.3816425120772947 on epoch=499
03/18/2022 08:27:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=504
03/18/2022 08:27:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=509
03/18/2022 08:27:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=514
03/18/2022 08:27:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=519
03/18/2022 08:27:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=524
03/18/2022 08:27:11 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.3816425120772947 on epoch=524
03/18/2022 08:27:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=529
03/18/2022 08:27:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=534
03/18/2022 08:27:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=539
03/18/2022 08:27:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=544
03/18/2022 08:27:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=549
03/18/2022 08:27:23 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.4385964912280702 on epoch=549
03/18/2022 08:27:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=554
03/18/2022 08:27:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=559
03/18/2022 08:27:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
03/18/2022 08:27:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=569
03/18/2022 08:27:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=574
03/18/2022 08:27:36 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.3816425120772947 on epoch=574
03/18/2022 08:27:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=579
03/18/2022 08:27:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=584
03/18/2022 08:27:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
03/18/2022 08:27:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=594
03/18/2022 08:27:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=599
03/18/2022 08:27:49 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=599
03/18/2022 08:27:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=604
03/18/2022 08:27:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=609
03/18/2022 08:27:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=614
03/18/2022 08:27:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=619
03/18/2022 08:28:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=624
03/18/2022 08:28:02 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.3552492046659597 on epoch=624
03/18/2022 08:28:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=629
03/18/2022 08:28:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
03/18/2022 08:28:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=639
03/18/2022 08:28:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
03/18/2022 08:28:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=649
03/18/2022 08:28:15 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.3992490613266583 on epoch=649
03/18/2022 08:28:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
03/18/2022 08:28:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=659
03/18/2022 08:28:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=664
03/18/2022 08:28:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
03/18/2022 08:28:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
03/18/2022 08:28:27 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=674
03/18/2022 08:28:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=679
03/18/2022 08:28:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=684
03/18/2022 08:28:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
03/18/2022 08:28:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=694
03/18/2022 08:28:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=699
03/18/2022 08:28:40 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=699
03/18/2022 08:28:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=704
03/18/2022 08:28:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
03/18/2022 08:28:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
03/18/2022 08:28:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
03/18/2022 08:28:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
03/18/2022 08:28:53 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=724
03/18/2022 08:28:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=729
03/18/2022 08:28:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=734
03/18/2022 08:29:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=739
03/18/2022 08:29:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
03/18/2022 08:29:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=749
03/18/2022 08:29:06 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3266888150609081 on epoch=749
03/18/2022 08:29:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=754
03/18/2022 08:29:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=759
03/18/2022 08:29:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=764
03/18/2022 08:29:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=769
03/18/2022 08:29:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=774
03/18/2022 08:29:19 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.3992490613266583 on epoch=774
03/18/2022 08:29:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
03/18/2022 08:29:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
03/18/2022 08:29:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
03/18/2022 08:29:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=794
03/18/2022 08:29:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
03/18/2022 08:29:31 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=799
03/18/2022 08:29:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
03/18/2022 08:29:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/18/2022 08:29:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=814
03/18/2022 08:29:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
03/18/2022 08:29:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=824
03/18/2022 08:29:44 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=824
03/18/2022 08:29:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=829
03/18/2022 08:29:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
03/18/2022 08:29:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=839
03/18/2022 08:29:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
03/18/2022 08:29:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=849
03/18/2022 08:29:57 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=849
03/18/2022 08:30:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
03/18/2022 08:30:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=859
03/18/2022 08:30:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
03/18/2022 08:30:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=869
03/18/2022 08:30:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
03/18/2022 08:30:10 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3816425120772947 on epoch=874
03/18/2022 08:30:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=879
03/18/2022 08:30:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=884
03/18/2022 08:30:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
03/18/2022 08:30:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
03/18/2022 08:30:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=899
03/18/2022 08:30:23 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.41700404858299595 on epoch=899
03/18/2022 08:30:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=904
03/18/2022 08:30:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=909
03/18/2022 08:30:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=914
03/18/2022 08:30:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
03/18/2022 08:30:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
03/18/2022 08:30:36 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.36374269005847953 on epoch=924
03/18/2022 08:30:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
03/18/2022 08:30:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.45 on epoch=934
03/18/2022 08:30:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=939
03/18/2022 08:30:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=944
03/18/2022 08:30:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=949
03/18/2022 08:30:49 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3992490613266583 on epoch=949
03/18/2022 08:30:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=954
03/18/2022 08:30:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=959
03/18/2022 08:30:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
03/18/2022 08:30:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
03/18/2022 08:31:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
03/18/2022 08:31:02 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.46843853820598 on epoch=974
03/18/2022 08:31:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=979
03/18/2022 08:31:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
03/18/2022 08:31:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
03/18/2022 08:31:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=994
03/18/2022 08:31:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
03/18/2022 08:31:15 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.5588547189819725 on epoch=999
03/18/2022 08:31:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4817813765182186 -> 0.5588547189819725 on epoch=999, global_step=2000
03/18/2022 08:31:15 - INFO - __main__ - save last model!
03/18/2022 08:31:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 08:31:15 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 08:31:15 - INFO - __main__ - Printing 3 examples
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:31:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:31:15 - INFO - __main__ - Printing 3 examples
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 08:31:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:31:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:31:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:31:15 - INFO - __main__ - Printing 3 examples
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:31:15 - INFO - __main__ - ['false']
03/18/2022 08:31:15 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:31:15 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:31:15 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:31:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:31:19 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 08:31:31 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:31:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:31:31 - INFO - __main__ - Starting training!
03/18/2022 08:32:09 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_87_0.5_8_predictions.txt
03/18/2022 08:32:09 - INFO - __main__ - Classification-F1 on test data: 0.4038
03/18/2022 08:32:09 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.5, bsz=8, dev_performance=0.5588547189819725, test_performance=0.4038292291132999
03/18/2022 08:32:09 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.4, bsz=8 ...
03/18/2022 08:32:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:32:10 - INFO - __main__ - Printing 3 examples
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:32:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:32:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:32:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:32:10 - INFO - __main__ - Printing 3 examples
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:32:10 - INFO - __main__ - ['false']
03/18/2022 08:32:10 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:32:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:32:10 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:32:26 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:32:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:32:27 - INFO - __main__ - Starting training!
03/18/2022 08:32:30 - INFO - __main__ - Step 10 Global step 10 Train loss 7.53 on epoch=4
03/18/2022 08:32:33 - INFO - __main__ - Step 20 Global step 20 Train loss 7.61 on epoch=9
03/18/2022 08:32:35 - INFO - __main__ - Step 30 Global step 30 Train loss 7.13 on epoch=14
03/18/2022 08:32:37 - INFO - __main__ - Step 40 Global step 40 Train loss 6.82 on epoch=19
03/18/2022 08:32:40 - INFO - __main__ - Step 50 Global step 50 Train loss 6.84 on epoch=24
03/18/2022 08:32:58 - INFO - __main__ - Global step 50 Train loss 7.19 Classification-F1 0.0 on epoch=24
03/18/2022 08:32:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/18/2022 08:33:01 - INFO - __main__ - Step 60 Global step 60 Train loss 6.29 on epoch=29
03/18/2022 08:33:03 - INFO - __main__ - Step 70 Global step 70 Train loss 5.57 on epoch=34
03/18/2022 08:33:05 - INFO - __main__ - Step 80 Global step 80 Train loss 6.02 on epoch=39
03/18/2022 08:33:08 - INFO - __main__ - Step 90 Global step 90 Train loss 5.96 on epoch=44
03/18/2022 08:33:10 - INFO - __main__ - Step 100 Global step 100 Train loss 6.35 on epoch=49
03/18/2022 08:33:23 - INFO - __main__ - Global step 100 Train loss 6.04 Classification-F1 0.0 on epoch=49
03/18/2022 08:33:26 - INFO - __main__ - Step 110 Global step 110 Train loss 7.37 on epoch=54
03/18/2022 08:33:28 - INFO - __main__ - Step 120 Global step 120 Train loss 7.44 on epoch=59
03/18/2022 08:33:31 - INFO - __main__ - Step 130 Global step 130 Train loss 7.29 on epoch=64
03/18/2022 08:33:33 - INFO - __main__ - Step 140 Global step 140 Train loss 7.28 on epoch=69
03/18/2022 08:33:35 - INFO - __main__ - Step 150 Global step 150 Train loss 6.87 on epoch=74
03/18/2022 08:33:46 - INFO - __main__ - Global step 150 Train loss 7.25 Classification-F1 0.0 on epoch=74
03/18/2022 08:33:48 - INFO - __main__ - Step 160 Global step 160 Train loss 6.96 on epoch=79
03/18/2022 08:33:51 - INFO - __main__ - Step 170 Global step 170 Train loss 6.86 on epoch=84
03/18/2022 08:33:53 - INFO - __main__ - Step 180 Global step 180 Train loss 7.08 on epoch=89
03/18/2022 08:33:55 - INFO - __main__ - Step 190 Global step 190 Train loss 7.09 on epoch=94
03/18/2022 08:33:58 - INFO - __main__ - Step 200 Global step 200 Train loss 6.79 on epoch=99
03/18/2022 08:34:16 - INFO - __main__ - Global step 200 Train loss 6.95 Classification-F1 0.0 on epoch=99
03/18/2022 08:34:18 - INFO - __main__ - Step 210 Global step 210 Train loss 6.76 on epoch=104
03/18/2022 08:34:20 - INFO - __main__ - Step 220 Global step 220 Train loss 6.36 on epoch=109
03/18/2022 08:34:23 - INFO - __main__ - Step 230 Global step 230 Train loss 6.14 on epoch=114
03/18/2022 08:34:25 - INFO - __main__ - Step 240 Global step 240 Train loss 6.42 on epoch=119
03/18/2022 08:34:28 - INFO - __main__ - Step 250 Global step 250 Train loss 6.11 on epoch=124
03/18/2022 08:34:29 - INFO - __main__ - Global step 250 Train loss 6.36 Classification-F1 0.0 on epoch=124
03/18/2022 08:34:31 - INFO - __main__ - Step 260 Global step 260 Train loss 5.60 on epoch=129
03/18/2022 08:34:34 - INFO - __main__ - Step 270 Global step 270 Train loss 5.27 on epoch=134
03/18/2022 08:34:36 - INFO - __main__ - Step 280 Global step 280 Train loss 5.31 on epoch=139
03/18/2022 08:34:39 - INFO - __main__ - Step 290 Global step 290 Train loss 5.33 on epoch=144
03/18/2022 08:34:41 - INFO - __main__ - Step 300 Global step 300 Train loss 4.78 on epoch=149
03/18/2022 08:34:45 - INFO - __main__ - Global step 300 Train loss 5.26 Classification-F1 0.027777777777777776 on epoch=149
03/18/2022 08:34:45 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.027777777777777776 on epoch=149, global_step=300
03/18/2022 08:34:48 - INFO - __main__ - Step 310 Global step 310 Train loss 4.38 on epoch=154
03/18/2022 08:34:50 - INFO - __main__ - Step 320 Global step 320 Train loss 4.30 on epoch=159
03/18/2022 08:34:53 - INFO - __main__ - Step 330 Global step 330 Train loss 4.09 on epoch=164
03/18/2022 08:34:55 - INFO - __main__ - Step 340 Global step 340 Train loss 4.03 on epoch=169
03/18/2022 08:34:57 - INFO - __main__ - Step 350 Global step 350 Train loss 4.02 on epoch=174
03/18/2022 08:35:00 - INFO - __main__ - Global step 350 Train loss 4.16 Classification-F1 0.037296037296037296 on epoch=174
03/18/2022 08:35:00 - INFO - __main__ - Saving model with best Classification-F1: 0.027777777777777776 -> 0.037296037296037296 on epoch=174, global_step=350
03/18/2022 08:35:03 - INFO - __main__ - Step 360 Global step 360 Train loss 3.90 on epoch=179
03/18/2022 08:35:05 - INFO - __main__ - Step 370 Global step 370 Train loss 3.82 on epoch=184
03/18/2022 08:35:08 - INFO - __main__ - Step 380 Global step 380 Train loss 3.71 on epoch=189
03/18/2022 08:35:10 - INFO - __main__ - Step 390 Global step 390 Train loss 3.46 on epoch=194
03/18/2022 08:35:12 - INFO - __main__ - Step 400 Global step 400 Train loss 3.33 on epoch=199
03/18/2022 08:35:15 - INFO - __main__ - Global step 400 Train loss 3.64 Classification-F1 0.13023255813953488 on epoch=199
03/18/2022 08:35:15 - INFO - __main__ - Saving model with best Classification-F1: 0.037296037296037296 -> 0.13023255813953488 on epoch=199, global_step=400
03/18/2022 08:35:17 - INFO - __main__ - Step 410 Global step 410 Train loss 3.12 on epoch=204
03/18/2022 08:35:20 - INFO - __main__ - Step 420 Global step 420 Train loss 3.13 on epoch=209
03/18/2022 08:35:22 - INFO - __main__ - Step 430 Global step 430 Train loss 3.00 on epoch=214
03/18/2022 08:35:25 - INFO - __main__ - Step 440 Global step 440 Train loss 3.05 on epoch=219
03/18/2022 08:35:27 - INFO - __main__ - Step 450 Global step 450 Train loss 2.69 on epoch=224
03/18/2022 08:35:29 - INFO - __main__ - Global step 450 Train loss 3.00 Classification-F1 0.1739130434782609 on epoch=224
03/18/2022 08:35:29 - INFO - __main__ - Saving model with best Classification-F1: 0.13023255813953488 -> 0.1739130434782609 on epoch=224, global_step=450
03/18/2022 08:35:32 - INFO - __main__ - Step 460 Global step 460 Train loss 2.79 on epoch=229
03/18/2022 08:35:34 - INFO - __main__ - Step 470 Global step 470 Train loss 2.69 on epoch=234
03/18/2022 08:35:36 - INFO - __main__ - Step 480 Global step 480 Train loss 2.67 on epoch=239
03/18/2022 08:35:39 - INFO - __main__ - Step 490 Global step 490 Train loss 2.73 on epoch=244
03/18/2022 08:35:41 - INFO - __main__ - Step 500 Global step 500 Train loss 2.31 on epoch=249
03/18/2022 08:35:46 - INFO - __main__ - Global step 500 Train loss 2.64 Classification-F1 0.11555555555555555 on epoch=249
03/18/2022 08:35:48 - INFO - __main__ - Step 510 Global step 510 Train loss 2.36 on epoch=254
03/18/2022 08:35:51 - INFO - __main__ - Step 520 Global step 520 Train loss 2.49 on epoch=259
03/18/2022 08:35:53 - INFO - __main__ - Step 530 Global step 530 Train loss 2.17 on epoch=264
03/18/2022 08:35:55 - INFO - __main__ - Step 540 Global step 540 Train loss 2.17 on epoch=269
03/18/2022 08:35:58 - INFO - __main__ - Step 550 Global step 550 Train loss 2.19 on epoch=274
03/18/2022 08:36:09 - INFO - __main__ - Global step 550 Train loss 2.28 Classification-F1 0.09848484848484847 on epoch=274
03/18/2022 08:36:11 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
03/18/2022 08:36:14 - INFO - __main__ - Step 570 Global step 570 Train loss 1.95 on epoch=284
03/18/2022 08:36:16 - INFO - __main__ - Step 580 Global step 580 Train loss 1.85 on epoch=289
03/18/2022 08:36:18 - INFO - __main__ - Step 590 Global step 590 Train loss 1.93 on epoch=294
03/18/2022 08:36:21 - INFO - __main__ - Step 600 Global step 600 Train loss 1.89 on epoch=299
03/18/2022 08:36:24 - INFO - __main__ - Global step 600 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 08:36:24 - INFO - __main__ - Saving model with best Classification-F1: 0.1739130434782609 -> 0.3333333333333333 on epoch=299, global_step=600
03/18/2022 08:36:26 - INFO - __main__ - Step 610 Global step 610 Train loss 1.70 on epoch=304
03/18/2022 08:36:29 - INFO - __main__ - Step 620 Global step 620 Train loss 1.55 on epoch=309
03/18/2022 08:36:31 - INFO - __main__ - Step 630 Global step 630 Train loss 1.59 on epoch=314
03/18/2022 08:36:33 - INFO - __main__ - Step 640 Global step 640 Train loss 1.54 on epoch=319
03/18/2022 08:36:36 - INFO - __main__ - Step 650 Global step 650 Train loss 1.57 on epoch=324
03/18/2022 08:36:38 - INFO - __main__ - Global step 650 Train loss 1.59 Classification-F1 0.4666666666666667 on epoch=324
03/18/2022 08:36:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4666666666666667 on epoch=324, global_step=650
03/18/2022 08:36:40 - INFO - __main__ - Step 660 Global step 660 Train loss 1.35 on epoch=329
03/18/2022 08:36:43 - INFO - __main__ - Step 670 Global step 670 Train loss 1.33 on epoch=334
03/18/2022 08:36:45 - INFO - __main__ - Step 680 Global step 680 Train loss 1.32 on epoch=339
03/18/2022 08:36:47 - INFO - __main__ - Step 690 Global step 690 Train loss 1.24 on epoch=344
03/18/2022 08:36:50 - INFO - __main__ - Step 700 Global step 700 Train loss 1.13 on epoch=349
03/18/2022 08:36:50 - INFO - __main__ - Global step 700 Train loss 1.27 Classification-F1 0.4375 on epoch=349
03/18/2022 08:36:53 - INFO - __main__ - Step 710 Global step 710 Train loss 1.13 on epoch=354
03/18/2022 08:36:55 - INFO - __main__ - Step 720 Global step 720 Train loss 1.01 on epoch=359
03/18/2022 08:36:58 - INFO - __main__ - Step 730 Global step 730 Train loss 1.06 on epoch=364
03/18/2022 08:37:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.97 on epoch=369
03/18/2022 08:37:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.99 on epoch=374
03/18/2022 08:37:03 - INFO - __main__ - Global step 750 Train loss 1.03 Classification-F1 0.37254901960784315 on epoch=374
03/18/2022 08:37:05 - INFO - __main__ - Step 760 Global step 760 Train loss 1.00 on epoch=379
03/18/2022 08:37:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=384
03/18/2022 08:37:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.93 on epoch=389
03/18/2022 08:37:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.91 on epoch=394
03/18/2022 08:37:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.76 on epoch=399
03/18/2022 08:37:15 - INFO - __main__ - Global step 800 Train loss 0.90 Classification-F1 0.375 on epoch=399
03/18/2022 08:37:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=404
03/18/2022 08:37:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.82 on epoch=409
03/18/2022 08:37:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.80 on epoch=414
03/18/2022 08:37:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.82 on epoch=419
03/18/2022 08:37:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.62 on epoch=424
03/18/2022 08:37:28 - INFO - __main__ - Global step 850 Train loss 0.79 Classification-F1 0.4666666666666667 on epoch=424
03/18/2022 08:37:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=429
03/18/2022 08:37:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.71 on epoch=434
03/18/2022 08:37:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.73 on epoch=439
03/18/2022 08:37:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.75 on epoch=444
03/18/2022 08:37:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.73 on epoch=449
03/18/2022 08:37:40 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.46843853820598 on epoch=449
03/18/2022 08:37:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.46843853820598 on epoch=449, global_step=900
03/18/2022 08:37:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.63 on epoch=454
03/18/2022 08:37:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.72 on epoch=459
03/18/2022 08:37:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.65 on epoch=464
03/18/2022 08:37:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.62 on epoch=469
03/18/2022 08:37:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.62 on epoch=474
03/18/2022 08:37:53 - INFO - __main__ - Global step 950 Train loss 0.65 Classification-F1 0.4909862142099682 on epoch=474
03/18/2022 08:37:53 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.4909862142099682 on epoch=474, global_step=950
03/18/2022 08:37:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.71 on epoch=479
03/18/2022 08:37:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.66 on epoch=484
03/18/2022 08:38:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.66 on epoch=489
03/18/2022 08:38:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.69 on epoch=494
03/18/2022 08:38:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.58 on epoch=499
03/18/2022 08:38:06 - INFO - __main__ - Global step 1000 Train loss 0.66 Classification-F1 0.4181818181818182 on epoch=499
03/18/2022 08:38:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.60 on epoch=504
03/18/2022 08:38:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=509
03/18/2022 08:38:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.64 on epoch=514
03/18/2022 08:38:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.61 on epoch=519
03/18/2022 08:38:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.58 on epoch=524
03/18/2022 08:38:18 - INFO - __main__ - Global step 1050 Train loss 0.60 Classification-F1 0.5195195195195195 on epoch=524
03/18/2022 08:38:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4909862142099682 -> 0.5195195195195195 on epoch=524, global_step=1050
03/18/2022 08:38:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.60 on epoch=529
03/18/2022 08:38:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.58 on epoch=534
03/18/2022 08:38:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.56 on epoch=539
03/18/2022 08:38:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=544
03/18/2022 08:38:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.57 on epoch=549
03/18/2022 08:38:31 - INFO - __main__ - Global step 1100 Train loss 0.58 Classification-F1 0.36374269005847953 on epoch=549
03/18/2022 08:38:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=554
03/18/2022 08:38:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.55 on epoch=559
03/18/2022 08:38:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=564
03/18/2022 08:38:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=569
03/18/2022 08:38:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=574
03/18/2022 08:38:43 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.36374269005847953 on epoch=574
03/18/2022 08:38:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.56 on epoch=579
03/18/2022 08:38:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.49 on epoch=584
03/18/2022 08:38:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.57 on epoch=589
03/18/2022 08:38:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
03/18/2022 08:38:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=599
03/18/2022 08:38:56 - INFO - __main__ - Global step 1200 Train loss 0.53 Classification-F1 0.375 on epoch=599
03/18/2022 08:38:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.50 on epoch=604
03/18/2022 08:39:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.60 on epoch=609
03/18/2022 08:39:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.61 on epoch=614
03/18/2022 08:39:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=619
03/18/2022 08:39:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=624
03/18/2022 08:39:08 - INFO - __main__ - Global step 1250 Train loss 0.54 Classification-F1 0.4458874458874459 on epoch=624
03/18/2022 08:39:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=629
03/18/2022 08:39:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=634
03/18/2022 08:39:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=639
03/18/2022 08:39:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.58 on epoch=644
03/18/2022 08:39:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/18/2022 08:39:21 - INFO - __main__ - Global step 1300 Train loss 0.55 Classification-F1 0.5195195195195195 on epoch=649
03/18/2022 08:39:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=654
03/18/2022 08:39:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=659
03/18/2022 08:39:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=664
03/18/2022 08:39:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.48 on epoch=669
03/18/2022 08:39:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.50 on epoch=674
03/18/2022 08:39:33 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.37662337662337664 on epoch=674
03/18/2022 08:39:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=679
03/18/2022 08:39:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
03/18/2022 08:39:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.52 on epoch=689
03/18/2022 08:39:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=694
03/18/2022 08:39:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=699
03/18/2022 08:39:46 - INFO - __main__ - Global step 1400 Train loss 0.51 Classification-F1 0.5333333333333333 on epoch=699
03/18/2022 08:39:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5333333333333333 on epoch=699, global_step=1400
03/18/2022 08:39:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.51 on epoch=704
03/18/2022 08:39:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=709
03/18/2022 08:39:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=714
03/18/2022 08:39:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.52 on epoch=719
03/18/2022 08:39:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=724
03/18/2022 08:39:58 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.3816425120772947 on epoch=724
03/18/2022 08:40:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=729
03/18/2022 08:40:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.64 on epoch=734
03/18/2022 08:40:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=739
03/18/2022 08:40:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=744
03/18/2022 08:40:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
03/18/2022 08:40:11 - INFO - __main__ - Global step 1500 Train loss 0.53 Classification-F1 0.3816425120772947 on epoch=749
03/18/2022 08:40:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=754
03/18/2022 08:40:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=759
03/18/2022 08:40:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
03/18/2022 08:40:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=769
03/18/2022 08:40:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.47 on epoch=774
03/18/2022 08:40:23 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.36374269005847953 on epoch=774
03/18/2022 08:40:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
03/18/2022 08:40:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
03/18/2022 08:40:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=789
03/18/2022 08:40:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=794
03/18/2022 08:40:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=799
03/18/2022 08:40:36 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.5 on epoch=799
03/18/2022 08:40:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
03/18/2022 08:40:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=809
03/18/2022 08:40:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.50 on epoch=814
03/18/2022 08:40:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.51 on epoch=819
03/18/2022 08:40:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
03/18/2022 08:40:48 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.4181818181818182 on epoch=824
03/18/2022 08:40:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=829
03/18/2022 08:40:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
03/18/2022 08:40:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.54 on epoch=839
03/18/2022 08:40:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=844
03/18/2022 08:41:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=849
03/18/2022 08:41:01 - INFO - __main__ - Global step 1700 Train loss 0.50 Classification-F1 0.3816425120772947 on epoch=849
03/18/2022 08:41:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=854
03/18/2022 08:41:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.51 on epoch=859
03/18/2022 08:41:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=864
03/18/2022 08:41:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
03/18/2022 08:41:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=874
03/18/2022 08:41:13 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.4231177094379639 on epoch=874
03/18/2022 08:41:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
03/18/2022 08:41:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=884
03/18/2022 08:41:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
03/18/2022 08:41:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
03/18/2022 08:41:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=899
03/18/2022 08:41:26 - INFO - __main__ - Global step 1800 Train loss 0.47 Classification-F1 0.5555555555555556 on epoch=899
03/18/2022 08:41:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5555555555555556 on epoch=899, global_step=1800
03/18/2022 08:41:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=904
03/18/2022 08:41:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=909
03/18/2022 08:41:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.51 on epoch=914
03/18/2022 08:41:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=919
03/18/2022 08:41:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=924
03/18/2022 08:41:38 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.5555555555555556 on epoch=924
03/18/2022 08:41:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=929
03/18/2022 08:41:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.51 on epoch=934
03/18/2022 08:41:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=939
03/18/2022 08:41:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=944
03/18/2022 08:41:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.51 on epoch=949
03/18/2022 08:41:51 - INFO - __main__ - Global step 1900 Train loss 0.49 Classification-F1 0.4980392156862745 on epoch=949
03/18/2022 08:41:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=954
03/18/2022 08:41:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.48 on epoch=959
03/18/2022 08:41:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
03/18/2022 08:42:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=969
03/18/2022 08:42:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=974
03/18/2022 08:42:03 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=974
03/18/2022 08:42:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
03/18/2022 08:42:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
03/18/2022 08:42:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=989
03/18/2022 08:42:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=994
03/18/2022 08:42:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
03/18/2022 08:42:16 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.4817813765182186 on epoch=999
03/18/2022 08:42:16 - INFO - __main__ - save last model!
03/18/2022 08:42:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 08:42:16 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 08:42:16 - INFO - __main__ - Printing 3 examples
03/18/2022 08:42:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 08:42:16 - INFO - __main__ - ['false']
03/18/2022 08:42:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 08:42:16 - INFO - __main__ - ['false']
03/18/2022 08:42:16 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 08:42:16 - INFO - __main__ - ['false']
03/18/2022 08:42:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:42:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:42:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:42:17 - INFO - __main__ - Printing 3 examples
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 08:42:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:42:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:42:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:42:17 - INFO - __main__ - Printing 3 examples
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:42:17 - INFO - __main__ - ['false']
03/18/2022 08:42:17 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:42:17 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:42:17 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:42:20 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 08:42:36 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:42:37 - INFO - __main__ - Starting training!
03/18/2022 08:43:01 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_87_0.4_8_predictions.txt
03/18/2022 08:43:01 - INFO - __main__ - Classification-F1 on test data: 0.2237
03/18/2022 08:43:01 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.4, bsz=8, dev_performance=0.5555555555555556, test_performance=0.22365418624627362
03/18/2022 08:43:01 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.3, bsz=8 ...
03/18/2022 08:43:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:43:02 - INFO - __main__ - Printing 3 examples
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:43:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:43:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:43:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:43:02 - INFO - __main__ - Printing 3 examples
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:43:02 - INFO - __main__ - ['false']
03/18/2022 08:43:02 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:43:02 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:43:02 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:43:20 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:43:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:43:21 - INFO - __main__ - Starting training!
03/18/2022 08:43:24 - INFO - __main__ - Step 10 Global step 10 Train loss 7.22 on epoch=4
03/18/2022 08:43:27 - INFO - __main__ - Step 20 Global step 20 Train loss 4.55 on epoch=9
03/18/2022 08:43:29 - INFO - __main__ - Step 30 Global step 30 Train loss 1.30 on epoch=14
03/18/2022 08:43:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.70 on epoch=19
03/18/2022 08:43:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=24
03/18/2022 08:43:35 - INFO - __main__ - Global step 50 Train loss 2.87 Classification-F1 0.3191489361702127 on epoch=24
03/18/2022 08:43:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
03/18/2022 08:43:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
03/18/2022 08:43:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
03/18/2022 08:43:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
03/18/2022 08:43:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=44
03/18/2022 08:43:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=49
03/18/2022 08:43:48 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 08:43:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3333333333333333 on epoch=49, global_step=100
03/18/2022 08:43:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
03/18/2022 08:43:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=59
03/18/2022 08:43:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
03/18/2022 08:43:58 - INFO - __main__ - Step 140 Global step 140 Train loss 1.23 on epoch=69
03/18/2022 08:44:00 - INFO - __main__ - Step 150 Global step 150 Train loss 2.91 on epoch=74
03/18/2022 08:44:01 - INFO - __main__ - Global step 150 Train loss 1.11 Classification-F1 0.3992490613266583 on epoch=74
03/18/2022 08:44:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=74, global_step=150
03/18/2022 08:44:03 - INFO - __main__ - Step 160 Global step 160 Train loss 3.66 on epoch=79
03/18/2022 08:44:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.83 on epoch=84
03/18/2022 08:44:08 - INFO - __main__ - Step 180 Global step 180 Train loss 2.57 on epoch=89
03/18/2022 08:44:10 - INFO - __main__ - Step 190 Global step 190 Train loss 2.61 on epoch=94
03/18/2022 08:44:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.01 on epoch=99
03/18/2022 08:44:13 - INFO - __main__ - Global step 200 Train loss 2.74 Classification-F1 0.3333333333333333 on epoch=99
03/18/2022 08:44:16 - INFO - __main__ - Step 210 Global step 210 Train loss 3.02 on epoch=104
03/18/2022 08:44:18 - INFO - __main__ - Step 220 Global step 220 Train loss 5.36 on epoch=109
03/18/2022 08:44:21 - INFO - __main__ - Step 230 Global step 230 Train loss 6.38 on epoch=114
03/18/2022 08:44:23 - INFO - __main__ - Step 240 Global step 240 Train loss 5.22 on epoch=119
03/18/2022 08:44:26 - INFO - __main__ - Step 250 Global step 250 Train loss 3.09 on epoch=124
03/18/2022 08:44:29 - INFO - __main__ - Global step 250 Train loss 4.61 Classification-F1 0.047314578005115085 on epoch=124
03/18/2022 08:44:31 - INFO - __main__ - Step 260 Global step 260 Train loss 3.33 on epoch=129
03/18/2022 08:44:34 - INFO - __main__ - Step 270 Global step 270 Train loss 1.59 on epoch=134
03/18/2022 08:44:36 - INFO - __main__ - Step 280 Global step 280 Train loss 1.34 on epoch=139
03/18/2022 08:44:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.00 on epoch=144
03/18/2022 08:44:41 - INFO - __main__ - Step 300 Global step 300 Train loss 1.64 on epoch=149
03/18/2022 08:44:42 - INFO - __main__ - Global step 300 Train loss 1.78 Classification-F1 0.3043478260869565 on epoch=149
03/18/2022 08:44:44 - INFO - __main__ - Step 310 Global step 310 Train loss 1.05 on epoch=154
03/18/2022 08:44:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.97 on epoch=159
03/18/2022 08:44:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=164
03/18/2022 08:44:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
03/18/2022 08:44:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.91 on epoch=174
03/18/2022 08:44:54 - INFO - __main__ - Global step 350 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=174
03/18/2022 08:44:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.75 on epoch=179
03/18/2022 08:44:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=184
03/18/2022 08:45:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=189
03/18/2022 08:45:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=194
03/18/2022 08:45:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=199
03/18/2022 08:45:06 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.3333333333333333 on epoch=199
03/18/2022 08:45:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=204
03/18/2022 08:45:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=209
03/18/2022 08:45:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=214
03/18/2022 08:45:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=219
03/18/2022 08:45:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=224
03/18/2022 08:45:18 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=224
03/18/2022 08:45:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=229
03/18/2022 08:45:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.57 on epoch=234
03/18/2022 08:45:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=239
03/18/2022 08:45:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=244
03/18/2022 08:45:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=249
03/18/2022 08:45:30 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=249
03/18/2022 08:45:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=254
03/18/2022 08:45:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=259
03/18/2022 08:45:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=264
03/18/2022 08:45:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=269
03/18/2022 08:45:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=274
03/18/2022 08:45:42 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=274
03/18/2022 08:45:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=279
03/18/2022 08:45:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=284
03/18/2022 08:45:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=289
03/18/2022 08:45:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=294
03/18/2022 08:45:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=299
03/18/2022 08:45:54 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=299
03/18/2022 08:45:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=304
03/18/2022 08:45:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=309
03/18/2022 08:46:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=314
03/18/2022 08:46:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=319
03/18/2022 08:46:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=324
03/18/2022 08:46:06 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=324
03/18/2022 08:46:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=329
03/18/2022 08:46:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=334
03/18/2022 08:46:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=339
03/18/2022 08:46:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=344
03/18/2022 08:46:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
03/18/2022 08:46:19 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=349
03/18/2022 08:46:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=354
03/18/2022 08:46:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=359
03/18/2022 08:46:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.55 on epoch=364
03/18/2022 08:46:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.56 on epoch=369
03/18/2022 08:46:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=374
03/18/2022 08:46:32 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=374
03/18/2022 08:46:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=379
03/18/2022 08:46:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
03/18/2022 08:46:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=389
03/18/2022 08:46:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=394
03/18/2022 08:46:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=399
03/18/2022 08:46:45 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=399
03/18/2022 08:46:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=404
03/18/2022 08:46:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=409
03/18/2022 08:46:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=414
03/18/2022 08:46:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=419
03/18/2022 08:46:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=424
03/18/2022 08:46:58 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=424
03/18/2022 08:47:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
03/18/2022 08:47:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=434
03/18/2022 08:47:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
03/18/2022 08:47:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=444
03/18/2022 08:47:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=449
03/18/2022 08:47:11 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=449
03/18/2022 08:47:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=454
03/18/2022 08:47:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=459
03/18/2022 08:47:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/18/2022 08:47:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=469
03/18/2022 08:47:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=474
03/18/2022 08:47:24 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=474
03/18/2022 08:47:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
03/18/2022 08:47:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=484
03/18/2022 08:47:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
03/18/2022 08:47:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=494
03/18/2022 08:47:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=499
03/18/2022 08:47:37 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=499
03/18/2022 08:47:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=504
03/18/2022 08:47:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=509
03/18/2022 08:47:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=514
03/18/2022 08:47:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=519
03/18/2022 08:47:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=524
03/18/2022 08:47:50 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=524
03/18/2022 08:47:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
03/18/2022 08:47:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=534
03/18/2022 08:47:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
03/18/2022 08:48:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=544
03/18/2022 08:48:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=549
03/18/2022 08:48:03 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=549
03/18/2022 08:48:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=554
03/18/2022 08:48:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=559
03/18/2022 08:48:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=564
03/18/2022 08:48:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=569
03/18/2022 08:48:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=574
03/18/2022 08:48:16 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=574
03/18/2022 08:48:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
03/18/2022 08:48:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=584
03/18/2022 08:48:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=589
03/18/2022 08:48:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=594
03/18/2022 08:48:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.48 on epoch=599
03/18/2022 08:48:29 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=599
03/18/2022 08:48:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
03/18/2022 08:48:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
03/18/2022 08:48:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=614
03/18/2022 08:48:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=619
03/18/2022 08:48:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=624
03/18/2022 08:48:42 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=624
03/18/2022 08:48:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=629
03/18/2022 08:48:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=634
03/18/2022 08:48:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
03/18/2022 08:48:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
03/18/2022 08:48:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/18/2022 08:48:55 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=649
03/18/2022 08:48:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=654
03/18/2022 08:48:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=659
03/18/2022 08:49:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=664
03/18/2022 08:49:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=669
03/18/2022 08:49:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=674
03/18/2022 08:49:07 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=674
03/18/2022 08:49:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=679
03/18/2022 08:49:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=684
03/18/2022 08:49:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=689
03/18/2022 08:49:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
03/18/2022 08:49:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=699
03/18/2022 08:49:20 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=699
03/18/2022 08:49:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=704
03/18/2022 08:49:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=709
03/18/2022 08:49:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=714
03/18/2022 08:49:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=719
03/18/2022 08:49:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=724
03/18/2022 08:49:33 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=724
03/18/2022 08:49:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
03/18/2022 08:49:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=734
03/18/2022 08:49:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=739
03/18/2022 08:49:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=744
03/18/2022 08:49:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=749
03/18/2022 08:49:46 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=749
03/18/2022 08:49:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
03/18/2022 08:49:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
03/18/2022 08:49:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=764
03/18/2022 08:49:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=769
03/18/2022 08:49:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=774
03/18/2022 08:49:59 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=774
03/18/2022 08:50:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
03/18/2022 08:50:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=784
03/18/2022 08:50:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=789
03/18/2022 08:50:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=794
03/18/2022 08:50:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
03/18/2022 08:50:12 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=799
03/18/2022 08:50:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=804
03/18/2022 08:50:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
03/18/2022 08:50:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=814
03/18/2022 08:50:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
03/18/2022 08:50:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
03/18/2022 08:50:25 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=824
03/18/2022 08:50:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
03/18/2022 08:50:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.48 on epoch=834
03/18/2022 08:50:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
03/18/2022 08:50:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
03/18/2022 08:50:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
03/18/2022 08:50:38 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=849
03/18/2022 08:50:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
03/18/2022 08:50:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=859
03/18/2022 08:50:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=864
03/18/2022 08:50:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
03/18/2022 08:50:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=874
03/18/2022 08:50:50 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=874
03/18/2022 08:50:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=879
03/18/2022 08:50:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
03/18/2022 08:50:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
03/18/2022 08:51:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
03/18/2022 08:51:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=899
03/18/2022 08:51:03 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=899
03/18/2022 08:51:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
03/18/2022 08:51:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
03/18/2022 08:51:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
03/18/2022 08:51:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=919
03/18/2022 08:51:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=924
03/18/2022 08:51:16 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=924
03/18/2022 08:51:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
03/18/2022 08:51:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=934
03/18/2022 08:51:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
03/18/2022 08:51:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
03/18/2022 08:51:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
03/18/2022 08:51:29 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=949
03/18/2022 08:51:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
03/18/2022 08:51:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
03/18/2022 08:51:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
03/18/2022 08:51:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
03/18/2022 08:51:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
03/18/2022 08:51:42 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=974
03/18/2022 08:51:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=979
03/18/2022 08:51:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=984
03/18/2022 08:51:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
03/18/2022 08:51:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
03/18/2022 08:51:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=999
03/18/2022 08:51:55 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=999
03/18/2022 08:51:55 - INFO - __main__ - save last model!
03/18/2022 08:51:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 08:51:55 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 08:51:55 - INFO - __main__ - Printing 3 examples
03/18/2022 08:51:55 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 08:51:55 - INFO - __main__ - ['false']
03/18/2022 08:51:55 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 08:51:55 - INFO - __main__ - ['false']
03/18/2022 08:51:55 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 08:51:55 - INFO - __main__ - ['false']
03/18/2022 08:51:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:51:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:51:56 - INFO - __main__ - Printing 3 examples
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 08:51:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:51:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:51:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:51:56 - INFO - __main__ - Printing 3 examples
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:51:56 - INFO - __main__ - ['false']
03/18/2022 08:51:56 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:51:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:51:56 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:51:56 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:51:59 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 08:52:14 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:52:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:52:15 - INFO - __main__ - Starting training!
03/18/2022 08:52:42 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_87_0.3_8_predictions.txt
03/18/2022 08:52:42 - INFO - __main__ - Classification-F1 on test data: 0.2577
03/18/2022 08:52:43 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.3, bsz=8, dev_performance=0.3992490613266583, test_performance=0.25771644712921865
03/18/2022 08:52:43 - INFO - __main__ - Running ... prefix=wiki_qa_16_87, lr=0.2, bsz=8 ...
03/18/2022 08:52:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:52:43 - INFO - __main__ - Printing 3 examples
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 08:52:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:52:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 08:52:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 08:52:43 - INFO - __main__ - Printing 3 examples
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: where are poison dart frog seen [SEP] answer: These amphibians are often called "dart frogs" due to the Amerindians ' indigenous use of their to poison the tips of blowdarts .
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: who invented geothermal energy technology [SEP] answer: From hot springs , geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation .
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ -  [wiki_qa] question: What did the Supreme Court determine in Dred Scott v. Sandford? [SEP] answer: Although the Supreme Court has never explicitly overruled the Dred Scott case, the Court stated in the Slaughter-House Cases that at least one part of it had already been overruled by the Fourteenth Amendment in 1868, which begins by stating, "All persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside."
03/18/2022 08:52:43 - INFO - __main__ - ['false']
03/18/2022 08:52:43 - INFO - __main__ - Tokenizing Input ...
03/18/2022 08:52:43 - INFO - __main__ - Tokenizing Output ...
03/18/2022 08:52:44 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 08:53:00 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 08:53:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 08:53:01 - INFO - __main__ - Starting training!
03/18/2022 08:53:04 - INFO - __main__ - Step 10 Global step 10 Train loss 7.12 on epoch=4
03/18/2022 08:53:06 - INFO - __main__ - Step 20 Global step 20 Train loss 3.36 on epoch=9
03/18/2022 08:53:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.40 on epoch=14
03/18/2022 08:53:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=19
03/18/2022 08:53:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=24
03/18/2022 08:53:14 - INFO - __main__ - Global step 50 Train loss 2.63 Classification-F1 0.3333333333333333 on epoch=24
03/18/2022 08:53:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/18/2022 08:53:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=29
03/18/2022 08:53:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
03/18/2022 08:53:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=39
03/18/2022 08:53:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=44
03/18/2022 08:53:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
03/18/2022 08:53:27 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=49
03/18/2022 08:53:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=54
03/18/2022 08:53:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
03/18/2022 08:53:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=64
03/18/2022 08:53:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
03/18/2022 08:53:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.35 on epoch=74
03/18/2022 08:53:39 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=74
03/18/2022 08:53:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=79
03/18/2022 08:53:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=84
03/18/2022 08:53:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
03/18/2022 08:53:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
03/18/2022 08:53:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
03/18/2022 08:53:52 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.3816425120772947 on epoch=99
03/18/2022 08:53:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=99, global_step=200
03/18/2022 08:53:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=104
03/18/2022 08:53:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
03/18/2022 08:53:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=114
03/18/2022 08:54:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
03/18/2022 08:54:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
03/18/2022 08:54:04 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=124
03/18/2022 08:54:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
03/18/2022 08:54:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=134
03/18/2022 08:54:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/18/2022 08:54:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=144
03/18/2022 08:54:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/18/2022 08:54:16 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=149
03/18/2022 08:54:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=154
03/18/2022 08:54:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=159
03/18/2022 08:54:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=164
03/18/2022 08:54:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=169
03/18/2022 08:54:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=174
03/18/2022 08:54:29 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.4920634920634921 on epoch=174
03/18/2022 08:54:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4920634920634921 on epoch=174, global_step=350
03/18/2022 08:54:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
03/18/2022 08:54:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
03/18/2022 08:54:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=189
03/18/2022 08:54:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
03/18/2022 08:54:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
03/18/2022 08:54:41 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=199
03/18/2022 08:54:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=204
03/18/2022 08:54:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=209
03/18/2022 08:54:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=214
03/18/2022 08:54:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=219
03/18/2022 08:54:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
03/18/2022 08:54:54 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.4420512820512821 on epoch=224
03/18/2022 08:54:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=229
03/18/2022 08:54:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=234
03/18/2022 08:55:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=239
03/18/2022 08:55:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=244
03/18/2022 08:55:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
03/18/2022 08:55:06 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.3191489361702127 on epoch=249
03/18/2022 08:55:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=254
03/18/2022 08:55:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
03/18/2022 08:55:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=264
03/18/2022 08:55:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=269
03/18/2022 08:55:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=274
03/18/2022 08:55:18 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.36374269005847953 on epoch=274
03/18/2022 08:55:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/18/2022 08:55:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=284
03/18/2022 08:55:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=289
03/18/2022 08:55:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
03/18/2022 08:55:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
03/18/2022 08:55:31 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.4385964912280702 on epoch=299
03/18/2022 08:55:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=304
03/18/2022 08:55:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=309
03/18/2022 08:55:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/18/2022 08:55:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
03/18/2022 08:55:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=324
03/18/2022 08:55:43 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.3816425120772947 on epoch=324
03/18/2022 08:55:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=329
03/18/2022 08:55:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
03/18/2022 08:55:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/18/2022 08:55:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
03/18/2022 08:55:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/18/2022 08:55:57 - INFO - __main__ - Global step 700 Train loss 0.26 Classification-F1 0.5607843137254902 on epoch=349
03/18/2022 08:55:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5607843137254902 on epoch=349, global_step=700
03/18/2022 08:55:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
03/18/2022 08:56:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
03/18/2022 08:56:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=364
03/18/2022 08:56:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
03/18/2022 08:56:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
03/18/2022 08:56:09 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.4666666666666667 on epoch=374
03/18/2022 08:56:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=379
03/18/2022 08:56:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/18/2022 08:56:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
03/18/2022 08:56:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/18/2022 08:56:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
03/18/2022 08:56:22 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.4666666666666667 on epoch=399
03/18/2022 08:56:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
03/18/2022 08:56:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/18/2022 08:56:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=414
03/18/2022 08:56:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=419
03/18/2022 08:56:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
03/18/2022 08:56:34 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.5270935960591133 on epoch=424
03/18/2022 08:56:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/18/2022 08:56:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
03/18/2022 08:56:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/18/2022 08:56:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=444
03/18/2022 08:56:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=449
03/18/2022 08:56:47 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.39756367663344405 on epoch=449
03/18/2022 08:56:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
03/18/2022 08:56:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=459
03/18/2022 08:56:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=464
03/18/2022 08:56:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
03/18/2022 08:56:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=474
03/18/2022 08:56:59 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.5465587044534412 on epoch=474
03/18/2022 08:57:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=479
03/18/2022 08:57:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=484
03/18/2022 08:57:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=489
03/18/2022 08:57:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
03/18/2022 08:57:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
03/18/2022 08:57:12 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.5465587044534412 on epoch=499
03/18/2022 08:57:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=504
03/18/2022 08:57:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
03/18/2022 08:57:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
03/18/2022 08:57:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=519
03/18/2022 08:57:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
03/18/2022 08:57:25 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.5465587044534412 on epoch=524
03/18/2022 08:57:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
03/18/2022 08:57:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=534
03/18/2022 08:57:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=539
03/18/2022 08:57:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
03/18/2022 08:57:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=549
03/18/2022 08:57:37 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.5607843137254902 on epoch=549
03/18/2022 08:57:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
03/18/2022 08:57:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=559
03/18/2022 08:57:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/18/2022 08:57:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
03/18/2022 08:57:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/18/2022 08:57:50 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.4666666666666667 on epoch=574
03/18/2022 08:57:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
03/18/2022 08:57:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
03/18/2022 08:57:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
03/18/2022 08:58:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
03/18/2022 08:58:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
03/18/2022 08:58:03 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5076923076923077 on epoch=599
03/18/2022 08:58:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
03/18/2022 08:58:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
03/18/2022 08:58:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
03/18/2022 08:58:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
03/18/2022 08:58:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
03/18/2022 08:58:15 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5270935960591133 on epoch=624
03/18/2022 08:58:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
03/18/2022 08:58:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
03/18/2022 08:58:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/18/2022 08:58:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
03/18/2022 08:58:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
03/18/2022 08:58:28 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5333333333333333 on epoch=649
03/18/2022 08:58:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
03/18/2022 08:58:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/18/2022 08:58:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
03/18/2022 08:58:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
03/18/2022 08:58:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
03/18/2022 08:58:40 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.4909862142099682 on epoch=674
03/18/2022 08:58:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/18/2022 08:58:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/18/2022 08:58:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
03/18/2022 08:58:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/18/2022 08:58:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/18/2022 08:58:54 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5901477832512315 on epoch=699
03/18/2022 08:58:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5901477832512315 on epoch=699, global_step=1400
03/18/2022 08:58:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/18/2022 08:58:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/18/2022 08:59:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/18/2022 08:59:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/18/2022 08:59:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
03/18/2022 08:59:06 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.4909862142099682 on epoch=724
03/18/2022 08:59:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
03/18/2022 08:59:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
03/18/2022 08:59:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/18/2022 08:59:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/18/2022 08:59:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 08:59:19 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=749
03/18/2022 08:59:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/18/2022 08:59:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 08:59:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/18/2022 08:59:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
03/18/2022 08:59:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=774
03/18/2022 08:59:32 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.5195195195195195 on epoch=774
03/18/2022 08:59:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
03/18/2022 08:59:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/18/2022 08:59:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 08:59:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/18/2022 08:59:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
03/18/2022 08:59:45 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6235294117647059 on epoch=799
03/18/2022 08:59:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.6235294117647059 on epoch=799, global_step=1600
03/18/2022 08:59:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/18/2022 08:59:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 08:59:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/18/2022 08:59:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/18/2022 08:59:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/18/2022 08:59:59 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5333333333333333 on epoch=824
03/18/2022 09:00:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/18/2022 09:00:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/18/2022 09:00:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 09:00:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/18/2022 09:00:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 09:00:12 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5333333333333333 on epoch=849
03/18/2022 09:00:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=854
03/18/2022 09:00:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/18/2022 09:00:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
03/18/2022 09:00:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 09:00:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/18/2022 09:00:25 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.5901477832512315 on epoch=874
03/18/2022 09:00:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 09:00:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 09:00:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/18/2022 09:00:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/18/2022 09:00:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/18/2022 09:00:38 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=899
03/18/2022 09:00:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 09:00:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 09:00:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 09:00:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 09:00:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 09:00:50 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.6476476476476476 on epoch=924
03/18/2022 09:00:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6235294117647059 -> 0.6476476476476476 on epoch=924, global_step=1850
03/18/2022 09:00:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/18/2022 09:00:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 09:00:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/18/2022 09:01:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 09:01:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 09:01:03 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=949
03/18/2022 09:01:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 09:01:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=959
03/18/2022 09:01:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 09:01:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 09:01:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 09:01:15 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5333333333333333 on epoch=974
03/18/2022 09:01:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 09:01:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 09:01:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=989
03/18/2022 09:01:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/18/2022 09:01:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 09:01:28 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6113360323886641 on epoch=999
03/18/2022 09:01:28 - INFO - __main__ - save last model!
03/18/2022 09:01:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 09:01:28 - INFO - __main__ - Start tokenizing ... 2733 instances
03/18/2022 09:01:28 - INFO - __main__ - Printing 3 examples
03/18/2022 09:01:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
03/18/2022 09:01:28 - INFO - __main__ - ['false']
03/18/2022 09:01:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
03/18/2022 09:01:28 - INFO - __main__ - ['false']
03/18/2022 09:01:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
03/18/2022 09:01:28 - INFO - __main__ - ['false']
03/18/2022 09:01:28 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:01:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:01:32 - INFO - __main__ - Loaded 2733 examples from test data
03/18/2022 09:03:14 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-wiki_qa/wiki_qa_16_87_0.2_8_predictions.txt
03/18/2022 09:03:14 - INFO - __main__ - Classification-F1 on test data: 0.2681
03/18/2022 09:03:15 - INFO - __main__ - prefix=wiki_qa_16_87, lr=0.2, bsz=8, dev_performance=0.6476476476476476, test_performance=0.2681010166243328
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (21149): No such process
Task: emo, Checkpoint: models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10
Output directory () already exists and is not empty.
03/18/2022 09:03:22 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 09:03:22 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo
03/18/2022 09:03:22 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 09:03:22 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo
03/18/2022 09:03:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 09:03:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 09:03:23 - INFO - __main__ - args.device: cuda:0
03/18/2022 09:03:23 - INFO - __main__ - Using 2 gpus
03/18/2022 09:03:23 - INFO - __main__ - args.device: cuda:1
03/18/2022 09:03:23 - INFO - __main__ - Using 2 gpus
03/18/2022 09:03:23 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/18/2022 09:03:23 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/18/2022 09:03:28 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
03/18/2022 09:03:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:03:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:03:29 - INFO - __main__ - Printing 3 examples
03/18/2022 09:03:29 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:03:29 - INFO - __main__ - Printing 3 examples
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:03:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:03:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:03:29 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:03:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:03:29 - INFO - __main__ - Printing 3 examples
03/18/2022 09:03:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:03:29 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:03:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:03:29 - INFO - __main__ - Printing 3 examples
03/18/2022 09:03:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:03:29 - INFO - __main__ - ['others']
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:03:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:03:29 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:03:29 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:03:46 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:03:47 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:03:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:03:49 - INFO - __main__ - Starting training!
03/18/2022 09:03:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:03:55 - INFO - __main__ - Starting training!
03/18/2022 09:03:58 - INFO - __main__ - Step 10 Global step 10 Train loss 6.00 on epoch=2
03/18/2022 09:04:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.10 on epoch=4
03/18/2022 09:04:03 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=7
03/18/2022 09:04:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=9
03/18/2022 09:04:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=12
03/18/2022 09:04:09 - INFO - __main__ - Global step 50 Train loss 2.31 Classification-F1 0.1 on epoch=12
03/18/2022 09:04:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 09:04:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=14
03/18/2022 09:04:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=17
03/18/2022 09:04:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=19
03/18/2022 09:04:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
03/18/2022 09:04:21 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=24
03/18/2022 09:04:22 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.2023486901535682 on epoch=24
03/18/2022 09:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2023486901535682 on epoch=24, global_step=100
03/18/2022 09:04:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=27
03/18/2022 09:04:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=29
03/18/2022 09:04:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
03/18/2022 09:04:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=34
03/18/2022 09:04:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
03/18/2022 09:04:35 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.10126582278481013 on epoch=37
03/18/2022 09:04:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
03/18/2022 09:04:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=42
03/18/2022 09:04:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=44
03/18/2022 09:04:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=47
03/18/2022 09:04:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=49
03/18/2022 09:04:47 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.4450793650793651 on epoch=49
03/18/2022 09:04:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2023486901535682 -> 0.4450793650793651 on epoch=49, global_step=200
03/18/2022 09:04:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=52
03/18/2022 09:04:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
03/18/2022 09:04:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=57
03/18/2022 09:04:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=59
03/18/2022 09:04:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
03/18/2022 09:05:00 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.12518037518037517 on epoch=62
03/18/2022 09:05:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=64
03/18/2022 09:05:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=67
03/18/2022 09:05:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
03/18/2022 09:05:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=72
03/18/2022 09:05:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=74
03/18/2022 09:05:13 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.40781739811912227 on epoch=74
03/18/2022 09:05:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=77
03/18/2022 09:05:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
03/18/2022 09:05:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=82
03/18/2022 09:05:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=84
03/18/2022 09:05:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=87
03/18/2022 09:05:26 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.1550284292821606 on epoch=87
03/18/2022 09:05:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=89
03/18/2022 09:05:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.64 on epoch=92
03/18/2022 09:05:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=94
03/18/2022 09:05:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=97
03/18/2022 09:05:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=99
03/18/2022 09:05:39 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.4015151515151515 on epoch=99
03/18/2022 09:05:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=102
03/18/2022 09:05:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=104
03/18/2022 09:05:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
03/18/2022 09:05:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=109
03/18/2022 09:05:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=112
03/18/2022 09:05:52 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.43523163631859285 on epoch=112
03/18/2022 09:05:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=114
03/18/2022 09:05:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
03/18/2022 09:05:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=119
03/18/2022 09:06:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
03/18/2022 09:06:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=124
03/18/2022 09:06:04 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.5157503172209055 on epoch=124
03/18/2022 09:06:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4450793650793651 -> 0.5157503172209055 on epoch=124, global_step=500
03/18/2022 09:06:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=127
03/18/2022 09:06:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
03/18/2022 09:06:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.13 on epoch=132
03/18/2022 09:06:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=134
03/18/2022 09:06:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=137
03/18/2022 09:06:17 - INFO - __main__ - Global step 550 Train loss 0.13 Classification-F1 0.5974151173261295 on epoch=137
03/18/2022 09:06:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5157503172209055 -> 0.5974151173261295 on epoch=137, global_step=550
03/18/2022 09:06:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
03/18/2022 09:06:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=142
03/18/2022 09:06:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=144
03/18/2022 09:06:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.09 on epoch=147
03/18/2022 09:06:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
03/18/2022 09:06:30 - INFO - __main__ - Global step 600 Train loss 0.10 Classification-F1 0.5334331797235023 on epoch=149
03/18/2022 09:06:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=152
03/18/2022 09:06:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=154
03/18/2022 09:06:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=157
03/18/2022 09:06:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
03/18/2022 09:06:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=162
03/18/2022 09:06:43 - INFO - __main__ - Global step 650 Train loss 0.08 Classification-F1 0.5406746031746031 on epoch=162
03/18/2022 09:06:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=164
03/18/2022 09:06:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
03/18/2022 09:06:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=169
03/18/2022 09:06:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=172
03/18/2022 09:06:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.10 on epoch=174
03/18/2022 09:06:56 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.6339017589017589 on epoch=174
03/18/2022 09:06:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5974151173261295 -> 0.6339017589017589 on epoch=174, global_step=700
03/18/2022 09:06:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=177
03/18/2022 09:07:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
03/18/2022 09:07:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=182
03/18/2022 09:07:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=184
03/18/2022 09:07:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=187
03/18/2022 09:07:09 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.5777526395173453 on epoch=187
03/18/2022 09:07:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
03/18/2022 09:07:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=192
03/18/2022 09:07:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
03/18/2022 09:07:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=197
03/18/2022 09:07:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
03/18/2022 09:07:22 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.5710369674185464 on epoch=199
03/18/2022 09:07:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
03/18/2022 09:07:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
03/18/2022 09:07:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=207
03/18/2022 09:07:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
03/18/2022 09:07:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
03/18/2022 09:07:35 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.5980392156862745 on epoch=212
03/18/2022 09:07:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
03/18/2022 09:07:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=217
03/18/2022 09:07:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
03/18/2022 09:07:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
03/18/2022 09:07:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
03/18/2022 09:07:48 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.6171595528455285 on epoch=224
03/18/2022 09:07:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
03/18/2022 09:07:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
03/18/2022 09:07:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
03/18/2022 09:07:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
03/18/2022 09:08:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=237
03/18/2022 09:08:01 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.5764892344497607 on epoch=237
03/18/2022 09:08:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
03/18/2022 09:08:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=242
03/18/2022 09:08:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
03/18/2022 09:08:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
03/18/2022 09:08:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
03/18/2022 09:08:14 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.5612140033879165 on epoch=249
03/18/2022 09:08:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
03/18/2022 09:08:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
03/18/2022 09:08:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
03/18/2022 09:08:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
03/18/2022 09:08:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=262
03/18/2022 09:08:27 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5904960143409368 on epoch=262
03/18/2022 09:08:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/18/2022 09:08:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
03/18/2022 09:08:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=269
03/18/2022 09:08:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
03/18/2022 09:08:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
03/18/2022 09:08:40 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.5514336917562723 on epoch=274
03/18/2022 09:08:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
03/18/2022 09:08:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
03/18/2022 09:08:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
03/18/2022 09:08:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
03/18/2022 09:08:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
03/18/2022 09:08:53 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6085877496159754 on epoch=287
03/18/2022 09:08:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
03/18/2022 09:08:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/18/2022 09:09:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
03/18/2022 09:09:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
03/18/2022 09:09:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
03/18/2022 09:09:06 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5950226244343891 on epoch=299
03/18/2022 09:09:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
03/18/2022 09:09:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
03/18/2022 09:09:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
03/18/2022 09:09:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
03/18/2022 09:09:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
03/18/2022 09:09:19 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.5985895445134575 on epoch=312
03/18/2022 09:09:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/18/2022 09:09:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/18/2022 09:09:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
03/18/2022 09:09:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/18/2022 09:09:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
03/18/2022 09:09:32 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5891453100429257 on epoch=324
03/18/2022 09:09:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
03/18/2022 09:09:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
03/18/2022 09:09:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
03/18/2022 09:09:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
03/18/2022 09:09:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
03/18/2022 09:09:45 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6118077324973876 on epoch=337
03/18/2022 09:09:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
03/18/2022 09:09:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
03/18/2022 09:09:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/18/2022 09:09:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
03/18/2022 09:09:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
03/18/2022 09:09:58 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5832206445796709 on epoch=349
03/18/2022 09:10:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 09:10:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
03/18/2022 09:10:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
03/18/2022 09:10:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/18/2022 09:10:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/18/2022 09:10:11 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5801857576051124 on epoch=362
03/18/2022 09:10:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
03/18/2022 09:10:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/18/2022 09:10:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/18/2022 09:10:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
03/18/2022 09:10:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
03/18/2022 09:10:24 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5204435204435205 on epoch=374
03/18/2022 09:10:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/18/2022 09:10:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 09:10:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
03/18/2022 09:10:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 09:10:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
03/18/2022 09:10:37 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5402484787018256 on epoch=387
03/18/2022 09:10:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
03/18/2022 09:10:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/18/2022 09:10:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/18/2022 09:10:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/18/2022 09:10:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/18/2022 09:10:50 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5526840166059181 on epoch=399
03/18/2022 09:10:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
03/18/2022 09:10:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/18/2022 09:10:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
03/18/2022 09:11:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
03/18/2022 09:11:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/18/2022 09:11:03 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.6016497997048281 on epoch=412
03/18/2022 09:11:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
03/18/2022 09:11:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
03/18/2022 09:11:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/18/2022 09:11:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
03/18/2022 09:11:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/18/2022 09:11:17 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6161259339571499 on epoch=424
03/18/2022 09:11:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 09:11:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 09:11:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
03/18/2022 09:11:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 09:11:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
03/18/2022 09:11:30 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5533205619412516 on epoch=437
03/18/2022 09:11:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
03/18/2022 09:11:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 09:11:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/18/2022 09:11:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
03/18/2022 09:11:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 09:11:43 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6041666666666666 on epoch=449
03/18/2022 09:11:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
03/18/2022 09:11:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 09:11:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/18/2022 09:11:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/18/2022 09:11:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 09:11:56 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6108776844070962 on epoch=462
03/18/2022 09:11:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 09:12:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 09:12:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 09:12:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 09:12:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 09:12:09 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.6104594330400782 on epoch=474
03/18/2022 09:12:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
03/18/2022 09:12:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 09:12:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 09:12:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 09:12:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/18/2022 09:12:22 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5377339624106865 on epoch=487
03/18/2022 09:12:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
03/18/2022 09:12:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 09:12:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 09:12:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 09:12:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 09:12:35 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5768259518259518 on epoch=499
03/18/2022 09:12:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 09:12:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 09:12:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 09:12:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/18/2022 09:12:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/18/2022 09:12:49 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6124789562289563 on epoch=512
03/18/2022 09:12:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
03/18/2022 09:12:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 09:12:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 09:12:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 09:13:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 09:13:02 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6021941296134844 on epoch=524
03/18/2022 09:13:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 09:13:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 09:13:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/18/2022 09:13:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 09:13:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 09:13:15 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.57850165155668 on epoch=537
03/18/2022 09:13:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 09:13:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 09:13:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/18/2022 09:13:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 09:13:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 09:13:29 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.614294035414725 on epoch=549
03/18/2022 09:13:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 09:13:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/18/2022 09:13:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 09:13:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 09:13:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 09:13:42 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6123161764705882 on epoch=562
03/18/2022 09:13:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 09:13:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 09:13:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 09:13:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 09:13:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 09:13:55 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.65039130780232 on epoch=574
03/18/2022 09:13:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6339017589017589 -> 0.65039130780232 on epoch=574, global_step=2300
03/18/2022 09:13:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 09:14:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 09:14:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 09:14:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 09:14:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 09:14:08 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.5775556680161943 on epoch=587
03/18/2022 09:14:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 09:14:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 09:14:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
03/18/2022 09:14:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 09:14:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 09:14:22 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5963269178639198 on epoch=599
03/18/2022 09:14:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 09:14:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 09:14:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 09:14:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/18/2022 09:14:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/18/2022 09:14:35 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5636836343732896 on epoch=612
03/18/2022 09:14:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 09:14:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 09:14:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 09:14:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 09:14:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 09:14:48 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6184605307640297 on epoch=624
03/18/2022 09:14:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 09:14:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 09:14:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 09:14:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 09:15:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
03/18/2022 09:15:01 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5491101055806938 on epoch=637
03/18/2022 09:15:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 09:15:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 09:15:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 09:15:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 09:15:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 09:15:15 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6053784577491474 on epoch=649
03/18/2022 09:15:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 09:15:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 09:15:22 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
03/18/2022 09:15:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 09:15:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 09:15:28 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6381641366223909 on epoch=662
03/18/2022 09:15:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 09:15:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
03/18/2022 09:15:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 09:15:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 09:15:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 09:15:41 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6044275774826059 on epoch=674
03/18/2022 09:15:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=677
03/18/2022 09:15:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 09:15:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 09:15:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/18/2022 09:15:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 09:15:54 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.5821869070208728 on epoch=687
03/18/2022 09:15:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 09:15:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
03/18/2022 09:16:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 09:16:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 09:16:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 09:16:07 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5667299177735611 on epoch=699
03/18/2022 09:16:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 09:16:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 09:16:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 09:16:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 09:16:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 09:16:21 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5837029569892472 on epoch=712
03/18/2022 09:16:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 09:16:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 09:16:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
03/18/2022 09:16:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 09:16:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 09:16:34 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5532639494333043 on epoch=724
03/18/2022 09:16:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 09:16:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 09:16:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 09:16:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 09:16:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 09:16:47 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5773331514497309 on epoch=737
03/18/2022 09:16:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 09:16:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 09:16:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
03/18/2022 09:16:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 09:16:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 09:17:00 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5774832717467292 on epoch=749
03/18/2022 09:17:00 - INFO - __main__ - save last model!
03/18/2022 09:17:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 09:17:00 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 09:17:00 - INFO - __main__ - Printing 3 examples
03/18/2022 09:17:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 09:17:00 - INFO - __main__ - ['others']
03/18/2022 09:17:00 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 09:17:00 - INFO - __main__ - ['others']
03/18/2022 09:17:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 09:17:00 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:17:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:17:01 - INFO - __main__ - Printing 3 examples
03/18/2022 09:17:01 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 09:17:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:17:01 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:17:01 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:17:01 - INFO - __main__ - Printing 3 examples
03/18/2022 09:17:01 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:17:01 - INFO - __main__ - ['others']
03/18/2022 09:17:01 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:17:01 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:17:01 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:17:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:17:08 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 09:17:19 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:17:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:17:20 - INFO - __main__ - Starting training!
03/18/2022 09:18:38 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_100_0.5_8_predictions.txt
03/18/2022 09:18:38 - INFO - __main__ - Classification-F1 on test data: 0.2728
03/18/2022 09:18:38 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.65039130780232, test_performance=0.2728201856776429
03/18/2022 09:18:38 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
03/18/2022 09:18:39 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:18:39 - INFO - __main__ - Printing 3 examples
03/18/2022 09:18:39 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:18:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:18:39 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:18:39 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:18:39 - INFO - __main__ - Printing 3 examples
03/18/2022 09:18:39 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:18:39 - INFO - __main__ - ['others']
03/18/2022 09:18:39 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:18:39 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:18:39 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:18:58 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:18:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:18:58 - INFO - __main__ - Starting training!
03/18/2022 09:19:01 - INFO - __main__ - Step 10 Global step 10 Train loss 7.34 on epoch=2
03/18/2022 09:19:04 - INFO - __main__ - Step 20 Global step 20 Train loss 3.56 on epoch=4
03/18/2022 09:19:06 - INFO - __main__ - Step 30 Global step 30 Train loss 2.27 on epoch=7
03/18/2022 09:19:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.74 on epoch=9
03/18/2022 09:19:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=12
03/18/2022 09:19:12 - INFO - __main__ - Global step 50 Train loss 3.26 Classification-F1 0.10126582278481013 on epoch=12
03/18/2022 09:19:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
03/18/2022 09:19:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=14
03/18/2022 09:19:17 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=17
03/18/2022 09:19:19 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
03/18/2022 09:19:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=22
03/18/2022 09:19:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=24
03/18/2022 09:19:25 - INFO - __main__ - Global step 100 Train loss 1.11 Classification-F1 0.228064963359081 on epoch=24
03/18/2022 09:19:25 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.228064963359081 on epoch=24, global_step=100
03/18/2022 09:19:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
03/18/2022 09:19:30 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=29
03/18/2022 09:19:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
03/18/2022 09:19:35 - INFO - __main__ - Step 140 Global step 140 Train loss 1.01 on epoch=34
03/18/2022 09:19:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=37
03/18/2022 09:19:38 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.1 on epoch=37
03/18/2022 09:19:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.03 on epoch=39
03/18/2022 09:19:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.94 on epoch=42
03/18/2022 09:19:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=44
03/18/2022 09:19:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
03/18/2022 09:19:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
03/18/2022 09:19:51 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.13751987281399045 on epoch=49
03/18/2022 09:19:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
03/18/2022 09:19:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
03/18/2022 09:19:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
03/18/2022 09:20:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
03/18/2022 09:20:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.01 on epoch=62
03/18/2022 09:20:04 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.10126582278481013 on epoch=62
03/18/2022 09:20:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=64
03/18/2022 09:20:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
03/18/2022 09:20:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
03/18/2022 09:20:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
03/18/2022 09:20:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
03/18/2022 09:20:17 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.1565276828434723 on epoch=74
03/18/2022 09:20:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=77
03/18/2022 09:20:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
03/18/2022 09:20:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=82
03/18/2022 09:20:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=84
03/18/2022 09:20:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
03/18/2022 09:20:30 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.22602316602316602 on epoch=87
03/18/2022 09:20:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=89
03/18/2022 09:20:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=92
03/18/2022 09:20:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=94
03/18/2022 09:20:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=97
03/18/2022 09:20:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
03/18/2022 09:20:43 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.10996955859969558 on epoch=99
03/18/2022 09:20:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=102
03/18/2022 09:20:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
03/18/2022 09:20:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.81 on epoch=107
03/18/2022 09:20:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.76 on epoch=109
03/18/2022 09:20:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=112
03/18/2022 09:20:56 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.19277155306567073 on epoch=112
03/18/2022 09:20:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=114
03/18/2022 09:21:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=117
03/18/2022 09:21:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=119
03/18/2022 09:21:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
03/18/2022 09:21:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.65 on epoch=124
03/18/2022 09:21:09 - INFO - __main__ - Global step 500 Train loss 0.73 Classification-F1 0.2649108403825385 on epoch=124
03/18/2022 09:21:09 - INFO - __main__ - Saving model with best Classification-F1: 0.228064963359081 -> 0.2649108403825385 on epoch=124, global_step=500
03/18/2022 09:21:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=127
03/18/2022 09:21:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
03/18/2022 09:21:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=132
03/18/2022 09:21:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=134
03/18/2022 09:21:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=137
03/18/2022 09:21:22 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.2674697629414611 on epoch=137
03/18/2022 09:21:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2649108403825385 -> 0.2674697629414611 on epoch=137, global_step=550
03/18/2022 09:21:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=139
03/18/2022 09:21:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=142
03/18/2022 09:21:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
03/18/2022 09:21:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=147
03/18/2022 09:21:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=149
03/18/2022 09:21:35 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2977114752316513 on epoch=149
03/18/2022 09:21:35 - INFO - __main__ - Saving model with best Classification-F1: 0.2674697629414611 -> 0.2977114752316513 on epoch=149, global_step=600
03/18/2022 09:21:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=152
03/18/2022 09:21:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=154
03/18/2022 09:21:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=157
03/18/2022 09:21:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=159
03/18/2022 09:21:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=162
03/18/2022 09:21:48 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.4049941927990708 on epoch=162
03/18/2022 09:21:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2977114752316513 -> 0.4049941927990708 on epoch=162, global_step=650
03/18/2022 09:21:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=164
03/18/2022 09:21:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=167
03/18/2022 09:21:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=169
03/18/2022 09:21:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=172
03/18/2022 09:22:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
03/18/2022 09:22:01 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.4412665345728226 on epoch=174
03/18/2022 09:22:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4049941927990708 -> 0.4412665345728226 on epoch=174, global_step=700
03/18/2022 09:22:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
03/18/2022 09:22:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
03/18/2022 09:22:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
03/18/2022 09:22:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
03/18/2022 09:22:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=187
03/18/2022 09:22:14 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.4892897576721106 on epoch=187
03/18/2022 09:22:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4412665345728226 -> 0.4892897576721106 on epoch=187, global_step=750
03/18/2022 09:22:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
03/18/2022 09:22:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
03/18/2022 09:22:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=194
03/18/2022 09:22:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
03/18/2022 09:22:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
03/18/2022 09:22:27 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.4650846192439947 on epoch=199
03/18/2022 09:22:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
03/18/2022 09:22:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
03/18/2022 09:22:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
03/18/2022 09:22:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
03/18/2022 09:22:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
03/18/2022 09:22:40 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.4766762429047565 on epoch=212
03/18/2022 09:22:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
03/18/2022 09:22:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
03/18/2022 09:22:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
03/18/2022 09:22:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
03/18/2022 09:22:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
03/18/2022 09:22:53 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.5444362984685566 on epoch=224
03/18/2022 09:22:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4892897576721106 -> 0.5444362984685566 on epoch=224, global_step=900
03/18/2022 09:22:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
03/18/2022 09:22:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
03/18/2022 09:23:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
03/18/2022 09:23:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
03/18/2022 09:23:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
03/18/2022 09:23:06 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.46363442057039617 on epoch=237
03/18/2022 09:23:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
03/18/2022 09:23:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
03/18/2022 09:23:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
03/18/2022 09:23:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
03/18/2022 09:23:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
03/18/2022 09:23:19 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.5930872048519107 on epoch=249
03/18/2022 09:23:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5444362984685566 -> 0.5930872048519107 on epoch=249, global_step=1000
03/18/2022 09:23:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
03/18/2022 09:23:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
03/18/2022 09:23:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
03/18/2022 09:23:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
03/18/2022 09:23:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
03/18/2022 09:23:32 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.5663461538461538 on epoch=262
03/18/2022 09:23:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
03/18/2022 09:23:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
03/18/2022 09:23:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
03/18/2022 09:23:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/18/2022 09:23:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
03/18/2022 09:23:45 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.5204901886824084 on epoch=274
03/18/2022 09:23:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
03/18/2022 09:23:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
03/18/2022 09:23:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
03/18/2022 09:23:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
03/18/2022 09:23:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
03/18/2022 09:23:58 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.5225961538461539 on epoch=287
03/18/2022 09:24:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/18/2022 09:24:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
03/18/2022 09:24:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
03/18/2022 09:24:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
03/18/2022 09:24:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
03/18/2022 09:24:11 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5331855791962175 on epoch=299
03/18/2022 09:24:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=302
03/18/2022 09:24:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
03/18/2022 09:24:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
03/18/2022 09:24:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/18/2022 09:24:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
03/18/2022 09:24:24 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.5594405594405594 on epoch=312
03/18/2022 09:24:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/18/2022 09:24:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
03/18/2022 09:24:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/18/2022 09:24:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
03/18/2022 09:24:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
03/18/2022 09:24:36 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5115153171371574 on epoch=324
03/18/2022 09:24:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/18/2022 09:24:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
03/18/2022 09:24:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
03/18/2022 09:24:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
03/18/2022 09:24:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
03/18/2022 09:24:49 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.563782991202346 on epoch=337
03/18/2022 09:24:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
03/18/2022 09:24:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/18/2022 09:24:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
03/18/2022 09:24:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
03/18/2022 09:25:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
03/18/2022 09:25:02 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.5490409764603312 on epoch=349
03/18/2022 09:25:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 09:25:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/18/2022 09:25:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
03/18/2022 09:25:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/18/2022 09:25:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
03/18/2022 09:25:15 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.5576036866359448 on epoch=362
03/18/2022 09:25:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/18/2022 09:25:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
03/18/2022 09:25:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
03/18/2022 09:25:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/18/2022 09:25:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
03/18/2022 09:25:28 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.522721466014713 on epoch=374
03/18/2022 09:25:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
03/18/2022 09:25:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 09:25:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/18/2022 09:25:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 09:25:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/18/2022 09:25:41 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5833333333333334 on epoch=387
03/18/2022 09:25:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/18/2022 09:25:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/18/2022 09:25:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/18/2022 09:25:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/18/2022 09:25:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/18/2022 09:25:54 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5385347985347986 on epoch=399
03/18/2022 09:25:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
03/18/2022 09:25:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
03/18/2022 09:26:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
03/18/2022 09:26:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/18/2022 09:26:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/18/2022 09:26:07 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.4974974974974975 on epoch=412
03/18/2022 09:26:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
03/18/2022 09:26:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/18/2022 09:26:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 09:26:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
03/18/2022 09:26:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
03/18/2022 09:26:20 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5253467753467753 on epoch=424
03/18/2022 09:26:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/18/2022 09:26:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
03/18/2022 09:26:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/18/2022 09:26:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
03/18/2022 09:26:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
03/18/2022 09:26:33 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.4929554655870445 on epoch=437
03/18/2022 09:26:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/18/2022 09:26:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 09:26:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 09:26:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/18/2022 09:26:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/18/2022 09:26:46 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5811686187266898 on epoch=449
03/18/2022 09:26:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 09:26:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 09:26:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 09:26:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 09:26:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 09:26:59 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5730706075533661 on epoch=462
03/18/2022 09:27:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/18/2022 09:27:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 09:27:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 09:27:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 09:27:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
03/18/2022 09:27:12 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5632237386269644 on epoch=474
03/18/2022 09:27:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/18/2022 09:27:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/18/2022 09:27:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 09:27:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/18/2022 09:27:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/18/2022 09:27:25 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5181569918678108 on epoch=487
03/18/2022 09:27:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 09:27:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 09:27:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 09:27:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 09:27:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 09:27:39 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5239613327848621 on epoch=499
03/18/2022 09:27:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 09:27:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 09:27:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/18/2022 09:27:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 09:27:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 09:27:52 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.5757839721254355 on epoch=512
03/18/2022 09:27:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 09:27:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 09:27:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 09:28:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 09:28:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 09:28:05 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.6342123373373373 on epoch=524
03/18/2022 09:28:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5930872048519107 -> 0.6342123373373373 on epoch=524, global_step=2100
03/18/2022 09:28:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 09:28:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
03/18/2022 09:28:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/18/2022 09:28:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 09:28:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 09:28:18 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5815024630541872 on epoch=537
03/18/2022 09:28:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 09:28:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 09:28:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 09:28:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 09:28:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 09:28:31 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.5574802036199095 on epoch=549
03/18/2022 09:28:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 09:28:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 09:28:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 09:28:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 09:28:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
03/18/2022 09:28:44 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5143793264369604 on epoch=562
03/18/2022 09:28:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 09:28:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 09:28:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 09:28:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 09:28:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 09:28:57 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5691188011968411 on epoch=574
03/18/2022 09:29:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 09:29:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
03/18/2022 09:29:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 09:29:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 09:29:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 09:29:11 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.5238461538461539 on epoch=587
03/18/2022 09:29:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 09:29:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 09:29:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 09:29:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 09:29:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 09:29:24 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.556656746031746 on epoch=599
03/18/2022 09:29:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 09:29:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 09:29:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 09:29:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 09:29:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 09:29:37 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.49252747252747253 on epoch=612
03/18/2022 09:29:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 09:29:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
03/18/2022 09:29:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 09:29:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/18/2022 09:29:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 09:29:50 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5193786605551312 on epoch=624
03/18/2022 09:29:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
03/18/2022 09:29:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 09:29:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 09:30:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 09:30:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 09:30:03 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5380298719772404 on epoch=637
03/18/2022 09:30:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 09:30:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 09:30:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 09:30:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
03/18/2022 09:30:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 09:30:17 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5705909671426912 on epoch=649
03/18/2022 09:30:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 09:30:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
03/18/2022 09:30:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 09:30:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 09:30:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
03/18/2022 09:30:30 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.62235803412274 on epoch=662
03/18/2022 09:30:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 09:30:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
03/18/2022 09:30:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 09:30:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 09:30:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
03/18/2022 09:30:43 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5425529952783789 on epoch=674
03/18/2022 09:30:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 09:30:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 09:30:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 09:30:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 09:30:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 09:30:57 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.5420843862020333 on epoch=687
03/18/2022 09:30:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 09:31:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 09:31:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 09:31:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 09:31:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 09:31:10 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.525 on epoch=699
03/18/2022 09:31:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 09:31:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 09:31:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 09:31:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=709
03/18/2022 09:31:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/18/2022 09:31:23 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6021924603174603 on epoch=712
03/18/2022 09:31:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 09:31:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 09:31:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 09:31:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 09:31:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 09:31:36 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.5862719092942217 on epoch=724
03/18/2022 09:31:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 09:31:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 09:31:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
03/18/2022 09:31:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 09:31:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 09:31:49 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5001815888064837 on epoch=737
03/18/2022 09:31:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 09:31:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 09:31:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 09:31:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 09:32:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
03/18/2022 09:32:03 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5918034511784511 on epoch=749
03/18/2022 09:32:03 - INFO - __main__ - save last model!
03/18/2022 09:32:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 09:32:03 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 09:32:03 - INFO - __main__ - Printing 3 examples
03/18/2022 09:32:03 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:32:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:32:03 - INFO - __main__ - Printing 3 examples
03/18/2022 09:32:03 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 09:32:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:32:03 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:32:03 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:32:03 - INFO - __main__ - Printing 3 examples
03/18/2022 09:32:03 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:32:03 - INFO - __main__ - ['others']
03/18/2022 09:32:03 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:32:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:32:03 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:32:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:32:10 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 09:32:22 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:32:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:32:22 - INFO - __main__ - Starting training!
03/18/2022 09:33:41 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_100_0.4_8_predictions.txt
03/18/2022 09:33:41 - INFO - __main__ - Classification-F1 on test data: 0.3573
03/18/2022 09:33:41 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.6342123373373373, test_performance=0.3573043629020395
03/18/2022 09:33:41 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
03/18/2022 09:33:42 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:33:42 - INFO - __main__ - Printing 3 examples
03/18/2022 09:33:42 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:33:42 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:33:42 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:33:42 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:33:42 - INFO - __main__ - Printing 3 examples
03/18/2022 09:33:42 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:33:42 - INFO - __main__ - ['others']
03/18/2022 09:33:42 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:33:42 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:33:42 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:33:57 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:33:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:33:58 - INFO - __main__ - Starting training!
03/18/2022 09:34:01 - INFO - __main__ - Step 10 Global step 10 Train loss 7.69 on epoch=2
03/18/2022 09:34:03 - INFO - __main__ - Step 20 Global step 20 Train loss 4.63 on epoch=4
03/18/2022 09:34:06 - INFO - __main__ - Step 30 Global step 30 Train loss 2.44 on epoch=7
03/18/2022 09:34:08 - INFO - __main__ - Step 40 Global step 40 Train loss 1.66 on epoch=9
03/18/2022 09:34:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
03/18/2022 09:34:12 - INFO - __main__ - Global step 50 Train loss 3.53 Classification-F1 0.1500341763499658 on epoch=12
03/18/2022 09:34:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1500341763499658 on epoch=12, global_step=50
03/18/2022 09:34:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=14
03/18/2022 09:34:16 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=17
03/18/2022 09:34:19 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
03/18/2022 09:34:21 - INFO - __main__ - Step 90 Global step 90 Train loss 1.08 on epoch=22
03/18/2022 09:34:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=24
03/18/2022 09:34:24 - INFO - __main__ - Global step 100 Train loss 1.07 Classification-F1 0.17277992277992277 on epoch=24
03/18/2022 09:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.1500341763499658 -> 0.17277992277992277 on epoch=24, global_step=100
03/18/2022 09:34:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=27
03/18/2022 09:34:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.02 on epoch=29
03/18/2022 09:34:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
03/18/2022 09:34:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=34
03/18/2022 09:34:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=37
03/18/2022 09:34:37 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.10126582278481013 on epoch=37
03/18/2022 09:34:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=39
03/18/2022 09:34:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=42
03/18/2022 09:34:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=44
03/18/2022 09:34:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=47
03/18/2022 09:34:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=49
03/18/2022 09:34:50 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.31769005847953213 on epoch=49
03/18/2022 09:34:50 - INFO - __main__ - Saving model with best Classification-F1: 0.17277992277992277 -> 0.31769005847953213 on epoch=49, global_step=200
03/18/2022 09:34:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=52
03/18/2022 09:34:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=54
03/18/2022 09:34:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=57
03/18/2022 09:35:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
03/18/2022 09:35:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=62
03/18/2022 09:35:03 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.09210526315789473 on epoch=62
03/18/2022 09:35:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
03/18/2022 09:35:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
03/18/2022 09:35:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=69
03/18/2022 09:35:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
03/18/2022 09:35:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
03/18/2022 09:35:16 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.13197586726998492 on epoch=74
03/18/2022 09:35:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=77
03/18/2022 09:35:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.88 on epoch=79
03/18/2022 09:35:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
03/18/2022 09:35:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=84
03/18/2022 09:35:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=87
03/18/2022 09:35:29 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.2840795232821811 on epoch=87
03/18/2022 09:35:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
03/18/2022 09:35:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=92
03/18/2022 09:35:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
03/18/2022 09:35:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=97
03/18/2022 09:35:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=99
03/18/2022 09:35:42 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.20017543859649123 on epoch=99
03/18/2022 09:35:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=102
03/18/2022 09:35:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.75 on epoch=104
03/18/2022 09:35:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=107
03/18/2022 09:35:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=109
03/18/2022 09:35:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=112
03/18/2022 09:35:55 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.17002442002442003 on epoch=112
03/18/2022 09:35:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=114
03/18/2022 09:36:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=117
03/18/2022 09:36:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=119
03/18/2022 09:36:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=122
03/18/2022 09:36:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=124
03/18/2022 09:36:08 - INFO - __main__ - Global step 500 Train loss 0.67 Classification-F1 0.32602331255082684 on epoch=124
03/18/2022 09:36:08 - INFO - __main__ - Saving model with best Classification-F1: 0.31769005847953213 -> 0.32602331255082684 on epoch=124, global_step=500
03/18/2022 09:36:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
03/18/2022 09:36:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=129
03/18/2022 09:36:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=132
03/18/2022 09:36:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=134
03/18/2022 09:36:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=137
03/18/2022 09:36:21 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.2968810601163543 on epoch=137
03/18/2022 09:36:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=139
03/18/2022 09:36:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=142
03/18/2022 09:36:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=144
03/18/2022 09:36:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=147
03/18/2022 09:36:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=149
03/18/2022 09:36:34 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.29472117794486213 on epoch=149
03/18/2022 09:36:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
03/18/2022 09:36:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=154
03/18/2022 09:36:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=157
03/18/2022 09:36:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=159
03/18/2022 09:36:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=162
03/18/2022 09:36:46 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.361732210822808 on epoch=162
03/18/2022 09:36:46 - INFO - __main__ - Saving model with best Classification-F1: 0.32602331255082684 -> 0.361732210822808 on epoch=162, global_step=650
03/18/2022 09:36:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=164
03/18/2022 09:36:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=167
03/18/2022 09:36:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=169
03/18/2022 09:36:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=172
03/18/2022 09:36:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
03/18/2022 09:36:59 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.37955376415135894 on epoch=174
03/18/2022 09:36:59 - INFO - __main__ - Saving model with best Classification-F1: 0.361732210822808 -> 0.37955376415135894 on epoch=174, global_step=700
03/18/2022 09:37:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=177
03/18/2022 09:37:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=179
03/18/2022 09:37:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=182
03/18/2022 09:37:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
03/18/2022 09:37:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
03/18/2022 09:37:12 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.42160303776683083 on epoch=187
03/18/2022 09:37:12 - INFO - __main__ - Saving model with best Classification-F1: 0.37955376415135894 -> 0.42160303776683083 on epoch=187, global_step=750
03/18/2022 09:37:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=189
03/18/2022 09:37:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=192
03/18/2022 09:37:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=194
03/18/2022 09:37:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
03/18/2022 09:37:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
03/18/2022 09:37:25 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.46071895424836606 on epoch=199
03/18/2022 09:37:25 - INFO - __main__ - Saving model with best Classification-F1: 0.42160303776683083 -> 0.46071895424836606 on epoch=199, global_step=800
03/18/2022 09:37:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
03/18/2022 09:37:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
03/18/2022 09:37:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
03/18/2022 09:37:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
03/18/2022 09:37:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
03/18/2022 09:37:38 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.38240589198036 on epoch=212
03/18/2022 09:37:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
03/18/2022 09:37:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=217
03/18/2022 09:37:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
03/18/2022 09:37:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
03/18/2022 09:37:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
03/18/2022 09:37:51 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.48078201368523954 on epoch=224
03/18/2022 09:37:51 - INFO - __main__ - Saving model with best Classification-F1: 0.46071895424836606 -> 0.48078201368523954 on epoch=224, global_step=900
03/18/2022 09:37:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
03/18/2022 09:37:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=229
03/18/2022 09:37:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
03/18/2022 09:38:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
03/18/2022 09:38:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
03/18/2022 09:38:04 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.48619347789805833 on epoch=237
03/18/2022 09:38:04 - INFO - __main__ - Saving model with best Classification-F1: 0.48078201368523954 -> 0.48619347789805833 on epoch=237, global_step=950
03/18/2022 09:38:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
03/18/2022 09:38:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
03/18/2022 09:38:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
03/18/2022 09:38:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
03/18/2022 09:38:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
03/18/2022 09:38:17 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.48209105552059717 on epoch=249
03/18/2022 09:38:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
03/18/2022 09:38:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
03/18/2022 09:38:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
03/18/2022 09:38:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
03/18/2022 09:38:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
03/18/2022 09:38:30 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.4984137363169621 on epoch=262
03/18/2022 09:38:30 - INFO - __main__ - Saving model with best Classification-F1: 0.48619347789805833 -> 0.4984137363169621 on epoch=262, global_step=1050
03/18/2022 09:38:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
03/18/2022 09:38:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
03/18/2022 09:38:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
03/18/2022 09:38:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
03/18/2022 09:38:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/18/2022 09:38:43 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.5198274650361937 on epoch=274
03/18/2022 09:38:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4984137363169621 -> 0.5198274650361937 on epoch=274, global_step=1100
03/18/2022 09:38:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
03/18/2022 09:38:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=279
03/18/2022 09:38:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
03/18/2022 09:38:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
03/18/2022 09:38:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
03/18/2022 09:38:56 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5214803312629399 on epoch=287
03/18/2022 09:38:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5198274650361937 -> 0.5214803312629399 on epoch=287, global_step=1150
03/18/2022 09:38:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
03/18/2022 09:39:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
03/18/2022 09:39:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
03/18/2022 09:39:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
03/18/2022 09:39:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
03/18/2022 09:39:09 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.5030094205703961 on epoch=299
03/18/2022 09:39:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/18/2022 09:39:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
03/18/2022 09:39:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
03/18/2022 09:39:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
03/18/2022 09:39:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
03/18/2022 09:39:22 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.4593137254901961 on epoch=312
03/18/2022 09:39:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
03/18/2022 09:39:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/18/2022 09:39:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
03/18/2022 09:39:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
03/18/2022 09:39:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/18/2022 09:39:35 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5451524794591447 on epoch=324
03/18/2022 09:39:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5214803312629399 -> 0.5451524794591447 on epoch=324, global_step=1300
03/18/2022 09:39:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
03/18/2022 09:39:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
03/18/2022 09:39:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
03/18/2022 09:39:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/18/2022 09:39:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/18/2022 09:39:48 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5460767302872566 on epoch=337
03/18/2022 09:39:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5451524794591447 -> 0.5460767302872566 on epoch=337, global_step=1350
03/18/2022 09:39:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
03/18/2022 09:39:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
03/18/2022 09:39:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/18/2022 09:39:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
03/18/2022 09:40:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=349
03/18/2022 09:40:01 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.5074398368116534 on epoch=349
03/18/2022 09:40:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
03/18/2022 09:40:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
03/18/2022 09:40:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
03/18/2022 09:40:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
03/18/2022 09:40:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
03/18/2022 09:40:14 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.5217148182665423 on epoch=362
03/18/2022 09:40:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/18/2022 09:40:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
03/18/2022 09:40:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/18/2022 09:40:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/18/2022 09:40:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
03/18/2022 09:40:27 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5200513538748832 on epoch=374
03/18/2022 09:40:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/18/2022 09:40:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/18/2022 09:40:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/18/2022 09:40:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 09:40:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
03/18/2022 09:40:40 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5390123896716189 on epoch=387
03/18/2022 09:40:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
03/18/2022 09:40:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/18/2022 09:40:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
03/18/2022 09:40:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/18/2022 09:40:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/18/2022 09:40:54 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.49421921921921924 on epoch=399
03/18/2022 09:40:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
03/18/2022 09:40:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/18/2022 09:41:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/18/2022 09:41:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
03/18/2022 09:41:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
03/18/2022 09:41:07 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5237403100775193 on epoch=412
03/18/2022 09:41:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/18/2022 09:41:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/18/2022 09:41:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/18/2022 09:41:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/18/2022 09:41:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/18/2022 09:41:20 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5268687965260546 on epoch=424
03/18/2022 09:41:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/18/2022 09:41:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 09:41:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
03/18/2022 09:41:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=434
03/18/2022 09:41:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/18/2022 09:41:33 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5165322580645162 on epoch=437
03/18/2022 09:41:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/18/2022 09:41:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/18/2022 09:41:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/18/2022 09:41:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 09:41:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
03/18/2022 09:41:46 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.4966793168880456 on epoch=449
03/18/2022 09:41:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 09:41:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/18/2022 09:41:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 09:41:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 09:41:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 09:41:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4939296439296439 on epoch=462
03/18/2022 09:42:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
03/18/2022 09:42:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
03/18/2022 09:42:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 09:42:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/18/2022 09:42:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 09:42:12 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.4966793168880456 on epoch=474
03/18/2022 09:42:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/18/2022 09:42:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
03/18/2022 09:42:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/18/2022 09:42:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/18/2022 09:42:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
03/18/2022 09:42:25 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5009476817042606 on epoch=487
03/18/2022 09:42:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
03/18/2022 09:42:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 09:42:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 09:42:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/18/2022 09:42:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
03/18/2022 09:42:39 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5172647527910685 on epoch=499
03/18/2022 09:42:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
03/18/2022 09:42:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/18/2022 09:42:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
03/18/2022 09:42:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/18/2022 09:42:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
03/18/2022 09:42:52 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5192396454256036 on epoch=512
03/18/2022 09:42:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/18/2022 09:42:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/18/2022 09:42:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 09:43:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 09:43:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
03/18/2022 09:43:05 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.5399698340874811 on epoch=524
03/18/2022 09:43:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/18/2022 09:43:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 09:43:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/18/2022 09:43:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
03/18/2022 09:43:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 09:43:18 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.506936882435975 on epoch=537
03/18/2022 09:43:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 09:43:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
03/18/2022 09:43:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/18/2022 09:43:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 09:43:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 09:43:32 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.5058257918552036 on epoch=549
03/18/2022 09:43:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
03/18/2022 09:43:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 09:43:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 09:43:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/18/2022 09:43:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 09:43:45 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.49306006493506493 on epoch=562
03/18/2022 09:43:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 09:43:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
03/18/2022 09:43:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 09:43:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
03/18/2022 09:43:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
03/18/2022 09:43:58 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5143738140417458 on epoch=574
03/18/2022 09:44:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 09:44:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/18/2022 09:44:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
03/18/2022 09:44:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
03/18/2022 09:44:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 09:44:11 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.49862351411747713 on epoch=587
03/18/2022 09:44:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 09:44:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 09:44:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/18/2022 09:44:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 09:44:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 09:44:25 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.4907407407407407 on epoch=599
03/18/2022 09:44:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
03/18/2022 09:44:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 09:44:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 09:44:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 09:44:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
03/18/2022 09:44:38 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.4792042042042042 on epoch=612
03/18/2022 09:44:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 09:44:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=617
03/18/2022 09:44:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 09:44:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 09:44:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
03/18/2022 09:44:51 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.49517217655362206 on epoch=624
03/18/2022 09:44:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 09:44:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 09:44:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 09:45:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 09:45:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 09:45:04 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.4863714660024584 on epoch=637
03/18/2022 09:45:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 09:45:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/18/2022 09:45:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=644
03/18/2022 09:45:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 09:45:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 09:45:17 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.4844492160191751 on epoch=649
03/18/2022 09:45:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 09:45:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 09:45:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/18/2022 09:45:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 09:45:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/18/2022 09:45:31 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5292988640814728 on epoch=662
03/18/2022 09:45:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 09:45:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
03/18/2022 09:45:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 09:45:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/18/2022 09:45:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 09:45:44 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5320306281035594 on epoch=674
03/18/2022 09:45:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 09:45:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 09:45:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/18/2022 09:45:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
03/18/2022 09:45:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 09:45:58 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5373391138003452 on epoch=687
03/18/2022 09:46:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
03/18/2022 09:46:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
03/18/2022 09:46:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=694
03/18/2022 09:46:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
03/18/2022 09:46:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 09:46:11 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.5080128205128205 on epoch=699
03/18/2022 09:46:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 09:46:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/18/2022 09:46:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=707
03/18/2022 09:46:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 09:46:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/18/2022 09:46:24 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.5444132444132445 on epoch=712
03/18/2022 09:46:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 09:46:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 09:46:32 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 09:46:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/18/2022 09:46:37 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 09:46:38 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.551195597235913 on epoch=724
03/18/2022 09:46:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5460767302872566 -> 0.551195597235913 on epoch=724, global_step=2900
03/18/2022 09:46:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 09:46:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/18/2022 09:46:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=732
03/18/2022 09:46:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/18/2022 09:46:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 09:46:51 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5023605621431708 on epoch=737
03/18/2022 09:46:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 09:46:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/18/2022 09:46:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 09:47:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/18/2022 09:47:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 09:47:04 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5025282259092183 on epoch=749
03/18/2022 09:47:04 - INFO - __main__ - save last model!
03/18/2022 09:47:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 09:47:05 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 09:47:05 - INFO - __main__ - Printing 3 examples
03/18/2022 09:47:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:47:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:47:05 - INFO - __main__ - Printing 3 examples
03/18/2022 09:47:05 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 09:47:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:47:05 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:47:05 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:47:05 - INFO - __main__ - Printing 3 examples
03/18/2022 09:47:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:47:05 - INFO - __main__ - ['others']
03/18/2022 09:47:05 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:47:05 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:47:05 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:47:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:47:12 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 09:47:20 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:47:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:47:21 - INFO - __main__ - Starting training!
03/18/2022 09:48:42 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_100_0.3_8_predictions.txt
03/18/2022 09:48:42 - INFO - __main__ - Classification-F1 on test data: 0.3024
03/18/2022 09:48:43 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.551195597235913, test_performance=0.302443498007047
03/18/2022 09:48:43 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
03/18/2022 09:48:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:48:44 - INFO - __main__ - Printing 3 examples
03/18/2022 09:48:44 - INFO - __main__ -  [emo] how cause yes am listening
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ -  [emo] ok that way i like living wwrong
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 09:48:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:48:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 09:48:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 09:48:44 - INFO - __main__ - Printing 3 examples
03/18/2022 09:48:44 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
03/18/2022 09:48:44 - INFO - __main__ - ['others']
03/18/2022 09:48:44 - INFO - __main__ - Tokenizing Input ...
03/18/2022 09:48:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 09:48:44 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 09:48:59 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 09:49:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 09:49:00 - INFO - __main__ - Starting training!
03/18/2022 09:49:02 - INFO - __main__ - Step 10 Global step 10 Train loss 7.44 on epoch=2
03/18/2022 09:49:05 - INFO - __main__ - Step 20 Global step 20 Train loss 5.45 on epoch=4
03/18/2022 09:49:07 - INFO - __main__ - Step 30 Global step 30 Train loss 3.23 on epoch=7
03/18/2022 09:49:09 - INFO - __main__ - Step 40 Global step 40 Train loss 2.01 on epoch=9
03/18/2022 09:49:12 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
03/18/2022 09:49:13 - INFO - __main__ - Global step 50 Train loss 3.92 Classification-F1 0.10126582278481013 on epoch=12
03/18/2022 09:49:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
03/18/2022 09:49:15 - INFO - __main__ - Step 60 Global step 60 Train loss 1.21 on epoch=14
03/18/2022 09:49:17 - INFO - __main__ - Step 70 Global step 70 Train loss 1.17 on epoch=17
03/18/2022 09:49:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=19
03/18/2022 09:49:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=22
03/18/2022 09:49:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
03/18/2022 09:49:25 - INFO - __main__ - Global step 100 Train loss 1.09 Classification-F1 0.24575634413743636 on epoch=24
03/18/2022 09:49:25 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.24575634413743636 on epoch=24, global_step=100
03/18/2022 09:49:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
03/18/2022 09:49:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=29
03/18/2022 09:49:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
03/18/2022 09:49:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
03/18/2022 09:49:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
03/18/2022 09:49:38 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.21256038647342995 on epoch=37
03/18/2022 09:49:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=39
03/18/2022 09:49:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
03/18/2022 09:49:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=44
03/18/2022 09:49:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.00 on epoch=47
03/18/2022 09:49:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.92 on epoch=49
03/18/2022 09:49:50 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.1658496732026144 on epoch=49
03/18/2022 09:49:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
03/18/2022 09:49:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
03/18/2022 09:49:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
03/18/2022 09:49:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=59
03/18/2022 09:50:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=62
03/18/2022 09:50:03 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.10126582278481013 on epoch=62
03/18/2022 09:50:05 - INFO - __main__ - Step 260 Global step 260 Train loss 1.00 on epoch=64
03/18/2022 09:50:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.91 on epoch=67
03/18/2022 09:50:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
03/18/2022 09:50:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
03/18/2022 09:50:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=74
03/18/2022 09:50:15 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.2634485612426789 on epoch=74
03/18/2022 09:50:15 - INFO - __main__ - Saving model with best Classification-F1: 0.24575634413743636 -> 0.2634485612426789 on epoch=74, global_step=300
03/18/2022 09:50:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=77
03/18/2022 09:50:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=79
03/18/2022 09:50:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
03/18/2022 09:50:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
03/18/2022 09:50:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.86 on epoch=87
03/18/2022 09:50:27 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.13197586726998492 on epoch=87
03/18/2022 09:50:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=89
03/18/2022 09:50:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
03/18/2022 09:50:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
03/18/2022 09:50:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=97
03/18/2022 09:50:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.80 on epoch=99
03/18/2022 09:50:40 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.3312937062937063 on epoch=99
03/18/2022 09:50:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2634485612426789 -> 0.3312937062937063 on epoch=99, global_step=400
03/18/2022 09:50:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
03/18/2022 09:50:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
03/18/2022 09:50:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=107
03/18/2022 09:50:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
03/18/2022 09:50:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=112
03/18/2022 09:50:52 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.19615384615384615 on epoch=112
03/18/2022 09:50:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=114
03/18/2022 09:50:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.79 on epoch=117
03/18/2022 09:50:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.85 on epoch=119
03/18/2022 09:51:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=122
03/18/2022 09:51:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.73 on epoch=124
03/18/2022 09:51:05 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.21844585561497326 on epoch=124
03/18/2022 09:51:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
03/18/2022 09:51:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.80 on epoch=129
03/18/2022 09:51:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=132
03/18/2022 09:51:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=134
03/18/2022 09:51:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.74 on epoch=137
03/18/2022 09:51:17 - INFO - __main__ - Global step 550 Train loss 0.75 Classification-F1 0.2747471166572131 on epoch=137
03/18/2022 09:51:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=139
03/18/2022 09:51:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=142
03/18/2022 09:51:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=144
03/18/2022 09:51:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.72 on epoch=147
03/18/2022 09:51:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
03/18/2022 09:51:30 - INFO - __main__ - Global step 600 Train loss 0.70 Classification-F1 0.16347687400318978 on epoch=149
03/18/2022 09:51:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.67 on epoch=152
03/18/2022 09:51:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.70 on epoch=154
03/18/2022 09:51:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=157
03/18/2022 09:51:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.71 on epoch=159
03/18/2022 09:51:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=162
03/18/2022 09:51:42 - INFO - __main__ - Global step 650 Train loss 0.66 Classification-F1 0.2776686033264981 on epoch=162
03/18/2022 09:51:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=164
03/18/2022 09:51:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=167
03/18/2022 09:51:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=169
03/18/2022 09:51:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=172
03/18/2022 09:51:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=174
03/18/2022 09:51:55 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.3106318082788671 on epoch=174
03/18/2022 09:51:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.63 on epoch=177
03/18/2022 09:51:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=179
03/18/2022 09:52:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.58 on epoch=182
03/18/2022 09:52:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=184
03/18/2022 09:52:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=187
03/18/2022 09:52:07 - INFO - __main__ - Global step 750 Train loss 0.57 Classification-F1 0.4534455128205128 on epoch=187
03/18/2022 09:52:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3312937062937063 -> 0.4534455128205128 on epoch=187, global_step=750
03/18/2022 09:52:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=189
03/18/2022 09:52:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=192
03/18/2022 09:52:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=194
03/18/2022 09:52:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=197
03/18/2022 09:52:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=199
03/18/2022 09:52:19 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.4134333964144865 on epoch=199
03/18/2022 09:52:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=202
03/18/2022 09:52:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=204
03/18/2022 09:52:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=207
03/18/2022 09:52:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=209
03/18/2022 09:52:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=212
03/18/2022 09:52:32 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.4454417012513362 on epoch=212
03/18/2022 09:52:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=214
03/18/2022 09:52:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=217
03/18/2022 09:52:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=219
03/18/2022 09:52:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=222
03/18/2022 09:52:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=224
03/18/2022 09:52:44 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.4335125448028674 on epoch=224
03/18/2022 09:52:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=227
03/18/2022 09:52:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=229
03/18/2022 09:52:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=232
03/18/2022 09:52:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=234
03/18/2022 09:52:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=237
03/18/2022 09:52:57 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.4356398297525295 on epoch=237
03/18/2022 09:52:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=239
03/18/2022 09:53:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
03/18/2022 09:53:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=244
03/18/2022 09:53:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=247
03/18/2022 09:53:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=249
03/18/2022 09:53:09 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.4160067873303167 on epoch=249
03/18/2022 09:53:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=252
03/18/2022 09:53:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
03/18/2022 09:53:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=257
03/18/2022 09:53:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=259
03/18/2022 09:53:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=262
03/18/2022 09:53:22 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.4679695052558157 on epoch=262
03/18/2022 09:53:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4534455128205128 -> 0.4679695052558157 on epoch=262, global_step=1050
03/18/2022 09:53:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
03/18/2022 09:53:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
03/18/2022 09:53:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=269
03/18/2022 09:53:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=272
03/18/2022 09:53:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=274
03/18/2022 09:53:34 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.5293945769050958 on epoch=274
03/18/2022 09:53:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4679695052558157 -> 0.5293945769050958 on epoch=274, global_step=1100
03/18/2022 09:53:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=277
03/18/2022 09:53:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=279
03/18/2022 09:53:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
03/18/2022 09:53:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
03/18/2022 09:53:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=287
03/18/2022 09:53:47 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.5102671451355661 on epoch=287
03/18/2022 09:53:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=289
03/18/2022 09:53:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=292
03/18/2022 09:53:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=294
03/18/2022 09:53:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=297
03/18/2022 09:53:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=299
03/18/2022 09:54:00 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.5829519879969992 on epoch=299
03/18/2022 09:54:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5293945769050958 -> 0.5829519879969992 on epoch=299, global_step=1200
03/18/2022 09:54:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
03/18/2022 09:54:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
03/18/2022 09:54:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
03/18/2022 09:54:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
03/18/2022 09:54:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=312
03/18/2022 09:54:12 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.5628054740957966 on epoch=312
03/18/2022 09:54:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
03/18/2022 09:54:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
03/18/2022 09:54:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
03/18/2022 09:54:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
03/18/2022 09:54:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
03/18/2022 09:54:25 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.6101895830796087 on epoch=324
03/18/2022 09:54:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5829519879969992 -> 0.6101895830796087 on epoch=324, global_step=1300
03/18/2022 09:54:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=327
03/18/2022 09:54:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/18/2022 09:54:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
03/18/2022 09:54:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
03/18/2022 09:54:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=337
03/18/2022 09:54:37 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.5895487530637699 on epoch=337
03/18/2022 09:54:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
03/18/2022 09:54:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
03/18/2022 09:54:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=344
03/18/2022 09:54:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
03/18/2022 09:54:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
03/18/2022 09:54:50 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.5411466885604816 on epoch=349
03/18/2022 09:54:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=352
03/18/2022 09:54:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=354
03/18/2022 09:54:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
03/18/2022 09:54:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
03/18/2022 09:55:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=362
03/18/2022 09:55:02 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.522791276823535 on epoch=362
03/18/2022 09:55:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
03/18/2022 09:55:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
03/18/2022 09:55:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
03/18/2022 09:55:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
03/18/2022 09:55:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
03/18/2022 09:55:15 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.5201067073170732 on epoch=374
03/18/2022 09:55:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/18/2022 09:55:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/18/2022 09:55:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
03/18/2022 09:55:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
03/18/2022 09:55:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
03/18/2022 09:55:28 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.5205425963488843 on epoch=387
03/18/2022 09:55:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
03/18/2022 09:55:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 09:55:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
03/18/2022 09:55:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
03/18/2022 09:55:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=399
03/18/2022 09:55:40 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.48889184292410104 on epoch=399
03/18/2022 09:55:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/18/2022 09:55:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
03/18/2022 09:55:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/18/2022 09:55:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
03/18/2022 09:55:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
03/18/2022 09:55:53 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.5401148297223879 on epoch=412
03/18/2022 09:55:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/18/2022 09:55:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=417
03/18/2022 09:56:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
03/18/2022 09:56:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
03/18/2022 09:56:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
03/18/2022 09:56:05 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.5740646833446023 on epoch=424
03/18/2022 09:56:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/18/2022 09:56:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 09:56:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
03/18/2022 09:56:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/18/2022 09:56:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/18/2022 09:56:18 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.5633459835547123 on epoch=437
03/18/2022 09:56:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/18/2022 09:56:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/18/2022 09:56:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/18/2022 09:56:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
03/18/2022 09:56:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
03/18/2022 09:56:31 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5141852770885028 on epoch=449
03/18/2022 09:56:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 09:56:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
03/18/2022 09:56:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
03/18/2022 09:56:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
03/18/2022 09:56:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/18/2022 09:56:43 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.48749999999999993 on epoch=462
03/18/2022 09:56:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 09:56:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
03/18/2022 09:56:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
03/18/2022 09:56:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 09:56:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
03/18/2022 09:56:56 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.5692915218531918 on epoch=474
03/18/2022 09:56:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
03/18/2022 09:57:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
03/18/2022 09:57:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 09:57:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 09:57:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 09:57:09 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5564311027725661 on epoch=487
03/18/2022 09:57:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
03/18/2022 09:57:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
03/18/2022 09:57:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/18/2022 09:57:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 09:57:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 09:57:21 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5396177427427427 on epoch=499
03/18/2022 09:57:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
03/18/2022 09:57:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
03/18/2022 09:57:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=507
03/18/2022 09:57:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/18/2022 09:57:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/18/2022 09:57:34 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.565229490337249 on epoch=512
03/18/2022 09:57:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
03/18/2022 09:57:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/18/2022 09:57:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
03/18/2022 09:57:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
03/18/2022 09:57:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
03/18/2022 09:57:47 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.5862633793668276 on epoch=524
03/18/2022 09:57:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/18/2022 09:57:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 09:57:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
03/18/2022 09:57:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 09:57:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/18/2022 09:57:59 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.5442703533026114 on epoch=537
03/18/2022 09:58:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/18/2022 09:58:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/18/2022 09:58:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
03/18/2022 09:58:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 09:58:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
03/18/2022 09:58:12 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.5801900584795322 on epoch=549
03/18/2022 09:58:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=552
03/18/2022 09:58:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/18/2022 09:58:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 09:58:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
03/18/2022 09:58:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 09:58:25 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5440934065934065 on epoch=562
03/18/2022 09:58:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/18/2022 09:58:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 09:58:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=569
03/18/2022 09:58:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 09:58:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
03/18/2022 09:58:38 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.5406649245063879 on epoch=574
03/18/2022 09:58:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 09:58:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 09:58:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
03/18/2022 09:58:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
03/18/2022 09:58:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/18/2022 09:58:50 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5181306306306306 on epoch=587
03/18/2022 09:58:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/18/2022 09:58:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 09:58:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
03/18/2022 09:59:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 09:59:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/18/2022 09:59:03 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.5467004005903436 on epoch=599
03/18/2022 09:59:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
03/18/2022 09:59:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 09:59:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/18/2022 09:59:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/18/2022 09:59:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 09:59:16 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.5198507166592272 on epoch=612
03/18/2022 09:59:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 09:59:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 09:59:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/18/2022 09:59:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/18/2022 09:59:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/18/2022 09:59:29 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5448796912211546 on epoch=624
03/18/2022 09:59:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 09:59:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=629
03/18/2022 09:59:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 09:59:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/18/2022 09:59:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
03/18/2022 09:59:41 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.5619354110207769 on epoch=637
03/18/2022 09:59:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/18/2022 09:59:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
03/18/2022 09:59:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 09:59:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 09:59:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
03/18/2022 09:59:54 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.4923668521462639 on epoch=649
03/18/2022 09:59:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
03/18/2022 09:59:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
03/18/2022 10:00:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=657
03/18/2022 10:00:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
03/18/2022 10:00:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
03/18/2022 10:00:07 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.5987116331943918 on epoch=662
03/18/2022 10:00:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 10:00:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
03/18/2022 10:00:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/18/2022 10:00:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 10:00:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/18/2022 10:00:20 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5515586436639068 on epoch=674
03/18/2022 10:00:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 10:00:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
03/18/2022 10:00:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=682
03/18/2022 10:00:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
03/18/2022 10:00:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
03/18/2022 10:00:33 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.5128384352522284 on epoch=687
03/18/2022 10:00:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 10:00:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 10:00:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 10:00:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
03/18/2022 10:00:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/18/2022 10:00:45 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5915567615935262 on epoch=699
03/18/2022 10:00:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/18/2022 10:00:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
03/18/2022 10:00:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 10:00:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
03/18/2022 10:00:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 10:00:58 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.5914381914381914 on epoch=712
03/18/2022 10:01:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 10:01:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
03/18/2022 10:01:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 10:01:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 10:01:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 10:01:11 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6031882868089764 on epoch=724
03/18/2022 10:01:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
03/18/2022 10:01:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
03/18/2022 10:01:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 10:01:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
03/18/2022 10:01:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 10:01:25 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5572757475083057 on epoch=737
03/18/2022 10:01:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 10:01:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
03/18/2022 10:01:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 10:01:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 10:01:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 10:01:38 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5450052115905775 on epoch=749
03/18/2022 10:01:38 - INFO - __main__ - save last model!
03/18/2022 10:01:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 10:01:38 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 10:01:38 - INFO - __main__ - Printing 3 examples
03/18/2022 10:01:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:01:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:01:38 - INFO - __main__ - Printing 3 examples
03/18/2022 10:01:38 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 10:01:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:01:38 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:01:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:01:38 - INFO - __main__ - Printing 3 examples
03/18/2022 10:01:38 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:01:38 - INFO - __main__ - ['others']
03/18/2022 10:01:38 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:01:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:01:38 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:01:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:01:45 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 10:01:53 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:01:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:01:54 - INFO - __main__ - Starting training!
03/18/2022 10:03:11 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_100_0.2_8_predictions.txt
03/18/2022 10:03:11 - INFO - __main__ - Classification-F1 on test data: 0.3707
03/18/2022 10:03:11 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.6101895830796087, test_performance=0.37070829585822745
03/18/2022 10:03:11 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
03/18/2022 10:03:12 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:03:12 - INFO - __main__ - Printing 3 examples
03/18/2022 10:03:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:03:12 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:03:12 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:03:12 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:03:12 - INFO - __main__ - Printing 3 examples
03/18/2022 10:03:12 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:03:12 - INFO - __main__ - ['others']
03/18/2022 10:03:12 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:03:12 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:03:12 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:03:27 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:03:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:03:28 - INFO - __main__ - Starting training!
03/18/2022 10:03:31 - INFO - __main__ - Step 10 Global step 10 Train loss 6.31 on epoch=2
03/18/2022 10:03:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.85 on epoch=4
03/18/2022 10:03:36 - INFO - __main__ - Step 30 Global step 30 Train loss 4.03 on epoch=7
03/18/2022 10:03:38 - INFO - __main__ - Step 40 Global step 40 Train loss 2.95 on epoch=9
03/18/2022 10:03:40 - INFO - __main__ - Step 50 Global step 50 Train loss 2.81 on epoch=12
03/18/2022 10:03:42 - INFO - __main__ - Global step 50 Train loss 3.99 Classification-F1 0.1 on epoch=12
03/18/2022 10:03:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 10:03:44 - INFO - __main__ - Step 60 Global step 60 Train loss 2.36 on epoch=14
03/18/2022 10:03:46 - INFO - __main__ - Step 70 Global step 70 Train loss 2.11 on epoch=17
03/18/2022 10:03:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.87 on epoch=19
03/18/2022 10:03:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.58 on epoch=22
03/18/2022 10:03:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.30 on epoch=24
03/18/2022 10:03:54 - INFO - __main__ - Global step 100 Train loss 1.85 Classification-F1 0.10294117647058824 on epoch=24
03/18/2022 10:03:54 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10294117647058824 on epoch=24, global_step=100
03/18/2022 10:03:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.29 on epoch=27
03/18/2022 10:03:59 - INFO - __main__ - Step 120 Global step 120 Train loss 1.29 on epoch=29
03/18/2022 10:04:01 - INFO - __main__ - Step 130 Global step 130 Train loss 1.24 on epoch=32
03/18/2022 10:04:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.18 on epoch=34
03/18/2022 10:04:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
03/18/2022 10:04:06 - INFO - __main__ - Global step 150 Train loss 1.22 Classification-F1 0.12631578947368421 on epoch=37
03/18/2022 10:04:06 - INFO - __main__ - Saving model with best Classification-F1: 0.10294117647058824 -> 0.12631578947368421 on epoch=37, global_step=150
03/18/2022 10:04:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.15 on epoch=39
03/18/2022 10:04:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
03/18/2022 10:04:13 - INFO - __main__ - Step 180 Global step 180 Train loss 1.06 on epoch=44
03/18/2022 10:04:16 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=47
03/18/2022 10:04:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.99 on epoch=49
03/18/2022 10:04:19 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.09589041095890412 on epoch=49
03/18/2022 10:04:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
03/18/2022 10:04:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
03/18/2022 10:04:26 - INFO - __main__ - Step 230 Global step 230 Train loss 1.01 on epoch=57
03/18/2022 10:04:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.02 on epoch=59
03/18/2022 10:04:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
03/18/2022 10:04:32 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.1640625 on epoch=62
03/18/2022 10:04:32 - INFO - __main__ - Saving model with best Classification-F1: 0.12631578947368421 -> 0.1640625 on epoch=62, global_step=250
03/18/2022 10:04:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.97 on epoch=64
03/18/2022 10:04:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.99 on epoch=67
03/18/2022 10:04:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
03/18/2022 10:04:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=72
03/18/2022 10:04:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
03/18/2022 10:04:45 - INFO - __main__ - Global step 300 Train loss 0.96 Classification-F1 0.1 on epoch=74
03/18/2022 10:04:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
03/18/2022 10:04:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=79
03/18/2022 10:04:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
03/18/2022 10:04:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
03/18/2022 10:04:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.89 on epoch=87
03/18/2022 10:04:58 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.1 on epoch=87
03/18/2022 10:05:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=89
03/18/2022 10:05:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
03/18/2022 10:05:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.91 on epoch=94
03/18/2022 10:05:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
03/18/2022 10:05:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=99
03/18/2022 10:05:11 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.1 on epoch=99
03/18/2022 10:05:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=102
03/18/2022 10:05:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
03/18/2022 10:05:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
03/18/2022 10:05:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=109
03/18/2022 10:05:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
03/18/2022 10:05:23 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.1565276828434723 on epoch=112
03/18/2022 10:05:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=114
03/18/2022 10:05:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.84 on epoch=117
03/18/2022 10:05:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.88 on epoch=119
03/18/2022 10:05:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=122
03/18/2022 10:05:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.77 on epoch=124
03/18/2022 10:05:36 - INFO - __main__ - Global step 500 Train loss 0.83 Classification-F1 0.28202860091404985 on epoch=124
03/18/2022 10:05:36 - INFO - __main__ - Saving model with best Classification-F1: 0.1640625 -> 0.28202860091404985 on epoch=124, global_step=500
03/18/2022 10:05:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
03/18/2022 10:05:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
03/18/2022 10:05:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=132
03/18/2022 10:05:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
03/18/2022 10:05:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.77 on epoch=137
03/18/2022 10:05:49 - INFO - __main__ - Global step 550 Train loss 0.79 Classification-F1 0.30147484094852517 on epoch=137
03/18/2022 10:05:49 - INFO - __main__ - Saving model with best Classification-F1: 0.28202860091404985 -> 0.30147484094852517 on epoch=137, global_step=550
03/18/2022 10:05:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=139
03/18/2022 10:05:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.75 on epoch=142
03/18/2022 10:05:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.79 on epoch=144
03/18/2022 10:05:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=147
03/18/2022 10:06:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.73 on epoch=149
03/18/2022 10:06:02 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.29019831999602363 on epoch=149
03/18/2022 10:06:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=152
03/18/2022 10:06:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=154
03/18/2022 10:06:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.87 on epoch=157
03/18/2022 10:06:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.88 on epoch=159
03/18/2022 10:06:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.79 on epoch=162
03/18/2022 10:06:15 - INFO - __main__ - Global step 650 Train loss 0.79 Classification-F1 0.14819427148194272 on epoch=162
03/18/2022 10:06:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=164
03/18/2022 10:06:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.71 on epoch=167
03/18/2022 10:06:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.66 on epoch=169
03/18/2022 10:06:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.75 on epoch=172
03/18/2022 10:06:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=174
03/18/2022 10:06:28 - INFO - __main__ - Global step 700 Train loss 0.72 Classification-F1 0.35052083333333334 on epoch=174
03/18/2022 10:06:28 - INFO - __main__ - Saving model with best Classification-F1: 0.30147484094852517 -> 0.35052083333333334 on epoch=174, global_step=700
03/18/2022 10:06:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.73 on epoch=177
03/18/2022 10:06:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=179
03/18/2022 10:06:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=182
03/18/2022 10:06:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.60 on epoch=184
03/18/2022 10:06:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=187
03/18/2022 10:06:41 - INFO - __main__ - Global step 750 Train loss 0.62 Classification-F1 0.28500797448165865 on epoch=187
03/18/2022 10:06:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=189
03/18/2022 10:06:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=192
03/18/2022 10:06:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=194
03/18/2022 10:06:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=197
03/18/2022 10:06:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=199
03/18/2022 10:06:54 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.414986559139785 on epoch=199
03/18/2022 10:06:54 - INFO - __main__ - Saving model with best Classification-F1: 0.35052083333333334 -> 0.414986559139785 on epoch=199, global_step=800
03/18/2022 10:06:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=202
03/18/2022 10:06:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=204
03/18/2022 10:07:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=207
03/18/2022 10:07:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=209
03/18/2022 10:07:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=212
03/18/2022 10:07:07 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.36760082023239915 on epoch=212
03/18/2022 10:07:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=214
03/18/2022 10:07:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=217
03/18/2022 10:07:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=219
03/18/2022 10:07:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=222
03/18/2022 10:07:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=224
03/18/2022 10:07:20 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.4784992784992785 on epoch=224
03/18/2022 10:07:20 - INFO - __main__ - Saving model with best Classification-F1: 0.414986559139785 -> 0.4784992784992785 on epoch=224, global_step=900
03/18/2022 10:07:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=227
03/18/2022 10:07:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=229
03/18/2022 10:07:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=232
03/18/2022 10:07:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
03/18/2022 10:07:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
03/18/2022 10:07:33 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.5318189900426743 on epoch=237
03/18/2022 10:07:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4784992784992785 -> 0.5318189900426743 on epoch=237, global_step=950
03/18/2022 10:07:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
03/18/2022 10:07:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
03/18/2022 10:07:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=244
03/18/2022 10:07:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
03/18/2022 10:07:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
03/18/2022 10:07:46 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.5436868686868687 on epoch=249
03/18/2022 10:07:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5318189900426743 -> 0.5436868686868687 on epoch=249, global_step=1000
03/18/2022 10:07:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
03/18/2022 10:07:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=254
03/18/2022 10:07:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=257
03/18/2022 10:07:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
03/18/2022 10:07:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
03/18/2022 10:07:59 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.5440839710461803 on epoch=262
03/18/2022 10:07:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5436868686868687 -> 0.5440839710461803 on epoch=262, global_step=1050
03/18/2022 10:08:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
03/18/2022 10:08:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
03/18/2022 10:08:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
03/18/2022 10:08:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
03/18/2022 10:08:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
03/18/2022 10:08:12 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.5378342245989305 on epoch=274
03/18/2022 10:08:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
03/18/2022 10:08:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
03/18/2022 10:08:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=282
03/18/2022 10:08:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
03/18/2022 10:08:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
03/18/2022 10:08:25 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.5178571428571428 on epoch=287
03/18/2022 10:08:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
03/18/2022 10:08:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
03/18/2022 10:08:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
03/18/2022 10:08:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=297
03/18/2022 10:08:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
03/18/2022 10:08:37 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.5726986523449505 on epoch=299
03/18/2022 10:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5440839710461803 -> 0.5726986523449505 on epoch=299, global_step=1200
03/18/2022 10:08:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
03/18/2022 10:08:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
03/18/2022 10:08:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
03/18/2022 10:08:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
03/18/2022 10:08:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
03/18/2022 10:08:51 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5650715841892312 on epoch=312
03/18/2022 10:08:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
03/18/2022 10:08:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
03/18/2022 10:08:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/18/2022 10:09:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
03/18/2022 10:09:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/18/2022 10:09:04 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.5807017543859649 on epoch=324
03/18/2022 10:09:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5726986523449505 -> 0.5807017543859649 on epoch=324, global_step=1300
03/18/2022 10:09:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
03/18/2022 10:09:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/18/2022 10:09:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
03/18/2022 10:09:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/18/2022 10:09:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
03/18/2022 10:09:17 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5729056979056979 on epoch=337
03/18/2022 10:09:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
03/18/2022 10:09:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
03/18/2022 10:09:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
03/18/2022 10:09:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/18/2022 10:09:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/18/2022 10:09:30 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.5452380952380953 on epoch=349
03/18/2022 10:09:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
03/18/2022 10:09:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
03/18/2022 10:09:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
03/18/2022 10:09:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
03/18/2022 10:09:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
03/18/2022 10:09:43 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5367317867317867 on epoch=362
03/18/2022 10:09:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
03/18/2022 10:09:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
03/18/2022 10:09:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/18/2022 10:09:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
03/18/2022 10:09:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 10:09:56 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.5838128451031677 on epoch=374
03/18/2022 10:09:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5807017543859649 -> 0.5838128451031677 on epoch=374, global_step=1500
03/18/2022 10:09:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
03/18/2022 10:10:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
03/18/2022 10:10:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
03/18/2022 10:10:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
03/18/2022 10:10:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
03/18/2022 10:10:09 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.5529312735195088 on epoch=387
03/18/2022 10:10:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
03/18/2022 10:10:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
03/18/2022 10:10:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
03/18/2022 10:10:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
03/18/2022 10:10:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
03/18/2022 10:10:22 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.5657009657009657 on epoch=399
03/18/2022 10:10:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
03/18/2022 10:10:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
03/18/2022 10:10:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 10:10:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/18/2022 10:10:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
03/18/2022 10:10:35 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.568082788671024 on epoch=412
03/18/2022 10:10:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/18/2022 10:10:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/18/2022 10:10:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 10:10:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/18/2022 10:10:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/18/2022 10:10:48 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5850840336134454 on epoch=424
03/18/2022 10:10:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5838128451031677 -> 0.5850840336134454 on epoch=424, global_step=1700
03/18/2022 10:10:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=427
03/18/2022 10:10:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 10:10:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
03/18/2022 10:10:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 10:11:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/18/2022 10:11:01 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6031784005468216 on epoch=437
03/18/2022 10:11:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5850840336134454 -> 0.6031784005468216 on epoch=437, global_step=1750
03/18/2022 10:11:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/18/2022 10:11:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 10:11:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/18/2022 10:11:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
03/18/2022 10:11:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
03/18/2022 10:11:14 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6008674915324532 on epoch=449
03/18/2022 10:11:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/18/2022 10:11:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
03/18/2022 10:11:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
03/18/2022 10:11:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 10:11:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/18/2022 10:11:27 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.5744166842364187 on epoch=462
03/18/2022 10:11:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/18/2022 10:11:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 10:11:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 10:11:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 10:11:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 10:11:40 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.557229788467111 on epoch=474
03/18/2022 10:11:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
03/18/2022 10:11:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/18/2022 10:11:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 10:11:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/18/2022 10:11:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/18/2022 10:11:53 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5572148617511521 on epoch=487
03/18/2022 10:11:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 10:11:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 10:12:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 10:12:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 10:12:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 10:12:06 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5689826302729528 on epoch=499
03/18/2022 10:12:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 10:12:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
03/18/2022 10:12:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 10:12:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 10:12:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
03/18/2022 10:12:19 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6234281105061505 on epoch=512
03/18/2022 10:12:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6031784005468216 -> 0.6234281105061505 on epoch=512, global_step=2050
03/18/2022 10:12:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/18/2022 10:12:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/18/2022 10:12:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 10:12:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
03/18/2022 10:12:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 10:12:33 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5287418452935694 on epoch=524
03/18/2022 10:12:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 10:12:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
03/18/2022 10:12:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/18/2022 10:12:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 10:12:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 10:12:46 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5559038662486939 on epoch=537
03/18/2022 10:12:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 10:12:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
03/18/2022 10:12:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 10:12:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 10:12:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 10:12:59 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5961538461538461 on epoch=549
03/18/2022 10:13:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 10:13:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/18/2022 10:13:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 10:13:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 10:13:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 10:13:12 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.613368336654627 on epoch=562
03/18/2022 10:13:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 10:13:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 10:13:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 10:13:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 10:13:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 10:13:25 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5725596724258464 on epoch=574
03/18/2022 10:13:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 10:13:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 10:13:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 10:13:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
03/18/2022 10:13:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/18/2022 10:13:37 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6087098087098086 on epoch=587
03/18/2022 10:13:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 10:13:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 10:13:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
03/18/2022 10:13:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 10:13:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 10:13:50 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5810983397190294 on epoch=599
03/18/2022 10:13:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 10:13:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 10:13:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 10:13:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 10:14:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 10:14:02 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6045062657965884 on epoch=612
03/18/2022 10:14:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 10:14:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 10:14:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 10:14:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 10:14:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 10:14:15 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6078169515669515 on epoch=624
03/18/2022 10:14:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 10:14:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/18/2022 10:14:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 10:14:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 10:14:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
03/18/2022 10:14:28 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6143927756830982 on epoch=637
03/18/2022 10:14:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 10:14:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 10:14:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 10:14:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 10:14:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 10:14:40 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6149080086580087 on epoch=649
03/18/2022 10:14:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 10:14:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 10:14:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 10:14:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 10:14:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
03/18/2022 10:14:53 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5890720081135903 on epoch=662
03/18/2022 10:14:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 10:14:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=667
03/18/2022 10:15:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 10:15:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
03/18/2022 10:15:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 10:15:06 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6309625073611203 on epoch=674
03/18/2022 10:15:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6234281105061505 -> 0.6309625073611203 on epoch=674, global_step=2700
03/18/2022 10:15:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 10:15:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/18/2022 10:15:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 10:15:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
03/18/2022 10:15:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 10:15:18 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5703372489579387 on epoch=687
03/18/2022 10:15:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 10:15:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 10:15:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 10:15:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 10:15:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 10:15:31 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.5847572362278246 on epoch=699
03/18/2022 10:15:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 10:15:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 10:15:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 10:15:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 10:15:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 10:15:44 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5938049899446959 on epoch=712
03/18/2022 10:15:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 10:15:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
03/18/2022 10:15:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 10:15:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
03/18/2022 10:15:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/18/2022 10:15:57 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5735734606787238 on epoch=724
03/18/2022 10:15:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 10:16:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=729
03/18/2022 10:16:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 10:16:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 10:16:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/18/2022 10:16:10 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5656795560682537 on epoch=737
03/18/2022 10:16:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 10:16:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 10:16:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 10:16:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 10:16:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 10:16:22 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5727344992050875 on epoch=749
03/18/2022 10:16:22 - INFO - __main__ - save last model!
03/18/2022 10:16:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 10:16:22 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 10:16:22 - INFO - __main__ - Printing 3 examples
03/18/2022 10:16:22 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 10:16:22 - INFO - __main__ - ['others']
03/18/2022 10:16:22 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 10:16:22 - INFO - __main__ - ['others']
03/18/2022 10:16:22 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 10:16:22 - INFO - __main__ - ['others']
03/18/2022 10:16:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:16:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:16:23 - INFO - __main__ - Printing 3 examples
03/18/2022 10:16:23 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 10:16:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:16:23 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:16:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:16:23 - INFO - __main__ - Printing 3 examples
03/18/2022 10:16:23 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:16:23 - INFO - __main__ - ['others']
03/18/2022 10:16:23 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:16:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:16:23 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:16:24 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:16:30 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 10:16:41 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:16:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:16:42 - INFO - __main__ - Starting training!
03/18/2022 10:17:46 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_13_0.5_8_predictions.txt
03/18/2022 10:17:46 - INFO - __main__ - Classification-F1 on test data: 0.2932
03/18/2022 10:17:47 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.6309625073611203, test_performance=0.29321159191359536
03/18/2022 10:17:47 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
03/18/2022 10:17:48 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:17:48 - INFO - __main__ - Printing 3 examples
03/18/2022 10:17:48 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:17:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:17:48 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:17:48 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:17:48 - INFO - __main__ - Printing 3 examples
03/18/2022 10:17:48 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:17:48 - INFO - __main__ - ['others']
03/18/2022 10:17:48 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:17:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:17:48 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:18:03 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:18:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:18:04 - INFO - __main__ - Starting training!
03/18/2022 10:18:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.69 on epoch=2
03/18/2022 10:18:09 - INFO - __main__ - Step 20 Global step 20 Train loss 2.90 on epoch=4
03/18/2022 10:18:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.43 on epoch=7
03/18/2022 10:18:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.13 on epoch=9
03/18/2022 10:18:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=12
03/18/2022 10:18:17 - INFO - __main__ - Global step 50 Train loss 2.63 Classification-F1 0.1 on epoch=12
03/18/2022 10:18:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 10:18:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=14
03/18/2022 10:18:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=17
03/18/2022 10:18:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=19
03/18/2022 10:18:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
03/18/2022 10:18:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=24
03/18/2022 10:18:30 - INFO - __main__ - Global step 100 Train loss 0.97 Classification-F1 0.1 on epoch=24
03/18/2022 10:18:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=27
03/18/2022 10:18:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=29
03/18/2022 10:18:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
03/18/2022 10:18:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
03/18/2022 10:18:42 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
03/18/2022 10:18:42 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.10126582278481013 on epoch=37
03/18/2022 10:18:42 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=37, global_step=150
03/18/2022 10:18:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
03/18/2022 10:18:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=42
03/18/2022 10:18:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
03/18/2022 10:18:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
03/18/2022 10:18:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
03/18/2022 10:18:55 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.1565276828434723 on epoch=49
03/18/2022 10:18:55 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.1565276828434723 on epoch=49, global_step=200
03/18/2022 10:18:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=52
03/18/2022 10:19:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
03/18/2022 10:19:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=57
03/18/2022 10:19:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=59
03/18/2022 10:19:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=62
03/18/2022 10:19:07 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.161078431372549 on epoch=62
03/18/2022 10:19:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1565276828434723 -> 0.161078431372549 on epoch=62, global_step=250
03/18/2022 10:19:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
03/18/2022 10:19:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=67
03/18/2022 10:19:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
03/18/2022 10:19:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=72
03/18/2022 10:19:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=74
03/18/2022 10:19:20 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.26832151300236406 on epoch=74
03/18/2022 10:19:20 - INFO - __main__ - Saving model with best Classification-F1: 0.161078431372549 -> 0.26832151300236406 on epoch=74, global_step=300
03/18/2022 10:19:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=77
03/18/2022 10:19:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
03/18/2022 10:19:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
03/18/2022 10:19:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=84
03/18/2022 10:19:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
03/18/2022 10:19:32 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.1238095238095238 on epoch=87
03/18/2022 10:19:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.75 on epoch=89
03/18/2022 10:19:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=92
03/18/2022 10:19:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.79 on epoch=94
03/18/2022 10:19:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=97
03/18/2022 10:19:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.79 on epoch=99
03/18/2022 10:19:45 - INFO - __main__ - Global step 400 Train loss 0.79 Classification-F1 0.3809677419354839 on epoch=99
03/18/2022 10:19:45 - INFO - __main__ - Saving model with best Classification-F1: 0.26832151300236406 -> 0.3809677419354839 on epoch=99, global_step=400
03/18/2022 10:19:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=102
03/18/2022 10:19:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=104
03/18/2022 10:19:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=107
03/18/2022 10:19:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.74 on epoch=109
03/18/2022 10:19:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.72 on epoch=112
03/18/2022 10:19:57 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.13034188034188032 on epoch=112
03/18/2022 10:20:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=114
03/18/2022 10:20:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=117
03/18/2022 10:20:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.66 on epoch=119
03/18/2022 10:20:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=122
03/18/2022 10:20:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=124
03/18/2022 10:20:10 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.5024966508342468 on epoch=124
03/18/2022 10:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3809677419354839 -> 0.5024966508342468 on epoch=124, global_step=500
03/18/2022 10:20:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=127
03/18/2022 10:20:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=129
03/18/2022 10:20:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=132
03/18/2022 10:20:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
03/18/2022 10:20:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=137
03/18/2022 10:20:22 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.3579931972789116 on epoch=137
03/18/2022 10:20:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
03/18/2022 10:20:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=142
03/18/2022 10:20:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=144
03/18/2022 10:20:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=147
03/18/2022 10:20:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
03/18/2022 10:20:35 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.5346560846560846 on epoch=149
03/18/2022 10:20:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5024966508342468 -> 0.5346560846560846 on epoch=149, global_step=600
03/18/2022 10:20:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=152
03/18/2022 10:20:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
03/18/2022 10:20:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=157
03/18/2022 10:20:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
03/18/2022 10:20:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=162
03/18/2022 10:20:47 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.5291206753943128 on epoch=162
03/18/2022 10:20:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
03/18/2022 10:20:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
03/18/2022 10:20:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
03/18/2022 10:20:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
03/18/2022 10:20:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=174
03/18/2022 10:21:00 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.5061824409250879 on epoch=174
03/18/2022 10:21:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=177
03/18/2022 10:21:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
03/18/2022 10:21:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
03/18/2022 10:21:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
03/18/2022 10:21:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
03/18/2022 10:21:12 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.5906653225806451 on epoch=187
03/18/2022 10:21:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5346560846560846 -> 0.5906653225806451 on epoch=187, global_step=750
03/18/2022 10:21:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
03/18/2022 10:21:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=192
03/18/2022 10:21:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
03/18/2022 10:21:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=197
03/18/2022 10:21:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
03/18/2022 10:21:25 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.5735621278544788 on epoch=199
03/18/2022 10:21:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
03/18/2022 10:21:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=204
03/18/2022 10:21:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
03/18/2022 10:21:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
03/18/2022 10:21:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
03/18/2022 10:21:37 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5786572622779519 on epoch=212
03/18/2022 10:21:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
03/18/2022 10:21:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
03/18/2022 10:21:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
03/18/2022 10:21:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
03/18/2022 10:21:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
03/18/2022 10:21:50 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.5769841269841269 on epoch=224
03/18/2022 10:21:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
03/18/2022 10:21:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
03/18/2022 10:21:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
03/18/2022 10:21:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
03/18/2022 10:22:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
03/18/2022 10:22:03 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.5219176818014027 on epoch=237
03/18/2022 10:22:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
03/18/2022 10:22:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
03/18/2022 10:22:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=244
03/18/2022 10:22:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
03/18/2022 10:22:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
03/18/2022 10:22:15 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5550595238095237 on epoch=249
03/18/2022 10:22:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
03/18/2022 10:22:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=254
03/18/2022 10:22:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
03/18/2022 10:22:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
03/18/2022 10:22:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
03/18/2022 10:22:28 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6460902636916835 on epoch=262
03/18/2022 10:22:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5906653225806451 -> 0.6460902636916835 on epoch=262, global_step=1050
03/18/2022 10:22:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/18/2022 10:22:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
03/18/2022 10:22:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
03/18/2022 10:22:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
03/18/2022 10:22:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
03/18/2022 10:22:41 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.6017579445571332 on epoch=274
03/18/2022 10:22:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
03/18/2022 10:22:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
03/18/2022 10:22:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
03/18/2022 10:22:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
03/18/2022 10:22:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
03/18/2022 10:22:54 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.62262660672352 on epoch=287
03/18/2022 10:22:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=289
03/18/2022 10:22:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
03/18/2022 10:23:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=294
03/18/2022 10:23:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
03/18/2022 10:23:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
03/18/2022 10:23:06 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5567864247821144 on epoch=299
03/18/2022 10:23:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
03/18/2022 10:23:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
03/18/2022 10:23:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
03/18/2022 10:23:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=309
03/18/2022 10:23:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
03/18/2022 10:23:19 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.5757316719791366 on epoch=312
03/18/2022 10:23:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
03/18/2022 10:23:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
03/18/2022 10:23:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
03/18/2022 10:23:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
03/18/2022 10:23:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/18/2022 10:23:31 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.5815789473684211 on epoch=324
03/18/2022 10:23:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/18/2022 10:23:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
03/18/2022 10:23:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
03/18/2022 10:23:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
03/18/2022 10:23:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
03/18/2022 10:23:44 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.6091575091575091 on epoch=337
03/18/2022 10:23:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
03/18/2022 10:23:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
03/18/2022 10:23:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
03/18/2022 10:23:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
03/18/2022 10:23:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/18/2022 10:23:57 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.6361315161315161 on epoch=349
03/18/2022 10:23:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 10:24:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
03/18/2022 10:24:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
03/18/2022 10:24:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/18/2022 10:24:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
03/18/2022 10:24:09 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6226760455692331 on epoch=362
03/18/2022 10:24:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
03/18/2022 10:24:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
03/18/2022 10:24:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
03/18/2022 10:24:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
03/18/2022 10:24:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
03/18/2022 10:24:22 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6019959172123859 on epoch=374
03/18/2022 10:24:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/18/2022 10:24:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
03/18/2022 10:24:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
03/18/2022 10:24:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 10:24:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
03/18/2022 10:24:35 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6142807642807643 on epoch=387
03/18/2022 10:24:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
03/18/2022 10:24:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
03/18/2022 10:24:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/18/2022 10:24:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
03/18/2022 10:24:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
03/18/2022 10:24:48 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.596103896103896 on epoch=399
03/18/2022 10:24:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/18/2022 10:24:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/18/2022 10:24:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 10:24:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/18/2022 10:24:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
03/18/2022 10:25:00 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6414767708885356 on epoch=412
03/18/2022 10:25:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/18/2022 10:25:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/18/2022 10:25:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 10:25:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
03/18/2022 10:25:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
03/18/2022 10:25:13 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6121080139372823 on epoch=424
03/18/2022 10:25:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 10:25:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 10:25:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
03/18/2022 10:25:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 10:25:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
03/18/2022 10:25:26 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.603571131101116 on epoch=437
03/18/2022 10:25:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
03/18/2022 10:25:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 10:25:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 10:25:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 10:25:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/18/2022 10:25:39 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7114055299539171 on epoch=449
03/18/2022 10:25:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6460902636916835 -> 0.7114055299539171 on epoch=449, global_step=1800
03/18/2022 10:25:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 10:25:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/18/2022 10:25:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 10:25:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=459
03/18/2022 10:25:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 10:25:51 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6444564694564695 on epoch=462
03/18/2022 10:25:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/18/2022 10:25:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 10:25:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 10:26:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 10:26:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 10:26:04 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6620579225842385 on epoch=474
03/18/2022 10:26:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 10:26:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 10:26:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 10:26:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/18/2022 10:26:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 10:26:17 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6435990506770906 on epoch=487
03/18/2022 10:26:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 10:26:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 10:26:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 10:26:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 10:26:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 10:26:30 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6876447876447876 on epoch=499
03/18/2022 10:26:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 10:26:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 10:26:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 10:26:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 10:26:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
03/18/2022 10:26:42 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6576535433693542 on epoch=512
03/18/2022 10:26:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
03/18/2022 10:26:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 10:26:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 10:26:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 10:26:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 10:26:55 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6938133874239352 on epoch=524
03/18/2022 10:26:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/18/2022 10:27:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 10:27:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/18/2022 10:27:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 10:27:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 10:27:08 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6614311002178649 on epoch=537
03/18/2022 10:27:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 10:27:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 10:27:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 10:27:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 10:27:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 10:27:21 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6479779411764706 on epoch=549
03/18/2022 10:27:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 10:27:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 10:27:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 10:27:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 10:27:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 10:27:33 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6540293040293039 on epoch=562
03/18/2022 10:27:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 10:27:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 10:27:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 10:27:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 10:27:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 10:27:46 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6536693548387097 on epoch=574
03/18/2022 10:27:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 10:27:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 10:27:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
03/18/2022 10:27:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 10:27:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 10:27:59 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6359012395854501 on epoch=587
03/18/2022 10:28:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 10:28:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 10:28:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 10:28:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 10:28:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
03/18/2022 10:28:12 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6300840336134454 on epoch=599
03/18/2022 10:28:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 10:28:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 10:28:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 10:28:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 10:28:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 10:28:24 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5898809523809525 on epoch=612
03/18/2022 10:28:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 10:28:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 10:28:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 10:28:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 10:28:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 10:28:37 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6709744697860148 on epoch=624
03/18/2022 10:28:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 10:28:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 10:28:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 10:28:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 10:28:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 10:28:50 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6127712639109698 on epoch=637
03/18/2022 10:28:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 10:28:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 10:28:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 10:28:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/18/2022 10:29:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 10:29:03 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6098499453762612 on epoch=649
03/18/2022 10:29:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 10:29:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 10:29:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=657
03/18/2022 10:29:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 10:29:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 10:29:15 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6426068926068926 on epoch=662
03/18/2022 10:29:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 10:29:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 10:29:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 10:29:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 10:29:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 10:29:28 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6361657348499454 on epoch=674
03/18/2022 10:29:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 10:29:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 10:29:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
03/18/2022 10:29:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 10:29:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 10:29:41 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6483599891569531 on epoch=687
03/18/2022 10:29:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
03/18/2022 10:29:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
03/18/2022 10:29:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 10:29:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 10:29:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 10:29:53 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6557450307450308 on epoch=699
03/18/2022 10:29:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 10:29:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
03/18/2022 10:30:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 10:30:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 10:30:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 10:30:06 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6344386795999699 on epoch=712
03/18/2022 10:30:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 10:30:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 10:30:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=719
03/18/2022 10:30:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
03/18/2022 10:30:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 10:30:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6365739081256323 on epoch=724
03/18/2022 10:30:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 10:30:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 10:30:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 10:30:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 10:30:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 10:30:32 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6365739081256323 on epoch=737
03/18/2022 10:30:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 10:30:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
03/18/2022 10:30:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 10:30:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/18/2022 10:30:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 10:30:45 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6101002506265665 on epoch=749
03/18/2022 10:30:45 - INFO - __main__ - save last model!
03/18/2022 10:30:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 10:30:45 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 10:30:45 - INFO - __main__ - Printing 3 examples
03/18/2022 10:30:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 10:30:45 - INFO - __main__ - ['others']
03/18/2022 10:30:45 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 10:30:45 - INFO - __main__ - ['others']
03/18/2022 10:30:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 10:30:45 - INFO - __main__ - ['others']
03/18/2022 10:30:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:30:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:30:46 - INFO - __main__ - Printing 3 examples
03/18/2022 10:30:46 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 10:30:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:30:46 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:30:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:30:46 - INFO - __main__ - Printing 3 examples
03/18/2022 10:30:46 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:30:46 - INFO - __main__ - ['others']
03/18/2022 10:30:46 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:30:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:30:46 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:30:47 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:30:53 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 10:31:04 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:31:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:31:05 - INFO - __main__ - Starting training!
03/18/2022 10:32:20 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_13_0.4_8_predictions.txt
03/18/2022 10:32:20 - INFO - __main__ - Classification-F1 on test data: 0.3958
03/18/2022 10:32:21 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.7114055299539171, test_performance=0.39579029249302233
03/18/2022 10:32:21 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
03/18/2022 10:32:21 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:32:21 - INFO - __main__ - Printing 3 examples
03/18/2022 10:32:21 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:32:21 - INFO - __main__ - ['others']
03/18/2022 10:32:21 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:32:21 - INFO - __main__ - ['others']
03/18/2022 10:32:21 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:32:21 - INFO - __main__ - ['others']
03/18/2022 10:32:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:32:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:32:22 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:32:22 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:32:22 - INFO - __main__ - Printing 3 examples
03/18/2022 10:32:22 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:32:22 - INFO - __main__ - ['others']
03/18/2022 10:32:22 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:32:22 - INFO - __main__ - ['others']
03/18/2022 10:32:22 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:32:22 - INFO - __main__ - ['others']
03/18/2022 10:32:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:32:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:32:22 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:32:40 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:32:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:32:41 - INFO - __main__ - Starting training!
03/18/2022 10:32:44 - INFO - __main__ - Step 10 Global step 10 Train loss 7.30 on epoch=2
03/18/2022 10:32:46 - INFO - __main__ - Step 20 Global step 20 Train loss 6.02 on epoch=4
03/18/2022 10:32:49 - INFO - __main__ - Step 30 Global step 30 Train loss 5.03 on epoch=7
03/18/2022 10:32:51 - INFO - __main__ - Step 40 Global step 40 Train loss 3.68 on epoch=9
03/18/2022 10:32:53 - INFO - __main__ - Step 50 Global step 50 Train loss 2.79 on epoch=12
03/18/2022 10:32:55 - INFO - __main__ - Global step 50 Train loss 4.96 Classification-F1 0.11859154929577466 on epoch=12
03/18/2022 10:32:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11859154929577466 on epoch=12, global_step=50
03/18/2022 10:32:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.95 on epoch=14
03/18/2022 10:32:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.58 on epoch=17
03/18/2022 10:33:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.36 on epoch=19
03/18/2022 10:33:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=22
03/18/2022 10:33:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=24
03/18/2022 10:33:07 - INFO - __main__ - Global step 100 Train loss 1.41 Classification-F1 0.1856060606060606 on epoch=24
03/18/2022 10:33:07 - INFO - __main__ - Saving model with best Classification-F1: 0.11859154929577466 -> 0.1856060606060606 on epoch=24, global_step=100
03/18/2022 10:33:10 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=27
03/18/2022 10:33:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.17 on epoch=29
03/18/2022 10:33:15 - INFO - __main__ - Step 130 Global step 130 Train loss 1.05 on epoch=32
03/18/2022 10:33:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
03/18/2022 10:33:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
03/18/2022 10:33:20 - INFO - __main__ - Global step 150 Train loss 1.03 Classification-F1 0.09493670886075949 on epoch=37
03/18/2022 10:33:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
03/18/2022 10:33:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
03/18/2022 10:33:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=44
03/18/2022 10:33:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=47
03/18/2022 10:33:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.98 on epoch=49
03/18/2022 10:33:33 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.18776953596687943 on epoch=49
03/18/2022 10:33:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1856060606060606 -> 0.18776953596687943 on epoch=49, global_step=200
03/18/2022 10:33:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
03/18/2022 10:33:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
03/18/2022 10:33:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
03/18/2022 10:33:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
03/18/2022 10:33:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
03/18/2022 10:33:46 - INFO - __main__ - Global step 250 Train loss 0.94 Classification-F1 0.09333333333333334 on epoch=62
03/18/2022 10:33:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.99 on epoch=64
03/18/2022 10:33:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=67
03/18/2022 10:33:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=69
03/18/2022 10:33:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
03/18/2022 10:33:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.91 on epoch=74
03/18/2022 10:33:59 - INFO - __main__ - Global step 300 Train loss 0.90 Classification-F1 0.21021325585220108 on epoch=74
03/18/2022 10:33:59 - INFO - __main__ - Saving model with best Classification-F1: 0.18776953596687943 -> 0.21021325585220108 on epoch=74, global_step=300
03/18/2022 10:34:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.79 on epoch=77
03/18/2022 10:34:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=79
03/18/2022 10:34:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=82
03/18/2022 10:34:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.76 on epoch=84
03/18/2022 10:34:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.89 on epoch=87
03/18/2022 10:34:11 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.25688405797101443 on epoch=87
03/18/2022 10:34:11 - INFO - __main__ - Saving model with best Classification-F1: 0.21021325585220108 -> 0.25688405797101443 on epoch=87, global_step=350
03/18/2022 10:34:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
03/18/2022 10:34:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.97 on epoch=92
03/18/2022 10:34:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=94
03/18/2022 10:34:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.85 on epoch=97
03/18/2022 10:34:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
03/18/2022 10:34:24 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.3843703148425787 on epoch=99
03/18/2022 10:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.25688405797101443 -> 0.3843703148425787 on epoch=99, global_step=400
03/18/2022 10:34:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
03/18/2022 10:34:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=104
03/18/2022 10:34:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=107
03/18/2022 10:34:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.65 on epoch=109
03/18/2022 10:34:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=112
03/18/2022 10:34:37 - INFO - __main__ - Global step 450 Train loss 0.65 Classification-F1 0.4537570272864391 on epoch=112
03/18/2022 10:34:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3843703148425787 -> 0.4537570272864391 on epoch=112, global_step=450
03/18/2022 10:34:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=114
03/18/2022 10:34:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=117
03/18/2022 10:34:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=119
03/18/2022 10:34:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=122
03/18/2022 10:34:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=124
03/18/2022 10:34:50 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.29822054213534943 on epoch=124
03/18/2022 10:34:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=127
03/18/2022 10:34:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=129
03/18/2022 10:34:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=132
03/18/2022 10:35:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=134
03/18/2022 10:35:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=137
03/18/2022 10:35:03 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.5782181390877043 on epoch=137
03/18/2022 10:35:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4537570272864391 -> 0.5782181390877043 on epoch=137, global_step=550
03/18/2022 10:35:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=139
03/18/2022 10:35:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=142
03/18/2022 10:35:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=144
03/18/2022 10:35:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=147
03/18/2022 10:35:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=149
03/18/2022 10:35:16 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.48162948162948166 on epoch=149
03/18/2022 10:35:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=152
03/18/2022 10:35:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
03/18/2022 10:35:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=157
03/18/2022 10:35:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
03/18/2022 10:35:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
03/18/2022 10:35:29 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.6692255717255717 on epoch=162
03/18/2022 10:35:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5782181390877043 -> 0.6692255717255717 on epoch=162, global_step=650
03/18/2022 10:35:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
03/18/2022 10:35:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
03/18/2022 10:35:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
03/18/2022 10:35:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
03/18/2022 10:35:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
03/18/2022 10:35:42 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6117389060887513 on epoch=174
03/18/2022 10:35:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
03/18/2022 10:35:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.13 on epoch=179
03/18/2022 10:35:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=182
03/18/2022 10:35:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
03/18/2022 10:35:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
03/18/2022 10:35:54 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.5753978687237864 on epoch=187
03/18/2022 10:35:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
03/18/2022 10:35:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
03/18/2022 10:36:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=194
03/18/2022 10:36:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
03/18/2022 10:36:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=199
03/18/2022 10:36:07 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.6222879565105175 on epoch=199
03/18/2022 10:36:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
03/18/2022 10:36:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
03/18/2022 10:36:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
03/18/2022 10:36:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=209
03/18/2022 10:36:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
03/18/2022 10:36:20 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.6006124181421295 on epoch=212
03/18/2022 10:36:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=214
03/18/2022 10:36:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
03/18/2022 10:36:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=219
03/18/2022 10:36:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
03/18/2022 10:36:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
03/18/2022 10:36:33 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.604459104883553 on epoch=224
03/18/2022 10:36:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
03/18/2022 10:36:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
03/18/2022 10:36:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
03/18/2022 10:36:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
03/18/2022 10:36:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
03/18/2022 10:36:46 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6721211725456206 on epoch=237
03/18/2022 10:36:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6692255717255717 -> 0.6721211725456206 on epoch=237, global_step=950
03/18/2022 10:36:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
03/18/2022 10:36:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
03/18/2022 10:36:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
03/18/2022 10:36:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
03/18/2022 10:36:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
03/18/2022 10:36:59 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.6604339380832231 on epoch=249
03/18/2022 10:37:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
03/18/2022 10:37:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
03/18/2022 10:37:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
03/18/2022 10:37:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
03/18/2022 10:37:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
03/18/2022 10:37:12 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.691547342621259 on epoch=262
03/18/2022 10:37:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6721211725456206 -> 0.691547342621259 on epoch=262, global_step=1050
03/18/2022 10:37:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/18/2022 10:37:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
03/18/2022 10:37:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
03/18/2022 10:37:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/18/2022 10:37:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
03/18/2022 10:37:25 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.7080781287677839 on epoch=274
03/18/2022 10:37:25 - INFO - __main__ - Saving model with best Classification-F1: 0.691547342621259 -> 0.7080781287677839 on epoch=274, global_step=1100
03/18/2022 10:37:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/18/2022 10:37:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
03/18/2022 10:37:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
03/18/2022 10:37:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
03/18/2022 10:37:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
03/18/2022 10:37:38 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6721377533537244 on epoch=287
03/18/2022 10:37:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
03/18/2022 10:37:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/18/2022 10:37:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
03/18/2022 10:37:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
03/18/2022 10:37:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/18/2022 10:37:51 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.5922283272283273 on epoch=299
03/18/2022 10:37:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
03/18/2022 10:37:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
03/18/2022 10:37:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
03/18/2022 10:38:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
03/18/2022 10:38:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/18/2022 10:38:04 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6493475274725274 on epoch=312
03/18/2022 10:38:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
03/18/2022 10:38:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
03/18/2022 10:38:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
03/18/2022 10:38:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
03/18/2022 10:38:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/18/2022 10:38:18 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.5954309917724552 on epoch=324
03/18/2022 10:38:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/18/2022 10:38:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/18/2022 10:38:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
03/18/2022 10:38:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
03/18/2022 10:38:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
03/18/2022 10:38:30 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6603880868586751 on epoch=337
03/18/2022 10:38:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=339
03/18/2022 10:38:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
03/18/2022 10:38:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/18/2022 10:38:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/18/2022 10:38:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
03/18/2022 10:38:43 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6584249084249084 on epoch=349
03/18/2022 10:38:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 10:38:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
03/18/2022 10:38:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
03/18/2022 10:38:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
03/18/2022 10:38:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
03/18/2022 10:38:56 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7092322795678828 on epoch=362
03/18/2022 10:38:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7080781287677839 -> 0.7092322795678828 on epoch=362, global_step=1450
03/18/2022 10:38:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
03/18/2022 10:39:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/18/2022 10:39:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/18/2022 10:39:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
03/18/2022 10:39:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 10:39:09 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.741379173290938 on epoch=374
03/18/2022 10:39:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7092322795678828 -> 0.741379173290938 on epoch=374, global_step=1500
03/18/2022 10:39:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
03/18/2022 10:39:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 10:39:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/18/2022 10:39:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/18/2022 10:39:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
03/18/2022 10:39:22 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7140804597701149 on epoch=387
03/18/2022 10:39:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/18/2022 10:39:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 10:39:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/18/2022 10:39:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/18/2022 10:39:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
03/18/2022 10:39:35 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.683068783068783 on epoch=399
03/18/2022 10:39:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
03/18/2022 10:39:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/18/2022 10:39:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 10:39:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
03/18/2022 10:39:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
03/18/2022 10:39:48 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.705525569409417 on epoch=412
03/18/2022 10:39:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/18/2022 10:39:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
03/18/2022 10:39:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/18/2022 10:39:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
03/18/2022 10:40:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/18/2022 10:40:01 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.690129173290938 on epoch=424
03/18/2022 10:40:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/18/2022 10:40:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
03/18/2022 10:40:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/18/2022 10:40:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 10:40:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
03/18/2022 10:40:14 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7084846866096866 on epoch=437
03/18/2022 10:40:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/18/2022 10:40:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 10:40:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
03/18/2022 10:40:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
03/18/2022 10:40:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 10:40:27 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6954636591478697 on epoch=449
03/18/2022 10:40:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 10:40:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
03/18/2022 10:40:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 10:40:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 10:40:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 10:40:40 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6966268563511113 on epoch=462
03/18/2022 10:40:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 10:40:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 10:40:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 10:40:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 10:40:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 10:40:53 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6912854030501089 on epoch=474
03/18/2022 10:40:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 10:40:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 10:41:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
03/18/2022 10:41:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/18/2022 10:41:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/18/2022 10:41:06 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7120337301587302 on epoch=487
03/18/2022 10:41:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 10:41:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 10:41:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 10:41:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 10:41:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 10:41:19 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.718448490507314 on epoch=499
03/18/2022 10:41:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 10:41:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
03/18/2022 10:41:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
03/18/2022 10:41:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
03/18/2022 10:41:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 10:41:32 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7247642204538757 on epoch=512
03/18/2022 10:41:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/18/2022 10:41:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/18/2022 10:41:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 10:41:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
03/18/2022 10:41:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 10:41:46 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7243657417850966 on epoch=524
03/18/2022 10:41:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 10:41:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 10:41:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
03/18/2022 10:41:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 10:41:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 10:41:59 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6640350877192982 on epoch=537
03/18/2022 10:42:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 10:42:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
03/18/2022 10:42:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 10:42:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 10:42:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 10:42:11 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6786818917814249 on epoch=549
03/18/2022 10:42:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
03/18/2022 10:42:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
03/18/2022 10:42:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/18/2022 10:42:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 10:42:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 10:42:24 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7264160401002507 on epoch=562
03/18/2022 10:42:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 10:42:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 10:42:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
03/18/2022 10:42:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 10:42:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 10:42:37 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7068863218056766 on epoch=574
03/18/2022 10:42:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 10:42:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/18/2022 10:42:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 10:42:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
03/18/2022 10:42:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 10:42:51 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6533613445378152 on epoch=587
03/18/2022 10:42:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/18/2022 10:42:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 10:42:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=594
03/18/2022 10:43:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 10:43:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=599
03/18/2022 10:43:03 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.7241258741258741 on epoch=599
03/18/2022 10:43:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 10:43:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/18/2022 10:43:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
03/18/2022 10:43:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 10:43:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 10:43:17 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7089285714285715 on epoch=612
03/18/2022 10:43:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 10:43:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 10:43:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/18/2022 10:43:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
03/18/2022 10:43:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
03/18/2022 10:43:30 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7279239766081872 on epoch=624
03/18/2022 10:43:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 10:43:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 10:43:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 10:43:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 10:43:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 10:43:43 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.7403030303030304 on epoch=637
03/18/2022 10:43:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 10:43:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 10:43:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 10:43:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 10:43:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/18/2022 10:43:56 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7253297608390864 on epoch=649
03/18/2022 10:43:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 10:44:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 10:44:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 10:44:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 10:44:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 10:44:09 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7395554895554896 on epoch=662
03/18/2022 10:44:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 10:44:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 10:44:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 10:44:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 10:44:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
03/18/2022 10:44:22 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7098901098901098 on epoch=674
03/18/2022 10:44:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 10:44:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 10:44:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/18/2022 10:44:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 10:44:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 10:44:35 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6832947754000386 on epoch=687
03/18/2022 10:44:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 10:44:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/18/2022 10:44:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 10:44:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 10:44:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 10:44:48 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7273088979985532 on epoch=699
03/18/2022 10:44:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 10:44:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 10:44:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 10:44:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 10:45:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 10:45:01 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7298738298738299 on epoch=712
03/18/2022 10:45:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/18/2022 10:45:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 10:45:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 10:45:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 10:45:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 10:45:14 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7298738298738299 on epoch=724
03/18/2022 10:45:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 10:45:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 10:45:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 10:45:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 10:45:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 10:45:27 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6510628275334157 on epoch=737
03/18/2022 10:45:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/18/2022 10:45:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/18/2022 10:45:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 10:45:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 10:45:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
03/18/2022 10:45:40 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7431244830438378 on epoch=749
03/18/2022 10:45:40 - INFO - __main__ - Saving model with best Classification-F1: 0.741379173290938 -> 0.7431244830438378 on epoch=749, global_step=3000
03/18/2022 10:45:40 - INFO - __main__ - save last model!
03/18/2022 10:45:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 10:45:40 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 10:45:40 - INFO - __main__ - Printing 3 examples
03/18/2022 10:45:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:45:40 - INFO - __main__ - Printing 3 examples
03/18/2022 10:45:40 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 10:45:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:45:40 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:45:40 - INFO - __main__ - Printing 3 examples
03/18/2022 10:45:40 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:45:40 - INFO - __main__ - ['others']
03/18/2022 10:45:40 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:45:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:45:40 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:45:42 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:45:48 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 10:45:59 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:46:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:46:00 - INFO - __main__ - Starting training!
03/18/2022 10:47:15 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_13_0.3_8_predictions.txt
03/18/2022 10:47:15 - INFO - __main__ - Classification-F1 on test data: 0.3871
03/18/2022 10:47:15 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7431244830438378, test_performance=0.38705055351665657
03/18/2022 10:47:15 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
03/18/2022 10:47:16 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:47:16 - INFO - __main__ - Printing 3 examples
03/18/2022 10:47:16 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 10:47:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:47:16 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 10:47:16 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 10:47:16 - INFO - __main__ - Printing 3 examples
03/18/2022 10:47:16 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ -  [emo] i did ask now you did tell ms
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ -  [emo] buddy how you tell me your contact no
03/18/2022 10:47:16 - INFO - __main__ - ['others']
03/18/2022 10:47:16 - INFO - __main__ - Tokenizing Input ...
03/18/2022 10:47:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 10:47:16 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 10:47:32 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 10:47:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 10:47:32 - INFO - __main__ - Starting training!
03/18/2022 10:47:35 - INFO - __main__ - Step 10 Global step 10 Train loss 7.50 on epoch=2
03/18/2022 10:47:38 - INFO - __main__ - Step 20 Global step 20 Train loss 4.66 on epoch=4
03/18/2022 10:47:40 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=7
03/18/2022 10:47:43 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=9
03/18/2022 10:47:45 - INFO - __main__ - Step 50 Global step 50 Train loss 1.39 on epoch=12
03/18/2022 10:47:47 - INFO - __main__ - Global step 50 Train loss 3.64 Classification-F1 0.16544477028347998 on epoch=12
03/18/2022 10:47:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16544477028347998 on epoch=12, global_step=50
03/18/2022 10:47:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=14
03/18/2022 10:47:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=17
03/18/2022 10:47:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.29 on epoch=19
03/18/2022 10:47:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.12 on epoch=22
03/18/2022 10:48:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=24
03/18/2022 10:48:01 - INFO - __main__ - Global step 100 Train loss 1.14 Classification-F1 0.10256410256410256 on epoch=24
03/18/2022 10:48:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=27
03/18/2022 10:48:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=29
03/18/2022 10:48:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=32
03/18/2022 10:48:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=34
03/18/2022 10:48:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=37
03/18/2022 10:48:14 - INFO - __main__ - Global step 150 Train loss 0.96 Classification-F1 0.1 on epoch=37
03/18/2022 10:48:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
03/18/2022 10:48:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=42
03/18/2022 10:48:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
03/18/2022 10:48:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
03/18/2022 10:48:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
03/18/2022 10:48:27 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.1095890410958904 on epoch=49
03/18/2022 10:48:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
03/18/2022 10:48:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
03/18/2022 10:48:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=57
03/18/2022 10:48:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=59
03/18/2022 10:48:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
03/18/2022 10:48:40 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.20980302336234538 on epoch=62
03/18/2022 10:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.16544477028347998 -> 0.20980302336234538 on epoch=62, global_step=250
03/18/2022 10:48:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=64
03/18/2022 10:48:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=67
03/18/2022 10:48:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
03/18/2022 10:48:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
03/18/2022 10:48:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
03/18/2022 10:48:53 - INFO - __main__ - Global step 300 Train loss 0.82 Classification-F1 0.10389610389610389 on epoch=74
03/18/2022 10:48:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=77
03/18/2022 10:48:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=79
03/18/2022 10:49:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
03/18/2022 10:49:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
03/18/2022 10:49:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=87
03/18/2022 10:49:06 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.1111111111111111 on epoch=87
03/18/2022 10:49:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.78 on epoch=89
03/18/2022 10:49:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=92
03/18/2022 10:49:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
03/18/2022 10:49:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=97
03/18/2022 10:49:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=99
03/18/2022 10:49:19 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.38231031543052 on epoch=99
03/18/2022 10:49:19 - INFO - __main__ - Saving model with best Classification-F1: 0.20980302336234538 -> 0.38231031543052 on epoch=99, global_step=400
03/18/2022 10:49:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=102
03/18/2022 10:49:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.80 on epoch=104
03/18/2022 10:49:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=107
03/18/2022 10:49:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
03/18/2022 10:49:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=112
03/18/2022 10:49:32 - INFO - __main__ - Global step 450 Train loss 0.85 Classification-F1 0.29088504088504086 on epoch=112
03/18/2022 10:49:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.76 on epoch=114
03/18/2022 10:49:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
03/18/2022 10:49:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
03/18/2022 10:49:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
03/18/2022 10:49:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.83 on epoch=124
03/18/2022 10:49:45 - INFO - __main__ - Global step 500 Train loss 0.82 Classification-F1 0.28412944125018424 on epoch=124
03/18/2022 10:49:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=127
03/18/2022 10:49:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.76 on epoch=129
03/18/2022 10:49:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=132
03/18/2022 10:49:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.73 on epoch=134
03/18/2022 10:49:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.80 on epoch=137
03/18/2022 10:49:58 - INFO - __main__ - Global step 550 Train loss 0.77 Classification-F1 0.29088504088504086 on epoch=137
03/18/2022 10:50:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.78 on epoch=139
03/18/2022 10:50:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=142
03/18/2022 10:50:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.85 on epoch=144
03/18/2022 10:50:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=147
03/18/2022 10:50:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=149
03/18/2022 10:50:11 - INFO - __main__ - Global step 600 Train loss 0.79 Classification-F1 0.2822721766051959 on epoch=149
03/18/2022 10:50:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
03/18/2022 10:50:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
03/18/2022 10:50:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.78 on epoch=157
03/18/2022 10:50:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.82 on epoch=159
03/18/2022 10:50:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=162
03/18/2022 10:50:24 - INFO - __main__ - Global step 650 Train loss 0.79 Classification-F1 0.12530217566478646 on epoch=162
03/18/2022 10:50:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.79 on epoch=164
03/18/2022 10:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.74 on epoch=167
03/18/2022 10:50:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.74 on epoch=169
03/18/2022 10:50:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.77 on epoch=172
03/18/2022 10:50:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.72 on epoch=174
03/18/2022 10:50:37 - INFO - __main__ - Global step 700 Train loss 0.75 Classification-F1 0.3644933497874674 on epoch=174
03/18/2022 10:50:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.78 on epoch=177
03/18/2022 10:50:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.77 on epoch=179
03/18/2022 10:50:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=182
03/18/2022 10:50:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.71 on epoch=184
03/18/2022 10:50:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.65 on epoch=187
03/18/2022 10:50:50 - INFO - __main__ - Global step 750 Train loss 0.72 Classification-F1 0.5479707792207792 on epoch=187
03/18/2022 10:50:50 - INFO - __main__ - Saving model with best Classification-F1: 0.38231031543052 -> 0.5479707792207792 on epoch=187, global_step=750
03/18/2022 10:50:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.68 on epoch=189
03/18/2022 10:50:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=192
03/18/2022 10:50:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=194
03/18/2022 10:51:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=197
03/18/2022 10:51:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.66 on epoch=199
03/18/2022 10:51:03 - INFO - __main__ - Global step 800 Train loss 0.66 Classification-F1 0.5705253121680731 on epoch=199
03/18/2022 10:51:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5479707792207792 -> 0.5705253121680731 on epoch=199, global_step=800
03/18/2022 10:51:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=202
03/18/2022 10:51:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.67 on epoch=204
03/18/2022 10:51:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=207
03/18/2022 10:51:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.60 on epoch=209
03/18/2022 10:51:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=212
03/18/2022 10:51:16 - INFO - __main__ - Global step 850 Train loss 0.62 Classification-F1 0.4855471113127646 on epoch=212
03/18/2022 10:51:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=214
03/18/2022 10:51:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=217
03/18/2022 10:51:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=219
03/18/2022 10:51:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
03/18/2022 10:51:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=224
03/18/2022 10:51:29 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.6840277777777777 on epoch=224
03/18/2022 10:51:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5705253121680731 -> 0.6840277777777777 on epoch=224, global_step=900
03/18/2022 10:51:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.57 on epoch=227
03/18/2022 10:51:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=229
03/18/2022 10:51:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=232
03/18/2022 10:51:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=234
03/18/2022 10:51:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=237
03/18/2022 10:51:42 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.6843822843822844 on epoch=237
03/18/2022 10:51:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6840277777777777 -> 0.6843822843822844 on epoch=237, global_step=950
03/18/2022 10:51:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=239
03/18/2022 10:51:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=242
03/18/2022 10:51:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=244
03/18/2022 10:51:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=247
03/18/2022 10:51:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=249
03/18/2022 10:51:55 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.5978619248968087 on epoch=249
03/18/2022 10:51:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=252
03/18/2022 10:52:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=254
03/18/2022 10:52:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=257
03/18/2022 10:52:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=259
03/18/2022 10:52:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=262
03/18/2022 10:52:08 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7065410901617798 on epoch=262
03/18/2022 10:52:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6843822843822844 -> 0.7065410901617798 on epoch=262, global_step=1050
03/18/2022 10:52:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=264
03/18/2022 10:52:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=267
03/18/2022 10:52:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=269
03/18/2022 10:52:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=272
03/18/2022 10:52:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=274
03/18/2022 10:52:22 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.6632732732732732 on epoch=274
03/18/2022 10:52:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=277
03/18/2022 10:52:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=279
03/18/2022 10:52:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=282
03/18/2022 10:52:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=284
03/18/2022 10:52:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=287
03/18/2022 10:52:35 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.5538523683345844 on epoch=287
03/18/2022 10:52:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=289
03/18/2022 10:52:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
03/18/2022 10:52:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
03/18/2022 10:52:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
03/18/2022 10:52:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=299
03/18/2022 10:52:48 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.594307064829868 on epoch=299
03/18/2022 10:52:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=302
03/18/2022 10:52:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
03/18/2022 10:52:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
03/18/2022 10:52:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
03/18/2022 10:53:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
03/18/2022 10:53:01 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.6307772756208755 on epoch=312
03/18/2022 10:53:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=314
03/18/2022 10:53:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
03/18/2022 10:53:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
03/18/2022 10:53:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=322
03/18/2022 10:53:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=324
03/18/2022 10:53:14 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.5501516324687057 on epoch=324
03/18/2022 10:53:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
03/18/2022 10:53:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=329
03/18/2022 10:53:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
03/18/2022 10:53:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
03/18/2022 10:53:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=337
03/18/2022 10:53:27 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.6103695464095908 on epoch=337
03/18/2022 10:53:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=339
03/18/2022 10:53:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
03/18/2022 10:53:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
03/18/2022 10:53:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
03/18/2022 10:53:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=349
03/18/2022 10:53:40 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.5507029007029006 on epoch=349
03/18/2022 10:53:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
03/18/2022 10:53:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
03/18/2022 10:53:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
03/18/2022 10:53:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=359
03/18/2022 10:53:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=362
03/18/2022 10:53:53 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.6508640855011092 on epoch=362
03/18/2022 10:53:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
03/18/2022 10:53:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
03/18/2022 10:54:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
03/18/2022 10:54:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
03/18/2022 10:54:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
03/18/2022 10:54:07 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.5961380662519182 on epoch=374
03/18/2022 10:54:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
03/18/2022 10:54:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
03/18/2022 10:54:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
03/18/2022 10:54:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
03/18/2022 10:54:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
03/18/2022 10:54:20 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.6233701025831171 on epoch=387
03/18/2022 10:54:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
03/18/2022 10:54:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
03/18/2022 10:54:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
03/18/2022 10:54:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
03/18/2022 10:54:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=399
03/18/2022 10:54:33 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.5901348039215686 on epoch=399
03/18/2022 10:54:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=402
03/18/2022 10:54:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
03/18/2022 10:54:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/18/2022 10:54:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
03/18/2022 10:54:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
03/18/2022 10:54:46 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6181617692497836 on epoch=412
03/18/2022 10:54:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/18/2022 10:54:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
03/18/2022 10:54:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
03/18/2022 10:54:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
03/18/2022 10:54:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
03/18/2022 10:55:00 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.5388071895424837 on epoch=424
03/18/2022 10:55:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
03/18/2022 10:55:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
03/18/2022 10:55:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
03/18/2022 10:55:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/18/2022 10:55:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/18/2022 10:55:13 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.5933528836754643 on epoch=437
03/18/2022 10:55:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=439
03/18/2022 10:55:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/18/2022 10:55:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
03/18/2022 10:55:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/18/2022 10:55:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/18/2022 10:55:26 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6645381645381645 on epoch=449
03/18/2022 10:55:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
03/18/2022 10:55:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
03/18/2022 10:55:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/18/2022 10:55:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
03/18/2022 10:55:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
03/18/2022 10:55:39 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6648076648076648 on epoch=462
03/18/2022 10:55:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/18/2022 10:55:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/18/2022 10:55:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
03/18/2022 10:55:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 10:55:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
03/18/2022 10:55:53 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5614583333333334 on epoch=474
03/18/2022 10:55:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
03/18/2022 10:55:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/18/2022 10:56:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
03/18/2022 10:56:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 10:56:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/18/2022 10:56:06 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.675121060569977 on epoch=487
03/18/2022 10:56:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
03/18/2022 10:56:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
03/18/2022 10:56:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
03/18/2022 10:56:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
03/18/2022 10:56:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/18/2022 10:56:19 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.6829861111111111 on epoch=499
03/18/2022 10:56:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=502
03/18/2022 10:56:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/18/2022 10:56:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/18/2022 10:56:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/18/2022 10:56:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
03/18/2022 10:56:32 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.6452958027151576 on epoch=512
03/18/2022 10:56:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/18/2022 10:56:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/18/2022 10:56:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/18/2022 10:56:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 10:56:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
03/18/2022 10:56:46 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6458558558558558 on epoch=524
03/18/2022 10:56:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
03/18/2022 10:56:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 10:56:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
03/18/2022 10:56:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 10:56:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 10:56:59 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.5842086834733894 on epoch=537
03/18/2022 10:57:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/18/2022 10:57:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
03/18/2022 10:57:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 10:57:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 10:57:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 10:57:12 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6026229916552498 on epoch=549
03/18/2022 10:57:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 10:57:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/18/2022 10:57:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
03/18/2022 10:57:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/18/2022 10:57:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 10:57:26 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6377616995073891 on epoch=562
03/18/2022 10:57:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/18/2022 10:57:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 10:57:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
03/18/2022 10:57:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
03/18/2022 10:57:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 10:57:39 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6361665813278716 on epoch=574
03/18/2022 10:57:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
03/18/2022 10:57:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/18/2022 10:57:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 10:57:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
03/18/2022 10:57:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
03/18/2022 10:57:52 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6154371119888362 on epoch=587
03/18/2022 10:57:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
03/18/2022 10:57:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 10:58:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/18/2022 10:58:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
03/18/2022 10:58:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 10:58:06 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.5425606641123882 on epoch=599
03/18/2022 10:58:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
03/18/2022 10:58:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 10:58:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 10:58:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 10:58:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/18/2022 10:58:19 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5518095644054724 on epoch=612
03/18/2022 10:58:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 10:58:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 10:58:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 10:58:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/18/2022 10:58:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
03/18/2022 10:58:32 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5904234654234655 on epoch=624
03/18/2022 10:58:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
03/18/2022 10:58:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/18/2022 10:58:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 10:58:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
03/18/2022 10:58:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 10:58:46 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6155309250136838 on epoch=637
03/18/2022 10:58:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
03/18/2022 10:58:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/18/2022 10:58:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/18/2022 10:58:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
03/18/2022 10:58:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
03/18/2022 10:58:59 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5880952380952381 on epoch=649
03/18/2022 10:59:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 10:59:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
03/18/2022 10:59:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/18/2022 10:59:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 10:59:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/18/2022 10:59:12 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6202380952380953 on epoch=662
03/18/2022 10:59:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/18/2022 10:59:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/18/2022 10:59:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/18/2022 10:59:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
03/18/2022 10:59:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 10:59:26 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6664614121510674 on epoch=674
03/18/2022 10:59:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
03/18/2022 10:59:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 10:59:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 10:59:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
03/18/2022 10:59:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 10:59:39 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.5236814011195106 on epoch=687
03/18/2022 10:59:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/18/2022 10:59:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
03/18/2022 10:59:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 10:59:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 10:59:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 10:59:52 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5413734115347019 on epoch=699
03/18/2022 10:59:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/18/2022 10:59:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 11:00:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 11:00:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
03/18/2022 11:00:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
03/18/2022 11:00:06 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6157563025210084 on epoch=712
03/18/2022 11:00:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 11:00:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 11:00:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 11:00:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
03/18/2022 11:00:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
03/18/2022 11:00:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5876984126984127 on epoch=724
03/18/2022 11:00:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 11:00:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/18/2022 11:00:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 11:00:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/18/2022 11:00:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 11:00:32 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6021241830065359 on epoch=737
03/18/2022 11:00:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 11:00:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/18/2022 11:00:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 11:00:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 11:00:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/18/2022 11:00:46 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5434695609992723 on epoch=749
03/18/2022 11:00:46 - INFO - __main__ - save last model!
03/18/2022 11:00:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 11:00:46 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 11:00:46 - INFO - __main__ - Printing 3 examples
03/18/2022 11:00:46 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 11:00:46 - INFO - __main__ - ['others']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 11:00:46 - INFO - __main__ - ['others']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 11:00:46 - INFO - __main__ - ['others']
03/18/2022 11:00:46 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:00:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:00:46 - INFO - __main__ - Printing 3 examples
03/18/2022 11:00:46 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 11:00:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:00:46 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:00:46 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:00:46 - INFO - __main__ - Printing 3 examples
03/18/2022 11:00:46 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:00:46 - INFO - __main__ - ['sad']
03/18/2022 11:00:46 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:00:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:00:46 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:00:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:00:53 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 11:01:02 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:01:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:01:03 - INFO - __main__ - Starting training!
03/18/2022 11:02:24 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_13_0.2_8_predictions.txt
03/18/2022 11:02:24 - INFO - __main__ - Classification-F1 on test data: 0.2863
03/18/2022 11:02:24 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.7065410901617798, test_performance=0.28626826373937825
03/18/2022 11:02:24 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
03/18/2022 11:02:25 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:02:25 - INFO - __main__ - Printing 3 examples
03/18/2022 11:02:25 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:02:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:02:25 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:02:25 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:02:25 - INFO - __main__ - Printing 3 examples
03/18/2022 11:02:25 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:02:25 - INFO - __main__ - ['sad']
03/18/2022 11:02:25 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:02:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:02:25 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:02:40 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:02:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:02:41 - INFO - __main__ - Starting training!
03/18/2022 11:02:44 - INFO - __main__ - Step 10 Global step 10 Train loss 7.13 on epoch=2
03/18/2022 11:02:47 - INFO - __main__ - Step 20 Global step 20 Train loss 3.49 on epoch=4
03/18/2022 11:02:49 - INFO - __main__ - Step 30 Global step 30 Train loss 1.78 on epoch=7
03/18/2022 11:02:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.28 on epoch=9
03/18/2022 11:02:54 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=12
03/18/2022 11:02:55 - INFO - __main__ - Global step 50 Train loss 2.94 Classification-F1 0.1 on epoch=12
03/18/2022 11:02:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 11:02:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.23 on epoch=14
03/18/2022 11:03:00 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=17
03/18/2022 11:03:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=19
03/18/2022 11:03:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
03/18/2022 11:03:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=24
03/18/2022 11:03:08 - INFO - __main__ - Global step 100 Train loss 1.04 Classification-F1 0.10126582278481013 on epoch=24
03/18/2022 11:03:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=24, global_step=100
03/18/2022 11:03:10 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
03/18/2022 11:03:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.34 on epoch=29
03/18/2022 11:03:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=32
03/18/2022 11:03:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=34
03/18/2022 11:03:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
03/18/2022 11:03:21 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.1 on epoch=37
03/18/2022 11:03:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=39
03/18/2022 11:03:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=42
03/18/2022 11:03:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
03/18/2022 11:03:31 - INFO - __main__ - Step 190 Global step 190 Train loss 1.01 on epoch=47
03/18/2022 11:03:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=49
03/18/2022 11:03:34 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.10126582278481013 on epoch=49
03/18/2022 11:03:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
03/18/2022 11:03:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=54
03/18/2022 11:03:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
03/18/2022 11:03:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=59
03/18/2022 11:03:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=62
03/18/2022 11:03:47 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.1 on epoch=62
03/18/2022 11:03:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=64
03/18/2022 11:03:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=67
03/18/2022 11:03:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=69
03/18/2022 11:03:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
03/18/2022 11:03:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
03/18/2022 11:04:00 - INFO - __main__ - Global step 300 Train loss 0.86 Classification-F1 0.23541666666666666 on epoch=74
03/18/2022 11:04:00 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.23541666666666666 on epoch=74, global_step=300
03/18/2022 11:04:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=77
03/18/2022 11:04:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=79
03/18/2022 11:04:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
03/18/2022 11:04:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
03/18/2022 11:04:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=87
03/18/2022 11:04:13 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.10126582278481013 on epoch=87
03/18/2022 11:04:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.82 on epoch=89
03/18/2022 11:04:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
03/18/2022 11:04:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=94
03/18/2022 11:04:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=97
03/18/2022 11:04:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.80 on epoch=99
03/18/2022 11:04:26 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.2799847211611917 on epoch=99
03/18/2022 11:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.23541666666666666 -> 0.2799847211611917 on epoch=99, global_step=400
03/18/2022 11:04:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.77 on epoch=102
03/18/2022 11:04:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
03/18/2022 11:04:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.93 on epoch=107
03/18/2022 11:04:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=109
03/18/2022 11:04:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
03/18/2022 11:04:38 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.17809523809523808 on epoch=112
03/18/2022 11:04:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=114
03/18/2022 11:04:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=117
03/18/2022 11:04:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=119
03/18/2022 11:04:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=122
03/18/2022 11:04:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=124
03/18/2022 11:04:52 - INFO - __main__ - Global step 500 Train loss 0.67 Classification-F1 0.42051282051282046 on epoch=124
03/18/2022 11:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2799847211611917 -> 0.42051282051282046 on epoch=124, global_step=500
03/18/2022 11:04:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=127
03/18/2022 11:04:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=129
03/18/2022 11:04:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=132
03/18/2022 11:05:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=134
03/18/2022 11:05:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=137
03/18/2022 11:05:05 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.48888888888888893 on epoch=137
03/18/2022 11:05:05 - INFO - __main__ - Saving model with best Classification-F1: 0.42051282051282046 -> 0.48888888888888893 on epoch=137, global_step=550
03/18/2022 11:05:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
03/18/2022 11:05:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=142
03/18/2022 11:05:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=144
03/18/2022 11:05:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
03/18/2022 11:05:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=149
03/18/2022 11:05:18 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.5304821098539264 on epoch=149
03/18/2022 11:05:18 - INFO - __main__ - Saving model with best Classification-F1: 0.48888888888888893 -> 0.5304821098539264 on epoch=149, global_step=600
03/18/2022 11:05:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=152
03/18/2022 11:05:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=154
03/18/2022 11:05:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=157
03/18/2022 11:05:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=159
03/18/2022 11:05:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
03/18/2022 11:05:31 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.6074162679425837 on epoch=162
03/18/2022 11:05:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5304821098539264 -> 0.6074162679425837 on epoch=162, global_step=650
03/18/2022 11:05:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
03/18/2022 11:05:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
03/18/2022 11:05:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
03/18/2022 11:05:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
03/18/2022 11:05:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=174
03/18/2022 11:05:44 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.5497474747474748 on epoch=174
03/18/2022 11:05:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
03/18/2022 11:05:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
03/18/2022 11:05:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=182
03/18/2022 11:05:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=184
03/18/2022 11:05:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
03/18/2022 11:05:57 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.616690577216893 on epoch=187
03/18/2022 11:05:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6074162679425837 -> 0.616690577216893 on epoch=187, global_step=750
03/18/2022 11:06:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=189
03/18/2022 11:06:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
03/18/2022 11:06:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
03/18/2022 11:06:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=197
03/18/2022 11:06:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
03/18/2022 11:06:10 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6257211538461538 on epoch=199
03/18/2022 11:06:10 - INFO - __main__ - Saving model with best Classification-F1: 0.616690577216893 -> 0.6257211538461538 on epoch=199, global_step=800
03/18/2022 11:06:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
03/18/2022 11:06:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
03/18/2022 11:06:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
03/18/2022 11:06:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
03/18/2022 11:06:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
03/18/2022 11:06:23 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.7155237854251013 on epoch=212
03/18/2022 11:06:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6257211538461538 -> 0.7155237854251013 on epoch=212, global_step=850
03/18/2022 11:06:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
03/18/2022 11:06:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
03/18/2022 11:06:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
03/18/2022 11:06:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
03/18/2022 11:06:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
03/18/2022 11:06:36 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.6540779437470206 on epoch=224
03/18/2022 11:06:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
03/18/2022 11:06:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
03/18/2022 11:06:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
03/18/2022 11:06:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
03/18/2022 11:06:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
03/18/2022 11:06:50 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.7035145035145036 on epoch=237
03/18/2022 11:06:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
03/18/2022 11:06:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
03/18/2022 11:06:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
03/18/2022 11:06:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
03/18/2022 11:07:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
03/18/2022 11:07:03 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.7009611838465793 on epoch=249
03/18/2022 11:07:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
03/18/2022 11:07:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
03/18/2022 11:07:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
03/18/2022 11:07:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
03/18/2022 11:07:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
03/18/2022 11:07:16 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6550323577706945 on epoch=262
03/18/2022 11:07:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
03/18/2022 11:07:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
03/18/2022 11:07:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
03/18/2022 11:07:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
03/18/2022 11:07:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=274
03/18/2022 11:07:30 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.6965391239584787 on epoch=274
03/18/2022 11:07:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=277
03/18/2022 11:07:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
03/18/2022 11:07:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
03/18/2022 11:07:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=284
03/18/2022 11:07:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
03/18/2022 11:07:43 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.642834595959596 on epoch=287
03/18/2022 11:07:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
03/18/2022 11:07:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
03/18/2022 11:07:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
03/18/2022 11:07:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
03/18/2022 11:07:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
03/18/2022 11:07:56 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.6274621212121212 on epoch=299
03/18/2022 11:07:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/18/2022 11:08:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
03/18/2022 11:08:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
03/18/2022 11:08:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/18/2022 11:08:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
03/18/2022 11:08:09 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.6834935897435898 on epoch=312
03/18/2022 11:08:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
03/18/2022 11:08:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/18/2022 11:08:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
03/18/2022 11:08:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
03/18/2022 11:08:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
03/18/2022 11:08:22 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7314102564102565 on epoch=324
03/18/2022 11:08:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7155237854251013 -> 0.7314102564102565 on epoch=324, global_step=1300
03/18/2022 11:08:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
03/18/2022 11:08:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
03/18/2022 11:08:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
03/18/2022 11:08:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
03/18/2022 11:08:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
03/18/2022 11:08:36 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6969159323648488 on epoch=337
03/18/2022 11:08:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
03/18/2022 11:08:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/18/2022 11:08:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/18/2022 11:08:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/18/2022 11:08:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
03/18/2022 11:08:49 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7189228021124573 on epoch=349
03/18/2022 11:08:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 11:08:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
03/18/2022 11:08:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
03/18/2022 11:08:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
03/18/2022 11:09:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
03/18/2022 11:09:02 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7127332144979204 on epoch=362
03/18/2022 11:09:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
03/18/2022 11:09:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
03/18/2022 11:09:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/18/2022 11:09:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
03/18/2022 11:09:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 11:09:15 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7016069951553823 on epoch=374
03/18/2022 11:09:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
03/18/2022 11:09:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 11:09:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
03/18/2022 11:09:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 11:09:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
03/18/2022 11:09:29 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.7153293449177764 on epoch=387
03/18/2022 11:09:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
03/18/2022 11:09:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
03/18/2022 11:09:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
03/18/2022 11:09:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/18/2022 11:09:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
03/18/2022 11:09:42 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.7308199363156259 on epoch=399
03/18/2022 11:09:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/18/2022 11:09:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
03/18/2022 11:09:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/18/2022 11:09:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/18/2022 11:09:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
03/18/2022 11:09:55 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7187676853423883 on epoch=412
03/18/2022 11:09:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
03/18/2022 11:10:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
03/18/2022 11:10:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 11:10:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/18/2022 11:10:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
03/18/2022 11:10:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.7332506203473946 on epoch=424
03/18/2022 11:10:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7314102564102565 -> 0.7332506203473946 on epoch=424, global_step=1700
03/18/2022 11:10:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 11:10:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 11:10:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
03/18/2022 11:10:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
03/18/2022 11:10:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
03/18/2022 11:10:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7130007247654307 on epoch=437
03/18/2022 11:10:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
03/18/2022 11:10:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/18/2022 11:10:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
03/18/2022 11:10:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
03/18/2022 11:10:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/18/2022 11:10:35 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7170138888888888 on epoch=449
03/18/2022 11:10:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
03/18/2022 11:10:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 11:10:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 11:10:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 11:10:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 11:10:48 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.7152732683982684 on epoch=462
03/18/2022 11:10:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 11:10:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 11:10:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 11:10:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 11:11:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 11:11:02 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7026002506265665 on epoch=474
03/18/2022 11:11:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 11:11:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 11:11:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 11:11:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
03/18/2022 11:11:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 11:11:15 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6872947454844007 on epoch=487
03/18/2022 11:11:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 11:11:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
03/18/2022 11:11:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 11:11:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 11:11:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 11:11:28 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7296505921612249 on epoch=499
03/18/2022 11:11:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/18/2022 11:11:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/18/2022 11:11:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 11:11:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
03/18/2022 11:11:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 11:11:42 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7187676853423883 on epoch=512
03/18/2022 11:11:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 11:11:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 11:11:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 11:11:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 11:11:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 11:11:55 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7465768620002491 on epoch=524
03/18/2022 11:11:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7332506203473946 -> 0.7465768620002491 on epoch=524, global_step=2100
03/18/2022 11:11:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 11:12:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 11:12:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/18/2022 11:12:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 11:12:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 11:12:08 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.71505499005499 on epoch=537
03/18/2022 11:12:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 11:12:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/18/2022 11:12:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 11:12:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 11:12:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 11:12:21 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6875 on epoch=549
03/18/2022 11:12:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 11:12:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 11:12:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 11:12:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 11:12:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 11:12:35 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6710526315789473 on epoch=562
03/18/2022 11:12:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 11:12:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 11:12:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 11:12:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 11:12:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 11:12:48 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7308982683982684 on epoch=574
03/18/2022 11:12:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 11:12:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 11:12:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 11:12:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 11:13:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 11:13:01 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7342553091191929 on epoch=587
03/18/2022 11:13:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 11:13:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 11:13:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 11:13:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 11:13:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 11:13:15 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.721499060150376 on epoch=599
03/18/2022 11:13:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/18/2022 11:13:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 11:13:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 11:13:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/18/2022 11:13:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 11:13:28 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7195628230110988 on epoch=612
03/18/2022 11:13:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 11:13:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 11:13:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 11:13:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 11:13:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 11:13:41 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7301587301587301 on epoch=624
03/18/2022 11:13:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 11:13:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 11:13:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 11:13:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/18/2022 11:13:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 11:13:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7165596123506874 on epoch=637
03/18/2022 11:13:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/18/2022 11:13:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 11:14:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 11:14:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 11:14:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 11:14:08 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6979037081339713 on epoch=649
03/18/2022 11:14:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 11:14:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 11:14:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 11:14:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 11:14:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 11:14:21 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7472222222222221 on epoch=662
03/18/2022 11:14:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7465768620002491 -> 0.7472222222222221 on epoch=662, global_step=2650
03/18/2022 11:14:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 11:14:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 11:14:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 11:14:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 11:14:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 11:14:35 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6832150113400113 on epoch=674
03/18/2022 11:14:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/18/2022 11:14:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 11:14:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 11:14:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 11:14:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 11:14:48 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7302492656981822 on epoch=687
03/18/2022 11:14:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
03/18/2022 11:14:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 11:14:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 11:14:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 11:15:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 11:15:01 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7304477132063338 on epoch=699
03/18/2022 11:15:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 11:15:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 11:15:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 11:15:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 11:15:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 11:15:14 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7304477132063338 on epoch=712
03/18/2022 11:15:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 11:15:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 11:15:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 11:15:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 11:15:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 11:15:28 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7251515151515151 on epoch=724
03/18/2022 11:15:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 11:15:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 11:15:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 11:15:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 11:15:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 11:15:41 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7251515151515151 on epoch=737
03/18/2022 11:15:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 11:15:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 11:15:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 11:15:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 11:15:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 11:15:54 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7121116138763198 on epoch=749
03/18/2022 11:15:54 - INFO - __main__ - save last model!
03/18/2022 11:15:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 11:15:54 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 11:15:54 - INFO - __main__ - Printing 3 examples
03/18/2022 11:15:54 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 11:15:54 - INFO - __main__ - ['others']
03/18/2022 11:15:54 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 11:15:54 - INFO - __main__ - ['others']
03/18/2022 11:15:54 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 11:15:54 - INFO - __main__ - ['others']
03/18/2022 11:15:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:15:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:15:55 - INFO - __main__ - Printing 3 examples
03/18/2022 11:15:55 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 11:15:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:15:55 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:15:55 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:15:55 - INFO - __main__ - Printing 3 examples
03/18/2022 11:15:55 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:15:55 - INFO - __main__ - ['sad']
03/18/2022 11:15:55 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:15:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:15:55 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:15:57 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:16:02 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 11:16:10 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:16:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:16:11 - INFO - __main__ - Starting training!
03/18/2022 11:17:39 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_21_0.5_8_predictions.txt
03/18/2022 11:17:39 - INFO - __main__ - Classification-F1 on test data: 0.2630
03/18/2022 11:17:39 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7472222222222221, test_performance=0.2629621974211733
03/18/2022 11:17:39 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
03/18/2022 11:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:17:40 - INFO - __main__ - Printing 3 examples
03/18/2022 11:17:40 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:17:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:17:40 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:17:40 - INFO - __main__ - Printing 3 examples
03/18/2022 11:17:40 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:17:40 - INFO - __main__ - ['sad']
03/18/2022 11:17:40 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:17:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:17:40 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:17:55 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:17:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:17:56 - INFO - __main__ - Starting training!
03/18/2022 11:17:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.42 on epoch=2
03/18/2022 11:18:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=4
03/18/2022 11:18:04 - INFO - __main__ - Step 30 Global step 30 Train loss 1.59 on epoch=7
03/18/2022 11:18:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=9
03/18/2022 11:18:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.17 on epoch=12
03/18/2022 11:18:10 - INFO - __main__ - Global step 50 Train loss 2.61 Classification-F1 0.09493670886075949 on epoch=12
03/18/2022 11:18:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09493670886075949 on epoch=12, global_step=50
03/18/2022 11:18:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=14
03/18/2022 11:18:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.26 on epoch=17
03/18/2022 11:18:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=19
03/18/2022 11:18:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
03/18/2022 11:18:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=24
03/18/2022 11:18:22 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.10256410256410256 on epoch=24
03/18/2022 11:18:23 - INFO - __main__ - Saving model with best Classification-F1: 0.09493670886075949 -> 0.10256410256410256 on epoch=24, global_step=100
03/18/2022 11:18:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=27
03/18/2022 11:18:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=29
03/18/2022 11:18:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=32
03/18/2022 11:18:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=34
03/18/2022 11:18:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.00 on epoch=37
03/18/2022 11:18:35 - INFO - __main__ - Global step 150 Train loss 0.98 Classification-F1 0.09493670886075949 on epoch=37
03/18/2022 11:18:38 - INFO - __main__ - Step 160 Global step 160 Train loss 1.12 on epoch=39
03/18/2022 11:18:40 - INFO - __main__ - Step 170 Global step 170 Train loss 2.19 on epoch=42
03/18/2022 11:18:43 - INFO - __main__ - Step 180 Global step 180 Train loss 1.47 on epoch=44
03/18/2022 11:18:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.46 on epoch=47
03/18/2022 11:18:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.99 on epoch=49
03/18/2022 11:18:48 - INFO - __main__ - Global step 200 Train loss 1.45 Classification-F1 0.18187134502923974 on epoch=49
03/18/2022 11:18:48 - INFO - __main__ - Saving model with best Classification-F1: 0.10256410256410256 -> 0.18187134502923974 on epoch=49, global_step=200
03/18/2022 11:18:51 - INFO - __main__ - Step 210 Global step 210 Train loss 1.04 on epoch=52
03/18/2022 11:18:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
03/18/2022 11:18:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
03/18/2022 11:18:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=59
03/18/2022 11:19:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=62
03/18/2022 11:19:01 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.2563170826328721 on epoch=62
03/18/2022 11:19:01 - INFO - __main__ - Saving model with best Classification-F1: 0.18187134502923974 -> 0.2563170826328721 on epoch=62, global_step=250
03/18/2022 11:19:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=64
03/18/2022 11:19:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.93 on epoch=67
03/18/2022 11:19:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=69
03/18/2022 11:19:11 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
03/18/2022 11:19:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
03/18/2022 11:19:14 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.1422354557947778 on epoch=74
03/18/2022 11:19:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=77
03/18/2022 11:19:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.90 on epoch=79
03/18/2022 11:19:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=82
03/18/2022 11:19:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
03/18/2022 11:19:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
03/18/2022 11:19:27 - INFO - __main__ - Global step 350 Train loss 0.85 Classification-F1 0.21760752688172041 on epoch=87
03/18/2022 11:19:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=89
03/18/2022 11:19:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
03/18/2022 11:19:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.95 on epoch=94
03/18/2022 11:19:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
03/18/2022 11:19:39 - INFO - __main__ - Step 400 Global step 400 Train loss 1.00 on epoch=99
03/18/2022 11:19:40 - INFO - __main__ - Global step 400 Train loss 0.92 Classification-F1 0.1839053803339518 on epoch=99
03/18/2022 11:19:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.96 on epoch=102
03/18/2022 11:19:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=104
03/18/2022 11:19:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=107
03/18/2022 11:19:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=109
03/18/2022 11:19:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=112
03/18/2022 11:19:53 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.1611111111111111 on epoch=112
03/18/2022 11:19:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
03/18/2022 11:19:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.77 on epoch=117
03/18/2022 11:20:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.90 on epoch=119
03/18/2022 11:20:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=122
03/18/2022 11:20:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
03/18/2022 11:20:06 - INFO - __main__ - Global step 500 Train loss 0.87 Classification-F1 0.12198067632850242 on epoch=124
03/18/2022 11:20:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=127
03/18/2022 11:20:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.80 on epoch=129
03/18/2022 11:20:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=132
03/18/2022 11:20:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=134
03/18/2022 11:20:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
03/18/2022 11:20:19 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.13067758749069247 on epoch=137
03/18/2022 11:20:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.89 on epoch=139
03/18/2022 11:20:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=142
03/18/2022 11:20:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=144
03/18/2022 11:20:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.84 on epoch=147
03/18/2022 11:20:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
03/18/2022 11:20:32 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.27022417153996103 on epoch=149
03/18/2022 11:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2563170826328721 -> 0.27022417153996103 on epoch=149, global_step=600
03/18/2022 11:20:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
03/18/2022 11:20:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.89 on epoch=154
03/18/2022 11:20:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.89 on epoch=157
03/18/2022 11:20:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=159
03/18/2022 11:20:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.88 on epoch=162
03/18/2022 11:20:45 - INFO - __main__ - Global step 650 Train loss 0.87 Classification-F1 0.1 on epoch=162
03/18/2022 11:20:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.85 on epoch=164
03/18/2022 11:20:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.83 on epoch=167
03/18/2022 11:20:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=169
03/18/2022 11:20:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.84 on epoch=172
03/18/2022 11:20:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.82 on epoch=174
03/18/2022 11:20:57 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.16239422084623323 on epoch=174
03/18/2022 11:21:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.91 on epoch=177
03/18/2022 11:21:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.82 on epoch=179
03/18/2022 11:21:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.84 on epoch=182
03/18/2022 11:21:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.81 on epoch=184
03/18/2022 11:21:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.80 on epoch=187
03/18/2022 11:21:10 - INFO - __main__ - Global step 750 Train loss 0.83 Classification-F1 0.10126582278481013 on epoch=187
03/18/2022 11:21:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=189
03/18/2022 11:21:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.86 on epoch=192
03/18/2022 11:21:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=194
03/18/2022 11:21:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.89 on epoch=197
03/18/2022 11:21:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=199
03/18/2022 11:21:23 - INFO - __main__ - Global step 800 Train loss 0.85 Classification-F1 0.12644927536231884 on epoch=199
03/18/2022 11:21:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.79 on epoch=202
03/18/2022 11:21:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.87 on epoch=204
03/18/2022 11:21:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.83 on epoch=207
03/18/2022 11:21:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
03/18/2022 11:21:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.84 on epoch=212
03/18/2022 11:21:36 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.13034188034188032 on epoch=212
03/18/2022 11:21:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.79 on epoch=214
03/18/2022 11:21:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
03/18/2022 11:21:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.75 on epoch=219
03/18/2022 11:21:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.81 on epoch=222
03/18/2022 11:21:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.78 on epoch=224
03/18/2022 11:21:49 - INFO - __main__ - Global step 900 Train loss 0.79 Classification-F1 0.13444444444444445 on epoch=224
03/18/2022 11:21:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=227
03/18/2022 11:21:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.83 on epoch=229
03/18/2022 11:21:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.82 on epoch=232
03/18/2022 11:21:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=234
03/18/2022 11:22:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.75 on epoch=237
03/18/2022 11:22:02 - INFO - __main__ - Global step 950 Train loss 0.80 Classification-F1 0.3415315950199671 on epoch=237
03/18/2022 11:22:02 - INFO - __main__ - Saving model with best Classification-F1: 0.27022417153996103 -> 0.3415315950199671 on epoch=237, global_step=950
03/18/2022 11:22:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=239
03/18/2022 11:22:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.79 on epoch=242
03/18/2022 11:22:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.73 on epoch=244
03/18/2022 11:22:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.72 on epoch=247
03/18/2022 11:22:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.60 on epoch=249
03/18/2022 11:22:15 - INFO - __main__ - Global step 1000 Train loss 0.71 Classification-F1 0.2646136618141097 on epoch=249
03/18/2022 11:22:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.76 on epoch=252
03/18/2022 11:22:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.68 on epoch=254
03/18/2022 11:22:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.68 on epoch=257
03/18/2022 11:22:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.58 on epoch=259
03/18/2022 11:22:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=262
03/18/2022 11:22:28 - INFO - __main__ - Global step 1050 Train loss 0.65 Classification-F1 0.4104093567251462 on epoch=262
03/18/2022 11:22:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3415315950199671 -> 0.4104093567251462 on epoch=262, global_step=1050
03/18/2022 11:22:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.59 on epoch=264
03/18/2022 11:22:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.61 on epoch=267
03/18/2022 11:22:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=269
03/18/2022 11:22:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=272
03/18/2022 11:22:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=274
03/18/2022 11:22:41 - INFO - __main__ - Global step 1100 Train loss 0.60 Classification-F1 0.34090883336166355 on epoch=274
03/18/2022 11:22:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=277
03/18/2022 11:22:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.57 on epoch=279
03/18/2022 11:22:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.60 on epoch=282
03/18/2022 11:22:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=284
03/18/2022 11:22:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=287
03/18/2022 11:22:54 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.6079404967244734 on epoch=287
03/18/2022 11:22:54 - INFO - __main__ - Saving model with best Classification-F1: 0.4104093567251462 -> 0.6079404967244734 on epoch=287, global_step=1150
03/18/2022 11:22:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.51 on epoch=289
03/18/2022 11:22:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=292
03/18/2022 11:23:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=294
03/18/2022 11:23:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=297
03/18/2022 11:23:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=299
03/18/2022 11:23:07 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.5375 on epoch=299
03/18/2022 11:23:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=302
03/18/2022 11:23:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=304
03/18/2022 11:23:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=307
03/18/2022 11:23:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=309
03/18/2022 11:23:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=312
03/18/2022 11:23:20 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.6849290466937525 on epoch=312
03/18/2022 11:23:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6079404967244734 -> 0.6849290466937525 on epoch=312, global_step=1250
03/18/2022 11:23:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=314
03/18/2022 11:23:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=317
03/18/2022 11:23:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=319
03/18/2022 11:23:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=322
03/18/2022 11:23:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=324
03/18/2022 11:23:33 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.49952287855513666 on epoch=324
03/18/2022 11:23:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=327
03/18/2022 11:23:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=329
03/18/2022 11:23:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=332
03/18/2022 11:23:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=334
03/18/2022 11:23:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=337
03/18/2022 11:23:46 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.7164880527109629 on epoch=337
03/18/2022 11:23:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6849290466937525 -> 0.7164880527109629 on epoch=337, global_step=1350
03/18/2022 11:23:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
03/18/2022 11:23:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=342
03/18/2022 11:23:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=344
03/18/2022 11:23:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=347
03/18/2022 11:23:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
03/18/2022 11:23:58 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7133613445378152 on epoch=349
03/18/2022 11:24:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=352
03/18/2022 11:24:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=354
03/18/2022 11:24:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=357
03/18/2022 11:24:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
03/18/2022 11:24:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=362
03/18/2022 11:24:11 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.6827103601297149 on epoch=362
03/18/2022 11:24:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=364
03/18/2022 11:24:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=367
03/18/2022 11:24:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=369
03/18/2022 11:24:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=372
03/18/2022 11:24:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=374
03/18/2022 11:24:24 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.6881451881451882 on epoch=374
03/18/2022 11:24:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
03/18/2022 11:24:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
03/18/2022 11:24:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=382
03/18/2022 11:24:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
03/18/2022 11:24:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
03/18/2022 11:24:36 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.6836576735957541 on epoch=387
03/18/2022 11:24:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=389
03/18/2022 11:24:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
03/18/2022 11:24:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=394
03/18/2022 11:24:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
03/18/2022 11:24:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
03/18/2022 11:24:49 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.7287984006734007 on epoch=399
03/18/2022 11:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7164880527109629 -> 0.7287984006734007 on epoch=399, global_step=1600
03/18/2022 11:24:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
03/18/2022 11:24:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
03/18/2022 11:24:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
03/18/2022 11:24:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=409
03/18/2022 11:25:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
03/18/2022 11:25:02 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7111473285486445 on epoch=412
03/18/2022 11:25:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
03/18/2022 11:25:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
03/18/2022 11:25:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
03/18/2022 11:25:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/18/2022 11:25:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
03/18/2022 11:25:15 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7463203463203464 on epoch=424
03/18/2022 11:25:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7287984006734007 -> 0.7463203463203464 on epoch=424, global_step=1700
03/18/2022 11:25:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
03/18/2022 11:25:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 11:25:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
03/18/2022 11:25:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/18/2022 11:25:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/18/2022 11:25:28 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7583477576711252 on epoch=437
03/18/2022 11:25:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7463203463203464 -> 0.7583477576711252 on epoch=437, global_step=1750
03/18/2022 11:25:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
03/18/2022 11:25:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/18/2022 11:25:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 11:25:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 11:25:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/18/2022 11:25:41 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6959760077677863 on epoch=449
03/18/2022 11:25:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
03/18/2022 11:25:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
03/18/2022 11:25:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
03/18/2022 11:25:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=459
03/18/2022 11:25:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
03/18/2022 11:25:54 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7450644162588634 on epoch=462
03/18/2022 11:25:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=464
03/18/2022 11:25:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 11:26:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 11:26:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=472
03/18/2022 11:26:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 11:26:07 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.7356251558288909 on epoch=474
03/18/2022 11:26:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
03/18/2022 11:26:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
03/18/2022 11:26:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=482
03/18/2022 11:26:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
03/18/2022 11:26:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
03/18/2022 11:26:20 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.728935384000799 on epoch=487
03/18/2022 11:26:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
03/18/2022 11:26:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
03/18/2022 11:26:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=494
03/18/2022 11:26:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 11:26:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 11:26:33 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7053098213701499 on epoch=499
03/18/2022 11:26:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
03/18/2022 11:26:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
03/18/2022 11:26:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
03/18/2022 11:26:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
03/18/2022 11:26:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=512
03/18/2022 11:26:45 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7292331507501786 on epoch=512
03/18/2022 11:26:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/18/2022 11:26:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/18/2022 11:26:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
03/18/2022 11:26:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
03/18/2022 11:26:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 11:26:58 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7614607614607616 on epoch=524
03/18/2022 11:26:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7583477576711252 -> 0.7614607614607616 on epoch=524, global_step=2100
03/18/2022 11:27:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
03/18/2022 11:27:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 11:27:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
03/18/2022 11:27:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 11:27:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 11:27:11 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.712611567924661 on epoch=537
03/18/2022 11:27:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
03/18/2022 11:27:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
03/18/2022 11:27:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
03/18/2022 11:27:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 11:27:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 11:27:24 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7466824229691876 on epoch=549
03/18/2022 11:27:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 11:27:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/18/2022 11:27:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
03/18/2022 11:27:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/18/2022 11:27:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 11:27:37 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7145880052900926 on epoch=562
03/18/2022 11:27:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/18/2022 11:27:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 11:27:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=569
03/18/2022 11:27:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 11:27:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/18/2022 11:27:50 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7623959152897981 on epoch=574
03/18/2022 11:27:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7614607614607616 -> 0.7623959152897981 on epoch=574, global_step=2300
03/18/2022 11:27:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 11:27:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/18/2022 11:27:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/18/2022 11:28:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
03/18/2022 11:28:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
03/18/2022 11:28:03 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7742205710955711 on epoch=587
03/18/2022 11:28:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7623959152897981 -> 0.7742205710955711 on epoch=587, global_step=2350
03/18/2022 11:28:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/18/2022 11:28:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 11:28:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
03/18/2022 11:28:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
03/18/2022 11:28:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 11:28:16 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7006628787878788 on epoch=599
03/18/2022 11:28:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 11:28:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/18/2022 11:28:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/18/2022 11:28:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/18/2022 11:28:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/18/2022 11:28:30 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7443052232854865 on epoch=612
03/18/2022 11:28:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 11:28:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 11:28:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
03/18/2022 11:28:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
03/18/2022 11:28:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 11:28:43 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7021040783084395 on epoch=624
03/18/2022 11:28:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 11:28:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/18/2022 11:28:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 11:28:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/18/2022 11:28:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 11:28:56 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7575694444444444 on epoch=637
03/18/2022 11:28:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 11:29:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 11:29:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 11:29:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/18/2022 11:29:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 11:29:09 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.741748277479459 on epoch=649
03/18/2022 11:29:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 11:29:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 11:29:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
03/18/2022 11:29:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 11:29:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/18/2022 11:29:22 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7503472222222223 on epoch=662
03/18/2022 11:29:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 11:29:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 11:29:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 11:29:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
03/18/2022 11:29:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 11:29:35 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7110653574068209 on epoch=674
03/18/2022 11:29:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/18/2022 11:29:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 11:29:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 11:29:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 11:29:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 11:29:48 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6960454296661193 on epoch=687
03/18/2022 11:29:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
03/18/2022 11:29:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 11:29:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 11:29:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 11:30:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=699
03/18/2022 11:30:01 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7420866733652202 on epoch=699
03/18/2022 11:30:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
03/18/2022 11:30:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 11:30:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 11:30:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 11:30:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
03/18/2022 11:30:14 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.728042328042328 on epoch=712
03/18/2022 11:30:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/18/2022 11:30:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 11:30:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/18/2022 11:30:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 11:30:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 11:30:27 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7960063953896971 on epoch=724
03/18/2022 11:30:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7742205710955711 -> 0.7960063953896971 on epoch=724, global_step=2900
03/18/2022 11:30:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/18/2022 11:30:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 11:30:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 11:30:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 11:30:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 11:30:40 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7809256156030351 on epoch=737
03/18/2022 11:30:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 11:30:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 11:30:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
03/18/2022 11:30:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
03/18/2022 11:30:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
03/18/2022 11:30:53 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7710775228745075 on epoch=749
03/18/2022 11:30:53 - INFO - __main__ - save last model!
03/18/2022 11:30:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 11:30:53 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 11:30:53 - INFO - __main__ - Printing 3 examples
03/18/2022 11:30:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 11:30:53 - INFO - __main__ - ['others']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 11:30:53 - INFO - __main__ - ['others']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 11:30:53 - INFO - __main__ - ['others']
03/18/2022 11:30:53 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:30:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:30:53 - INFO - __main__ - Printing 3 examples
03/18/2022 11:30:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 11:30:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:30:53 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:30:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:30:53 - INFO - __main__ - Printing 3 examples
03/18/2022 11:30:53 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:30:53 - INFO - __main__ - ['sad']
03/18/2022 11:30:53 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:30:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:30:53 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:30:55 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:31:01 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 11:31:08 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:31:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:31:09 - INFO - __main__ - Starting training!
03/18/2022 11:32:36 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_21_0.4_8_predictions.txt
03/18/2022 11:32:36 - INFO - __main__ - Classification-F1 on test data: 0.3137
03/18/2022 11:32:37 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.7960063953896971, test_performance=0.31369241826789973
03/18/2022 11:32:37 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
03/18/2022 11:32:37 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:32:37 - INFO - __main__ - Printing 3 examples
03/18/2022 11:32:37 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:32:37 - INFO - __main__ - ['sad']
03/18/2022 11:32:37 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:32:37 - INFO - __main__ - ['sad']
03/18/2022 11:32:37 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:32:37 - INFO - __main__ - ['sad']
03/18/2022 11:32:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:32:37 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:32:38 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:32:38 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:32:38 - INFO - __main__ - Printing 3 examples
03/18/2022 11:32:38 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:32:38 - INFO - __main__ - ['sad']
03/18/2022 11:32:38 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:32:38 - INFO - __main__ - ['sad']
03/18/2022 11:32:38 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:32:38 - INFO - __main__ - ['sad']
03/18/2022 11:32:38 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:32:38 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:32:38 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:32:56 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:32:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:32:57 - INFO - __main__ - Starting training!
03/18/2022 11:33:00 - INFO - __main__ - Step 10 Global step 10 Train loss 7.07 on epoch=2
03/18/2022 11:33:02 - INFO - __main__ - Step 20 Global step 20 Train loss 3.90 on epoch=4
03/18/2022 11:33:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.24 on epoch=7
03/18/2022 11:33:07 - INFO - __main__ - Step 40 Global step 40 Train loss 1.39 on epoch=9
03/18/2022 11:33:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.21 on epoch=12
03/18/2022 11:33:10 - INFO - __main__ - Global step 50 Train loss 3.16 Classification-F1 0.10126582278481013 on epoch=12
03/18/2022 11:33:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
03/18/2022 11:33:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=14
03/18/2022 11:33:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.29 on epoch=17
03/18/2022 11:33:18 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=19
03/18/2022 11:33:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.08 on epoch=22
03/18/2022 11:33:23 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=24
03/18/2022 11:33:23 - INFO - __main__ - Global step 100 Train loss 1.11 Classification-F1 0.13197586726998492 on epoch=24
03/18/2022 11:33:24 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.13197586726998492 on epoch=24, global_step=100
03/18/2022 11:33:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=27
03/18/2022 11:33:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.04 on epoch=29
03/18/2022 11:33:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.05 on epoch=32
03/18/2022 11:33:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=34
03/18/2022 11:33:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=37
03/18/2022 11:33:37 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.09493670886075949 on epoch=37
03/18/2022 11:33:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
03/18/2022 11:33:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=42
03/18/2022 11:33:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=44
03/18/2022 11:33:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.93 on epoch=47
03/18/2022 11:33:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
03/18/2022 11:33:50 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.19259032455603184 on epoch=49
03/18/2022 11:33:50 - INFO - __main__ - Saving model with best Classification-F1: 0.13197586726998492 -> 0.19259032455603184 on epoch=49, global_step=200
03/18/2022 11:33:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
03/18/2022 11:33:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
03/18/2022 11:33:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
03/18/2022 11:34:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
03/18/2022 11:34:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=62
03/18/2022 11:34:03 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.1 on epoch=62
03/18/2022 11:34:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
03/18/2022 11:34:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
03/18/2022 11:34:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
03/18/2022 11:34:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
03/18/2022 11:34:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=74
03/18/2022 11:34:16 - INFO - __main__ - Global step 300 Train loss 0.86 Classification-F1 0.15135135135135136 on epoch=74
03/18/2022 11:34:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=77
03/18/2022 11:34:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.88 on epoch=79
03/18/2022 11:34:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
03/18/2022 11:34:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
03/18/2022 11:34:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.86 on epoch=87
03/18/2022 11:34:29 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.1256338028169014 on epoch=87
03/18/2022 11:34:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
03/18/2022 11:34:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=92
03/18/2022 11:34:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=94
03/18/2022 11:34:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=97
03/18/2022 11:34:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
03/18/2022 11:34:42 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.27253302253302253 on epoch=99
03/18/2022 11:34:42 - INFO - __main__ - Saving model with best Classification-F1: 0.19259032455603184 -> 0.27253302253302253 on epoch=99, global_step=400
03/18/2022 11:34:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=102
03/18/2022 11:34:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.74 on epoch=104
03/18/2022 11:34:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=107
03/18/2022 11:34:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=109
03/18/2022 11:34:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=112
03/18/2022 11:34:55 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.1237183868762816 on epoch=112
03/18/2022 11:34:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.77 on epoch=114
03/18/2022 11:35:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
03/18/2022 11:35:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=119
03/18/2022 11:35:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=122
03/18/2022 11:35:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
03/18/2022 11:35:08 - INFO - __main__ - Global step 500 Train loss 0.78 Classification-F1 0.17907586763518968 on epoch=124
03/18/2022 11:35:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.84 on epoch=127
03/18/2022 11:35:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
03/18/2022 11:35:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=132
03/18/2022 11:35:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=134
03/18/2022 11:35:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.79 on epoch=137
03/18/2022 11:35:21 - INFO - __main__ - Global step 550 Train loss 0.76 Classification-F1 0.1640625 on epoch=137
03/18/2022 11:35:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.78 on epoch=139
03/18/2022 11:35:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=142
03/18/2022 11:35:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=144
03/18/2022 11:35:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.74 on epoch=147
03/18/2022 11:35:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.70 on epoch=149
03/18/2022 11:35:34 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.22033435314685315 on epoch=149
03/18/2022 11:35:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.82 on epoch=152
03/18/2022 11:35:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.73 on epoch=154
03/18/2022 11:35:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.70 on epoch=157
03/18/2022 11:35:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=159
03/18/2022 11:35:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=162
03/18/2022 11:35:47 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.19960165030587568 on epoch=162
03/18/2022 11:35:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.66 on epoch=164
03/18/2022 11:35:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
03/18/2022 11:35:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=169
03/18/2022 11:35:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.62 on epoch=172
03/18/2022 11:35:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.64 on epoch=174
03/18/2022 11:36:00 - INFO - __main__ - Global step 700 Train loss 0.65 Classification-F1 0.3371028491586173 on epoch=174
03/18/2022 11:36:00 - INFO - __main__ - Saving model with best Classification-F1: 0.27253302253302253 -> 0.3371028491586173 on epoch=174, global_step=700
03/18/2022 11:36:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.61 on epoch=177
03/18/2022 11:36:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=179
03/18/2022 11:36:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=182
03/18/2022 11:36:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=184
03/18/2022 11:36:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=187
03/18/2022 11:36:13 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.49241758241758243 on epoch=187
03/18/2022 11:36:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3371028491586173 -> 0.49241758241758243 on epoch=187, global_step=750
03/18/2022 11:36:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=189
03/18/2022 11:36:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=192
03/18/2022 11:36:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=194
03/18/2022 11:36:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=197
03/18/2022 11:36:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=199
03/18/2022 11:36:27 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.45556198187777136 on epoch=199
03/18/2022 11:36:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=202
03/18/2022 11:36:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=204
03/18/2022 11:36:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=207
03/18/2022 11:36:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=209
03/18/2022 11:36:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=212
03/18/2022 11:36:39 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.5771571021571021 on epoch=212
03/18/2022 11:36:39 - INFO - __main__ - Saving model with best Classification-F1: 0.49241758241758243 -> 0.5771571021571021 on epoch=212, global_step=850
03/18/2022 11:36:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
03/18/2022 11:36:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=217
03/18/2022 11:36:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=219
03/18/2022 11:36:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=222
03/18/2022 11:36:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=224
03/18/2022 11:36:53 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.7345232447817837 on epoch=224
03/18/2022 11:36:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5771571021571021 -> 0.7345232447817837 on epoch=224, global_step=900
03/18/2022 11:36:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=227
03/18/2022 11:36:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=229
03/18/2022 11:37:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
03/18/2022 11:37:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
03/18/2022 11:37:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=237
03/18/2022 11:37:06 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.6842089658444023 on epoch=237
03/18/2022 11:37:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
03/18/2022 11:37:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
03/18/2022 11:37:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
03/18/2022 11:37:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
03/18/2022 11:37:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
03/18/2022 11:37:19 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.7092943548387096 on epoch=249
03/18/2022 11:37:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
03/18/2022 11:37:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
03/18/2022 11:37:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
03/18/2022 11:37:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=259
03/18/2022 11:37:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
03/18/2022 11:37:32 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.7830750193833358 on epoch=262
03/18/2022 11:37:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7345232447817837 -> 0.7830750193833358 on epoch=262, global_step=1050
03/18/2022 11:37:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
03/18/2022 11:37:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
03/18/2022 11:37:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
03/18/2022 11:37:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/18/2022 11:37:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
03/18/2022 11:37:45 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.7691845510811028 on epoch=274
03/18/2022 11:37:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
03/18/2022 11:37:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/18/2022 11:37:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/18/2022 11:37:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
03/18/2022 11:37:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
03/18/2022 11:37:59 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.7125953829807785 on epoch=287
03/18/2022 11:38:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/18/2022 11:38:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/18/2022 11:38:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
03/18/2022 11:38:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
03/18/2022 11:38:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/18/2022 11:38:12 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.7055821418724645 on epoch=299
03/18/2022 11:38:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
03/18/2022 11:38:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
03/18/2022 11:38:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
03/18/2022 11:38:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
03/18/2022 11:38:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/18/2022 11:38:25 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.7962640518084066 on epoch=312
03/18/2022 11:38:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7830750193833358 -> 0.7962640518084066 on epoch=312, global_step=1250
03/18/2022 11:38:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
03/18/2022 11:38:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
03/18/2022 11:38:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
03/18/2022 11:38:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
03/18/2022 11:38:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
03/18/2022 11:38:39 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7427731092436975 on epoch=324
03/18/2022 11:38:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
03/18/2022 11:38:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
03/18/2022 11:38:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
03/18/2022 11:38:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/18/2022 11:38:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
03/18/2022 11:38:52 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.7964285714285715 on epoch=337
03/18/2022 11:38:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7962640518084066 -> 0.7964285714285715 on epoch=337, global_step=1350
03/18/2022 11:38:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
03/18/2022 11:38:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=342
03/18/2022 11:38:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
03/18/2022 11:39:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
03/18/2022 11:39:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
03/18/2022 11:39:05 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7489461143695015 on epoch=349
03/18/2022 11:39:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
03/18/2022 11:39:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
03/18/2022 11:39:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/18/2022 11:39:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/18/2022 11:39:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
03/18/2022 11:39:19 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7666923624288425 on epoch=362
03/18/2022 11:39:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/18/2022 11:39:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/18/2022 11:39:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
03/18/2022 11:39:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/18/2022 11:39:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
03/18/2022 11:39:32 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.765022137887413 on epoch=374
03/18/2022 11:39:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
03/18/2022 11:39:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/18/2022 11:39:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
03/18/2022 11:39:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/18/2022 11:39:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
03/18/2022 11:39:46 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7943472545252299 on epoch=387
03/18/2022 11:39:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/18/2022 11:39:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 11:39:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/18/2022 11:39:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
03/18/2022 11:39:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
03/18/2022 11:39:59 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.8082203884688672 on epoch=399
03/18/2022 11:39:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7964285714285715 -> 0.8082203884688672 on epoch=399, global_step=1600
03/18/2022 11:40:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/18/2022 11:40:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
03/18/2022 11:40:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
03/18/2022 11:40:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/18/2022 11:40:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
03/18/2022 11:40:12 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7670955882352941 on epoch=412
03/18/2022 11:40:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/18/2022 11:40:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/18/2022 11:40:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
03/18/2022 11:40:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/18/2022 11:40:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/18/2022 11:40:25 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7472027743244127 on epoch=424
03/18/2022 11:40:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/18/2022 11:40:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 11:40:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/18/2022 11:40:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 11:40:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 11:40:39 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7955543507725671 on epoch=437
03/18/2022 11:40:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/18/2022 11:40:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 11:40:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 11:40:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 11:40:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/18/2022 11:40:52 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7555555555555555 on epoch=449
03/18/2022 11:40:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
03/18/2022 11:40:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 11:41:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/18/2022 11:41:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 11:41:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 11:41:06 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7666350411132195 on epoch=462
03/18/2022 11:41:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 11:41:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/18/2022 11:41:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 11:41:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 11:41:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 11:41:19 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.744144670140875 on epoch=474
03/18/2022 11:41:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/18/2022 11:41:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 11:41:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
03/18/2022 11:41:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 11:41:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/18/2022 11:41:32 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7602588383838383 on epoch=487
03/18/2022 11:41:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/18/2022 11:41:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 11:41:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/18/2022 11:41:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 11:41:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 11:41:45 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7926383053221289 on epoch=499
03/18/2022 11:41:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=502
03/18/2022 11:41:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
03/18/2022 11:41:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
03/18/2022 11:41:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/18/2022 11:41:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 11:41:59 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7872549019607842 on epoch=512
03/18/2022 11:42:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/18/2022 11:42:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
03/18/2022 11:42:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
03/18/2022 11:42:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 11:42:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 11:42:13 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.8261460761460762 on epoch=524
03/18/2022 11:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8082203884688672 -> 0.8261460761460762 on epoch=524, global_step=2100
03/18/2022 11:42:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 11:42:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 11:42:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/18/2022 11:42:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 11:42:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 11:42:26 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.8279258098223616 on epoch=537
03/18/2022 11:42:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8261460761460762 -> 0.8279258098223616 on epoch=537, global_step=2150
03/18/2022 11:42:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/18/2022 11:42:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 11:42:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 11:42:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 11:42:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 11:42:39 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.8261924962292609 on epoch=549
03/18/2022 11:42:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 11:42:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 11:42:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 11:42:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 11:42:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 11:42:53 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.748015873015873 on epoch=562
03/18/2022 11:42:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 11:42:58 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
03/18/2022 11:43:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 11:43:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 11:43:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
03/18/2022 11:43:06 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7503162555344718 on epoch=574
03/18/2022 11:43:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 11:43:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 11:43:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 11:43:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 11:43:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
03/18/2022 11:43:19 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7308558558558558 on epoch=587
03/18/2022 11:43:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
03/18/2022 11:43:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 11:43:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 11:43:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 11:43:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 11:43:33 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7114065551565552 on epoch=599
03/18/2022 11:43:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 11:43:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 11:43:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
03/18/2022 11:43:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
03/18/2022 11:43:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 11:43:46 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7604499864461913 on epoch=612
03/18/2022 11:43:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
03/18/2022 11:43:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 11:43:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 11:43:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
03/18/2022 11:43:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 11:44:00 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6776018099547512 on epoch=624
03/18/2022 11:44:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
03/18/2022 11:44:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
03/18/2022 11:44:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
03/18/2022 11:44:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 11:44:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 11:44:13 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7529894716507293 on epoch=637
03/18/2022 11:44:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 11:44:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/18/2022 11:44:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 11:44:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 11:44:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 11:44:27 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7666666666666666 on epoch=649
03/18/2022 11:44:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 11:44:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 11:44:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 11:44:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 11:44:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 11:44:40 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7854637020376731 on epoch=662
03/18/2022 11:44:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/18/2022 11:44:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 11:44:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 11:44:50 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/18/2022 11:44:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 11:44:54 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7213352007469654 on epoch=674
03/18/2022 11:44:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 11:44:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/18/2022 11:45:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 11:45:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/18/2022 11:45:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 11:45:07 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7354754440961337 on epoch=687
03/18/2022 11:45:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 11:45:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 11:45:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 11:45:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 11:45:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/18/2022 11:45:20 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7453829807785183 on epoch=699
03/18/2022 11:45:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 11:45:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/18/2022 11:45:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 11:45:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 11:45:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 11:45:34 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7468985601285378 on epoch=712
03/18/2022 11:45:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 11:45:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/18/2022 11:45:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 11:45:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 11:45:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 11:45:47 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7381908494991659 on epoch=724
03/18/2022 11:45:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 11:45:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 11:45:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 11:45:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 11:45:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 11:46:01 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7657355209537373 on epoch=737
03/18/2022 11:46:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
03/18/2022 11:46:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
03/18/2022 11:46:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 11:46:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 11:46:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 11:46:14 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:46:14 - INFO - __main__ - Printing 3 examples
03/18/2022 11:46:14 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 11:46:14 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7043103448275863 on epoch=749
03/18/2022 11:46:14 - INFO - __main__ - save last model!
03/18/2022 11:46:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:46:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 11:46:14 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 11:46:14 - INFO - __main__ - Printing 3 examples
03/18/2022 11:46:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 11:46:14 - INFO - __main__ - ['others']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 11:46:14 - INFO - __main__ - ['others']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 11:46:14 - INFO - __main__ - ['others']
03/18/2022 11:46:14 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:46:14 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:46:14 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:46:14 - INFO - __main__ - Printing 3 examples
03/18/2022 11:46:14 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:46:14 - INFO - __main__ - ['sad']
03/18/2022 11:46:14 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:46:14 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:46:14 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:46:16 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:46:22 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 11:46:29 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:46:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:46:30 - INFO - __main__ - Starting training!
03/18/2022 11:47:57 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_21_0.3_8_predictions.txt
03/18/2022 11:47:57 - INFO - __main__ - Classification-F1 on test data: 0.3295
03/18/2022 11:47:57 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.8279258098223616, test_performance=0.32952812830632056
03/18/2022 11:47:57 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
03/18/2022 11:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:47:58 - INFO - __main__ - Printing 3 examples
03/18/2022 11:47:58 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 11:47:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:47:58 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 11:47:58 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 11:47:58 - INFO - __main__ - Printing 3 examples
03/18/2022 11:47:58 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
03/18/2022 11:47:58 - INFO - __main__ - ['sad']
03/18/2022 11:47:58 - INFO - __main__ - Tokenizing Input ...
03/18/2022 11:47:58 - INFO - __main__ - Tokenizing Output ...
03/18/2022 11:47:58 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 11:48:17 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 11:48:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 11:48:17 - INFO - __main__ - Starting training!
03/18/2022 11:48:22 - INFO - __main__ - Step 10 Global step 10 Train loss 7.83 on epoch=2
03/18/2022 11:48:24 - INFO - __main__ - Step 20 Global step 20 Train loss 7.35 on epoch=4
03/18/2022 11:48:26 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=7
03/18/2022 11:48:29 - INFO - __main__ - Step 40 Global step 40 Train loss 5.74 on epoch=9
03/18/2022 11:48:31 - INFO - __main__ - Step 50 Global step 50 Train loss 4.61 on epoch=12
03/18/2022 11:48:42 - INFO - __main__ - Global step 50 Train loss 6.48 Classification-F1 0.023076923076923078 on epoch=12
03/18/2022 11:48:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.023076923076923078 on epoch=12, global_step=50
03/18/2022 11:48:44 - INFO - __main__ - Step 60 Global step 60 Train loss 3.65 on epoch=14
03/18/2022 11:48:47 - INFO - __main__ - Step 70 Global step 70 Train loss 3.04 on epoch=17
03/18/2022 11:48:49 - INFO - __main__ - Step 80 Global step 80 Train loss 2.18 on epoch=19
03/18/2022 11:48:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.92 on epoch=22
03/18/2022 11:48:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.64 on epoch=24
03/18/2022 11:48:55 - INFO - __main__ - Global step 100 Train loss 2.49 Classification-F1 0.11879960317460317 on epoch=24
03/18/2022 11:48:55 - INFO - __main__ - Saving model with best Classification-F1: 0.023076923076923078 -> 0.11879960317460317 on epoch=24, global_step=100
03/18/2022 11:48:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.49 on epoch=27
03/18/2022 11:49:00 - INFO - __main__ - Step 120 Global step 120 Train loss 1.29 on epoch=29
03/18/2022 11:49:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=32
03/18/2022 11:49:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.20 on epoch=34
03/18/2022 11:49:07 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
03/18/2022 11:49:07 - INFO - __main__ - Global step 150 Train loss 1.25 Classification-F1 0.1118421052631579 on epoch=37
03/18/2022 11:49:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.15 on epoch=39
03/18/2022 11:49:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.00 on epoch=42
03/18/2022 11:49:15 - INFO - __main__ - Step 180 Global step 180 Train loss 1.08 on epoch=44
03/18/2022 11:49:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
03/18/2022 11:49:19 - INFO - __main__ - Step 200 Global step 200 Train loss 1.02 on epoch=49
03/18/2022 11:49:20 - INFO - __main__ - Global step 200 Train loss 1.04 Classification-F1 0.13123993558776167 on epoch=49
03/18/2022 11:49:20 - INFO - __main__ - Saving model with best Classification-F1: 0.11879960317460317 -> 0.13123993558776167 on epoch=49, global_step=200
03/18/2022 11:49:23 - INFO - __main__ - Step 210 Global step 210 Train loss 1.11 on epoch=52
03/18/2022 11:49:25 - INFO - __main__ - Step 220 Global step 220 Train loss 1.02 on epoch=54
03/18/2022 11:49:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=57
03/18/2022 11:49:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
03/18/2022 11:49:32 - INFO - __main__ - Step 250 Global step 250 Train loss 1.09 on epoch=62
03/18/2022 11:49:33 - INFO - __main__ - Global step 250 Train loss 1.01 Classification-F1 0.09493670886075949 on epoch=62
03/18/2022 11:49:35 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
03/18/2022 11:49:38 - INFO - __main__ - Step 270 Global step 270 Train loss 1.00 on epoch=67
03/18/2022 11:49:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
03/18/2022 11:49:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
03/18/2022 11:49:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
03/18/2022 11:49:46 - INFO - __main__ - Global step 300 Train loss 0.97 Classification-F1 0.154707233065442 on epoch=74
03/18/2022 11:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.13123993558776167 -> 0.154707233065442 on epoch=74, global_step=300
03/18/2022 11:49:48 - INFO - __main__ - Step 310 Global step 310 Train loss 1.08 on epoch=77
03/18/2022 11:49:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=79
03/18/2022 11:49:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
03/18/2022 11:49:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
03/18/2022 11:49:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=87
03/18/2022 11:49:58 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.09493670886075949 on epoch=87
03/18/2022 11:50:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
03/18/2022 11:50:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=92
03/18/2022 11:50:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=94
03/18/2022 11:50:08 - INFO - __main__ - Step 390 Global step 390 Train loss 1.00 on epoch=97
03/18/2022 11:50:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=99
03/18/2022 11:50:11 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.19735128093790708 on epoch=99
03/18/2022 11:50:11 - INFO - __main__ - Saving model with best Classification-F1: 0.154707233065442 -> 0.19735128093790708 on epoch=99, global_step=400
03/18/2022 11:50:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
03/18/2022 11:50:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
03/18/2022 11:50:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=107
03/18/2022 11:50:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
03/18/2022 11:50:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.93 on epoch=112
03/18/2022 11:50:24 - INFO - __main__ - Global step 450 Train loss 0.89 Classification-F1 0.260423197492163 on epoch=112
03/18/2022 11:50:24 - INFO - __main__ - Saving model with best Classification-F1: 0.19735128093790708 -> 0.260423197492163 on epoch=112, global_step=450
03/18/2022 11:50:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
03/18/2022 11:50:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.94 on epoch=117
03/18/2022 11:50:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
03/18/2022 11:50:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.89 on epoch=122
03/18/2022 11:50:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
03/18/2022 11:50:37 - INFO - __main__ - Global step 500 Train loss 0.88 Classification-F1 0.21528091240107128 on epoch=124
03/18/2022 11:50:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
03/18/2022 11:50:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.92 on epoch=129
03/18/2022 11:50:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
03/18/2022 11:50:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
03/18/2022 11:50:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
03/18/2022 11:50:50 - INFO - __main__ - Global step 550 Train loss 0.84 Classification-F1 0.1490608875128999 on epoch=137
03/18/2022 11:50:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=139
03/18/2022 11:50:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=142
03/18/2022 11:50:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
03/18/2022 11:50:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=147
03/18/2022 11:51:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
03/18/2022 11:51:02 - INFO - __main__ - Global step 600 Train loss 0.83 Classification-F1 0.17838910025038207 on epoch=149
03/18/2022 11:51:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
03/18/2022 11:51:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
03/18/2022 11:51:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=157
03/18/2022 11:51:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.75 on epoch=159
03/18/2022 11:51:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
03/18/2022 11:51:15 - INFO - __main__ - Global step 650 Train loss 0.82 Classification-F1 0.09493670886075949 on epoch=162
03/18/2022 11:51:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.73 on epoch=164
03/18/2022 11:51:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=167
03/18/2022 11:51:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.73 on epoch=169
03/18/2022 11:51:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.83 on epoch=172
03/18/2022 11:51:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.72 on epoch=174
03/18/2022 11:51:28 - INFO - __main__ - Global step 700 Train loss 0.76 Classification-F1 0.202020202020202 on epoch=174
03/18/2022 11:51:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.75 on epoch=177
03/18/2022 11:51:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.73 on epoch=179
03/18/2022 11:51:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=182
03/18/2022 11:51:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.70 on epoch=184
03/18/2022 11:51:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=187
03/18/2022 11:51:41 - INFO - __main__ - Global step 750 Train loss 0.71 Classification-F1 0.20755186132423986 on epoch=187
03/18/2022 11:51:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=189
03/18/2022 11:51:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=192
03/18/2022 11:51:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.73 on epoch=194
03/18/2022 11:51:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=197
03/18/2022 11:51:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.71 on epoch=199
03/18/2022 11:51:53 - INFO - __main__ - Global step 800 Train loss 0.69 Classification-F1 0.3287312484108823 on epoch=199
03/18/2022 11:51:53 - INFO - __main__ - Saving model with best Classification-F1: 0.260423197492163 -> 0.3287312484108823 on epoch=199, global_step=800
03/18/2022 11:51:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.73 on epoch=202
03/18/2022 11:51:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.68 on epoch=204
03/18/2022 11:52:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=207
03/18/2022 11:52:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.54 on epoch=209
03/18/2022 11:52:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.61 on epoch=212
03/18/2022 11:52:06 - INFO - __main__ - Global step 850 Train loss 0.62 Classification-F1 0.41887075330259393 on epoch=212
03/18/2022 11:52:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3287312484108823 -> 0.41887075330259393 on epoch=212, global_step=850
03/18/2022 11:52:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.62 on epoch=214
03/18/2022 11:52:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=217
03/18/2022 11:52:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=219
03/18/2022 11:52:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=222
03/18/2022 11:52:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=224
03/18/2022 11:52:19 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.46408602150537637 on epoch=224
03/18/2022 11:52:19 - INFO - __main__ - Saving model with best Classification-F1: 0.41887075330259393 -> 0.46408602150537637 on epoch=224, global_step=900
03/18/2022 11:52:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.57 on epoch=227
03/18/2022 11:52:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=229
03/18/2022 11:52:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=232
03/18/2022 11:52:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=234
03/18/2022 11:52:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=237
03/18/2022 11:52:32 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.5021464646464646 on epoch=237
03/18/2022 11:52:32 - INFO - __main__ - Saving model with best Classification-F1: 0.46408602150537637 -> 0.5021464646464646 on epoch=237, global_step=950
03/18/2022 11:52:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=239
03/18/2022 11:52:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=242
03/18/2022 11:52:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=244
03/18/2022 11:52:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=247
03/18/2022 11:52:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=249
03/18/2022 11:52:45 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.4875 on epoch=249
03/18/2022 11:52:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=252
03/18/2022 11:52:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=254
03/18/2022 11:52:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=257
03/18/2022 11:52:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=259
03/18/2022 11:52:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=262
03/18/2022 11:52:57 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6143162393162392 on epoch=262
03/18/2022 11:52:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5021464646464646 -> 0.6143162393162392 on epoch=262, global_step=1050
03/18/2022 11:53:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=264
03/18/2022 11:53:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=267
03/18/2022 11:53:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=269
03/18/2022 11:53:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=272
03/18/2022 11:53:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=274
03/18/2022 11:53:10 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.6187584711303877 on epoch=274
03/18/2022 11:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6143162393162392 -> 0.6187584711303877 on epoch=274, global_step=1100
03/18/2022 11:53:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=277
03/18/2022 11:53:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=279
03/18/2022 11:53:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=282
03/18/2022 11:53:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=284
03/18/2022 11:53:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
03/18/2022 11:53:23 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.5927735812037083 on epoch=287
03/18/2022 11:53:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=289
03/18/2022 11:53:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=292
03/18/2022 11:53:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=294
03/18/2022 11:53:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=297
03/18/2022 11:53:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=299
03/18/2022 11:53:36 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.6586505860699409 on epoch=299
03/18/2022 11:53:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6187584711303877 -> 0.6586505860699409 on epoch=299, global_step=1200
03/18/2022 11:53:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
03/18/2022 11:53:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
03/18/2022 11:53:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=307
03/18/2022 11:53:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=309
03/18/2022 11:53:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=312
03/18/2022 11:53:49 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.683262850773669 on epoch=312
03/18/2022 11:53:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6586505860699409 -> 0.683262850773669 on epoch=312, global_step=1250
03/18/2022 11:53:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=314
03/18/2022 11:53:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=317
03/18/2022 11:53:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=319
03/18/2022 11:53:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=322
03/18/2022 11:54:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
03/18/2022 11:54:02 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.7257739817857785 on epoch=324
03/18/2022 11:54:02 - INFO - __main__ - Saving model with best Classification-F1: 0.683262850773669 -> 0.7257739817857785 on epoch=324, global_step=1300
03/18/2022 11:54:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
03/18/2022 11:54:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
03/18/2022 11:54:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
03/18/2022 11:54:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
03/18/2022 11:54:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
03/18/2022 11:54:16 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.6851712928232058 on epoch=337
03/18/2022 11:54:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
03/18/2022 11:54:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
03/18/2022 11:54:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=344
03/18/2022 11:54:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
03/18/2022 11:54:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=349
03/18/2022 11:54:29 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.6174866740723517 on epoch=349
03/18/2022 11:54:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=352
03/18/2022 11:54:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
03/18/2022 11:54:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
03/18/2022 11:54:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
03/18/2022 11:54:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
03/18/2022 11:54:42 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.6303180692566882 on epoch=362
03/18/2022 11:54:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/18/2022 11:54:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/18/2022 11:54:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
03/18/2022 11:54:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=372
03/18/2022 11:54:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
03/18/2022 11:54:55 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.5939190542977358 on epoch=374
03/18/2022 11:54:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=377
03/18/2022 11:54:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
03/18/2022 11:55:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=382
03/18/2022 11:55:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/18/2022 11:55:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
03/18/2022 11:55:08 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6883303624480095 on epoch=387
03/18/2022 11:55:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
03/18/2022 11:55:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
03/18/2022 11:55:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/18/2022 11:55:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
03/18/2022 11:55:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
03/18/2022 11:55:21 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.5720410065237651 on epoch=399
03/18/2022 11:55:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
03/18/2022 11:55:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=404
03/18/2022 11:55:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
03/18/2022 11:55:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
03/18/2022 11:55:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
03/18/2022 11:55:34 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6996527777777778 on epoch=412
03/18/2022 11:55:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/18/2022 11:55:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/18/2022 11:55:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
03/18/2022 11:55:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
03/18/2022 11:55:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
03/18/2022 11:55:47 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6523809523809523 on epoch=424
03/18/2022 11:55:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
03/18/2022 11:55:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 11:55:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
03/18/2022 11:55:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 11:55:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/18/2022 11:56:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6496362433862434 on epoch=437
03/18/2022 11:56:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/18/2022 11:56:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
03/18/2022 11:56:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
03/18/2022 11:56:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/18/2022 11:56:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/18/2022 11:56:13 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7132910401002506 on epoch=449
03/18/2022 11:56:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
03/18/2022 11:56:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/18/2022 11:56:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
03/18/2022 11:56:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/18/2022 11:56:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/18/2022 11:56:26 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6590151515151516 on epoch=462
03/18/2022 11:56:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/18/2022 11:56:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 11:56:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
03/18/2022 11:56:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/18/2022 11:56:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 11:56:39 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7724480095068331 on epoch=474
03/18/2022 11:56:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7257739817857785 -> 0.7724480095068331 on epoch=474, global_step=1900
03/18/2022 11:56:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
03/18/2022 11:56:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/18/2022 11:56:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/18/2022 11:56:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
03/18/2022 11:56:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/18/2022 11:56:52 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6733692248398131 on epoch=487
03/18/2022 11:56:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
03/18/2022 11:56:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
03/18/2022 11:56:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/18/2022 11:57:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/18/2022 11:57:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 11:57:05 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7474049248242797 on epoch=499
03/18/2022 11:57:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/18/2022 11:57:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
03/18/2022 11:57:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/18/2022 11:57:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
03/18/2022 11:57:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
03/18/2022 11:57:18 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6944798865851498 on epoch=512
03/18/2022 11:57:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/18/2022 11:57:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
03/18/2022 11:57:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/18/2022 11:57:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
03/18/2022 11:57:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 11:57:31 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6737394957983194 on epoch=524
03/18/2022 11:57:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
03/18/2022 11:57:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 11:57:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
03/18/2022 11:57:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
03/18/2022 11:57:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
03/18/2022 11:57:44 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6723856209150327 on epoch=537
03/18/2022 11:57:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
03/18/2022 11:57:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/18/2022 11:57:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/18/2022 11:57:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
03/18/2022 11:57:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
03/18/2022 11:57:57 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6931045751633987 on epoch=549
03/18/2022 11:58:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 11:58:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
03/18/2022 11:58:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/18/2022 11:58:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
03/18/2022 11:58:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
03/18/2022 11:58:10 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7387146074646075 on epoch=562
03/18/2022 11:58:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
03/18/2022 11:58:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
03/18/2022 11:58:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
03/18/2022 11:58:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 11:58:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
03/18/2022 11:58:23 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7238844393592677 on epoch=574
03/18/2022 11:58:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 11:58:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
03/18/2022 11:58:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 11:58:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
03/18/2022 11:58:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/18/2022 11:58:36 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7550438596491228 on epoch=587
03/18/2022 11:58:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
03/18/2022 11:58:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 11:58:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 11:58:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 11:58:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
03/18/2022 11:58:49 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6783245349421819 on epoch=599
03/18/2022 11:58:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/18/2022 11:58:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 11:58:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/18/2022 11:58:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/18/2022 11:59:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
03/18/2022 11:59:02 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7025186071238703 on epoch=612
03/18/2022 11:59:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 11:59:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
03/18/2022 11:59:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
03/18/2022 11:59:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
03/18/2022 11:59:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
03/18/2022 11:59:15 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6291666666666667 on epoch=624
03/18/2022 11:59:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 11:59:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
03/18/2022 11:59:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 11:59:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 11:59:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 11:59:28 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7248066917184565 on epoch=637
03/18/2022 11:59:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
03/18/2022 11:59:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/18/2022 11:59:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/18/2022 11:59:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
03/18/2022 11:59:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/18/2022 11:59:41 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.5980233739837398 on epoch=649
03/18/2022 11:59:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 11:59:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/18/2022 11:59:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
03/18/2022 11:59:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 11:59:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 11:59:54 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5912878787878788 on epoch=662
03/18/2022 11:59:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/18/2022 11:59:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 12:00:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
03/18/2022 12:00:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/18/2022 12:00:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 12:00:07 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6505791505791505 on epoch=674
03/18/2022 12:00:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 12:00:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/18/2022 12:00:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/18/2022 12:00:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/18/2022 12:00:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 12:00:20 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6744570463320463 on epoch=687
03/18/2022 12:00:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/18/2022 12:00:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 12:00:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
03/18/2022 12:00:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
03/18/2022 12:00:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 12:00:33 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6817766533136551 on epoch=699
03/18/2022 12:00:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 12:00:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/18/2022 12:00:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 12:00:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
03/18/2022 12:00:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 12:00:46 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.656319073083779 on epoch=712
03/18/2022 12:00:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
03/18/2022 12:00:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/18/2022 12:00:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
03/18/2022 12:00:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
03/18/2022 12:00:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 12:00:59 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7392720977279801 on epoch=724
03/18/2022 12:01:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
03/18/2022 12:01:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 12:01:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 12:01:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 12:01:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
03/18/2022 12:01:12 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.684109795197278 on epoch=737
03/18/2022 12:01:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 12:01:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
03/18/2022 12:01:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 12:01:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 12:01:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 12:01:25 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7235497835497835 on epoch=749
03/18/2022 12:01:25 - INFO - __main__ - save last model!
03/18/2022 12:01:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 12:01:25 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 12:01:26 - INFO - __main__ - Printing 3 examples
03/18/2022 12:01:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 12:01:26 - INFO - __main__ - ['others']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 12:01:26 - INFO - __main__ - ['others']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 12:01:26 - INFO - __main__ - ['others']
03/18/2022 12:01:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:01:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:01:26 - INFO - __main__ - Printing 3 examples
03/18/2022 12:01:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 12:01:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:01:26 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:01:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:01:26 - INFO - __main__ - Printing 3 examples
03/18/2022 12:01:26 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:01:26 - INFO - __main__ - ['happy']
03/18/2022 12:01:26 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:01:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:01:26 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:01:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:01:33 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 12:01:44 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:01:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:01:45 - INFO - __main__ - Starting training!
03/18/2022 12:03:07 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_21_0.2_8_predictions.txt
03/18/2022 12:03:07 - INFO - __main__ - Classification-F1 on test data: 0.3990
03/18/2022 12:03:07 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7724480095068331, test_performance=0.39897275593374903
03/18/2022 12:03:07 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
03/18/2022 12:03:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:03:08 - INFO - __main__ - Printing 3 examples
03/18/2022 12:03:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:03:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:03:08 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:03:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:03:08 - INFO - __main__ - Printing 3 examples
03/18/2022 12:03:08 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:03:08 - INFO - __main__ - ['happy']
03/18/2022 12:03:08 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:03:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:03:08 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:03:25 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:03:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:03:26 - INFO - __main__ - Starting training!
03/18/2022 12:03:29 - INFO - __main__ - Step 10 Global step 10 Train loss 6.32 on epoch=2
03/18/2022 12:03:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.31 on epoch=4
03/18/2022 12:03:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=7
03/18/2022 12:03:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=9
03/18/2022 12:03:38 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=12
03/18/2022 12:03:39 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.22252747252747254 on epoch=12
03/18/2022 12:03:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22252747252747254 on epoch=12, global_step=50
03/18/2022 12:03:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
03/18/2022 12:03:44 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=17
03/18/2022 12:03:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=19
03/18/2022 12:03:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
03/18/2022 12:03:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
03/18/2022 12:03:52 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.20043478260869568 on epoch=24
03/18/2022 12:03:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=27
03/18/2022 12:03:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=29
03/18/2022 12:04:00 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=32
03/18/2022 12:04:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
03/18/2022 12:04:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=37
03/18/2022 12:04:06 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.13067758749069247 on epoch=37
03/18/2022 12:04:08 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=39
03/18/2022 12:04:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.32 on epoch=42
03/18/2022 12:04:13 - INFO - __main__ - Step 180 Global step 180 Train loss 2.66 on epoch=44
03/18/2022 12:04:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=47
03/18/2022 12:04:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=49
03/18/2022 12:04:19 - INFO - __main__ - Global step 200 Train loss 1.54 Classification-F1 0.20169082125603863 on epoch=49
03/18/2022 12:04:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
03/18/2022 12:04:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
03/18/2022 12:04:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
03/18/2022 12:04:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=59
03/18/2022 12:04:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=62
03/18/2022 12:04:32 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.27941176470588236 on epoch=62
03/18/2022 12:04:32 - INFO - __main__ - Saving model with best Classification-F1: 0.22252747252747254 -> 0.27941176470588236 on epoch=62, global_step=250
03/18/2022 12:04:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
03/18/2022 12:04:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
03/18/2022 12:04:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=69
03/18/2022 12:04:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
03/18/2022 12:04:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
03/18/2022 12:04:45 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.32779720279720276 on epoch=74
03/18/2022 12:04:45 - INFO - __main__ - Saving model with best Classification-F1: 0.27941176470588236 -> 0.32779720279720276 on epoch=74, global_step=300
03/18/2022 12:04:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=77
03/18/2022 12:04:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=79
03/18/2022 12:04:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
03/18/2022 12:04:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
03/18/2022 12:04:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
03/18/2022 12:04:58 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.2780830280830281 on epoch=87
03/18/2022 12:05:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.82 on epoch=89
03/18/2022 12:05:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
03/18/2022 12:05:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
03/18/2022 12:05:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=97
03/18/2022 12:05:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=99
03/18/2022 12:05:11 - INFO - __main__ - Global step 400 Train loss 0.85 Classification-F1 0.13330786860198623 on epoch=99
03/18/2022 12:05:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.84 on epoch=102
03/18/2022 12:05:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
03/18/2022 12:05:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=107
03/18/2022 12:05:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.84 on epoch=109
03/18/2022 12:05:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
03/18/2022 12:05:24 - INFO - __main__ - Global step 450 Train loss 0.83 Classification-F1 0.38886806596701645 on epoch=112
03/18/2022 12:05:24 - INFO - __main__ - Saving model with best Classification-F1: 0.32779720279720276 -> 0.38886806596701645 on epoch=112, global_step=450
03/18/2022 12:05:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=114
03/18/2022 12:05:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.91 on epoch=117
03/18/2022 12:05:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
03/18/2022 12:05:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=122
03/18/2022 12:05:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
03/18/2022 12:05:37 - INFO - __main__ - Global step 500 Train loss 0.82 Classification-F1 0.3313836544197635 on epoch=124
03/18/2022 12:05:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
03/18/2022 12:05:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=129
03/18/2022 12:05:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=132
03/18/2022 12:05:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
03/18/2022 12:05:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=137
03/18/2022 12:05:51 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.33909774436090223 on epoch=137
03/18/2022 12:05:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=139
03/18/2022 12:05:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.74 on epoch=142
03/18/2022 12:05:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.87 on epoch=144
03/18/2022 12:06:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.66 on epoch=147
03/18/2022 12:06:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.75 on epoch=149
03/18/2022 12:06:04 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.24116798846431145 on epoch=149
03/18/2022 12:06:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.74 on epoch=152
03/18/2022 12:06:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
03/18/2022 12:06:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.65 on epoch=157
03/18/2022 12:06:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.78 on epoch=159
03/18/2022 12:06:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.66 on epoch=162
03/18/2022 12:06:17 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.4393939393939394 on epoch=162
03/18/2022 12:06:17 - INFO - __main__ - Saving model with best Classification-F1: 0.38886806596701645 -> 0.4393939393939394 on epoch=162, global_step=650
03/18/2022 12:06:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=164
03/18/2022 12:06:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=167
03/18/2022 12:06:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.74 on epoch=169
03/18/2022 12:06:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=172
03/18/2022 12:06:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=174
03/18/2022 12:06:30 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.5089565004887586 on epoch=174
03/18/2022 12:06:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4393939393939394 -> 0.5089565004887586 on epoch=174, global_step=700
03/18/2022 12:06:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=177
03/18/2022 12:06:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=179
03/18/2022 12:06:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=182
03/18/2022 12:06:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=184
03/18/2022 12:06:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=187
03/18/2022 12:06:43 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.4647678408547973 on epoch=187
03/18/2022 12:06:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=189
03/18/2022 12:06:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=192
03/18/2022 12:06:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=194
03/18/2022 12:06:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=197
03/18/2022 12:06:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=199
03/18/2022 12:06:56 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.4941060717668105 on epoch=199
03/18/2022 12:06:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=202
03/18/2022 12:07:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=204
03/18/2022 12:07:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=207
03/18/2022 12:07:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=209
03/18/2022 12:07:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=212
03/18/2022 12:07:09 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.4962586334327293 on epoch=212
03/18/2022 12:07:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
03/18/2022 12:07:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=217
03/18/2022 12:07:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=219
03/18/2022 12:07:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
03/18/2022 12:07:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=224
03/18/2022 12:07:23 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.5089153732446415 on epoch=224
03/18/2022 12:07:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
03/18/2022 12:07:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
03/18/2022 12:07:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=232
03/18/2022 12:07:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
03/18/2022 12:07:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
03/18/2022 12:07:36 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.47327731092436975 on epoch=237
03/18/2022 12:07:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=239
03/18/2022 12:07:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
03/18/2022 12:07:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
03/18/2022 12:07:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=247
03/18/2022 12:07:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
03/18/2022 12:07:50 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.546141862170088 on epoch=249
03/18/2022 12:07:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5089565004887586 -> 0.546141862170088 on epoch=249, global_step=1000
03/18/2022 12:07:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
03/18/2022 12:07:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
03/18/2022 12:07:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
03/18/2022 12:08:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
03/18/2022 12:08:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
03/18/2022 12:08:04 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.5390688533477908 on epoch=262
03/18/2022 12:08:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
03/18/2022 12:08:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
03/18/2022 12:08:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
03/18/2022 12:08:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=272
03/18/2022 12:08:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
03/18/2022 12:08:17 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.5328920361247947 on epoch=274
03/18/2022 12:08:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
03/18/2022 12:08:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
03/18/2022 12:08:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
03/18/2022 12:08:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
03/18/2022 12:08:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=287
03/18/2022 12:08:30 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.5943451526952245 on epoch=287
03/18/2022 12:08:30 - INFO - __main__ - Saving model with best Classification-F1: 0.546141862170088 -> 0.5943451526952245 on epoch=287, global_step=1150
03/18/2022 12:08:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
03/18/2022 12:08:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
03/18/2022 12:08:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
03/18/2022 12:08:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
03/18/2022 12:08:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/18/2022 12:08:44 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6254289215686275 on epoch=299
03/18/2022 12:08:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5943451526952245 -> 0.6254289215686275 on epoch=299, global_step=1200
03/18/2022 12:08:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/18/2022 12:08:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
03/18/2022 12:08:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
03/18/2022 12:08:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=309
03/18/2022 12:08:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/18/2022 12:08:57 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.4121693121693121 on epoch=312
03/18/2022 12:08:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
03/18/2022 12:09:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
03/18/2022 12:09:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
03/18/2022 12:09:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/18/2022 12:09:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
03/18/2022 12:09:10 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6019009725906278 on epoch=324
03/18/2022 12:09:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
03/18/2022 12:09:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
03/18/2022 12:09:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
03/18/2022 12:09:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
03/18/2022 12:09:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
03/18/2022 12:09:23 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.5444154741692943 on epoch=337
03/18/2022 12:09:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
03/18/2022 12:09:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/18/2022 12:09:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
03/18/2022 12:09:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=347
03/18/2022 12:09:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
03/18/2022 12:09:37 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.5097542794462984 on epoch=349
03/18/2022 12:09:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
03/18/2022 12:09:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
03/18/2022 12:09:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
03/18/2022 12:09:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/18/2022 12:09:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
03/18/2022 12:09:50 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.562270714737508 on epoch=362
03/18/2022 12:09:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/18/2022 12:09:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
03/18/2022 12:09:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
03/18/2022 12:10:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
03/18/2022 12:10:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 12:10:04 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5815047021943573 on epoch=374
03/18/2022 12:10:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
03/18/2022 12:10:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/18/2022 12:10:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/18/2022 12:10:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
03/18/2022 12:10:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/18/2022 12:10:17 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5475865749525617 on epoch=387
03/18/2022 12:10:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/18/2022 12:10:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 12:10:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
03/18/2022 12:10:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
03/18/2022 12:10:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
03/18/2022 12:10:30 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.5795255819449368 on epoch=399
03/18/2022 12:10:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
03/18/2022 12:10:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/18/2022 12:10:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 12:10:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
03/18/2022 12:10:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
03/18/2022 12:10:44 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.54373460591133 on epoch=412
03/18/2022 12:10:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
03/18/2022 12:10:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/18/2022 12:10:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
03/18/2022 12:10:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
03/18/2022 12:10:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
03/18/2022 12:10:57 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.62519981599678 on epoch=424
03/18/2022 12:10:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 12:11:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
03/18/2022 12:11:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/18/2022 12:11:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 12:11:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 12:11:10 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5768453644983123 on epoch=437
03/18/2022 12:11:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
03/18/2022 12:11:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/18/2022 12:11:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 12:11:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
03/18/2022 12:11:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 12:11:24 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5611235696624691 on epoch=449
03/18/2022 12:11:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
03/18/2022 12:11:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 12:11:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=457
03/18/2022 12:11:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/18/2022 12:11:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 12:11:37 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.5597537878787879 on epoch=462
03/18/2022 12:11:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 12:11:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
03/18/2022 12:11:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 12:11:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/18/2022 12:11:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 12:11:50 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6170745272525028 on epoch=474
03/18/2022 12:11:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 12:11:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
03/18/2022 12:11:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/18/2022 12:12:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/18/2022 12:12:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 12:12:04 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.574905303030303 on epoch=487
03/18/2022 12:12:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 12:12:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 12:12:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 12:12:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/18/2022 12:12:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 12:12:17 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6267364312496159 on epoch=499
03/18/2022 12:12:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6254289215686275 -> 0.6267364312496159 on epoch=499, global_step=2000
03/18/2022 12:12:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 12:12:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 12:12:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 12:12:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 12:12:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 12:12:31 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.5805871212121212 on epoch=512
03/18/2022 12:12:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 12:12:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 12:12:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
03/18/2022 12:12:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 12:12:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 12:12:44 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5213287921999357 on epoch=524
03/18/2022 12:12:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
03/18/2022 12:12:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 12:12:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
03/18/2022 12:12:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
03/18/2022 12:12:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
03/18/2022 12:12:58 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.5880824372759856 on epoch=537
03/18/2022 12:13:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 12:13:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/18/2022 12:13:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
03/18/2022 12:13:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 12:13:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 12:13:11 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.617003367003367 on epoch=549
03/18/2022 12:13:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 12:13:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 12:13:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 12:13:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 12:13:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 12:13:24 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.5510416666666668 on epoch=562
03/18/2022 12:13:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
03/18/2022 12:13:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 12:13:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 12:13:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 12:13:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 12:13:38 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.5187386800290026 on epoch=574
03/18/2022 12:13:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
03/18/2022 12:13:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 12:13:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
03/18/2022 12:13:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
03/18/2022 12:13:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 12:13:51 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5869037892934952 on epoch=587
03/18/2022 12:13:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
03/18/2022 12:13:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 12:13:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 12:14:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 12:14:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 12:14:04 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5176913391422568 on epoch=599
03/18/2022 12:14:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
03/18/2022 12:14:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
03/18/2022 12:14:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 12:14:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 12:14:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 12:14:17 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5153471782978425 on epoch=612
03/18/2022 12:14:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=614
03/18/2022 12:14:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 12:14:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 12:14:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 12:14:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 12:14:31 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5282966250708186 on epoch=624
03/18/2022 12:14:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 12:14:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 12:14:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 12:14:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 12:14:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 12:14:44 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.5323130641145347 on epoch=637
03/18/2022 12:14:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 12:14:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/18/2022 12:14:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
03/18/2022 12:14:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 12:14:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 12:14:57 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5396516671184604 on epoch=649
03/18/2022 12:15:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
03/18/2022 12:15:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/18/2022 12:15:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 12:15:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
03/18/2022 12:15:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 12:15:10 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.524753593719111 on epoch=662
03/18/2022 12:15:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
03/18/2022 12:15:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=667
03/18/2022 12:15:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 12:15:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 12:15:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 12:15:24 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5456040480708413 on epoch=674
03/18/2022 12:15:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/18/2022 12:15:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 12:15:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 12:15:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
03/18/2022 12:15:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 12:15:37 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5693616029822925 on epoch=687
03/18/2022 12:15:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 12:15:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/18/2022 12:15:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 12:15:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
03/18/2022 12:15:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 12:15:50 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5318260368663594 on epoch=699
03/18/2022 12:15:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 12:15:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 12:15:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 12:16:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 12:16:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 12:16:04 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5313583720473729 on epoch=712
03/18/2022 12:16:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 12:16:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 12:16:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
03/18/2022 12:16:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 12:16:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
03/18/2022 12:16:17 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5294117647058824 on epoch=724
03/18/2022 12:16:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 12:16:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 12:16:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 12:16:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 12:16:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 12:16:31 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5422696890438826 on epoch=737
03/18/2022 12:16:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 12:16:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 12:16:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 12:16:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 12:16:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 12:16:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:16:44 - INFO - __main__ - Printing 3 examples
03/18/2022 12:16:44 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 12:16:44 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5647656201979224 on epoch=749
03/18/2022 12:16:44 - INFO - __main__ - save last model!
03/18/2022 12:16:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:16:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 12:16:44 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 12:16:44 - INFO - __main__ - Printing 3 examples
03/18/2022 12:16:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 12:16:44 - INFO - __main__ - ['others']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 12:16:44 - INFO - __main__ - ['others']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 12:16:44 - INFO - __main__ - ['others']
03/18/2022 12:16:44 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:16:44 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:16:44 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:16:44 - INFO - __main__ - Printing 3 examples
03/18/2022 12:16:44 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:16:44 - INFO - __main__ - ['happy']
03/18/2022 12:16:44 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:16:44 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:16:44 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:16:46 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:16:52 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 12:17:03 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:17:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:17:03 - INFO - __main__ - Starting training!
03/18/2022 12:18:28 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_42_0.5_8_predictions.txt
03/18/2022 12:18:28 - INFO - __main__ - Classification-F1 on test data: 0.1558
03/18/2022 12:18:28 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.6267364312496159, test_performance=0.15583911440562945
03/18/2022 12:18:28 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
03/18/2022 12:18:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:18:29 - INFO - __main__ - Printing 3 examples
03/18/2022 12:18:29 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:18:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:18:29 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:18:29 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:18:29 - INFO - __main__ - Printing 3 examples
03/18/2022 12:18:29 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:18:29 - INFO - __main__ - ['happy']
03/18/2022 12:18:29 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:18:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:18:29 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:18:48 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:18:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:18:48 - INFO - __main__ - Starting training!
03/18/2022 12:18:53 - INFO - __main__ - Step 10 Global step 10 Train loss 7.13 on epoch=2
03/18/2022 12:18:55 - INFO - __main__ - Step 20 Global step 20 Train loss 4.02 on epoch=4
03/18/2022 12:18:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.99 on epoch=7
03/18/2022 12:19:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.42 on epoch=9
03/18/2022 12:19:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.17 on epoch=12
03/18/2022 12:19:03 - INFO - __main__ - Global step 50 Train loss 3.14 Classification-F1 0.18907563025210083 on epoch=12
03/18/2022 12:19:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18907563025210083 on epoch=12, global_step=50
03/18/2022 12:19:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=14
03/18/2022 12:19:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=17
03/18/2022 12:19:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=19
03/18/2022 12:19:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=22
03/18/2022 12:19:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=24
03/18/2022 12:19:16 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.14160839160839161 on epoch=24
03/18/2022 12:19:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=27
03/18/2022 12:19:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
03/18/2022 12:19:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
03/18/2022 12:19:26 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=34
03/18/2022 12:19:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
03/18/2022 12:19:29 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.10126582278481013 on epoch=37
03/18/2022 12:19:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
03/18/2022 12:19:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=42
03/18/2022 12:19:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
03/18/2022 12:19:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=47
03/18/2022 12:19:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
03/18/2022 12:19:42 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.1480549199084668 on epoch=49
03/18/2022 12:19:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=52
03/18/2022 12:19:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
03/18/2022 12:19:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
03/18/2022 12:19:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=59
03/18/2022 12:19:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
03/18/2022 12:19:54 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.17267605633802818 on epoch=62
03/18/2022 12:19:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=64
03/18/2022 12:19:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
03/18/2022 12:20:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=69
03/18/2022 12:20:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=72
03/18/2022 12:20:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
03/18/2022 12:20:07 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.1581196581196581 on epoch=74
03/18/2022 12:20:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.85 on epoch=77
03/18/2022 12:20:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
03/18/2022 12:20:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
03/18/2022 12:20:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
03/18/2022 12:20:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=87
03/18/2022 12:20:20 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.3147224991691592 on epoch=87
03/18/2022 12:20:20 - INFO - __main__ - Saving model with best Classification-F1: 0.18907563025210083 -> 0.3147224991691592 on epoch=87, global_step=350
03/18/2022 12:20:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
03/18/2022 12:20:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=92
03/18/2022 12:20:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=94
03/18/2022 12:20:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=97
03/18/2022 12:20:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.71 on epoch=99
03/18/2022 12:20:32 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.332116523192038 on epoch=99
03/18/2022 12:20:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3147224991691592 -> 0.332116523192038 on epoch=99, global_step=400
03/18/2022 12:20:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=102
03/18/2022 12:20:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.71 on epoch=104
03/18/2022 12:20:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=107
03/18/2022 12:20:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.63 on epoch=109
03/18/2022 12:20:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=112
03/18/2022 12:20:45 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.4053103626274358 on epoch=112
03/18/2022 12:20:45 - INFO - __main__ - Saving model with best Classification-F1: 0.332116523192038 -> 0.4053103626274358 on epoch=112, global_step=450
03/18/2022 12:20:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=114
03/18/2022 12:20:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=117
03/18/2022 12:20:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=119
03/18/2022 12:20:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=122
03/18/2022 12:20:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=124
03/18/2022 12:20:58 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.5207577602199696 on epoch=124
03/18/2022 12:20:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4053103626274358 -> 0.5207577602199696 on epoch=124, global_step=500
03/18/2022 12:21:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=127
03/18/2022 12:21:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=129
03/18/2022 12:21:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=132
03/18/2022 12:21:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=134
03/18/2022 12:21:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
03/18/2022 12:21:10 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.595348616900341 on epoch=137
03/18/2022 12:21:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5207577602199696 -> 0.595348616900341 on epoch=137, global_step=550
03/18/2022 12:21:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=139
03/18/2022 12:21:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=142
03/18/2022 12:21:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
03/18/2022 12:21:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
03/18/2022 12:21:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=149
03/18/2022 12:21:23 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.6404293310987024 on epoch=149
03/18/2022 12:21:23 - INFO - __main__ - Saving model with best Classification-F1: 0.595348616900341 -> 0.6404293310987024 on epoch=149, global_step=600
03/18/2022 12:21:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
03/18/2022 12:21:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
03/18/2022 12:21:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=157
03/18/2022 12:21:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=159
03/18/2022 12:21:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=162
03/18/2022 12:21:36 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.5176102699144174 on epoch=162
03/18/2022 12:21:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=164
03/18/2022 12:21:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
03/18/2022 12:21:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=169
03/18/2022 12:21:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
03/18/2022 12:21:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
03/18/2022 12:21:49 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.6857142857142858 on epoch=174
03/18/2022 12:21:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6404293310987024 -> 0.6857142857142858 on epoch=174, global_step=700
03/18/2022 12:21:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
03/18/2022 12:21:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
03/18/2022 12:21:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=182
03/18/2022 12:21:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=184
03/18/2022 12:22:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
03/18/2022 12:22:01 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.7007002801120448 on epoch=187
03/18/2022 12:22:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6857142857142858 -> 0.7007002801120448 on epoch=187, global_step=750
03/18/2022 12:22:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
03/18/2022 12:22:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=192
03/18/2022 12:22:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=194
03/18/2022 12:22:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
03/18/2022 12:22:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
03/18/2022 12:22:14 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.6703384418901659 on epoch=199
03/18/2022 12:22:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
03/18/2022 12:22:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
03/18/2022 12:22:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
03/18/2022 12:22:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
03/18/2022 12:22:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=212
03/18/2022 12:22:27 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.607981993851559 on epoch=212
03/18/2022 12:22:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
03/18/2022 12:22:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=217
03/18/2022 12:22:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
03/18/2022 12:22:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
03/18/2022 12:22:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
03/18/2022 12:22:40 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.6704573934837093 on epoch=224
03/18/2022 12:22:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
03/18/2022 12:22:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
03/18/2022 12:22:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
03/18/2022 12:22:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
03/18/2022 12:22:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
03/18/2022 12:22:53 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.6702107279693487 on epoch=237
03/18/2022 12:22:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
03/18/2022 12:22:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
03/18/2022 12:23:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
03/18/2022 12:23:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
03/18/2022 12:23:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
03/18/2022 12:23:06 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.6986892736892737 on epoch=249
03/18/2022 12:23:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
03/18/2022 12:23:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
03/18/2022 12:23:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
03/18/2022 12:23:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
03/18/2022 12:23:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
03/18/2022 12:23:19 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6547619047619048 on epoch=262
03/18/2022 12:23:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
03/18/2022 12:23:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
03/18/2022 12:23:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
03/18/2022 12:23:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
03/18/2022 12:23:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
03/18/2022 12:23:32 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6845061810579052 on epoch=274
03/18/2022 12:23:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
03/18/2022 12:23:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
03/18/2022 12:23:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=282
03/18/2022 12:23:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=284
03/18/2022 12:23:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
03/18/2022 12:23:45 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.6264880952380952 on epoch=287
03/18/2022 12:23:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
03/18/2022 12:23:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
03/18/2022 12:23:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
03/18/2022 12:23:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/18/2022 12:23:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
03/18/2022 12:23:59 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.6727722082985241 on epoch=299
03/18/2022 12:24:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
03/18/2022 12:24:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
03/18/2022 12:24:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
03/18/2022 12:24:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/18/2022 12:24:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
03/18/2022 12:24:12 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.5720087094767402 on epoch=312
03/18/2022 12:24:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
03/18/2022 12:24:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
03/18/2022 12:24:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/18/2022 12:24:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
03/18/2022 12:24:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
03/18/2022 12:24:25 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6259478974996215 on epoch=324
03/18/2022 12:24:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
03/18/2022 12:24:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
03/18/2022 12:24:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
03/18/2022 12:24:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
03/18/2022 12:24:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
03/18/2022 12:24:38 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6442223473473473 on epoch=337
03/18/2022 12:24:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
03/18/2022 12:24:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
03/18/2022 12:24:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=344
03/18/2022 12:24:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
03/18/2022 12:24:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
03/18/2022 12:24:51 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.6862503622138509 on epoch=349
03/18/2022 12:24:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
03/18/2022 12:24:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
03/18/2022 12:24:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
03/18/2022 12:25:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
03/18/2022 12:25:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/18/2022 12:25:04 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.657354698151662 on epoch=362
03/18/2022 12:25:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/18/2022 12:25:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
03/18/2022 12:25:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/18/2022 12:25:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
03/18/2022 12:25:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 12:25:17 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6865079365079365 on epoch=374
03/18/2022 12:25:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
03/18/2022 12:25:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 12:25:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
03/18/2022 12:25:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 12:25:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/18/2022 12:25:30 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6867525833043076 on epoch=387
03/18/2022 12:25:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
03/18/2022 12:25:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/18/2022 12:25:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/18/2022 12:25:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
03/18/2022 12:25:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
03/18/2022 12:25:43 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6581716916157143 on epoch=399
03/18/2022 12:25:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
03/18/2022 12:25:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/18/2022 12:25:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
03/18/2022 12:25:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
03/18/2022 12:25:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
03/18/2022 12:25:56 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.6542867042867043 on epoch=412
03/18/2022 12:25:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/18/2022 12:26:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
03/18/2022 12:26:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 12:26:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
03/18/2022 12:26:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
03/18/2022 12:26:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6718571512689159 on epoch=424
03/18/2022 12:26:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 12:26:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
03/18/2022 12:26:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
03/18/2022 12:26:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
03/18/2022 12:26:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
03/18/2022 12:26:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6708994708994709 on epoch=437
03/18/2022 12:26:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
03/18/2022 12:26:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 12:26:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
03/18/2022 12:26:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/18/2022 12:26:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 12:26:35 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6718571512689159 on epoch=449
03/18/2022 12:26:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 12:26:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 12:26:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 12:26:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
03/18/2022 12:26:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
03/18/2022 12:26:48 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6865408470671628 on epoch=462
03/18/2022 12:26:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 12:26:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 12:26:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 12:26:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 12:27:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 12:27:01 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.639070246148286 on epoch=474
03/18/2022 12:27:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 12:27:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
03/18/2022 12:27:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/18/2022 12:27:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/18/2022 12:27:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 12:27:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6571512689159748 on epoch=487
03/18/2022 12:27:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 12:27:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
03/18/2022 12:27:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
03/18/2022 12:27:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 12:27:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 12:27:27 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6610100884294432 on epoch=499
03/18/2022 12:27:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
03/18/2022 12:27:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 12:27:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 12:27:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 12:27:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 12:27:40 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6724548440065681 on epoch=512
03/18/2022 12:27:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 12:27:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 12:27:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 12:27:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 12:27:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 12:27:53 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.657354698151662 on epoch=524
03/18/2022 12:27:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/18/2022 12:27:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 12:28:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
03/18/2022 12:28:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 12:28:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 12:28:06 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.653350299902024 on epoch=537
03/18/2022 12:28:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 12:28:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 12:28:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
03/18/2022 12:28:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 12:28:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 12:28:19 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6703384418901659 on epoch=549
03/18/2022 12:28:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 12:28:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 12:28:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 12:28:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 12:28:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=562
03/18/2022 12:28:32 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6586571716028422 on epoch=562
03/18/2022 12:28:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 12:28:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 12:28:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 12:28:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 12:28:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 12:28:45 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6412141779788838 on epoch=574
03/18/2022 12:28:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 12:28:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 12:28:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 12:28:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 12:28:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 12:28:59 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6853453218351264 on epoch=587
03/18/2022 12:29:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 12:29:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 12:29:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 12:29:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 12:29:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 12:29:12 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6836354702208362 on epoch=599
03/18/2022 12:29:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 12:29:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 12:29:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 12:29:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/18/2022 12:29:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 12:29:25 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6412980378497619 on epoch=612
03/18/2022 12:29:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 12:29:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 12:29:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 12:29:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 12:29:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 12:29:38 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6845061810579052 on epoch=624
03/18/2022 12:29:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 12:29:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
03/18/2022 12:29:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=632
03/18/2022 12:29:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
03/18/2022 12:29:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 12:29:51 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6845061810579052 on epoch=637
03/18/2022 12:29:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 12:29:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 12:29:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 12:30:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 12:30:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 12:30:04 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6845061810579052 on epoch=649
03/18/2022 12:30:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 12:30:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 12:30:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 12:30:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 12:30:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 12:30:17 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.684729064039409 on epoch=662
03/18/2022 12:30:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 12:30:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 12:30:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 12:30:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 12:30:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/18/2022 12:30:30 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6512613602316989 on epoch=674
03/18/2022 12:30:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=677
03/18/2022 12:30:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=679
03/18/2022 12:30:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 12:30:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 12:30:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 12:30:43 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7174107142857143 on epoch=687
03/18/2022 12:30:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7007002801120448 -> 0.7174107142857143 on epoch=687, global_step=2750
03/18/2022 12:30:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 12:30:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
03/18/2022 12:30:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 12:30:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 12:30:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 12:30:56 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7006756756756757 on epoch=699
03/18/2022 12:30:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 12:31:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 12:31:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 12:31:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 12:31:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
03/18/2022 12:31:09 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6706647824294882 on epoch=712
03/18/2022 12:31:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 12:31:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 12:31:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 12:31:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 12:31:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 12:31:22 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6806747634333842 on epoch=724
03/18/2022 12:31:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
03/18/2022 12:31:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 12:31:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
03/18/2022 12:31:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 12:31:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 12:31:35 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6514073476548122 on epoch=737
03/18/2022 12:31:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
03/18/2022 12:31:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 12:31:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 12:31:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 12:31:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 12:31:48 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6801778201159006 on epoch=749
03/18/2022 12:31:48 - INFO - __main__ - save last model!
03/18/2022 12:31:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 12:31:48 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 12:31:48 - INFO - __main__ - Printing 3 examples
03/18/2022 12:31:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 12:31:48 - INFO - __main__ - ['others']
03/18/2022 12:31:48 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 12:31:48 - INFO - __main__ - ['others']
03/18/2022 12:31:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 12:31:48 - INFO - __main__ - ['others']
03/18/2022 12:31:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:31:48 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:31:48 - INFO - __main__ - Printing 3 examples
03/18/2022 12:31:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:31:48 - INFO - __main__ - ['happy']
03/18/2022 12:31:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:31:48 - INFO - __main__ - ['happy']
03/18/2022 12:31:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:31:48 - INFO - __main__ - ['happy']
03/18/2022 12:31:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 12:31:48 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:31:49 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:31:49 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:31:49 - INFO - __main__ - Printing 3 examples
03/18/2022 12:31:49 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:31:49 - INFO - __main__ - ['happy']
03/18/2022 12:31:49 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:31:49 - INFO - __main__ - ['happy']
03/18/2022 12:31:49 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:31:49 - INFO - __main__ - ['happy']
03/18/2022 12:31:49 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:31:49 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:31:49 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:31:50 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:31:56 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 12:32:07 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:32:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:32:08 - INFO - __main__ - Starting training!
03/18/2022 12:33:30 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_42_0.4_8_predictions.txt
03/18/2022 12:33:30 - INFO - __main__ - Classification-F1 on test data: 0.1083
03/18/2022 12:33:30 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.7174107142857143, test_performance=0.10825291761984113
03/18/2022 12:33:30 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
03/18/2022 12:33:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:33:31 - INFO - __main__ - Printing 3 examples
03/18/2022 12:33:31 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:33:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:33:31 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:33:31 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:33:31 - INFO - __main__ - Printing 3 examples
03/18/2022 12:33:31 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:33:31 - INFO - __main__ - ['happy']
03/18/2022 12:33:31 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:33:31 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:33:31 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:33:47 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:33:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:33:47 - INFO - __main__ - Starting training!
03/18/2022 12:33:52 - INFO - __main__ - Step 10 Global step 10 Train loss 6.98 on epoch=2
03/18/2022 12:33:55 - INFO - __main__ - Step 20 Global step 20 Train loss 4.21 on epoch=4
03/18/2022 12:33:57 - INFO - __main__ - Step 30 Global step 30 Train loss 2.39 on epoch=7
03/18/2022 12:34:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.49 on epoch=9
03/18/2022 12:34:02 - INFO - __main__ - Step 50 Global step 50 Train loss 1.33 on epoch=12
03/18/2022 12:34:03 - INFO - __main__ - Global step 50 Train loss 3.28 Classification-F1 0.27345938375350143 on epoch=12
03/18/2022 12:34:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27345938375350143 on epoch=12, global_step=50
03/18/2022 12:34:05 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=14
03/18/2022 12:34:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.12 on epoch=17
03/18/2022 12:34:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=19
03/18/2022 12:34:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=22
03/18/2022 12:34:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=24
03/18/2022 12:34:16 - INFO - __main__ - Global step 100 Train loss 1.05 Classification-F1 0.20526695526695526 on epoch=24
03/18/2022 12:34:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
03/18/2022 12:34:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
03/18/2022 12:34:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
03/18/2022 12:34:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=34
03/18/2022 12:34:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
03/18/2022 12:34:29 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.2698412698412699 on epoch=37
03/18/2022 12:34:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=39
03/18/2022 12:34:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=42
03/18/2022 12:34:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
03/18/2022 12:34:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=47
03/18/2022 12:34:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
03/18/2022 12:34:42 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.1281512605042017 on epoch=49
03/18/2022 12:34:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=52
03/18/2022 12:34:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
03/18/2022 12:34:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
03/18/2022 12:34:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
03/18/2022 12:34:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
03/18/2022 12:34:55 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.1302118933697881 on epoch=62
03/18/2022 12:34:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
03/18/2022 12:35:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
03/18/2022 12:35:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=69
03/18/2022 12:35:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
03/18/2022 12:35:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
03/18/2022 12:35:08 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.1 on epoch=74
03/18/2022 12:35:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
03/18/2022 12:35:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.90 on epoch=79
03/18/2022 12:35:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.84 on epoch=82
03/18/2022 12:35:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
03/18/2022 12:35:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
03/18/2022 12:35:21 - INFO - __main__ - Global step 350 Train loss 0.86 Classification-F1 0.40439063556457067 on epoch=87
03/18/2022 12:35:21 - INFO - __main__ - Saving model with best Classification-F1: 0.27345938375350143 -> 0.40439063556457067 on epoch=87, global_step=350
03/18/2022 12:35:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=89
03/18/2022 12:35:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=92
03/18/2022 12:35:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=94
03/18/2022 12:35:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=97
03/18/2022 12:35:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=99
03/18/2022 12:35:34 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3500355366027008 on epoch=99
03/18/2022 12:35:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=102
03/18/2022 12:35:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=104
03/18/2022 12:35:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
03/18/2022 12:35:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=109
03/18/2022 12:35:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
03/18/2022 12:35:47 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.13067758749069247 on epoch=112
03/18/2022 12:35:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.78 on epoch=114
03/18/2022 12:35:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=117
03/18/2022 12:35:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
03/18/2022 12:35:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=122
03/18/2022 12:36:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
03/18/2022 12:36:00 - INFO - __main__ - Global step 500 Train loss 0.80 Classification-F1 0.11111111111111112 on epoch=124
03/18/2022 12:36:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
03/18/2022 12:36:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=129
03/18/2022 12:36:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
03/18/2022 12:36:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.82 on epoch=134
03/18/2022 12:36:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.79 on epoch=137
03/18/2022 12:36:13 - INFO - __main__ - Global step 550 Train loss 0.80 Classification-F1 0.2614870509607352 on epoch=137
03/18/2022 12:36:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=139
03/18/2022 12:36:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.83 on epoch=142
03/18/2022 12:36:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.74 on epoch=144
03/18/2022 12:36:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=147
03/18/2022 12:36:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.71 on epoch=149
03/18/2022 12:36:26 - INFO - __main__ - Global step 600 Train loss 0.74 Classification-F1 0.6044910309996516 on epoch=149
03/18/2022 12:36:26 - INFO - __main__ - Saving model with best Classification-F1: 0.40439063556457067 -> 0.6044910309996516 on epoch=149, global_step=600
03/18/2022 12:36:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
03/18/2022 12:36:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.73 on epoch=154
03/18/2022 12:36:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.72 on epoch=157
03/18/2022 12:36:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=159
03/18/2022 12:36:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=162
03/18/2022 12:36:40 - INFO - __main__ - Global step 650 Train loss 0.71 Classification-F1 0.4673835125448028 on epoch=162
03/18/2022 12:36:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
03/18/2022 12:36:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=167
03/18/2022 12:36:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=169
03/18/2022 12:36:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=172
03/18/2022 12:36:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=174
03/18/2022 12:36:53 - INFO - __main__ - Global step 700 Train loss 0.58 Classification-F1 0.5918444300797242 on epoch=174
03/18/2022 12:36:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=177
03/18/2022 12:36:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=179
03/18/2022 12:37:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=182
03/18/2022 12:37:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=184
03/18/2022 12:37:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=187
03/18/2022 12:37:06 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.4207247245247845 on epoch=187
03/18/2022 12:37:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=189
03/18/2022 12:37:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=192
03/18/2022 12:37:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=194
03/18/2022 12:37:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=197
03/18/2022 12:37:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=199
03/18/2022 12:37:19 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.6066875653082549 on epoch=199
03/18/2022 12:37:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6044910309996516 -> 0.6066875653082549 on epoch=199, global_step=800
03/18/2022 12:37:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=202
03/18/2022 12:37:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=204
03/18/2022 12:37:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=207
03/18/2022 12:37:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=209
03/18/2022 12:37:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=212
03/18/2022 12:37:32 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.4962546007854749 on epoch=212
03/18/2022 12:37:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
03/18/2022 12:37:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=217
03/18/2022 12:37:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=219
03/18/2022 12:37:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
03/18/2022 12:37:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
03/18/2022 12:37:45 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.5217727674624226 on epoch=224
03/18/2022 12:37:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
03/18/2022 12:37:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
03/18/2022 12:37:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=232
03/18/2022 12:37:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
03/18/2022 12:37:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
03/18/2022 12:37:58 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.6439927944862156 on epoch=237
03/18/2022 12:37:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6066875653082549 -> 0.6439927944862156 on epoch=237, global_step=950
03/18/2022 12:38:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=239
03/18/2022 12:38:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=242
03/18/2022 12:38:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
03/18/2022 12:38:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
03/18/2022 12:38:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
03/18/2022 12:38:11 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5520285087719299 on epoch=249
03/18/2022 12:38:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
03/18/2022 12:38:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=254
03/18/2022 12:38:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
03/18/2022 12:38:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=259
03/18/2022 12:38:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
03/18/2022 12:38:25 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.5925055017347106 on epoch=262
03/18/2022 12:38:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
03/18/2022 12:38:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
03/18/2022 12:38:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
03/18/2022 12:38:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
03/18/2022 12:38:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
03/18/2022 12:38:38 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.5750880997560315 on epoch=274
03/18/2022 12:38:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
03/18/2022 12:38:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
03/18/2022 12:38:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
03/18/2022 12:38:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=284
03/18/2022 12:38:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
03/18/2022 12:38:51 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6017569989344183 on epoch=287
03/18/2022 12:38:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
03/18/2022 12:38:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=292
03/18/2022 12:38:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
03/18/2022 12:39:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
03/18/2022 12:39:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
03/18/2022 12:39:04 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.5986607142857143 on epoch=299
03/18/2022 12:39:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
03/18/2022 12:39:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=304
03/18/2022 12:39:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
03/18/2022 12:39:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
03/18/2022 12:39:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
03/18/2022 12:39:17 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5827678571428572 on epoch=312
03/18/2022 12:39:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
03/18/2022 12:39:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
03/18/2022 12:39:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
03/18/2022 12:39:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
03/18/2022 12:39:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
03/18/2022 12:39:31 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.6263045540796963 on epoch=324
03/18/2022 12:39:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
03/18/2022 12:39:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
03/18/2022 12:39:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
03/18/2022 12:39:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
03/18/2022 12:39:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
03/18/2022 12:39:44 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.6007834757834758 on epoch=337
03/18/2022 12:39:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
03/18/2022 12:39:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/18/2022 12:39:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
03/18/2022 12:39:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
03/18/2022 12:39:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
03/18/2022 12:39:57 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6417687309527916 on epoch=349
03/18/2022 12:39:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
03/18/2022 12:40:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
03/18/2022 12:40:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=357
03/18/2022 12:40:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
03/18/2022 12:40:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
03/18/2022 12:40:10 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6287965865552073 on epoch=362
03/18/2022 12:40:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
03/18/2022 12:40:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
03/18/2022 12:40:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
03/18/2022 12:40:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
03/18/2022 12:40:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 12:40:24 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.5905893766674752 on epoch=374
03/18/2022 12:40:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
03/18/2022 12:40:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
03/18/2022 12:40:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
03/18/2022 12:40:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/18/2022 12:40:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
03/18/2022 12:40:37 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6287762416794676 on epoch=387
03/18/2022 12:40:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
03/18/2022 12:40:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 12:40:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
03/18/2022 12:40:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
03/18/2022 12:40:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
03/18/2022 12:40:50 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.639784946236559 on epoch=399
03/18/2022 12:40:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/18/2022 12:40:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
03/18/2022 12:40:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
03/18/2022 12:41:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
03/18/2022 12:41:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
03/18/2022 12:41:04 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6123626373626374 on epoch=412
03/18/2022 12:41:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
03/18/2022 12:41:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
03/18/2022 12:41:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
03/18/2022 12:41:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/18/2022 12:41:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
03/18/2022 12:41:17 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6485008262391629 on epoch=424
03/18/2022 12:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6439927944862156 -> 0.6485008262391629 on epoch=424, global_step=1700
03/18/2022 12:41:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
03/18/2022 12:41:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
03/18/2022 12:41:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
03/18/2022 12:41:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
03/18/2022 12:41:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 12:41:30 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6273809523809524 on epoch=437
03/18/2022 12:41:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
03/18/2022 12:41:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/18/2022 12:41:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 12:41:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 12:41:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/18/2022 12:41:44 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6137334887334888 on epoch=449
03/18/2022 12:41:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 12:41:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/18/2022 12:41:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 12:41:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/18/2022 12:41:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
03/18/2022 12:41:57 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6205235310498468 on epoch=462
03/18/2022 12:41:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/18/2022 12:42:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 12:42:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
03/18/2022 12:42:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
03/18/2022 12:42:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=474
03/18/2022 12:42:10 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5884224598930481 on epoch=474
03/18/2022 12:42:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/18/2022 12:42:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
03/18/2022 12:42:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/18/2022 12:42:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 12:42:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 12:42:23 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.63008658008658 on epoch=487
03/18/2022 12:42:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/18/2022 12:42:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/18/2022 12:42:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/18/2022 12:42:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
03/18/2022 12:42:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
03/18/2022 12:42:36 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6696003928866832 on epoch=499
03/18/2022 12:42:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6485008262391629 -> 0.6696003928866832 on epoch=499, global_step=2000
03/18/2022 12:42:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/18/2022 12:42:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/18/2022 12:42:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
03/18/2022 12:42:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
03/18/2022 12:42:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
03/18/2022 12:42:50 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6120298160620742 on epoch=512
03/18/2022 12:42:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 12:42:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/18/2022 12:42:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 12:43:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 12:43:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
03/18/2022 12:43:04 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5779516253654184 on epoch=524
03/18/2022 12:43:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/18/2022 12:43:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 12:43:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
03/18/2022 12:43:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 12:43:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 12:43:17 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6224289224289224 on epoch=537
03/18/2022 12:43:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
03/18/2022 12:43:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 12:43:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
03/18/2022 12:43:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 12:43:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 12:43:30 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6013982732732732 on epoch=549
03/18/2022 12:43:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 12:43:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 12:43:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/18/2022 12:43:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
03/18/2022 12:43:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
03/18/2022 12:43:44 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6531609195402299 on epoch=562
03/18/2022 12:43:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/18/2022 12:43:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 12:43:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 12:43:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
03/18/2022 12:43:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 12:43:57 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.5868055555555556 on epoch=574
03/18/2022 12:44:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 12:44:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 12:44:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 12:44:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 12:44:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/18/2022 12:44:11 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.5805003819709702 on epoch=587
03/18/2022 12:44:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 12:44:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 12:44:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 12:44:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 12:44:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
03/18/2022 12:44:24 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5766056378959605 on epoch=599
03/18/2022 12:44:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 12:44:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 12:44:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/18/2022 12:44:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 12:44:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/18/2022 12:44:38 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5875350140056023 on epoch=612
03/18/2022 12:44:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 12:44:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
03/18/2022 12:44:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/18/2022 12:44:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
03/18/2022 12:44:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 12:44:51 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6154771944620141 on epoch=624
03/18/2022 12:44:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 12:44:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 12:44:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 12:45:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 12:45:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 12:45:05 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5765309642918751 on epoch=637
03/18/2022 12:45:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/18/2022 12:45:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
03/18/2022 12:45:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/18/2022 12:45:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/18/2022 12:45:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
03/18/2022 12:45:18 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6859681372549019 on epoch=649
03/18/2022 12:45:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6696003928866832 -> 0.6859681372549019 on epoch=649, global_step=2600
03/18/2022 12:45:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 12:45:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 12:45:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 12:45:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 12:45:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
03/18/2022 12:45:32 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6248686974789917 on epoch=662
03/18/2022 12:45:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
03/18/2022 12:45:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 12:45:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/18/2022 12:45:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 12:45:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
03/18/2022 12:45:45 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6432723045626271 on epoch=674
03/18/2022 12:45:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 12:45:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 12:45:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 12:45:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
03/18/2022 12:45:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 12:45:59 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6282444067370538 on epoch=687
03/18/2022 12:46:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 12:46:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 12:46:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 12:46:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 12:46:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 12:46:13 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6122652063828534 on epoch=699
03/18/2022 12:46:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 12:46:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 12:46:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 12:46:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 12:46:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/18/2022 12:46:26 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6285168566577735 on epoch=712
03/18/2022 12:46:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 12:46:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 12:46:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 12:46:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
03/18/2022 12:46:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 12:46:40 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6594967532467533 on epoch=724
03/18/2022 12:46:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
03/18/2022 12:46:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 12:46:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 12:46:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/18/2022 12:46:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 12:46:53 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6586511527687998 on epoch=737
03/18/2022 12:46:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 12:46:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 12:47:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
03/18/2022 12:47:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
03/18/2022 12:47:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 12:47:06 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.617365373732097 on epoch=749
03/18/2022 12:47:06 - INFO - __main__ - save last model!
03/18/2022 12:47:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 12:47:06 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 12:47:06 - INFO - __main__ - Printing 3 examples
03/18/2022 12:47:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 12:47:06 - INFO - __main__ - ['others']
03/18/2022 12:47:06 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 12:47:06 - INFO - __main__ - ['others']
03/18/2022 12:47:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 12:47:06 - INFO - __main__ - ['others']
03/18/2022 12:47:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:47:06 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:47:06 - INFO - __main__ - Printing 3 examples
03/18/2022 12:47:06 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:47:06 - INFO - __main__ - ['happy']
03/18/2022 12:47:06 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:47:06 - INFO - __main__ - ['happy']
03/18/2022 12:47:06 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:47:06 - INFO - __main__ - ['happy']
03/18/2022 12:47:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 12:47:06 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:47:07 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:47:07 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:47:07 - INFO - __main__ - Printing 3 examples
03/18/2022 12:47:07 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:47:07 - INFO - __main__ - ['happy']
03/18/2022 12:47:07 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:47:07 - INFO - __main__ - ['happy']
03/18/2022 12:47:07 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:47:07 - INFO - __main__ - ['happy']
03/18/2022 12:47:07 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:47:07 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:47:07 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:47:09 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:47:14 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 12:47:22 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:47:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:47:23 - INFO - __main__ - Starting training!
03/18/2022 12:48:58 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_42_0.3_8_predictions.txt
03/18/2022 12:48:58 - INFO - __main__ - Classification-F1 on test data: 0.2798
03/18/2022 12:48:59 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.6859681372549019, test_performance=0.2797812495569385
03/18/2022 12:48:59 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
03/18/2022 12:49:00 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:49:00 - INFO - __main__ - Printing 3 examples
03/18/2022 12:49:00 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ -  [emo] your right i'm always right i am impressed
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 12:49:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:49:00 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 12:49:00 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 12:49:00 - INFO - __main__ - Printing 3 examples
03/18/2022 12:49:00 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
03/18/2022 12:49:00 - INFO - __main__ - ['happy']
03/18/2022 12:49:00 - INFO - __main__ - Tokenizing Input ...
03/18/2022 12:49:00 - INFO - __main__ - Tokenizing Output ...
03/18/2022 12:49:00 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 12:49:15 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 12:49:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 12:49:16 - INFO - __main__ - Starting training!
03/18/2022 12:49:21 - INFO - __main__ - Step 10 Global step 10 Train loss 7.60 on epoch=2
03/18/2022 12:49:23 - INFO - __main__ - Step 20 Global step 20 Train loss 5.02 on epoch=4
03/18/2022 12:49:26 - INFO - __main__ - Step 30 Global step 30 Train loss 2.97 on epoch=7
03/18/2022 12:49:28 - INFO - __main__ - Step 40 Global step 40 Train loss 1.88 on epoch=9
03/18/2022 12:49:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.46 on epoch=12
03/18/2022 12:49:32 - INFO - __main__ - Global step 50 Train loss 3.79 Classification-F1 0.17197802197802198 on epoch=12
03/18/2022 12:49:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17197802197802198 on epoch=12, global_step=50
03/18/2022 12:49:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=14
03/18/2022 12:49:36 - INFO - __main__ - Step 70 Global step 70 Train loss 1.18 on epoch=17
03/18/2022 12:49:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=19
03/18/2022 12:49:41 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=22
03/18/2022 12:49:44 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=24
03/18/2022 12:49:44 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.16059379217273953 on epoch=24
03/18/2022 12:49:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
03/18/2022 12:49:49 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=29
03/18/2022 12:49:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=32
03/18/2022 12:49:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.02 on epoch=34
03/18/2022 12:49:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.96 on epoch=37
03/18/2022 12:49:58 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.18853801169590642 on epoch=37
03/18/2022 12:49:58 - INFO - __main__ - Saving model with best Classification-F1: 0.17197802197802198 -> 0.18853801169590642 on epoch=37, global_step=150
03/18/2022 12:50:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
03/18/2022 12:50:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=42
03/18/2022 12:50:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
03/18/2022 12:50:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
03/18/2022 12:50:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
03/18/2022 12:50:11 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.17943456452348377 on epoch=49
03/18/2022 12:50:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=52
03/18/2022 12:50:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=54
03/18/2022 12:50:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
03/18/2022 12:50:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.91 on epoch=59
03/18/2022 12:50:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=62
03/18/2022 12:50:23 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.10256410256410256 on epoch=62
03/18/2022 12:50:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=64
03/18/2022 12:50:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=67
03/18/2022 12:50:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=69
03/18/2022 12:50:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
03/18/2022 12:50:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=74
03/18/2022 12:50:36 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.1500341763499658 on epoch=74
03/18/2022 12:50:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
03/18/2022 12:50:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
03/18/2022 12:50:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
03/18/2022 12:50:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.94 on epoch=84
03/18/2022 12:50:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
03/18/2022 12:50:49 - INFO - __main__ - Global step 350 Train loss 0.91 Classification-F1 0.23116615067079466 on epoch=87
03/18/2022 12:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.18853801169590642 -> 0.23116615067079466 on epoch=87, global_step=350
03/18/2022 12:50:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=89
03/18/2022 12:50:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=92
03/18/2022 12:50:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.83 on epoch=94
03/18/2022 12:50:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=97
03/18/2022 12:51:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.77 on epoch=99
03/18/2022 12:51:02 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.17752100840336132 on epoch=99
03/18/2022 12:51:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=102
03/18/2022 12:51:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.79 on epoch=104
03/18/2022 12:51:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.83 on epoch=107
03/18/2022 12:51:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=109
03/18/2022 12:51:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.78 on epoch=112
03/18/2022 12:51:15 - INFO - __main__ - Global step 450 Train loss 0.80 Classification-F1 0.1798701298701299 on epoch=112
03/18/2022 12:51:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.78 on epoch=114
03/18/2022 12:51:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
03/18/2022 12:51:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=119
03/18/2022 12:51:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=122
03/18/2022 12:51:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
03/18/2022 12:51:28 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.2992592592592592 on epoch=124
03/18/2022 12:51:28 - INFO - __main__ - Saving model with best Classification-F1: 0.23116615067079466 -> 0.2992592592592592 on epoch=124, global_step=500
03/18/2022 12:51:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.81 on epoch=127
03/18/2022 12:51:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=129
03/18/2022 12:51:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=132
03/18/2022 12:51:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=134
03/18/2022 12:51:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=137
03/18/2022 12:51:41 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.226010101010101 on epoch=137
03/18/2022 12:51:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.78 on epoch=139
03/18/2022 12:51:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.65 on epoch=142
03/18/2022 12:51:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.72 on epoch=144
03/18/2022 12:51:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=147
03/18/2022 12:51:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.73 on epoch=149
03/18/2022 12:51:53 - INFO - __main__ - Global step 600 Train loss 0.70 Classification-F1 0.29464285714285715 on epoch=149
03/18/2022 12:51:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=152
03/18/2022 12:51:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.75 on epoch=154
03/18/2022 12:52:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=157
03/18/2022 12:52:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.59 on epoch=159
03/18/2022 12:52:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=162
03/18/2022 12:52:06 - INFO - __main__ - Global step 650 Train loss 0.67 Classification-F1 0.38206987775953294 on epoch=162
03/18/2022 12:52:06 - INFO - __main__ - Saving model with best Classification-F1: 0.2992592592592592 -> 0.38206987775953294 on epoch=162, global_step=650
03/18/2022 12:52:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=164
03/18/2022 12:52:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=167
03/18/2022 12:52:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=169
03/18/2022 12:52:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.53 on epoch=172
03/18/2022 12:52:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=174
03/18/2022 12:52:19 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.4176641810570382 on epoch=174
03/18/2022 12:52:19 - INFO - __main__ - Saving model with best Classification-F1: 0.38206987775953294 -> 0.4176641810570382 on epoch=174, global_step=700
03/18/2022 12:52:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.55 on epoch=177
03/18/2022 12:52:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.57 on epoch=179
03/18/2022 12:52:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=182
03/18/2022 12:52:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=184
03/18/2022 12:52:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=187
03/18/2022 12:52:32 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.3911658456486043 on epoch=187
03/18/2022 12:52:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=189
03/18/2022 12:52:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=192
03/18/2022 12:52:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=194
03/18/2022 12:52:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=197
03/18/2022 12:52:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=199
03/18/2022 12:52:45 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.5181089743589744 on epoch=199
03/18/2022 12:52:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4176641810570382 -> 0.5181089743589744 on epoch=199, global_step=800
03/18/2022 12:52:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=202
03/18/2022 12:52:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=204
03/18/2022 12:52:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=207
03/18/2022 12:52:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=209
03/18/2022 12:52:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=212
03/18/2022 12:52:58 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.5061897840531562 on epoch=212
03/18/2022 12:53:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=214
03/18/2022 12:53:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=217
03/18/2022 12:53:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=219
03/18/2022 12:53:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=222
03/18/2022 12:53:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=224
03/18/2022 12:53:11 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.5047869093970325 on epoch=224
03/18/2022 12:53:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=227
03/18/2022 12:53:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=229
03/18/2022 12:53:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=232
03/18/2022 12:53:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
03/18/2022 12:53:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=237
03/18/2022 12:53:24 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.5542929292929293 on epoch=237
03/18/2022 12:53:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5181089743589744 -> 0.5542929292929293 on epoch=237, global_step=950
03/18/2022 12:53:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=239
03/18/2022 12:53:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=242
03/18/2022 12:53:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=244
03/18/2022 12:53:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
03/18/2022 12:53:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=249
03/18/2022 12:53:37 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.5989670868347339 on epoch=249
03/18/2022 12:53:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5542929292929293 -> 0.5989670868347339 on epoch=249, global_step=1000
03/18/2022 12:53:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=252
03/18/2022 12:53:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=254
03/18/2022 12:53:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=257
03/18/2022 12:53:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
03/18/2022 12:53:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=262
03/18/2022 12:53:50 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.6119922969187676 on epoch=262
03/18/2022 12:53:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5989670868347339 -> 0.6119922969187676 on epoch=262, global_step=1050
03/18/2022 12:53:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=264
03/18/2022 12:53:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
03/18/2022 12:53:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=269
03/18/2022 12:54:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
03/18/2022 12:54:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
03/18/2022 12:54:03 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.5847672064777328 on epoch=274
03/18/2022 12:54:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
03/18/2022 12:54:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
03/18/2022 12:54:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
03/18/2022 12:54:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
03/18/2022 12:54:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
03/18/2022 12:54:16 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.5614583333333334 on epoch=287
03/18/2022 12:54:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
03/18/2022 12:54:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
03/18/2022 12:54:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
03/18/2022 12:54:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
03/18/2022 12:54:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
03/18/2022 12:54:29 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.5707459207459208 on epoch=299
03/18/2022 12:54:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/18/2022 12:54:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
03/18/2022 12:54:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
03/18/2022 12:54:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
03/18/2022 12:54:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
03/18/2022 12:54:43 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.5342832233154814 on epoch=312
03/18/2022 12:54:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
03/18/2022 12:54:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
03/18/2022 12:54:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
03/18/2022 12:54:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
03/18/2022 12:54:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
03/18/2022 12:54:56 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.5126111003720112 on epoch=324
03/18/2022 12:54:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
03/18/2022 12:55:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
03/18/2022 12:55:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
03/18/2022 12:55:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
03/18/2022 12:55:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
03/18/2022 12:55:09 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.591231684981685 on epoch=337
03/18/2022 12:55:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
03/18/2022 12:55:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/18/2022 12:55:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
03/18/2022 12:55:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
03/18/2022 12:55:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
03/18/2022 12:55:22 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6307975113122172 on epoch=349
03/18/2022 12:55:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6119922969187676 -> 0.6307975113122172 on epoch=349, global_step=1400
03/18/2022 12:55:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
03/18/2022 12:55:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
03/18/2022 12:55:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
03/18/2022 12:55:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
03/18/2022 12:55:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/18/2022 12:55:36 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.6572916666666667 on epoch=362
03/18/2022 12:55:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6307975113122172 -> 0.6572916666666667 on epoch=362, global_step=1450
03/18/2022 12:55:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
03/18/2022 12:55:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
03/18/2022 12:55:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
03/18/2022 12:55:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
03/18/2022 12:55:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
03/18/2022 12:55:49 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.5922016579911317 on epoch=374
03/18/2022 12:55:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
03/18/2022 12:55:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
03/18/2022 12:55:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/18/2022 12:55:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
03/18/2022 12:56:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
03/18/2022 12:56:02 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.5606203606604568 on epoch=387
03/18/2022 12:56:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
03/18/2022 12:56:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
03/18/2022 12:56:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
03/18/2022 12:56:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=397
03/18/2022 12:56:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=399
03/18/2022 12:56:15 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.599792960662526 on epoch=399
03/18/2022 12:56:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/18/2022 12:56:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
03/18/2022 12:56:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
03/18/2022 12:56:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
03/18/2022 12:56:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
03/18/2022 12:56:28 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6743643970127235 on epoch=412
03/18/2022 12:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6572916666666667 -> 0.6743643970127235 on epoch=412, global_step=1650
03/18/2022 12:56:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
03/18/2022 12:56:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
03/18/2022 12:56:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
03/18/2022 12:56:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
03/18/2022 12:56:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/18/2022 12:56:42 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6300595238095238 on epoch=424
03/18/2022 12:56:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
03/18/2022 12:56:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 12:56:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
03/18/2022 12:56:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
03/18/2022 12:56:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
03/18/2022 12:56:55 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5883012820512821 on epoch=437
03/18/2022 12:56:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
03/18/2022 12:57:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
03/18/2022 12:57:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
03/18/2022 12:57:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
03/18/2022 12:57:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
03/18/2022 12:57:08 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5596124031007752 on epoch=449
03/18/2022 12:57:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 12:57:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
03/18/2022 12:57:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
03/18/2022 12:57:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/18/2022 12:57:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/18/2022 12:57:21 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6194986388384756 on epoch=462
03/18/2022 12:57:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
03/18/2022 12:57:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/18/2022 12:57:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 12:57:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
03/18/2022 12:57:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 12:57:34 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6463516009852217 on epoch=474
03/18/2022 12:57:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
03/18/2022 12:57:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/18/2022 12:57:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
03/18/2022 12:57:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
03/18/2022 12:57:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
03/18/2022 12:57:47 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6114894233555491 on epoch=487
03/18/2022 12:57:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
03/18/2022 12:57:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/18/2022 12:57:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
03/18/2022 12:57:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
03/18/2022 12:57:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 12:58:00 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6016043405156308 on epoch=499
03/18/2022 12:58:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 12:58:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
03/18/2022 12:58:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 12:58:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/18/2022 12:58:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 12:58:14 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.640530303030303 on epoch=512
03/18/2022 12:58:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
03/18/2022 12:58:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
03/18/2022 12:58:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 12:58:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
03/18/2022 12:58:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 12:58:27 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.5651340996168582 on epoch=524
03/18/2022 12:58:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/18/2022 12:58:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 12:58:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
03/18/2022 12:58:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 12:58:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
03/18/2022 12:58:40 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6716286069488975 on epoch=537
03/18/2022 12:58:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/18/2022 12:58:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
03/18/2022 12:58:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 12:58:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 12:58:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
03/18/2022 12:58:54 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.5901736055935166 on epoch=549
03/18/2022 12:58:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
03/18/2022 12:58:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
03/18/2022 12:59:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/18/2022 12:59:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 12:59:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 12:59:07 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6022008145363409 on epoch=562
03/18/2022 12:59:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/18/2022 12:59:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 12:59:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
03/18/2022 12:59:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
03/18/2022 12:59:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/18/2022 12:59:20 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5921638387155629 on epoch=574
03/18/2022 12:59:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 12:59:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
03/18/2022 12:59:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
03/18/2022 12:59:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
03/18/2022 12:59:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/18/2022 12:59:33 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.5163698370594922 on epoch=587
03/18/2022 12:59:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
03/18/2022 12:59:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 12:59:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
03/18/2022 12:59:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 12:59:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 12:59:47 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6320658866995075 on epoch=599
03/18/2022 12:59:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/18/2022 12:59:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
03/18/2022 12:59:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 12:59:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
03/18/2022 12:59:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 13:00:00 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.4879892037786774 on epoch=612
03/18/2022 13:00:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 13:00:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 13:00:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/18/2022 13:00:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 13:00:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
03/18/2022 13:00:13 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5857561415370746 on epoch=624
03/18/2022 13:00:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 13:00:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
03/18/2022 13:00:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
03/18/2022 13:00:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 13:00:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 13:00:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5513820470717022 on epoch=637
03/18/2022 13:00:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
03/18/2022 13:00:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 13:00:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 13:00:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 13:00:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/18/2022 13:00:40 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.625667909288599 on epoch=649
03/18/2022 13:00:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 13:00:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 13:00:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
03/18/2022 13:00:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 13:00:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/18/2022 13:00:53 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6592128818612084 on epoch=662
03/18/2022 13:00:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 13:00:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
03/18/2022 13:01:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 13:01:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
03/18/2022 13:01:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
03/18/2022 13:01:06 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5260509605337191 on epoch=674
03/18/2022 13:01:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/18/2022 13:01:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 13:01:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 13:01:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
03/18/2022 13:01:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 13:01:19 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6040132805219012 on epoch=687
03/18/2022 13:01:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 13:01:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
03/18/2022 13:01:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 13:01:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 13:01:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
03/18/2022 13:01:33 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5762968214581118 on epoch=699
03/18/2022 13:01:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 13:01:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 13:01:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 13:01:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 13:01:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
03/18/2022 13:01:46 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.5670250896057347 on epoch=712
03/18/2022 13:01:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/18/2022 13:01:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/18/2022 13:01:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 13:01:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 13:01:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 13:01:59 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5734179197994987 on epoch=724
03/18/2022 13:02:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
03/18/2022 13:02:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=729
03/18/2022 13:02:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 13:02:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 13:02:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/18/2022 13:02:12 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.5583479020979021 on epoch=737
03/18/2022 13:02:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 13:02:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/18/2022 13:02:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 13:02:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 13:02:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 13:02:25 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6037965865552073 on epoch=749
03/18/2022 13:02:25 - INFO - __main__ - save last model!
03/18/2022 13:02:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 13:02:25 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 13:02:26 - INFO - __main__ - Printing 3 examples
03/18/2022 13:02:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:02:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:02:26 - INFO - __main__ - Printing 3 examples
03/18/2022 13:02:26 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 13:02:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:02:26 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:02:26 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:02:26 - INFO - __main__ - Printing 3 examples
03/18/2022 13:02:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:02:26 - INFO - __main__ - ['others']
03/18/2022 13:02:26 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:02:26 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:02:26 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:02:28 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:02:33 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 13:02:44 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:02:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:02:45 - INFO - __main__ - Starting training!
03/18/2022 13:04:08 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_42_0.2_8_predictions.txt
03/18/2022 13:04:08 - INFO - __main__ - Classification-F1 on test data: 0.1095
03/18/2022 13:04:09 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.6743643970127235, test_performance=0.10949907949663548
03/18/2022 13:04:09 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
03/18/2022 13:04:10 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:04:10 - INFO - __main__ - Printing 3 examples
03/18/2022 13:04:10 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:04:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:04:10 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:04:10 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:04:10 - INFO - __main__ - Printing 3 examples
03/18/2022 13:04:10 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:04:10 - INFO - __main__ - ['others']
03/18/2022 13:04:10 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:04:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:04:10 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:04:28 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:04:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:04:29 - INFO - __main__ - Starting training!
03/18/2022 13:04:32 - INFO - __main__ - Step 10 Global step 10 Train loss 6.91 on epoch=2
03/18/2022 13:04:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.76 on epoch=4
03/18/2022 13:04:37 - INFO - __main__ - Step 30 Global step 30 Train loss 2.21 on epoch=7
03/18/2022 13:04:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.43 on epoch=9
03/18/2022 13:04:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
03/18/2022 13:04:43 - INFO - __main__ - Global step 50 Train loss 3.11 Classification-F1 0.1 on epoch=12
03/18/2022 13:04:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 13:04:45 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=14
03/18/2022 13:04:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
03/18/2022 13:04:50 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=19
03/18/2022 13:04:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=22
03/18/2022 13:04:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=24
03/18/2022 13:04:56 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.09285714285714285 on epoch=24
03/18/2022 13:04:58 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=27
03/18/2022 13:05:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=29
03/18/2022 13:05:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
03/18/2022 13:05:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=34
03/18/2022 13:05:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
03/18/2022 13:05:09 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.1 on epoch=37
03/18/2022 13:05:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=39
03/18/2022 13:05:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
03/18/2022 13:05:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=44
03/18/2022 13:05:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=47
03/18/2022 13:05:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=49
03/18/2022 13:05:22 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.18279569892473116 on epoch=49
03/18/2022 13:05:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.18279569892473116 on epoch=49, global_step=200
03/18/2022 13:05:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.95 on epoch=52
03/18/2022 13:05:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
03/18/2022 13:05:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=57
03/18/2022 13:05:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
03/18/2022 13:05:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=62
03/18/2022 13:05:34 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.1 on epoch=62
03/18/2022 13:05:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=64
03/18/2022 13:05:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=67
03/18/2022 13:05:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=69
03/18/2022 13:05:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=72
03/18/2022 13:05:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
03/18/2022 13:05:47 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.15920920607048789 on epoch=74
03/18/2022 13:05:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=77
03/18/2022 13:05:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
03/18/2022 13:05:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.84 on epoch=82
03/18/2022 13:05:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=84
03/18/2022 13:05:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
03/18/2022 13:06:00 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.4523989898989899 on epoch=87
03/18/2022 13:06:00 - INFO - __main__ - Saving model with best Classification-F1: 0.18279569892473116 -> 0.4523989898989899 on epoch=87, global_step=350
03/18/2022 13:06:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
03/18/2022 13:06:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.77 on epoch=92
03/18/2022 13:06:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=94
03/18/2022 13:06:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=97
03/18/2022 13:06:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=99
03/18/2022 13:06:13 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.33110228040101813 on epoch=99
03/18/2022 13:06:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=102
03/18/2022 13:06:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=104
03/18/2022 13:06:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=107
03/18/2022 13:06:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.75 on epoch=109
03/18/2022 13:06:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=112
03/18/2022 13:06:26 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.6255585105330437 on epoch=112
03/18/2022 13:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4523989898989899 -> 0.6255585105330437 on epoch=112, global_step=450
03/18/2022 13:06:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.67 on epoch=114
03/18/2022 13:06:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.71 on epoch=117
03/18/2022 13:06:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=119
03/18/2022 13:06:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=122
03/18/2022 13:06:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=124
03/18/2022 13:06:39 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.48658203193236205 on epoch=124
03/18/2022 13:06:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=127
03/18/2022 13:06:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=129
03/18/2022 13:06:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
03/18/2022 13:06:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
03/18/2022 13:06:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
03/18/2022 13:06:52 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.5833356175648264 on epoch=137
03/18/2022 13:06:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
03/18/2022 13:06:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
03/18/2022 13:06:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
03/18/2022 13:07:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
03/18/2022 13:07:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=149
03/18/2022 13:07:05 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.5936824877250408 on epoch=149
03/18/2022 13:07:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=152
03/18/2022 13:07:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=154
03/18/2022 13:07:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=157
03/18/2022 13:07:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=159
03/18/2022 13:07:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=162
03/18/2022 13:07:18 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.7134212769988632 on epoch=162
03/18/2022 13:07:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6255585105330437 -> 0.7134212769988632 on epoch=162, global_step=650
03/18/2022 13:07:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=164
03/18/2022 13:07:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=167
03/18/2022 13:07:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=169
03/18/2022 13:07:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
03/18/2022 13:07:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=174
03/18/2022 13:07:31 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.6902931361643656 on epoch=174
03/18/2022 13:07:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
03/18/2022 13:07:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
03/18/2022 13:07:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=182
03/18/2022 13:07:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=184
03/18/2022 13:07:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
03/18/2022 13:07:45 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.6417758738539139 on epoch=187
03/18/2022 13:07:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=189
03/18/2022 13:07:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=192
03/18/2022 13:07:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
03/18/2022 13:07:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
03/18/2022 13:07:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=199
03/18/2022 13:07:58 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6839931393470413 on epoch=199
03/18/2022 13:08:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
03/18/2022 13:08:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
03/18/2022 13:08:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
03/18/2022 13:08:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
03/18/2022 13:08:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
03/18/2022 13:08:11 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.7188879456706282 on epoch=212
03/18/2022 13:08:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7134212769988632 -> 0.7188879456706282 on epoch=212, global_step=850
03/18/2022 13:08:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
03/18/2022 13:08:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
03/18/2022 13:08:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
03/18/2022 13:08:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
03/18/2022 13:08:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=224
03/18/2022 13:08:24 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.6622716352317871 on epoch=224
03/18/2022 13:08:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
03/18/2022 13:08:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
03/18/2022 13:08:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
03/18/2022 13:08:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
03/18/2022 13:08:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
03/18/2022 13:08:37 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.6663890491476698 on epoch=237
03/18/2022 13:08:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
03/18/2022 13:08:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
03/18/2022 13:08:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
03/18/2022 13:08:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
03/18/2022 13:08:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
03/18/2022 13:08:50 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7320560232101995 on epoch=249
03/18/2022 13:08:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7188879456706282 -> 0.7320560232101995 on epoch=249, global_step=1000
03/18/2022 13:08:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
03/18/2022 13:08:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
03/18/2022 13:08:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
03/18/2022 13:09:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
03/18/2022 13:09:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
03/18/2022 13:09:04 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.7057371794871795 on epoch=262
03/18/2022 13:09:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
03/18/2022 13:09:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
03/18/2022 13:09:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
03/18/2022 13:09:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
03/18/2022 13:09:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
03/18/2022 13:09:17 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.6627177177177177 on epoch=274
03/18/2022 13:09:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
03/18/2022 13:09:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
03/18/2022 13:09:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
03/18/2022 13:09:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
03/18/2022 13:09:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
03/18/2022 13:09:30 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6957416785002992 on epoch=287
03/18/2022 13:09:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
03/18/2022 13:09:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
03/18/2022 13:09:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
03/18/2022 13:09:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
03/18/2022 13:09:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
03/18/2022 13:09:43 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.7136925385564225 on epoch=299
03/18/2022 13:09:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
03/18/2022 13:09:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
03/18/2022 13:09:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
03/18/2022 13:09:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
03/18/2022 13:09:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
03/18/2022 13:09:56 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.7107885304659499 on epoch=312
03/18/2022 13:09:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
03/18/2022 13:10:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
03/18/2022 13:10:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
03/18/2022 13:10:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
03/18/2022 13:10:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
03/18/2022 13:10:10 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7568825510001981 on epoch=324
03/18/2022 13:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7320560232101995 -> 0.7568825510001981 on epoch=324, global_step=1300
03/18/2022 13:10:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
03/18/2022 13:10:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
03/18/2022 13:10:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
03/18/2022 13:10:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
03/18/2022 13:10:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
03/18/2022 13:10:23 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6766081871345029 on epoch=337
03/18/2022 13:10:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
03/18/2022 13:10:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
03/18/2022 13:10:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
03/18/2022 13:10:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
03/18/2022 13:10:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
03/18/2022 13:10:37 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7049783549783549 on epoch=349
03/18/2022 13:10:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
03/18/2022 13:10:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
03/18/2022 13:10:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
03/18/2022 13:10:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/18/2022 13:10:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
03/18/2022 13:10:51 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7048965262379896 on epoch=362
03/18/2022 13:10:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
03/18/2022 13:10:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
03/18/2022 13:10:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
03/18/2022 13:11:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
03/18/2022 13:11:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
03/18/2022 13:11:04 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6816137566137566 on epoch=374
03/18/2022 13:11:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
03/18/2022 13:11:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
03/18/2022 13:11:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
03/18/2022 13:11:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
03/18/2022 13:11:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
03/18/2022 13:11:18 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.7108089826839827 on epoch=387
03/18/2022 13:11:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
03/18/2022 13:11:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
03/18/2022 13:11:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
03/18/2022 13:11:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
03/18/2022 13:11:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
03/18/2022 13:11:31 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6540229885057471 on epoch=399
03/18/2022 13:11:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/18/2022 13:11:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
03/18/2022 13:11:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 13:11:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
03/18/2022 13:11:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
03/18/2022 13:11:44 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7028257763741635 on epoch=412
03/18/2022 13:11:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
03/18/2022 13:11:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
03/18/2022 13:11:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
03/18/2022 13:11:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
03/18/2022 13:11:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
03/18/2022 13:11:57 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7191151142764046 on epoch=424
03/18/2022 13:12:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
03/18/2022 13:12:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
03/18/2022 13:12:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
03/18/2022 13:12:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
03/18/2022 13:12:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 13:12:10 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.738954808632228 on epoch=437
03/18/2022 13:12:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
03/18/2022 13:12:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
03/18/2022 13:12:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
03/18/2022 13:12:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
03/18/2022 13:12:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 13:12:24 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6681879648411088 on epoch=449
03/18/2022 13:12:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
03/18/2022 13:12:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 13:12:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
03/18/2022 13:12:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 13:12:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/18/2022 13:12:37 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.6520955539718217 on epoch=462
03/18/2022 13:12:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
03/18/2022 13:12:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
03/18/2022 13:12:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
03/18/2022 13:12:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
03/18/2022 13:12:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
03/18/2022 13:12:50 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6870856988765777 on epoch=474
03/18/2022 13:12:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
03/18/2022 13:12:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 13:12:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
03/18/2022 13:12:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
03/18/2022 13:13:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
03/18/2022 13:13:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6804935767410413 on epoch=487
03/18/2022 13:13:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
03/18/2022 13:13:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/18/2022 13:13:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/18/2022 13:13:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 13:13:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 13:13:16 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6958353956992795 on epoch=499
03/18/2022 13:13:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 13:13:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 13:13:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 13:13:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
03/18/2022 13:13:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
03/18/2022 13:13:29 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6446931809160912 on epoch=512
03/18/2022 13:13:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
03/18/2022 13:13:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
03/18/2022 13:13:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 13:13:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 13:13:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 13:13:42 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.6747845325431532 on epoch=524
03/18/2022 13:13:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
03/18/2022 13:13:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
03/18/2022 13:13:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/18/2022 13:13:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
03/18/2022 13:13:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 13:13:55 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6815073186777041 on epoch=537
03/18/2022 13:13:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
03/18/2022 13:14:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
03/18/2022 13:14:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 13:14:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
03/18/2022 13:14:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
03/18/2022 13:14:08 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6312741312741312 on epoch=549
03/18/2022 13:14:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 13:14:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/18/2022 13:14:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
03/18/2022 13:14:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
03/18/2022 13:14:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
03/18/2022 13:14:22 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6167988640814728 on epoch=562
03/18/2022 13:14:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 13:14:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
03/18/2022 13:14:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 13:14:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 13:14:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
03/18/2022 13:14:35 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6968722437472438 on epoch=574
03/18/2022 13:14:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 13:14:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
03/18/2022 13:14:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
03/18/2022 13:14:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 13:14:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
03/18/2022 13:14:48 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6973556483053513 on epoch=587
03/18/2022 13:14:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 13:14:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 13:14:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 13:14:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 13:15:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 13:15:01 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6968722437472438 on epoch=599
03/18/2022 13:15:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 13:15:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
03/18/2022 13:15:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 13:15:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/18/2022 13:15:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 13:15:15 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.719815668202765 on epoch=612
03/18/2022 13:15:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 13:15:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 13:15:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
03/18/2022 13:15:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 13:15:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 13:15:28 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6544749963867611 on epoch=624
03/18/2022 13:15:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 13:15:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
03/18/2022 13:15:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
03/18/2022 13:15:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 13:15:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 13:15:41 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6764636327503974 on epoch=637
03/18/2022 13:15:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 13:15:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 13:15:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 13:15:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 13:15:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
03/18/2022 13:15:54 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6661764705882353 on epoch=649
03/18/2022 13:15:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 13:15:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 13:16:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 13:16:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
03/18/2022 13:16:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 13:16:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6795093795093796 on epoch=662
03/18/2022 13:16:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 13:16:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 13:16:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 13:16:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 13:16:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 13:16:20 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6899336929129019 on epoch=674
03/18/2022 13:16:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 13:16:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
03/18/2022 13:16:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 13:16:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 13:16:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 13:16:33 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6746594982078853 on epoch=687
03/18/2022 13:16:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 13:16:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 13:16:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 13:16:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 13:16:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
03/18/2022 13:16:47 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7246830286168522 on epoch=699
03/18/2022 13:16:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 13:16:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 13:16:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
03/18/2022 13:16:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/18/2022 13:16:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 13:17:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6901108695226343 on epoch=712
03/18/2022 13:17:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
03/18/2022 13:17:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 13:17:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 13:17:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 13:17:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
03/18/2022 13:17:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7210171568627451 on epoch=724
03/18/2022 13:17:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 13:17:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 13:17:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 13:17:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 13:17:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 13:17:26 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7036931818181819 on epoch=737
03/18/2022 13:17:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
03/18/2022 13:17:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 13:17:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
03/18/2022 13:17:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
03/18/2022 13:17:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 13:17:39 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7134846805634159 on epoch=749
03/18/2022 13:17:39 - INFO - __main__ - save last model!
03/18/2022 13:17:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 13:17:39 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 13:17:39 - INFO - __main__ - Printing 3 examples
03/18/2022 13:17:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 13:17:39 - INFO - __main__ - ['others']
03/18/2022 13:17:39 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 13:17:39 - INFO - __main__ - ['others']
03/18/2022 13:17:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 13:17:39 - INFO - __main__ - ['others']
03/18/2022 13:17:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:17:40 - INFO - __main__ - Printing 3 examples
03/18/2022 13:17:40 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 13:17:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:17:40 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:17:40 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:17:40 - INFO - __main__ - Printing 3 examples
03/18/2022 13:17:40 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:17:40 - INFO - __main__ - ['others']
03/18/2022 13:17:40 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:17:40 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:17:40 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:17:42 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:17:47 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 13:17:55 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:17:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:17:56 - INFO - __main__ - Starting training!
03/18/2022 13:19:26 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_87_0.5_8_predictions.txt
03/18/2022 13:19:26 - INFO - __main__ - Classification-F1 on test data: 0.1893
03/18/2022 13:19:26 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.7568825510001981, test_performance=0.18929925197597067
03/18/2022 13:19:26 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
03/18/2022 13:19:27 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:19:27 - INFO - __main__ - Printing 3 examples
03/18/2022 13:19:27 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:19:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:19:27 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:19:27 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:19:27 - INFO - __main__ - Printing 3 examples
03/18/2022 13:19:27 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:19:27 - INFO - __main__ - ['others']
03/18/2022 13:19:27 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:19:27 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:19:27 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:19:42 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:19:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:19:43 - INFO - __main__ - Starting training!
03/18/2022 13:19:46 - INFO - __main__ - Step 10 Global step 10 Train loss 7.62 on epoch=2
03/18/2022 13:19:49 - INFO - __main__ - Step 20 Global step 20 Train loss 7.62 on epoch=4
03/18/2022 13:19:51 - INFO - __main__ - Step 30 Global step 30 Train loss 7.14 on epoch=7
03/18/2022 13:19:54 - INFO - __main__ - Step 40 Global step 40 Train loss 6.85 on epoch=9
03/18/2022 13:19:56 - INFO - __main__ - Step 50 Global step 50 Train loss 6.42 on epoch=12
03/18/2022 13:20:10 - INFO - __main__ - Global step 50 Train loss 7.13 Classification-F1 0.0 on epoch=12
03/18/2022 13:20:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
03/18/2022 13:20:13 - INFO - __main__ - Step 60 Global step 60 Train loss 5.69 on epoch=14
03/18/2022 13:20:15 - INFO - __main__ - Step 70 Global step 70 Train loss 5.16 on epoch=17
03/18/2022 13:20:18 - INFO - __main__ - Step 80 Global step 80 Train loss 4.40 on epoch=19
03/18/2022 13:20:20 - INFO - __main__ - Step 90 Global step 90 Train loss 4.12 on epoch=22
03/18/2022 13:20:23 - INFO - __main__ - Step 100 Global step 100 Train loss 3.41 on epoch=24
03/18/2022 13:20:25 - INFO - __main__ - Global step 100 Train loss 4.56 Classification-F1 0.12681436210847974 on epoch=24
03/18/2022 13:20:25 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.12681436210847974 on epoch=24, global_step=100
03/18/2022 13:20:27 - INFO - __main__ - Step 110 Global step 110 Train loss 3.09 on epoch=27
03/18/2022 13:20:29 - INFO - __main__ - Step 120 Global step 120 Train loss 2.62 on epoch=29
03/18/2022 13:20:32 - INFO - __main__ - Step 130 Global step 130 Train loss 2.51 on epoch=32
03/18/2022 13:20:34 - INFO - __main__ - Step 140 Global step 140 Train loss 2.00 on epoch=34
03/18/2022 13:20:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.75 on epoch=37
03/18/2022 13:20:37 - INFO - __main__ - Global step 150 Train loss 2.39 Classification-F1 0.2078123115274818 on epoch=37
03/18/2022 13:20:37 - INFO - __main__ - Saving model with best Classification-F1: 0.12681436210847974 -> 0.2078123115274818 on epoch=37, global_step=150
03/18/2022 13:20:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.56 on epoch=39
03/18/2022 13:20:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.41 on epoch=42
03/18/2022 13:20:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.38 on epoch=44
03/18/2022 13:20:47 - INFO - __main__ - Step 190 Global step 190 Train loss 1.28 on epoch=47
03/18/2022 13:20:50 - INFO - __main__ - Step 200 Global step 200 Train loss 1.29 on epoch=49
03/18/2022 13:20:50 - INFO - __main__ - Global step 200 Train loss 1.38 Classification-F1 0.10126582278481013 on epoch=49
03/18/2022 13:20:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.08 on epoch=52
03/18/2022 13:20:55 - INFO - __main__ - Step 220 Global step 220 Train loss 1.20 on epoch=54
03/18/2022 13:20:58 - INFO - __main__ - Step 230 Global step 230 Train loss 1.15 on epoch=57
03/18/2022 13:21:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.26 on epoch=59
03/18/2022 13:21:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.09 on epoch=62
03/18/2022 13:21:03 - INFO - __main__ - Global step 250 Train loss 1.16 Classification-F1 0.27232814466857014 on epoch=62
03/18/2022 13:21:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2078123115274818 -> 0.27232814466857014 on epoch=62, global_step=250
03/18/2022 13:21:06 - INFO - __main__ - Step 260 Global step 260 Train loss 1.04 on epoch=64
03/18/2022 13:21:08 - INFO - __main__ - Step 270 Global step 270 Train loss 1.13 on epoch=67
03/18/2022 13:21:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=69
03/18/2022 13:21:13 - INFO - __main__ - Step 290 Global step 290 Train loss 1.00 on epoch=72
03/18/2022 13:21:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
03/18/2022 13:21:16 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.13197586726998492 on epoch=74
03/18/2022 13:21:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.94 on epoch=77
03/18/2022 13:21:21 - INFO - __main__ - Step 320 Global step 320 Train loss 1.02 on epoch=79
03/18/2022 13:21:23 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=82
03/18/2022 13:21:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.95 on epoch=84
03/18/2022 13:21:28 - INFO - __main__ - Step 350 Global step 350 Train loss 1.11 on epoch=87
03/18/2022 13:21:29 - INFO - __main__ - Global step 350 Train loss 1.01 Classification-F1 0.2994775339602926 on epoch=87
03/18/2022 13:21:29 - INFO - __main__ - Saving model with best Classification-F1: 0.27232814466857014 -> 0.2994775339602926 on epoch=87, global_step=350
03/18/2022 13:21:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.02 on epoch=89
03/18/2022 13:21:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.97 on epoch=92
03/18/2022 13:21:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.97 on epoch=94
03/18/2022 13:21:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.99 on epoch=97
03/18/2022 13:21:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=99
03/18/2022 13:21:42 - INFO - __main__ - Global step 400 Train loss 0.98 Classification-F1 0.13197586726998492 on epoch=99
03/18/2022 13:21:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
03/18/2022 13:21:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.93 on epoch=104
03/18/2022 13:21:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.99 on epoch=107
03/18/2022 13:21:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
03/18/2022 13:21:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
03/18/2022 13:21:55 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.3002473716759432 on epoch=112
03/18/2022 13:21:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2994775339602926 -> 0.3002473716759432 on epoch=112, global_step=450
03/18/2022 13:21:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.99 on epoch=114
03/18/2022 13:22:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.96 on epoch=117
03/18/2022 13:22:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
03/18/2022 13:22:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.91 on epoch=122
03/18/2022 13:22:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=124
03/18/2022 13:22:08 - INFO - __main__ - Global step 500 Train loss 0.91 Classification-F1 0.2801172707889126 on epoch=124
03/18/2022 13:22:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.99 on epoch=127
03/18/2022 13:22:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.94 on epoch=129
03/18/2022 13:22:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.82 on epoch=132
03/18/2022 13:22:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.90 on epoch=134
03/18/2022 13:22:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.86 on epoch=137
03/18/2022 13:22:21 - INFO - __main__ - Global step 550 Train loss 0.90 Classification-F1 0.1 on epoch=137
03/18/2022 13:22:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.91 on epoch=139
03/18/2022 13:22:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.94 on epoch=142
03/18/2022 13:22:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=144
03/18/2022 13:22:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.89 on epoch=147
03/18/2022 13:22:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.94 on epoch=149
03/18/2022 13:22:34 - INFO - __main__ - Global step 600 Train loss 0.92 Classification-F1 0.1746498599439776 on epoch=149
03/18/2022 13:22:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.94 on epoch=152
03/18/2022 13:22:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.84 on epoch=154
03/18/2022 13:22:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.89 on epoch=157
03/18/2022 13:22:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.79 on epoch=159
03/18/2022 13:22:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=162
03/18/2022 13:22:47 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.23861607142857144 on epoch=162
03/18/2022 13:22:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.82 on epoch=164
03/18/2022 13:22:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.89 on epoch=167
03/18/2022 13:22:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.76 on epoch=169
03/18/2022 13:22:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.81 on epoch=172
03/18/2022 13:22:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
03/18/2022 13:23:00 - INFO - __main__ - Global step 700 Train loss 0.83 Classification-F1 0.3854166666666667 on epoch=174
03/18/2022 13:23:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3002473716759432 -> 0.3854166666666667 on epoch=174, global_step=700
03/18/2022 13:23:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.91 on epoch=177
03/18/2022 13:23:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.84 on epoch=179
03/18/2022 13:23:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.84 on epoch=182
03/18/2022 13:23:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.80 on epoch=184
03/18/2022 13:23:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.94 on epoch=187
03/18/2022 13:23:13 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.2562167261944875 on epoch=187
03/18/2022 13:23:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.95 on epoch=189
03/18/2022 13:23:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
03/18/2022 13:23:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.86 on epoch=194
03/18/2022 13:23:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.82 on epoch=197
03/18/2022 13:23:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.90 on epoch=199
03/18/2022 13:23:26 - INFO - __main__ - Global step 800 Train loss 0.88 Classification-F1 0.294953640370877 on epoch=199
03/18/2022 13:23:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
03/18/2022 13:23:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.80 on epoch=204
03/18/2022 13:23:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.79 on epoch=207
03/18/2022 13:23:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
03/18/2022 13:23:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=212
03/18/2022 13:23:39 - INFO - __main__ - Global step 850 Train loss 0.83 Classification-F1 0.13034188034188032 on epoch=212
03/18/2022 13:23:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
03/18/2022 13:23:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=217
03/18/2022 13:23:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.78 on epoch=219
03/18/2022 13:23:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=222
03/18/2022 13:23:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=224
03/18/2022 13:23:52 - INFO - __main__ - Global step 900 Train loss 0.82 Classification-F1 0.18026315789473685 on epoch=224
03/18/2022 13:23:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.78 on epoch=227
03/18/2022 13:23:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
03/18/2022 13:23:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.80 on epoch=232
03/18/2022 13:24:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.82 on epoch=234
03/18/2022 13:24:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.84 on epoch=237
03/18/2022 13:24:05 - INFO - __main__ - Global step 950 Train loss 0.80 Classification-F1 0.3992590742590743 on epoch=237
03/18/2022 13:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3854166666666667 -> 0.3992590742590743 on epoch=237, global_step=950
03/18/2022 13:24:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.77 on epoch=239
03/18/2022 13:24:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.75 on epoch=242
03/18/2022 13:24:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.78 on epoch=244
03/18/2022 13:24:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
03/18/2022 13:24:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.83 on epoch=249
03/18/2022 13:24:18 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.4626993576161168 on epoch=249
03/18/2022 13:24:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3992590742590743 -> 0.4626993576161168 on epoch=249, global_step=1000
03/18/2022 13:24:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.74 on epoch=252
03/18/2022 13:24:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.80 on epoch=254
03/18/2022 13:24:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.73 on epoch=257
03/18/2022 13:24:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.71 on epoch=259
03/18/2022 13:24:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.78 on epoch=262
03/18/2022 13:24:31 - INFO - __main__ - Global step 1050 Train loss 0.75 Classification-F1 0.5888888888888889 on epoch=262
03/18/2022 13:24:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4626993576161168 -> 0.5888888888888889 on epoch=262, global_step=1050
03/18/2022 13:24:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=264
03/18/2022 13:24:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.84 on epoch=267
03/18/2022 13:24:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=269
03/18/2022 13:24:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=272
03/18/2022 13:24:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=274
03/18/2022 13:24:44 - INFO - __main__ - Global step 1100 Train loss 0.71 Classification-F1 0.4450416948388551 on epoch=274
03/18/2022 13:24:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.62 on epoch=277
03/18/2022 13:24:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.62 on epoch=279
03/18/2022 13:24:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.66 on epoch=282
03/18/2022 13:24:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.61 on epoch=284
03/18/2022 13:24:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=287
03/18/2022 13:24:57 - INFO - __main__ - Global step 1150 Train loss 0.61 Classification-F1 0.540719696969697 on epoch=287
03/18/2022 13:25:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=289
03/18/2022 13:25:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.55 on epoch=292
03/18/2022 13:25:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=294
03/18/2022 13:25:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=297
03/18/2022 13:25:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=299
03/18/2022 13:25:10 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.4409722222222222 on epoch=299
03/18/2022 13:25:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=302
03/18/2022 13:25:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=304
03/18/2022 13:25:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=307
03/18/2022 13:25:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=309
03/18/2022 13:25:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=312
03/18/2022 13:25:23 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.6533492822966507 on epoch=312
03/18/2022 13:25:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5888888888888889 -> 0.6533492822966507 on epoch=312, global_step=1250
03/18/2022 13:25:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=314
03/18/2022 13:25:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=317
03/18/2022 13:25:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=319
03/18/2022 13:25:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=322
03/18/2022 13:25:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=324
03/18/2022 13:25:37 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.5698535222529031 on epoch=324
03/18/2022 13:25:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=327
03/18/2022 13:25:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=329
03/18/2022 13:25:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=332
03/18/2022 13:25:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
03/18/2022 13:25:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=337
03/18/2022 13:25:50 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.682140565508677 on epoch=337
03/18/2022 13:25:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6533492822966507 -> 0.682140565508677 on epoch=337, global_step=1350
03/18/2022 13:25:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=339
03/18/2022 13:25:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=342
03/18/2022 13:25:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=344
03/18/2022 13:25:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=347
03/18/2022 13:26:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=349
03/18/2022 13:26:03 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.7102909345556404 on epoch=349
03/18/2022 13:26:03 - INFO - __main__ - Saving model with best Classification-F1: 0.682140565508677 -> 0.7102909345556404 on epoch=349, global_step=1400
03/18/2022 13:26:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=352
03/18/2022 13:26:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=354
03/18/2022 13:26:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=357
03/18/2022 13:26:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
03/18/2022 13:26:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=362
03/18/2022 13:26:16 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.7333596583596583 on epoch=362
03/18/2022 13:26:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7102909345556404 -> 0.7333596583596583 on epoch=362, global_step=1450
03/18/2022 13:26:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
03/18/2022 13:26:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
03/18/2022 13:26:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
03/18/2022 13:26:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
03/18/2022 13:26:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
03/18/2022 13:26:29 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.672846889952153 on epoch=374
03/18/2022 13:26:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
03/18/2022 13:26:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
03/18/2022 13:26:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
03/18/2022 13:26:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
03/18/2022 13:26:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
03/18/2022 13:26:42 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.7756875477463714 on epoch=387
03/18/2022 13:26:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7333596583596583 -> 0.7756875477463714 on epoch=387, global_step=1550
03/18/2022 13:26:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
03/18/2022 13:26:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
03/18/2022 13:26:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
03/18/2022 13:26:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=397
03/18/2022 13:26:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
03/18/2022 13:26:55 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7631944444444444 on epoch=399
03/18/2022 13:26:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
03/18/2022 13:27:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
03/18/2022 13:27:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
03/18/2022 13:27:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=409
03/18/2022 13:27:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
03/18/2022 13:27:09 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7358333333333333 on epoch=412
03/18/2022 13:27:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
03/18/2022 13:27:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
03/18/2022 13:27:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
03/18/2022 13:27:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
03/18/2022 13:27:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
03/18/2022 13:27:22 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7574645748987854 on epoch=424
03/18/2022 13:27:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
03/18/2022 13:27:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
03/18/2022 13:27:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
03/18/2022 13:27:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
03/18/2022 13:27:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 13:27:35 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7589087301587302 on epoch=437
03/18/2022 13:27:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
03/18/2022 13:27:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
03/18/2022 13:27:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
03/18/2022 13:27:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
03/18/2022 13:27:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=449
03/18/2022 13:27:48 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7591831588291069 on epoch=449
03/18/2022 13:27:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
03/18/2022 13:27:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=454
03/18/2022 13:27:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
03/18/2022 13:27:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
03/18/2022 13:28:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=462
03/18/2022 13:28:01 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7479155885227992 on epoch=462
03/18/2022 13:28:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
03/18/2022 13:28:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
03/18/2022 13:28:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
03/18/2022 13:28:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
03/18/2022 13:28:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
03/18/2022 13:28:15 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7330633484867356 on epoch=474
03/18/2022 13:28:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=477
03/18/2022 13:28:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
03/18/2022 13:28:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
03/18/2022 13:28:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 13:28:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
03/18/2022 13:28:28 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.759976212145289 on epoch=487
03/18/2022 13:28:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/18/2022 13:28:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
03/18/2022 13:28:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
03/18/2022 13:28:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/18/2022 13:28:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
03/18/2022 13:28:41 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7435873373373373 on epoch=499
03/18/2022 13:28:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 13:28:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
03/18/2022 13:28:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
03/18/2022 13:28:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/18/2022 13:28:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
03/18/2022 13:28:55 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7451839826839827 on epoch=512
03/18/2022 13:28:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
03/18/2022 13:29:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
03/18/2022 13:29:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
03/18/2022 13:29:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
03/18/2022 13:29:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 13:29:08 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7308312056295928 on epoch=524
03/18/2022 13:29:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
03/18/2022 13:29:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
03/18/2022 13:29:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
03/18/2022 13:29:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
03/18/2022 13:29:20 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
03/18/2022 13:29:21 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.7305719805631989 on epoch=537
03/18/2022 13:29:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
03/18/2022 13:29:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
03/18/2022 13:29:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
03/18/2022 13:29:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
03/18/2022 13:29:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
03/18/2022 13:29:34 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7608283293767165 on epoch=549
03/18/2022 13:29:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
03/18/2022 13:29:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
03/18/2022 13:29:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 13:29:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
03/18/2022 13:29:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 13:29:48 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.687866568914956 on epoch=562
03/18/2022 13:29:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
03/18/2022 13:29:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 13:29:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 13:29:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 13:30:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
03/18/2022 13:30:01 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6821827816492451 on epoch=574
03/18/2022 13:30:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 13:30:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
03/18/2022 13:30:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
03/18/2022 13:30:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 13:30:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
03/18/2022 13:30:14 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7430069365553237 on epoch=587
03/18/2022 13:30:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
03/18/2022 13:30:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
03/18/2022 13:30:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=594
03/18/2022 13:30:24 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
03/18/2022 13:30:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 13:30:28 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7285685035685036 on epoch=599
03/18/2022 13:30:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/18/2022 13:30:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/18/2022 13:30:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
03/18/2022 13:30:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
03/18/2022 13:30:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 13:30:41 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7296960072595282 on epoch=612
03/18/2022 13:30:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
03/18/2022 13:30:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 13:30:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
03/18/2022 13:30:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 13:30:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 13:30:54 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.759469696969697 on epoch=624
03/18/2022 13:30:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 13:30:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 13:31:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
03/18/2022 13:31:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
03/18/2022 13:31:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 13:31:08 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7460497835497836 on epoch=637
03/18/2022 13:31:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 13:31:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=642
03/18/2022 13:31:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 13:31:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
03/18/2022 13:31:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
03/18/2022 13:31:21 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7286956647357091 on epoch=649
03/18/2022 13:31:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
03/18/2022 13:31:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
03/18/2022 13:31:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
03/18/2022 13:31:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
03/18/2022 13:31:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 13:31:34 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7571719654262349 on epoch=662
03/18/2022 13:31:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/18/2022 13:31:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 13:31:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 13:31:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 13:31:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
03/18/2022 13:31:48 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7207017543859648 on epoch=674
03/18/2022 13:31:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 13:31:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 13:31:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/18/2022 13:31:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 13:32:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 13:32:01 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7720356892682869 on epoch=687
03/18/2022 13:32:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
03/18/2022 13:32:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=692
03/18/2022 13:32:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 13:32:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 13:32:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 13:32:15 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7702677224736049 on epoch=699
03/18/2022 13:32:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 13:32:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
03/18/2022 13:32:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 13:32:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
03/18/2022 13:32:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
03/18/2022 13:32:28 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7436035248535248 on epoch=712
03/18/2022 13:32:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 13:32:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
03/18/2022 13:32:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
03/18/2022 13:32:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
03/18/2022 13:32:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 13:32:41 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7562363834422657 on epoch=724
03/18/2022 13:32:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
03/18/2022 13:32:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/18/2022 13:32:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 13:32:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
03/18/2022 13:32:54 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
03/18/2022 13:32:55 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7446570615607119 on epoch=737
03/18/2022 13:32:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
03/18/2022 13:33:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 13:33:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 13:33:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 13:33:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 13:33:08 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7287084498291395 on epoch=749
03/18/2022 13:33:08 - INFO - __main__ - save last model!
03/18/2022 13:33:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 13:33:08 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 13:33:08 - INFO - __main__ - Printing 3 examples
03/18/2022 13:33:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:33:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:33:08 - INFO - __main__ - Printing 3 examples
03/18/2022 13:33:08 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 13:33:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:33:08 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:33:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:33:08 - INFO - __main__ - Printing 3 examples
03/18/2022 13:33:08 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:33:08 - INFO - __main__ - ['others']
03/18/2022 13:33:08 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:33:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:33:08 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:33:10 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:33:16 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 13:33:24 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:33:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:33:25 - INFO - __main__ - Starting training!
03/18/2022 13:34:51 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_87_0.4_8_predictions.txt
03/18/2022 13:34:51 - INFO - __main__ - Classification-F1 on test data: 0.3457
03/18/2022 13:34:52 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7756875477463714, test_performance=0.34570650836234407
03/18/2022 13:34:52 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
03/18/2022 13:34:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:34:53 - INFO - __main__ - Printing 3 examples
03/18/2022 13:34:53 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:34:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:34:53 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:34:53 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:34:53 - INFO - __main__ - Printing 3 examples
03/18/2022 13:34:53 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:34:53 - INFO - __main__ - ['others']
03/18/2022 13:34:53 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:34:53 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:34:53 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:35:11 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:35:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:35:12 - INFO - __main__ - Starting training!
03/18/2022 13:35:15 - INFO - __main__ - Step 10 Global step 10 Train loss 7.08 on epoch=2
03/18/2022 13:35:17 - INFO - __main__ - Step 20 Global step 20 Train loss 3.86 on epoch=4
03/18/2022 13:35:20 - INFO - __main__ - Step 30 Global step 30 Train loss 1.94 on epoch=7
03/18/2022 13:35:22 - INFO - __main__ - Step 40 Global step 40 Train loss 1.37 on epoch=9
03/18/2022 13:35:25 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=12
03/18/2022 13:35:26 - INFO - __main__ - Global step 50 Train loss 3.08 Classification-F1 0.1 on epoch=12
03/18/2022 13:35:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
03/18/2022 13:35:28 - INFO - __main__ - Step 60 Global step 60 Train loss 1.05 on epoch=14
03/18/2022 13:35:30 - INFO - __main__ - Step 70 Global step 70 Train loss 1.07 on epoch=17
03/18/2022 13:35:33 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=19
03/18/2022 13:35:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=22
03/18/2022 13:35:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=24
03/18/2022 13:35:39 - INFO - __main__ - Global step 100 Train loss 1.04 Classification-F1 0.12809597523219815 on epoch=24
03/18/2022 13:35:39 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.12809597523219815 on epoch=24, global_step=100
03/18/2022 13:35:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=27
03/18/2022 13:35:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.21 on epoch=29
03/18/2022 13:35:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=32
03/18/2022 13:35:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=34
03/18/2022 13:35:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
03/18/2022 13:35:52 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.13067758749069247 on epoch=37
03/18/2022 13:35:52 - INFO - __main__ - Saving model with best Classification-F1: 0.12809597523219815 -> 0.13067758749069247 on epoch=37, global_step=150
03/18/2022 13:35:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=39
03/18/2022 13:35:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
03/18/2022 13:35:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
03/18/2022 13:36:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
03/18/2022 13:36:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
03/18/2022 13:36:05 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.23206615979725223 on epoch=49
03/18/2022 13:36:05 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.23206615979725223 on epoch=49, global_step=200
03/18/2022 13:36:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=52
03/18/2022 13:36:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
03/18/2022 13:36:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
03/18/2022 13:36:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
03/18/2022 13:36:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=62
03/18/2022 13:36:18 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.16953316953316955 on epoch=62
03/18/2022 13:36:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=64
03/18/2022 13:36:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=67
03/18/2022 13:36:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.89 on epoch=69
03/18/2022 13:36:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
03/18/2022 13:36:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=74
03/18/2022 13:36:30 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.12809597523219815 on epoch=74
03/18/2022 13:36:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=77
03/18/2022 13:36:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=79
03/18/2022 13:36:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
03/18/2022 13:36:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
03/18/2022 13:36:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
03/18/2022 13:36:43 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.13034188034188032 on epoch=87
03/18/2022 13:36:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
03/18/2022 13:36:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
03/18/2022 13:36:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=94
03/18/2022 13:36:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.87 on epoch=97
03/18/2022 13:36:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.94 on epoch=99
03/18/2022 13:36:56 - INFO - __main__ - Global step 400 Train loss 0.88 Classification-F1 0.15945165945165946 on epoch=99
03/18/2022 13:36:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.83 on epoch=102
03/18/2022 13:37:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.86 on epoch=104
03/18/2022 13:37:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.84 on epoch=107
03/18/2022 13:37:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.82 on epoch=109
03/18/2022 13:37:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=112
03/18/2022 13:37:09 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.45 on epoch=112
03/18/2022 13:37:09 - INFO - __main__ - Saving model with best Classification-F1: 0.23206615979725223 -> 0.45 on epoch=112, global_step=450
03/18/2022 13:37:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
03/18/2022 13:37:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=117
03/18/2022 13:37:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.81 on epoch=119
03/18/2022 13:37:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=122
03/18/2022 13:37:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
03/18/2022 13:37:22 - INFO - __main__ - Global step 500 Train loss 0.83 Classification-F1 0.18284347231715653 on epoch=124
03/18/2022 13:37:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.86 on epoch=127
03/18/2022 13:37:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.80 on epoch=129
03/18/2022 13:37:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.81 on epoch=132
03/18/2022 13:37:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.76 on epoch=134
03/18/2022 13:37:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=137
03/18/2022 13:37:35 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.3189632442124136 on epoch=137
03/18/2022 13:37:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=139
03/18/2022 13:37:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.81 on epoch=142
03/18/2022 13:37:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.77 on epoch=144
03/18/2022 13:37:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.78 on epoch=147
03/18/2022 13:37:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.71 on epoch=149
03/18/2022 13:37:48 - INFO - __main__ - Global step 600 Train loss 0.78 Classification-F1 0.36089273235499647 on epoch=149
03/18/2022 13:37:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.74 on epoch=152
03/18/2022 13:37:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=154
03/18/2022 13:37:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.70 on epoch=157
03/18/2022 13:37:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=159
03/18/2022 13:38:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=162
03/18/2022 13:38:01 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.4957348957348958 on epoch=162
03/18/2022 13:38:01 - INFO - __main__ - Saving model with best Classification-F1: 0.45 -> 0.4957348957348958 on epoch=162, global_step=650
03/18/2022 13:38:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=164
03/18/2022 13:38:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
03/18/2022 13:38:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=169
03/18/2022 13:38:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.60 on epoch=172
03/18/2022 13:38:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.57 on epoch=174
03/18/2022 13:38:14 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.39851393188854495 on epoch=174
03/18/2022 13:38:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=177
03/18/2022 13:38:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=179
03/18/2022 13:38:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=182
03/18/2022 13:38:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=184
03/18/2022 13:38:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=187
03/18/2022 13:38:27 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.5070855614973262 on epoch=187
03/18/2022 13:38:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4957348957348958 -> 0.5070855614973262 on epoch=187, global_step=750
03/18/2022 13:38:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=189
03/18/2022 13:38:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=192
03/18/2022 13:38:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=194
03/18/2022 13:38:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=197
03/18/2022 13:38:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=199
03/18/2022 13:38:40 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.5414285714285715 on epoch=199
03/18/2022 13:38:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5070855614973262 -> 0.5414285714285715 on epoch=199, global_step=800
03/18/2022 13:38:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=202
03/18/2022 13:38:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=204
03/18/2022 13:38:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=207
03/18/2022 13:38:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=209
03/18/2022 13:38:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=212
03/18/2022 13:38:53 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.4764285714285714 on epoch=212
03/18/2022 13:38:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=214
03/18/2022 13:38:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=217
03/18/2022 13:39:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=219
03/18/2022 13:39:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=222
03/18/2022 13:39:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=224
03/18/2022 13:39:06 - INFO - __main__ - Global step 900 Train loss 0.32 Classification-F1 0.4771270396270396 on epoch=224
03/18/2022 13:39:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=227
03/18/2022 13:39:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=229
03/18/2022 13:39:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=232
03/18/2022 13:39:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
03/18/2022 13:39:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=237
03/18/2022 13:39:20 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.6222504279489574 on epoch=237
03/18/2022 13:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5414285714285715 -> 0.6222504279489574 on epoch=237, global_step=950
03/18/2022 13:39:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
03/18/2022 13:39:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
03/18/2022 13:39:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
03/18/2022 13:39:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
03/18/2022 13:39:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
03/18/2022 13:39:33 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6308322388885048 on epoch=249
03/18/2022 13:39:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6222504279489574 -> 0.6308322388885048 on epoch=249, global_step=1000
03/18/2022 13:39:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
03/18/2022 13:39:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
03/18/2022 13:39:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=257
03/18/2022 13:39:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
03/18/2022 13:39:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
03/18/2022 13:39:46 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.6070207688338494 on epoch=262
03/18/2022 13:39:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
03/18/2022 13:39:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
03/18/2022 13:39:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
03/18/2022 13:39:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=272
03/18/2022 13:39:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
03/18/2022 13:39:59 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6293859649122807 on epoch=274
03/18/2022 13:40:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
03/18/2022 13:40:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
03/18/2022 13:40:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
03/18/2022 13:40:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
03/18/2022 13:40:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
03/18/2022 13:40:12 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6442687979199135 on epoch=287
03/18/2022 13:40:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6308322388885048 -> 0.6442687979199135 on epoch=287, global_step=1150
03/18/2022 13:40:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
03/18/2022 13:40:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
03/18/2022 13:40:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
03/18/2022 13:40:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
03/18/2022 13:40:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
03/18/2022 13:40:25 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6440113976625133 on epoch=299
03/18/2022 13:40:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
03/18/2022 13:40:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
03/18/2022 13:40:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
03/18/2022 13:40:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
03/18/2022 13:40:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
03/18/2022 13:40:38 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.6495430495430495 on epoch=312
03/18/2022 13:40:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6442687979199135 -> 0.6495430495430495 on epoch=312, global_step=1250
03/18/2022 13:40:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
03/18/2022 13:40:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
03/18/2022 13:40:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
03/18/2022 13:40:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
03/18/2022 13:40:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=324
03/18/2022 13:40:51 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.7163943865981217 on epoch=324
03/18/2022 13:40:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6495430495430495 -> 0.7163943865981217 on epoch=324, global_step=1300
03/18/2022 13:40:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
03/18/2022 13:40:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
03/18/2022 13:40:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
03/18/2022 13:41:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
03/18/2022 13:41:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
03/18/2022 13:41:04 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6724881937863683 on epoch=337
03/18/2022 13:41:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
03/18/2022 13:41:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
03/18/2022 13:41:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
03/18/2022 13:41:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
03/18/2022 13:41:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
03/18/2022 13:41:17 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.7144414735591207 on epoch=349
03/18/2022 13:41:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
03/18/2022 13:41:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
03/18/2022 13:41:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
03/18/2022 13:41:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
03/18/2022 13:41:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
03/18/2022 13:41:31 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6892089093701996 on epoch=362
03/18/2022 13:41:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
03/18/2022 13:41:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
03/18/2022 13:41:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
03/18/2022 13:41:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
03/18/2022 13:41:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
03/18/2022 13:41:44 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.7144360269360269 on epoch=374
03/18/2022 13:41:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
03/18/2022 13:41:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
03/18/2022 13:41:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
03/18/2022 13:41:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
03/18/2022 13:41:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
03/18/2022 13:41:57 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6568376068376067 on epoch=387
03/18/2022 13:42:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
03/18/2022 13:42:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
03/18/2022 13:42:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
03/18/2022 13:42:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
03/18/2022 13:42:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
03/18/2022 13:42:11 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6587594635132835 on epoch=399
03/18/2022 13:42:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
03/18/2022 13:42:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
03/18/2022 13:42:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
03/18/2022 13:42:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
03/18/2022 13:42:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=412
03/18/2022 13:42:24 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.668473715651135 on epoch=412
03/18/2022 13:42:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
03/18/2022 13:42:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
03/18/2022 13:42:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
03/18/2022 13:42:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
03/18/2022 13:42:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
03/18/2022 13:42:37 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6754785409714497 on epoch=424
03/18/2022 13:42:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
03/18/2022 13:42:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
03/18/2022 13:42:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
03/18/2022 13:42:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
03/18/2022 13:42:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 13:42:50 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6193204079451957 on epoch=437
03/18/2022 13:42:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
03/18/2022 13:42:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
03/18/2022 13:42:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
03/18/2022 13:43:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
03/18/2022 13:43:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
03/18/2022 13:43:04 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5986577733860343 on epoch=449
03/18/2022 13:43:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
03/18/2022 13:43:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
03/18/2022 13:43:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/18/2022 13:43:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
03/18/2022 13:43:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
03/18/2022 13:43:17 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6234195402298851 on epoch=462
03/18/2022 13:43:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
03/18/2022 13:43:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
03/18/2022 13:43:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=469
03/18/2022 13:43:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
03/18/2022 13:43:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
03/18/2022 13:43:31 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6407181966449207 on epoch=474
03/18/2022 13:43:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/18/2022 13:43:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
03/18/2022 13:43:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
03/18/2022 13:43:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
03/18/2022 13:43:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
03/18/2022 13:43:44 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6538359788359789 on epoch=487
03/18/2022 13:43:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
03/18/2022 13:43:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
03/18/2022 13:43:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
03/18/2022 13:43:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
03/18/2022 13:43:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
03/18/2022 13:43:57 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7030721966205837 on epoch=499
03/18/2022 13:44:00 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
03/18/2022 13:44:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
03/18/2022 13:44:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
03/18/2022 13:44:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
03/18/2022 13:44:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
03/18/2022 13:44:11 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6280289350909823 on epoch=512
03/18/2022 13:44:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
03/18/2022 13:44:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
03/18/2022 13:44:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/18/2022 13:44:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
03/18/2022 13:44:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
03/18/2022 13:44:24 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.69064039408867 on epoch=524
03/18/2022 13:44:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
03/18/2022 13:44:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
03/18/2022 13:44:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
03/18/2022 13:44:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
03/18/2022 13:44:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
03/18/2022 13:44:37 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6267738593723434 on epoch=537
03/18/2022 13:44:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
03/18/2022 13:44:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
03/18/2022 13:44:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
03/18/2022 13:44:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
03/18/2022 13:44:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
03/18/2022 13:44:51 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6659899749373432 on epoch=549
03/18/2022 13:44:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 13:44:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
03/18/2022 13:44:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
03/18/2022 13:45:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/18/2022 13:45:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
03/18/2022 13:45:04 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6910984848484848 on epoch=562
03/18/2022 13:45:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
03/18/2022 13:45:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
03/18/2022 13:45:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=569
03/18/2022 13:45:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
03/18/2022 13:45:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
03/18/2022 13:45:17 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6740358005063888 on epoch=574
03/18/2022 13:45:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
03/18/2022 13:45:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
03/18/2022 13:45:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
03/18/2022 13:45:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
03/18/2022 13:45:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/18/2022 13:45:30 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6175366300366301 on epoch=587
03/18/2022 13:45:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
03/18/2022 13:45:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
03/18/2022 13:45:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
03/18/2022 13:45:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 13:45:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
03/18/2022 13:45:44 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6318528175276903 on epoch=599
03/18/2022 13:45:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
03/18/2022 13:45:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
03/18/2022 13:45:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
03/18/2022 13:45:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
03/18/2022 13:45:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
03/18/2022 13:45:57 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6701058201058201 on epoch=612
03/18/2022 13:45:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
03/18/2022 13:46:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
03/18/2022 13:46:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
03/18/2022 13:46:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 13:46:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 13:46:10 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.664222376593658 on epoch=624
03/18/2022 13:46:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
03/18/2022 13:46:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 13:46:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
03/18/2022 13:46:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/18/2022 13:46:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
03/18/2022 13:46:23 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6045977011494252 on epoch=637
03/18/2022 13:46:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
03/18/2022 13:46:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
03/18/2022 13:46:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
03/18/2022 13:46:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
03/18/2022 13:46:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/18/2022 13:46:37 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6723079996005193 on epoch=649
03/18/2022 13:46:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
03/18/2022 13:46:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/18/2022 13:46:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
03/18/2022 13:46:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
03/18/2022 13:46:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
03/18/2022 13:46:50 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.656043956043956 on epoch=662
03/18/2022 13:46:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
03/18/2022 13:46:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
03/18/2022 13:46:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
03/18/2022 13:47:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 13:47:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 13:47:03 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6709011813537675 on epoch=674
03/18/2022 13:47:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
03/18/2022 13:47:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 13:47:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
03/18/2022 13:47:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 13:47:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
03/18/2022 13:47:17 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7172738020394688 on epoch=687
03/18/2022 13:47:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7163943865981217 -> 0.7172738020394688 on epoch=687, global_step=2750
03/18/2022 13:47:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
03/18/2022 13:47:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 13:47:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
03/18/2022 13:47:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
03/18/2022 13:47:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
03/18/2022 13:47:30 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6806372549019608 on epoch=699
03/18/2022 13:47:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
03/18/2022 13:47:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
03/18/2022 13:47:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
03/18/2022 13:47:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
03/18/2022 13:47:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
03/18/2022 13:47:43 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6843915343915343 on epoch=712
03/18/2022 13:47:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
03/18/2022 13:47:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
03/18/2022 13:47:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 13:47:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
03/18/2022 13:47:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
03/18/2022 13:47:57 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6940790720594366 on epoch=724
03/18/2022 13:47:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 13:48:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
03/18/2022 13:48:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 13:48:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
03/18/2022 13:48:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 13:48:10 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7168117426181941 on epoch=737
03/18/2022 13:48:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
03/18/2022 13:48:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
03/18/2022 13:48:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
03/18/2022 13:48:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 13:48:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
03/18/2022 13:48:23 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7035794735035722 on epoch=749
03/18/2022 13:48:23 - INFO - __main__ - save last model!
03/18/2022 13:48:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 13:48:23 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 13:48:23 - INFO - __main__ - Printing 3 examples
03/18/2022 13:48:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:48:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:48:23 - INFO - __main__ - Printing 3 examples
03/18/2022 13:48:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 13:48:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:48:23 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:48:23 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:48:23 - INFO - __main__ - Printing 3 examples
03/18/2022 13:48:23 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:48:23 - INFO - __main__ - ['others']
03/18/2022 13:48:23 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:48:23 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:48:23 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:48:25 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:48:31 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 13:48:39 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:48:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:48:39 - INFO - __main__ - Starting training!
03/18/2022 13:50:06 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_87_0.3_8_predictions.txt
03/18/2022 13:50:07 - INFO - __main__ - Classification-F1 on test data: 0.3820
03/18/2022 13:50:07 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7172738020394688, test_performance=0.3820159790905956
03/18/2022 13:50:07 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
03/18/2022 13:50:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:50:08 - INFO - __main__ - Printing 3 examples
03/18/2022 13:50:08 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 13:50:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:50:08 - INFO - __main__ - Loaded 64 examples from train data
use DistributedSampler
03/18/2022 13:50:08 - INFO - __main__ - Start tokenizing ... 64 instances
03/18/2022 13:50:08 - INFO - __main__ - Printing 3 examples
03/18/2022 13:50:08 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
03/18/2022 13:50:08 - INFO - __main__ - ['others']
03/18/2022 13:50:08 - INFO - __main__ - Tokenizing Input ...
03/18/2022 13:50:08 - INFO - __main__ - Tokenizing Output ...
03/18/2022 13:50:08 - INFO - __main__ - Loaded 64 examples from dev data
03/18/2022 13:50:26 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 13:50:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 13:50:27 - INFO - __main__ - Starting training!
03/18/2022 13:50:30 - INFO - __main__ - Step 10 Global step 10 Train loss 7.75 on epoch=2
03/18/2022 13:50:33 - INFO - __main__ - Step 20 Global step 20 Train loss 5.55 on epoch=4
03/18/2022 13:50:35 - INFO - __main__ - Step 30 Global step 30 Train loss 4.10 on epoch=7
03/18/2022 13:50:37 - INFO - __main__ - Step 40 Global step 40 Train loss 2.91 on epoch=9
03/18/2022 13:50:40 - INFO - __main__ - Step 50 Global step 50 Train loss 2.40 on epoch=12
03/18/2022 13:50:41 - INFO - __main__ - Global step 50 Train loss 4.54 Classification-F1 0.23460317460317462 on epoch=12
03/18/2022 13:50:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23460317460317462 on epoch=12, global_step=50
03/18/2022 13:50:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.75 on epoch=14
03/18/2022 13:50:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.40 on epoch=17
03/18/2022 13:50:48 - INFO - __main__ - Step 80 Global step 80 Train loss 1.34 on epoch=19
03/18/2022 13:50:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.28 on epoch=22
03/18/2022 13:50:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=24
03/18/2022 13:50:54 - INFO - __main__ - Global step 100 Train loss 1.36 Classification-F1 0.10126582278481013 on epoch=24
03/18/2022 13:50:56 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=27
03/18/2022 13:50:59 - INFO - __main__ - Step 120 Global step 120 Train loss 1.07 on epoch=29
03/18/2022 13:51:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=32
03/18/2022 13:51:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.93 on epoch=34
03/18/2022 13:51:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
03/18/2022 13:51:07 - INFO - __main__ - Global step 150 Train loss 1.00 Classification-F1 0.3221727411227001 on epoch=37
03/18/2022 13:51:07 - INFO - __main__ - Saving model with best Classification-F1: 0.23460317460317462 -> 0.3221727411227001 on epoch=37, global_step=150
03/18/2022 13:51:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=39
03/18/2022 13:51:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.99 on epoch=42
03/18/2022 13:51:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=44
03/18/2022 13:51:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=47
03/18/2022 13:51:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
03/18/2022 13:51:20 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.10126582278481013 on epoch=49
03/18/2022 13:51:22 - INFO - __main__ - Step 210 Global step 210 Train loss 1.04 on epoch=52
03/18/2022 13:51:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.91 on epoch=54
03/18/2022 13:51:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=57
03/18/2022 13:51:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=59
03/18/2022 13:51:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.94 on epoch=62
03/18/2022 13:51:33 - INFO - __main__ - Global step 250 Train loss 0.96 Classification-F1 0.2618885448916409 on epoch=62
03/18/2022 13:51:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=64
03/18/2022 13:51:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.89 on epoch=67
03/18/2022 13:51:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.93 on epoch=69
03/18/2022 13:51:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.90 on epoch=72
03/18/2022 13:51:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
03/18/2022 13:51:45 - INFO - __main__ - Global step 300 Train loss 0.90 Classification-F1 0.15144596651445968 on epoch=74
03/18/2022 13:51:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.94 on epoch=77
03/18/2022 13:51:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
03/18/2022 13:51:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
03/18/2022 13:51:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.89 on epoch=84
03/18/2022 13:51:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
03/18/2022 13:51:58 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.3101851851851852 on epoch=87
03/18/2022 13:52:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.90 on epoch=89
03/18/2022 13:52:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
03/18/2022 13:52:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.93 on epoch=94
03/18/2022 13:52:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=97
03/18/2022 13:52:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.87 on epoch=99
03/18/2022 13:52:12 - INFO - __main__ - Global step 400 Train loss 0.87 Classification-F1 0.1307631160572337 on epoch=99
03/18/2022 13:52:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=102
03/18/2022 13:52:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
03/18/2022 13:52:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=107
03/18/2022 13:52:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=109
03/18/2022 13:52:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.82 on epoch=112
03/18/2022 13:52:24 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.398076923076923 on epoch=112
03/18/2022 13:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3221727411227001 -> 0.398076923076923 on epoch=112, global_step=450
03/18/2022 13:52:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
03/18/2022 13:52:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=117
03/18/2022 13:52:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=119
03/18/2022 13:52:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=122
03/18/2022 13:52:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.85 on epoch=124
03/18/2022 13:52:37 - INFO - __main__ - Global step 500 Train loss 0.83 Classification-F1 0.31067226890756305 on epoch=124
03/18/2022 13:52:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=127
03/18/2022 13:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=129
03/18/2022 13:52:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
03/18/2022 13:52:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.70 on epoch=134
03/18/2022 13:52:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.81 on epoch=137
03/18/2022 13:52:50 - INFO - __main__ - Global step 550 Train loss 0.79 Classification-F1 0.22774845825426945 on epoch=137
03/18/2022 13:52:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=139
03/18/2022 13:52:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.70 on epoch=142
03/18/2022 13:52:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.78 on epoch=144
03/18/2022 13:53:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=147
03/18/2022 13:53:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=149
03/18/2022 13:53:03 - INFO - __main__ - Global step 600 Train loss 0.76 Classification-F1 0.3585434173669468 on epoch=149
03/18/2022 13:53:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
03/18/2022 13:53:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=154
03/18/2022 13:53:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.76 on epoch=157
03/18/2022 13:53:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.73 on epoch=159
03/18/2022 13:53:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=162
03/18/2022 13:53:16 - INFO - __main__ - Global step 650 Train loss 0.74 Classification-F1 0.3872294372294372 on epoch=162
03/18/2022 13:53:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
03/18/2022 13:53:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.71 on epoch=167
03/18/2022 13:53:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=169
03/18/2022 13:53:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.69 on epoch=172
03/18/2022 13:53:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=174
03/18/2022 13:53:29 - INFO - __main__ - Global step 700 Train loss 0.64 Classification-F1 0.46105600674566194 on epoch=174
03/18/2022 13:53:29 - INFO - __main__ - Saving model with best Classification-F1: 0.398076923076923 -> 0.46105600674566194 on epoch=174, global_step=700
03/18/2022 13:53:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.65 on epoch=177
03/18/2022 13:53:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=179
03/18/2022 13:53:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=182
03/18/2022 13:53:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.59 on epoch=184
03/18/2022 13:53:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.64 on epoch=187
03/18/2022 13:53:42 - INFO - __main__ - Global step 750 Train loss 0.62 Classification-F1 0.42683682668469686 on epoch=187
03/18/2022 13:53:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=189
03/18/2022 13:53:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=192
03/18/2022 13:53:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=194
03/18/2022 13:53:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=197
03/18/2022 13:53:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=199
03/18/2022 13:53:55 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.4871872887085869 on epoch=199
03/18/2022 13:53:55 - INFO - __main__ - Saving model with best Classification-F1: 0.46105600674566194 -> 0.4871872887085869 on epoch=199, global_step=800
03/18/2022 13:53:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=202
03/18/2022 13:54:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=204
03/18/2022 13:54:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=207
03/18/2022 13:54:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=209
03/18/2022 13:54:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=212
03/18/2022 13:54:08 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.45256611069641084 on epoch=212
03/18/2022 13:54:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=214
03/18/2022 13:54:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=217
03/18/2022 13:54:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=219
03/18/2022 13:54:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=222
03/18/2022 13:54:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=224
03/18/2022 13:54:21 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.5023634984833165 on epoch=224
03/18/2022 13:54:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4871872887085869 -> 0.5023634984833165 on epoch=224, global_step=900
03/18/2022 13:54:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=227
03/18/2022 13:54:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=229
03/18/2022 13:54:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=232
03/18/2022 13:54:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=234
03/18/2022 13:54:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=237
03/18/2022 13:54:34 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.4097106866002215 on epoch=237
03/18/2022 13:54:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=239
03/18/2022 13:54:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=242
03/18/2022 13:54:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
03/18/2022 13:54:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=247
03/18/2022 13:54:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=249
03/18/2022 13:54:47 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.5192192934597286 on epoch=249
03/18/2022 13:54:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5023634984833165 -> 0.5192192934597286 on epoch=249, global_step=1000
03/18/2022 13:54:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=252
03/18/2022 13:54:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
03/18/2022 13:54:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=257
03/18/2022 13:54:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=259
03/18/2022 13:54:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=262
03/18/2022 13:55:00 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.4820289855072464 on epoch=262
03/18/2022 13:55:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
03/18/2022 13:55:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=267
03/18/2022 13:55:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=269
03/18/2022 13:55:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=272
03/18/2022 13:55:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=274
03/18/2022 13:55:13 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.5595251293422026 on epoch=274
03/18/2022 13:55:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5192192934597286 -> 0.5595251293422026 on epoch=274, global_step=1100
03/18/2022 13:55:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
03/18/2022 13:55:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
03/18/2022 13:55:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=282
03/18/2022 13:55:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=284
03/18/2022 13:55:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
03/18/2022 13:55:26 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.5627531472359059 on epoch=287
03/18/2022 13:55:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5595251293422026 -> 0.5627531472359059 on epoch=287, global_step=1150
03/18/2022 13:55:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=289
03/18/2022 13:55:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=292
03/18/2022 13:55:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=294
03/18/2022 13:55:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=297
03/18/2022 13:55:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
03/18/2022 13:55:39 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.5758750428695354 on epoch=299
03/18/2022 13:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5627531472359059 -> 0.5758750428695354 on epoch=299, global_step=1200
03/18/2022 13:55:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
03/18/2022 13:55:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=304
03/18/2022 13:55:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
03/18/2022 13:55:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=309
03/18/2022 13:55:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
03/18/2022 13:55:52 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.5439271749755621 on epoch=312
03/18/2022 13:55:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
03/18/2022 13:55:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
03/18/2022 13:56:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=319
03/18/2022 13:56:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=322
03/18/2022 13:56:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=324
03/18/2022 13:56:05 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.5607561793045664 on epoch=324
03/18/2022 13:56:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
03/18/2022 13:56:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
03/18/2022 13:56:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=332
03/18/2022 13:56:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=334
03/18/2022 13:56:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=337
03/18/2022 13:56:19 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.55821835563554 on epoch=337
03/18/2022 13:56:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
03/18/2022 13:56:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=342
03/18/2022 13:56:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
03/18/2022 13:56:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=347
03/18/2022 13:56:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=349
03/18/2022 13:56:32 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.600937950937951 on epoch=349
03/18/2022 13:56:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5758750428695354 -> 0.600937950937951 on epoch=349, global_step=1400
03/18/2022 13:56:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
03/18/2022 13:56:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
03/18/2022 13:56:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
03/18/2022 13:56:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
03/18/2022 13:56:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
03/18/2022 13:56:45 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.5379928315412186 on epoch=362
03/18/2022 13:56:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=364
03/18/2022 13:56:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
03/18/2022 13:56:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
03/18/2022 13:56:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
03/18/2022 13:56:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
03/18/2022 13:56:58 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.5128176142718046 on epoch=374
03/18/2022 13:57:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
03/18/2022 13:57:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
03/18/2022 13:57:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
03/18/2022 13:57:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
03/18/2022 13:57:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
03/18/2022 13:57:12 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.5833057623380205 on epoch=387
03/18/2022 13:57:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
03/18/2022 13:57:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
03/18/2022 13:57:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
03/18/2022 13:57:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
03/18/2022 13:57:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
03/18/2022 13:57:25 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.5841772807290049 on epoch=399
03/18/2022 13:57:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
03/18/2022 13:57:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=404
03/18/2022 13:57:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
03/18/2022 13:57:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
03/18/2022 13:57:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
03/18/2022 13:57:38 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5990196078431372 on epoch=412
03/18/2022 13:57:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
03/18/2022 13:57:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
03/18/2022 13:57:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
03/18/2022 13:57:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
03/18/2022 13:57:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=424
03/18/2022 13:57:51 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.5995337995337995 on epoch=424
03/18/2022 13:57:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
03/18/2022 13:57:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
03/18/2022 13:57:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
03/18/2022 13:58:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
03/18/2022 13:58:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
03/18/2022 13:58:05 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.5902777777777778 on epoch=437
03/18/2022 13:58:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=439
03/18/2022 13:58:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
03/18/2022 13:58:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
03/18/2022 13:58:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=447
03/18/2022 13:58:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
03/18/2022 13:58:18 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.5811069155248679 on epoch=449
03/18/2022 13:58:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
03/18/2022 13:58:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
03/18/2022 13:58:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
03/18/2022 13:58:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
03/18/2022 13:58:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
03/18/2022 13:58:31 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.5634201819685691 on epoch=462
03/18/2022 13:58:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
03/18/2022 13:58:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
03/18/2022 13:58:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
03/18/2022 13:58:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
03/18/2022 13:58:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
03/18/2022 13:58:44 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.567273811459858 on epoch=474
03/18/2022 13:58:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
03/18/2022 13:58:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
03/18/2022 13:58:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
03/18/2022 13:58:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
03/18/2022 13:58:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
03/18/2022 13:58:58 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6245277535600116 on epoch=487
03/18/2022 13:58:58 - INFO - __main__ - Saving model with best Classification-F1: 0.600937950937951 -> 0.6245277535600116 on epoch=487, global_step=1950
03/18/2022 13:59:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
03/18/2022 13:59:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
03/18/2022 13:59:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=494
03/18/2022 13:59:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
03/18/2022 13:59:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
03/18/2022 13:59:11 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.5833057623380205 on epoch=499
03/18/2022 13:59:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
03/18/2022 13:59:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
03/18/2022 13:59:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
03/18/2022 13:59:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
03/18/2022 13:59:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
03/18/2022 13:59:24 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5952331002331003 on epoch=512
03/18/2022 13:59:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
03/18/2022 13:59:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
03/18/2022 13:59:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
03/18/2022 13:59:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
03/18/2022 13:59:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
03/18/2022 13:59:37 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6108801108801108 on epoch=524
03/18/2022 13:59:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=527
03/18/2022 13:59:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
03/18/2022 13:59:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
03/18/2022 13:59:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=534
03/18/2022 13:59:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
03/18/2022 13:59:51 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.6626492702344097 on epoch=537
03/18/2022 13:59:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6245277535600116 -> 0.6626492702344097 on epoch=537, global_step=2150
03/18/2022 13:59:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
03/18/2022 13:59:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
03/18/2022 13:59:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
03/18/2022 14:00:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
03/18/2022 14:00:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
03/18/2022 14:00:04 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6226084538375973 on epoch=549
03/18/2022 14:00:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
03/18/2022 14:00:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=554
03/18/2022 14:00:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
03/18/2022 14:00:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
03/18/2022 14:00:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
03/18/2022 14:00:17 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5789682539682539 on epoch=562
03/18/2022 14:00:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
03/18/2022 14:00:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
03/18/2022 14:00:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
03/18/2022 14:00:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
03/18/2022 14:00:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
03/18/2022 14:00:31 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.610476605637896 on epoch=574
03/18/2022 14:00:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
03/18/2022 14:00:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
03/18/2022 14:00:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
03/18/2022 14:00:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
03/18/2022 14:00:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
03/18/2022 14:00:45 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.595434712346477 on epoch=587
03/18/2022 14:00:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
03/18/2022 14:00:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
03/18/2022 14:00:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
03/18/2022 14:00:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
03/18/2022 14:00:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
03/18/2022 14:00:58 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6382618113693578 on epoch=599
03/18/2022 14:01:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
03/18/2022 14:01:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
03/18/2022 14:01:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
03/18/2022 14:01:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
03/18/2022 14:01:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
03/18/2022 14:01:11 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6088276876911536 on epoch=612
03/18/2022 14:01:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
03/18/2022 14:01:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
03/18/2022 14:01:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=619
03/18/2022 14:01:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
03/18/2022 14:01:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
03/18/2022 14:01:25 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.666052043422733 on epoch=624
03/18/2022 14:01:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6626492702344097 -> 0.666052043422733 on epoch=624, global_step=2500
03/18/2022 14:01:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
03/18/2022 14:01:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
03/18/2022 14:01:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
03/18/2022 14:01:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
03/18/2022 14:01:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
03/18/2022 14:01:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6289209401709401 on epoch=637
03/18/2022 14:01:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
03/18/2022 14:01:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
03/18/2022 14:01:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
03/18/2022 14:01:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
03/18/2022 14:01:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
03/18/2022 14:01:51 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5941645133505599 on epoch=649
03/18/2022 14:01:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
03/18/2022 14:01:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
03/18/2022 14:01:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
03/18/2022 14:02:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
03/18/2022 14:02:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
03/18/2022 14:02:05 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6210737179487179 on epoch=662
03/18/2022 14:02:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
03/18/2022 14:02:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
03/18/2022 14:02:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
03/18/2022 14:02:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
03/18/2022 14:02:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
03/18/2022 14:02:18 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.5987335722819594 on epoch=674
03/18/2022 14:02:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
03/18/2022 14:02:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
03/18/2022 14:02:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
03/18/2022 14:02:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
03/18/2022 14:02:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
03/18/2022 14:02:32 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5974580974580975 on epoch=687
03/18/2022 14:02:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
03/18/2022 14:02:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
03/18/2022 14:02:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
03/18/2022 14:02:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
03/18/2022 14:02:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
03/18/2022 14:02:45 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5946156240273888 on epoch=699
03/18/2022 14:02:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
03/18/2022 14:02:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
03/18/2022 14:02:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
03/18/2022 14:02:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
03/18/2022 14:02:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
03/18/2022 14:02:58 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.618095737462217 on epoch=712
03/18/2022 14:03:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
03/18/2022 14:03:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
03/18/2022 14:03:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
03/18/2022 14:03:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
03/18/2022 14:03:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
03/18/2022 14:03:12 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6314935064935066 on epoch=724
03/18/2022 14:03:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
03/18/2022 14:03:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
03/18/2022 14:03:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
03/18/2022 14:03:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=734
03/18/2022 14:03:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
03/18/2022 14:03:25 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6057278742762614 on epoch=737
03/18/2022 14:03:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
03/18/2022 14:03:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
03/18/2022 14:03:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
03/18/2022 14:03:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
03/18/2022 14:03:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
03/18/2022 14:03:39 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6333384646962233 on epoch=749
03/18/2022 14:03:39 - INFO - __main__ - save last model!
03/18/2022 14:03:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 14:03:39 - INFO - __main__ - Start tokenizing ... 5509 instances
03/18/2022 14:03:39 - INFO - __main__ - Printing 3 examples
03/18/2022 14:03:39 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
03/18/2022 14:03:39 - INFO - __main__ - ['others']
03/18/2022 14:03:39 - INFO - __main__ -  [emo] what you like very little things ok
03/18/2022 14:03:39 - INFO - __main__ - ['others']
03/18/2022 14:03:39 - INFO - __main__ -  [emo] yes how so i want to fuck babu
03/18/2022 14:03:39 - INFO - __main__ - ['others']
03/18/2022 14:03:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 14:03:41 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:03:46 - INFO - __main__ - Loaded 5509 examples from test data
03/18/2022 14:05:19 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-emo/emo_16_87_0.2_8_predictions.txt
03/18/2022 14:05:19 - INFO - __main__ - Classification-F1 on test data: 0.3946
03/18/2022 14:05:20 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.666052043422733, test_performance=0.39455478936606847
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (21203): No such process
Task: yelp_polarity, Checkpoint: models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt, Identifier: T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10
03/18/2022 14:05:26 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 14:05:26 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-yelp_polarity
Output directory () already exists and is not empty.
03/18/2022 14:05:26 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-cls2cls-3e-5-2-5000-5e-1-10/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
03/18/2022 14:05:26 - INFO - __main__ - models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-yelp_polarity
03/18/2022 14:05:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/18/2022 14:05:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/18/2022 14:05:27 - INFO - __main__ - args.device: cuda:0
03/18/2022 14:05:27 - INFO - __main__ - Using 2 gpus
03/18/2022 14:05:27 - INFO - __main__ - args.device: cuda:1
03/18/2022 14:05:27 - INFO - __main__ - Using 2 gpus
03/18/2022 14:05:27 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/18/2022 14:05:27 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/18/2022 14:05:32 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.5, bsz=8 ...
03/18/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:05:33 - INFO - __main__ - Printing 3 examples
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:05:33 - INFO - __main__ - Printing 3 examples
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:05:33 - INFO - __main__ - Printing 3 examples
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 14:05:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:05:33 - INFO - __main__ - Printing 3 examples
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/18/2022 14:05:33 - INFO - __main__ - ['negative']
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Input ...
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:05:33 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 14:05:33 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 14:05:50 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 14:05:51 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 14:05:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 14:05:51 - INFO - __main__ - Starting training!
03/18/2022 14:05:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 14:05:56 - INFO - __main__ - Starting training!
03/18/2022 14:06:01 - INFO - __main__ - Step 10 Global step 10 Train loss 6.82 on epoch=4
03/18/2022 14:06:05 - INFO - __main__ - Step 20 Global step 20 Train loss 6.42 on epoch=9
03/18/2022 14:06:10 - INFO - __main__ - Step 30 Global step 30 Train loss 5.65 on epoch=14
03/18/2022 14:06:14 - INFO - __main__ - Step 40 Global step 40 Train loss 5.71 on epoch=19
03/18/2022 14:06:18 - INFO - __main__ - Step 50 Global step 50 Train loss 6.00 on epoch=24
03/18/2022 14:06:42 - INFO - __main__ - Global step 50 Train loss 6.12 Classification-F1 0.0 on epoch=24
03/18/2022 14:06:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=24, global_step=50
03/18/2022 14:06:46 - INFO - __main__ - Step 60 Global step 60 Train loss 7.13 on epoch=29
03/18/2022 14:06:51 - INFO - __main__ - Step 70 Global step 70 Train loss 6.22 on epoch=34
03/18/2022 14:06:55 - INFO - __main__ - Step 80 Global step 80 Train loss 5.91 on epoch=39
03/18/2022 14:06:59 - INFO - __main__ - Step 90 Global step 90 Train loss 5.19 on epoch=44
03/18/2022 14:07:04 - INFO - __main__ - Step 100 Global step 100 Train loss 5.01 on epoch=49
03/18/2022 14:07:26 - INFO - __main__ - Global step 100 Train loss 5.89 Classification-F1 0.0 on epoch=49
03/18/2022 14:07:30 - INFO - __main__ - Step 110 Global step 110 Train loss 4.29 on epoch=54
03/18/2022 14:07:35 - INFO - __main__ - Step 120 Global step 120 Train loss 3.59 on epoch=59
03/18/2022 14:07:39 - INFO - __main__ - Step 130 Global step 130 Train loss 3.11 on epoch=64
03/18/2022 14:07:43 - INFO - __main__ - Step 140 Global step 140 Train loss 2.65 on epoch=69
03/18/2022 14:07:48 - INFO - __main__ - Step 150 Global step 150 Train loss 2.26 on epoch=74
03/18/2022 14:07:49 - INFO - __main__ - Global step 150 Train loss 3.18 Classification-F1 0.3992490613266583 on epoch=74
03/18/2022 14:07:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3992490613266583 on epoch=74, global_step=150
03/18/2022 14:07:53 - INFO - __main__ - Step 160 Global step 160 Train loss 2.17 on epoch=79
03/18/2022 14:07:58 - INFO - __main__ - Step 170 Global step 170 Train loss 1.64 on epoch=84
03/18/2022 14:08:02 - INFO - __main__ - Step 180 Global step 180 Train loss 1.33 on epoch=89
03/18/2022 14:08:07 - INFO - __main__ - Step 190 Global step 190 Train loss 1.30 on epoch=94
03/18/2022 14:08:11 - INFO - __main__ - Step 200 Global step 200 Train loss 1.12 on epoch=99
03/18/2022 14:08:12 - INFO - __main__ - Global step 200 Train loss 1.51 Classification-F1 0.4385964912280702 on epoch=99
03/18/2022 14:08:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4385964912280702 on epoch=99, global_step=200
03/18/2022 14:08:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=104
03/18/2022 14:08:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=109
03/18/2022 14:08:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=114
03/18/2022 14:08:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=119
03/18/2022 14:08:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=124
03/18/2022 14:08:35 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.3992490613266583 on epoch=124
03/18/2022 14:08:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=129
03/18/2022 14:08:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=134
03/18/2022 14:08:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=139
03/18/2022 14:08:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=144
03/18/2022 14:08:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=149
03/18/2022 14:08:58 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=149
03/18/2022 14:09:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=154
03/18/2022 14:09:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=159
03/18/2022 14:09:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=164
03/18/2022 14:09:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=169
03/18/2022 14:09:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=174
03/18/2022 14:09:21 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.3992490613266583 on epoch=174
03/18/2022 14:09:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=179
03/18/2022 14:09:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=184
03/18/2022 14:09:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=189
03/18/2022 14:09:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=194
03/18/2022 14:09:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=199
03/18/2022 14:09:44 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.3992490613266583 on epoch=199
03/18/2022 14:09:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=204
03/18/2022 14:09:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=209
03/18/2022 14:09:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=214
03/18/2022 14:10:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=219
03/18/2022 14:10:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=224
03/18/2022 14:10:07 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.3992490613266583 on epoch=224
03/18/2022 14:10:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=229
03/18/2022 14:10:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=234
03/18/2022 14:10:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=239
03/18/2022 14:10:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=244
03/18/2022 14:10:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=249
03/18/2022 14:10:30 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.539313399778516 on epoch=249
03/18/2022 14:10:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.539313399778516 on epoch=249, global_step=500
03/18/2022 14:10:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=254
03/18/2022 14:10:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
03/18/2022 14:10:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=264
03/18/2022 14:10:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=269
03/18/2022 14:10:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.90 on epoch=274
03/18/2022 14:10:53 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.4909862142099682 on epoch=274
03/18/2022 14:10:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=279
03/18/2022 14:11:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=284
03/18/2022 14:11:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=289
03/18/2022 14:11:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/18/2022 14:11:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=299
03/18/2022 14:11:17 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.5134502923976608 on epoch=299
03/18/2022 14:11:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
03/18/2022 14:11:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=309
03/18/2022 14:11:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=314
03/18/2022 14:11:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=319
03/18/2022 14:11:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=324
03/18/2022 14:11:40 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.6113360323886641 on epoch=324
03/18/2022 14:11:40 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.6113360323886641 on epoch=324, global_step=650
03/18/2022 14:11:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=329
03/18/2022 14:11:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=334
03/18/2022 14:11:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=339
03/18/2022 14:11:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=344
03/18/2022 14:12:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
03/18/2022 14:12:03 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.6825396825396826 on epoch=349
03/18/2022 14:12:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6113360323886641 -> 0.6825396825396826 on epoch=349, global_step=700
03/18/2022 14:12:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
03/18/2022 14:12:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=359
03/18/2022 14:12:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=364
03/18/2022 14:12:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=369
03/18/2022 14:12:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=374
03/18/2022 14:12:26 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.6559139784946237 on epoch=374
03/18/2022 14:12:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=379
03/18/2022 14:12:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=384
03/18/2022 14:12:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=389
03/18/2022 14:12:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=394
03/18/2022 14:12:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=399
03/18/2022 14:12:49 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.625 on epoch=399
03/18/2022 14:12:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=404
03/18/2022 14:12:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=409
03/18/2022 14:13:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=414
03/18/2022 14:13:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=419
03/18/2022 14:13:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=424
03/18/2022 14:13:12 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.5835835835835835 on epoch=424
03/18/2022 14:13:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=429
03/18/2022 14:13:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
03/18/2022 14:13:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=439
03/18/2022 14:13:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/18/2022 14:13:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=449
03/18/2022 14:13:35 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.5607843137254902 on epoch=449
03/18/2022 14:13:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=454
03/18/2022 14:13:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/18/2022 14:13:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=464
03/18/2022 14:13:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/18/2022 14:13:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/18/2022 14:13:58 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.5901477832512315 on epoch=474
03/18/2022 14:14:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=479
03/18/2022 14:14:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=484
03/18/2022 14:14:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=489
03/18/2022 14:14:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
03/18/2022 14:14:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/18/2022 14:14:21 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.5607843137254902 on epoch=499
03/18/2022 14:14:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
03/18/2022 14:14:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=509
03/18/2022 14:14:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=514
03/18/2022 14:14:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=519
03/18/2022 14:14:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/18/2022 14:14:44 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5607843137254902 on epoch=524
03/18/2022 14:14:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
03/18/2022 14:14:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
03/18/2022 14:14:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
03/18/2022 14:15:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
03/18/2022 14:15:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
03/18/2022 14:15:07 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.5607843137254902 on epoch=549
03/18/2022 14:15:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/18/2022 14:15:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=559
03/18/2022 14:15:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=564
03/18/2022 14:15:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=569
03/18/2022 14:15:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
03/18/2022 14:15:30 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.5835835835835835 on epoch=574
03/18/2022 14:15:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=579
03/18/2022 14:15:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=584
03/18/2022 14:15:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=589
03/18/2022 14:15:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=594
03/18/2022 14:15:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
03/18/2022 14:15:53 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.6666666666666667 on epoch=599
03/18/2022 14:15:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=604
03/18/2022 14:16:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=609
03/18/2022 14:16:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=614
03/18/2022 14:16:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
03/18/2022 14:16:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=624
03/18/2022 14:16:16 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.5307917888563051 on epoch=624
03/18/2022 14:16:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=629
03/18/2022 14:16:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=634
03/18/2022 14:16:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=639
03/18/2022 14:16:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=644
03/18/2022 14:16:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=649
03/18/2022 14:16:39 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.5901477832512315 on epoch=649
03/18/2022 14:16:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=654
03/18/2022 14:16:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=659
03/18/2022 14:16:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/18/2022 14:16:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=669
03/18/2022 14:17:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
03/18/2022 14:17:02 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.625 on epoch=674
03/18/2022 14:17:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
03/18/2022 14:17:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
03/18/2022 14:17:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
03/18/2022 14:17:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=694
03/18/2022 14:17:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=699
03/18/2022 14:17:25 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.6476476476476476 on epoch=699
03/18/2022 14:17:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
03/18/2022 14:17:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=709
03/18/2022 14:17:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=714
03/18/2022 14:17:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
03/18/2022 14:17:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=724
03/18/2022 14:17:48 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.6825396825396826 on epoch=724
03/18/2022 14:17:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=729
03/18/2022 14:17:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=734
03/18/2022 14:18:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=739
03/18/2022 14:18:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=744
03/18/2022 14:18:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
03/18/2022 14:18:11 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.6761133603238867 on epoch=749
03/18/2022 14:18:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=754
03/18/2022 14:18:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=759
03/18/2022 14:18:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=764
03/18/2022 14:18:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=769
03/18/2022 14:18:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/18/2022 14:18:35 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.6536796536796536 on epoch=774
03/18/2022 14:18:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/18/2022 14:18:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=784
03/18/2022 14:18:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/18/2022 14:18:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=794
03/18/2022 14:18:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=799
03/18/2022 14:18:58 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.875 on epoch=799
03/18/2022 14:18:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6825396825396826 -> 0.875 on epoch=799, global_step=1600
03/18/2022 14:19:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=804
03/18/2022 14:19:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/18/2022 14:19:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=814
03/18/2022 14:19:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=819
03/18/2022 14:19:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=824
03/18/2022 14:19:22 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.8745098039215686 on epoch=824
03/18/2022 14:19:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=829
03/18/2022 14:19:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/18/2022 14:19:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=839
03/18/2022 14:19:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=844
03/18/2022 14:19:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=849
03/18/2022 14:19:47 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.906158357771261 on epoch=849
03/18/2022 14:19:47 - INFO - __main__ - Saving model with best Classification-F1: 0.875 -> 0.906158357771261 on epoch=849, global_step=1700
03/18/2022 14:19:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=854
03/18/2022 14:19:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=859
03/18/2022 14:20:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=864
03/18/2022 14:20:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=869
03/18/2022 14:20:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=874
03/18/2022 14:20:12 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.8745098039215686 on epoch=874
03/18/2022 14:20:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=879
03/18/2022 14:20:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=884
03/18/2022 14:20:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=889
03/18/2022 14:20:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=894
03/18/2022 14:20:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=899
03/18/2022 14:20:48 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.873015873015873 on epoch=899
03/18/2022 14:20:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
03/18/2022 14:20:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
03/18/2022 14:21:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
03/18/2022 14:21:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/18/2022 14:21:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/18/2022 14:21:23 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7702564102564102 on epoch=924
03/18/2022 14:21:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
03/18/2022 14:21:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/18/2022 14:21:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
03/18/2022 14:21:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
03/18/2022 14:21:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
03/18/2022 14:21:56 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.873015873015873 on epoch=949
03/18/2022 14:22:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
03/18/2022 14:22:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
03/18/2022 14:22:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
03/18/2022 14:22:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
03/18/2022 14:22:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
03/18/2022 14:22:30 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.873015873015873 on epoch=974
03/18/2022 14:22:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/18/2022 14:22:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
03/18/2022 14:22:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/18/2022 14:22:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
03/18/2022 14:22:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
03/18/2022 14:22:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:22:54 - INFO - __main__ - Printing 3 examples
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 14:22:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:22:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 14:22:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 14:22:54 - INFO - __main__ - Printing 3 examples
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/18/2022 14:22:54 - INFO - __main__ - ['negative']
03/18/2022 14:22:54 - INFO - __main__ - Tokenizing Input ...
03/18/2022 14:22:54 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:22:54 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 14:22:57 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.873015873015873 on epoch=999
03/18/2022 14:22:57 - INFO - __main__ - save last model!
03/18/2022 14:22:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 14:22:57 - INFO - __main__ - Start tokenizing ... 7600 instances
03/18/2022 14:22:57 - INFO - __main__ - Printing 3 examples
03/18/2022 14:22:57 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/18/2022 14:22:57 - INFO - __main__ - ['negative']
03/18/2022 14:22:57 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/18/2022 14:22:57 - INFO - __main__ - ['negative']
03/18/2022 14:22:57 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/18/2022 14:22:57 - INFO - __main__ - ['negative']
03/18/2022 14:22:57 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 14:23:03 - INFO - __main__ - Tokenizing Output ...
03/18/2022 14:23:11 - INFO - __main__ - Loaded 7600 examples from test data
03/18/2022 14:23:12 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 14:23:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 14:23:13 - INFO - __main__ - Starting training!
03/18/2022 15:09:58 - INFO - __main__ - Saved prediction in models/T5-large-reptile-cls2cls-3e-5-2-5000-5e-1-10/singletask-yelp_polarity/yelp_polarity_16_100_0.5_8_predictions.txt
03/18/2022 15:09:58 - INFO - __main__ - Classification-F1 on test data: 0.1757
03/18/2022 15:09:58 - INFO - __main__ - prefix=yelp_polarity_16_100, lr=0.5, bsz=8, dev_performance=0.906158357771261, test_performance=0.1756673577765721
03/18/2022 15:09:58 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.4, bsz=8 ...
03/18/2022 15:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:09:59 - INFO - __main__ - Printing 3 examples
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 15:09:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:09:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 15:09:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:09:59 - INFO - __main__ - Printing 3 examples
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/18/2022 15:09:59 - INFO - __main__ - ['negative']
03/18/2022 15:09:59 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:09:59 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:09:59 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:10:13 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 15:10:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 15:10:14 - INFO - __main__ - Starting training!
03/18/2022 15:10:20 - INFO - __main__ - Step 10 Global step 10 Train loss 8.00 on epoch=4
03/18/2022 15:10:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.17 on epoch=9
03/18/2022 15:10:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
03/18/2022 15:10:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=19
03/18/2022 15:10:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=24
03/18/2022 15:10:40 - INFO - __main__ - Global step 50 Train loss 2.59 Classification-F1 0.5134502923976608 on epoch=24
03/18/2022 15:10:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5134502923976608 on epoch=24, global_step=50
03/18/2022 15:10:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
03/18/2022 15:10:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/18/2022 15:10:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
03/18/2022 15:10:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
03/18/2022 15:11:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
03/18/2022 15:11:03 - INFO - __main__ - Global step 100 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=49
03/18/2022 15:11:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
03/18/2022 15:11:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
03/18/2022 15:11:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
03/18/2022 15:11:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
03/18/2022 15:11:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
03/18/2022 15:11:26 - INFO - __main__ - Global step 150 Train loss 0.34 Classification-F1 0.716256157635468 on epoch=74
03/18/2022 15:11:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.716256157635468 on epoch=74, global_step=150
03/18/2022 15:11:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
03/18/2022 15:11:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
03/18/2022 15:11:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
03/18/2022 15:11:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=94
03/18/2022 15:11:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/18/2022 15:11:49 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.7793103448275862 on epoch=99
03/18/2022 15:11:49 - INFO - __main__ - Saving model with best Classification-F1: 0.716256157635468 -> 0.7793103448275862 on epoch=99, global_step=200
03/18/2022 15:11:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/18/2022 15:11:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/18/2022 15:12:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/18/2022 15:12:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
03/18/2022 15:12:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
03/18/2022 15:12:15 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.906158357771261 on epoch=124
03/18/2022 15:12:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7793103448275862 -> 0.906158357771261 on epoch=124, global_step=250
03/18/2022 15:12:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
03/18/2022 15:12:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/18/2022 15:12:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
03/18/2022 15:12:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.08 on epoch=144
03/18/2022 15:12:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.08 on epoch=149
03/18/2022 15:12:43 - INFO - __main__ - Global step 300 Train loss 0.12 Classification-F1 0.8095238095238095 on epoch=149
03/18/2022 15:12:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.09 on epoch=154
03/18/2022 15:12:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
03/18/2022 15:12:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.08 on epoch=164
03/18/2022 15:13:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.07 on epoch=169
03/18/2022 15:13:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.05 on epoch=174
03/18/2022 15:13:19 - INFO - __main__ - Global step 350 Train loss 0.07 Classification-F1 0.9054187192118226 on epoch=174
03/18/2022 15:13:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.06 on epoch=179
03/18/2022 15:13:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.07 on epoch=184
03/18/2022 15:13:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
03/18/2022 15:13:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
03/18/2022 15:13:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
03/18/2022 15:13:47 - INFO - __main__ - Global step 400 Train loss 0.05 Classification-F1 0.8398398398398398 on epoch=199
03/18/2022 15:13:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
03/18/2022 15:13:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
03/18/2022 15:14:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.02 on epoch=214
03/18/2022 15:14:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
03/18/2022 15:14:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.03 on epoch=224
03/18/2022 15:14:21 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.9375 on epoch=224
03/18/2022 15:14:21 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9375 on epoch=224, global_step=450
03/18/2022 15:14:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/18/2022 15:14:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/18/2022 15:14:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
03/18/2022 15:14:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.03 on epoch=244
03/18/2022 15:14:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
03/18/2022 15:14:48 - INFO - __main__ - Global step 500 Train loss 0.03 Classification-F1 0.9054187192118226 on epoch=249
03/18/2022 15:14:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/18/2022 15:14:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.01 on epoch=259
03/18/2022 15:15:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/18/2022 15:15:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/18/2022 15:15:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/18/2022 15:15:14 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.873015873015873 on epoch=274
03/18/2022 15:15:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
03/18/2022 15:15:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/18/2022 15:15:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/18/2022 15:15:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/18/2022 15:15:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/18/2022 15:15:42 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.9054187192118226 on epoch=299
03/18/2022 15:15:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
03/18/2022 15:15:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/18/2022 15:15:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/18/2022 15:15:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/18/2022 15:16:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
03/18/2022 15:16:07 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.9054187192118226 on epoch=324
03/18/2022 15:16:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/18/2022 15:16:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/18/2022 15:16:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/18/2022 15:16:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/18/2022 15:16:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/18/2022 15:16:34 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=349
03/18/2022 15:16:34 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=349, global_step=700
03/18/2022 15:16:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/18/2022 15:16:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/18/2022 15:16:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/18/2022 15:16:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/18/2022 15:16:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/18/2022 15:17:03 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.8398398398398398 on epoch=374
03/18/2022 15:17:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
03/18/2022 15:17:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/18/2022 15:17:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/18/2022 15:17:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/18/2022 15:17:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/18/2022 15:17:29 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=399
03/18/2022 15:17:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
03/18/2022 15:17:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/18/2022 15:17:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/18/2022 15:17:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/18/2022 15:17:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/18/2022 15:17:57 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=424
03/18/2022 15:18:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/18/2022 15:18:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/18/2022 15:18:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
03/18/2022 15:18:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/18/2022 15:18:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=449
03/18/2022 15:18:23 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.9687194525904204 on epoch=449
03/18/2022 15:18:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/18/2022 15:18:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/18/2022 15:18:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/18/2022 15:18:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/18/2022 15:18:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/18/2022 15:18:52 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/18/2022 15:18:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/18/2022 15:19:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/18/2022 15:19:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/18/2022 15:19:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/18/2022 15:19:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/18/2022 15:19:21 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=499
03/18/2022 15:19:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/18/2022 15:19:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/18/2022 15:19:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/18/2022 15:19:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/18/2022 15:19:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/18/2022 15:19:49 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/18/2022 15:19:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/18/2022 15:19:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
03/18/2022 15:20:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/18/2022 15:20:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/18/2022 15:20:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/18/2022 15:20:15 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=549
03/18/2022 15:20:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/18/2022 15:20:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/18/2022 15:20:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/18/2022 15:20:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/18/2022 15:20:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/18/2022 15:20:44 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/18/2022 15:20:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/18/2022 15:20:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/18/2022 15:20:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/18/2022 15:21:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/18/2022 15:21:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/18/2022 15:21:12 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/18/2022 15:21:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/18/2022 15:21:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/18/2022 15:21:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/18/2022 15:21:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/18/2022 15:21:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/18/2022 15:21:39 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/18/2022 15:21:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/18/2022 15:21:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/18/2022 15:21:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/18/2022 15:21:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/18/2022 15:22:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/18/2022 15:22:06 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/18/2022 15:22:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/18/2022 15:22:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/18/2022 15:22:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/18/2022 15:22:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/18/2022 15:22:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/18/2022 15:22:33 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/18/2022 15:22:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/18/2022 15:22:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/18/2022 15:22:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/18/2022 15:22:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/18/2022 15:22:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/18/2022 15:23:00 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9375 on epoch=699
03/18/2022 15:23:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/18/2022 15:23:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/18/2022 15:23:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/18/2022 15:23:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/18/2022 15:23:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/18/2022 15:23:26 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9375 on epoch=724
03/18/2022 15:23:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/18/2022 15:23:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/18/2022 15:23:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/18/2022 15:23:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/18/2022 15:23:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/18/2022 15:23:52 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/18/2022 15:23:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/18/2022 15:24:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/18/2022 15:24:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/18/2022 15:24:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/18/2022 15:24:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/18/2022 15:24:20 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/18/2022 15:24:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/18/2022 15:24:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/18/2022 15:24:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/18/2022 15:24:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/18/2022 15:24:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/18/2022 15:24:47 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9375 on epoch=799
03/18/2022 15:24:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/18/2022 15:24:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/18/2022 15:25:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/18/2022 15:25:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/18/2022 15:25:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/18/2022 15:25:14 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/18/2022 15:25:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/18/2022 15:25:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/18/2022 15:25:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/18/2022 15:25:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/18/2022 15:25:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/18/2022 15:25:52 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9375 on epoch=849
03/18/2022 15:25:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/18/2022 15:26:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/18/2022 15:26:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/18/2022 15:26:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/18/2022 15:26:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/18/2022 15:26:17 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9375 on epoch=874
03/18/2022 15:26:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/18/2022 15:26:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/18/2022 15:26:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/18/2022 15:26:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/18/2022 15:26:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/18/2022 15:26:42 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=899
03/18/2022 15:26:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/18/2022 15:26:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/18/2022 15:26:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/18/2022 15:27:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/18/2022 15:27:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/18/2022 15:27:07 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=924
03/18/2022 15:27:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/18/2022 15:27:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/18/2022 15:27:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/18/2022 15:27:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/18/2022 15:27:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/18/2022 15:27:32 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.9375 on epoch=949
03/18/2022 15:27:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/18/2022 15:27:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/18/2022 15:27:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/18/2022 15:27:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/18/2022 15:27:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/18/2022 15:27:57 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/18/2022 15:28:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/18/2022 15:28:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/18/2022 15:28:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/18/2022 15:28:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/18/2022 15:28:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/18/2022 15:28:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:28:22 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] Soggy pizza, $19 burger with no side, 5% concession fee whatever. This place is SO overpriced, and the food is McDonald's just way overpriced.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] I originally went to the AZ Heart Institute due to fainting spells and periods of breathlessness when I wasn't moving.  In addition to an extremely abrupt manner - saying things like, \""Doesn't matter.  Nope.  Doesn't matter,\"" when I told him about my family history of heart problems (didn't matter because it was my uncle, not my parents.  Because there's no way that my grandmother could have passed the defective heart gene to my mother, who could have passed it to me without showing symptoms herself.  And the fact that my father at the time had heart issues, eventually dying from a heart attack, was inconsequential.), the doctor seemed extremely bored and had a \""why are you wasting my time\"" demeanor.  I was wasting his time because I was in my early twenties, but was having chest pains, hard thumps in my chest, and was passing out in clusters.  The icing on the cake however, and the reason I left and never came back, was when he shut the door and it bounced slightly open, and I was able to hear him making fun of me to a nurse right outside the door.  Humiliating, to say the least.  Now, some years later, I continue to have clusters of fainting, gasping for air, pounding in my chest, and memory problems.  I'm in the process of having the issue diagnosed with another cardiology practice, one who actually listens and has the courtesy to at least wait until I leave to snort and laugh about whether or not you can believe this chick.  I would never ever return to this place, nor would I ever recommend anyone go there.  As a matter of fact, I have steered a couple of friends away from the AZ Heart Institute due to my experience.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] I have just came here for staying two days. Feel so upset because of customer service. Room is not as clean as I thought. There is no reason to stay here, but to watch mystere.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/18/2022 15:28:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/18/2022 15:28:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/18/2022 15:28:22 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] My wife and I brought round trip tickets from  and to McCarran.  The person at the desk informed me that I do not have to call for pick-up service because it is noted on the ticket and to be sure to be at the designated location by 11:30 PM. The night of my pick-up we went to the designated pick up area at 11:20 PM.  At 11:26 PM I called just to check if the shuttle would be on time.  The dispatcher informed me that they are running about 5 to 10 minutes late.  At 11:40 PM I called back and asked for an update.  The dispatcher informed me that the shuttle driver came by at 11:35 PM and no one was there.  I informed the dispatcher that we have been out front from 11:20 PM and the door man of the hotel was out front since 11:00 PM and no airport shuttle came by. We had to catch a cab to the airport.  If they can lie about this,  I hate to think what else they would do.  I will attempt to get reimbursement for my cab fare which I think they will not do.  A follow up review to follow after I talk to them.   Paul Y.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] The snacks are more expensive than the harkins... And the seats look nice but are hella uncomfortable.  I mean go-to-the-chiropractor uncomfortable.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] Ok.... the two stars are ONLY because of the mac n cheese dish .... WHAT HAPPENED??? The first time I tried the mac n cheese I was in love!  I have been to scooter's two more times since my first review and I gotta say the mac n cheese was a complete disappointment! Very bland and watered down .... no flavor at all! I don't know if they have changed cooks? The recipe?  Blah!  Ok .... done with my rant! lol   Outside of the mac n cheese disaster, scooters is still one of my favorite places to go for a drink! and all the other dishes I have tried have been top notch!
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ - Tokenizing Input ...
03/18/2022 15:28:22 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:22 - INFO - __main__ - Loaded 32 examples from dev data
03/18/2022 15:28:22 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9375 on epoch=999
03/18/2022 15:28:22 - INFO - __main__ - save last model!
03/18/2022 15:28:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/18/2022 15:28:22 - INFO - __main__ - Start tokenizing ... 7600 instances
03/18/2022 15:28:22 - INFO - __main__ - Printing 3 examples
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] I expected the prices of the entrees to be a little bit higher but the quality of the Chinese food was not worth the money I paid for the dishes. I got the 18 monk noodle and the traditional dimsum. If I could describe the food  in one word-terrible! Making the dimsum look pretty by topping it with gold flakes did not do anything to make up for the flavor of the dimsum. It  seemed too starchy and you can hardly taste the meat. The noodles looked like a sad , greasy slop of Mai fun type noodles (noodles were stuck together) saturated with soy sauce for color, and garnished with a few pieces of shitake mushrooms, green onions and fine threads of carrots. And yes, portions were small, but that's not really the worst part of the whole experience. Just poorly prepared, way overpriced Chinese food...sorry.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] Review of Buffet:  UGH!  It was very very underwhelming.   Maybe regular menu is great, but do not get the buffet IMHO.  About half the restaurant was eating the buffet... unfortunately I was in the idiot half of the crowd.
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ -  [yelp_polarity] If you value your life, don't go to Banner Boswell.  My husband was told to go to the ER by his doctor's office.  He arrived at Boswell around 10 a.m.  He is diabetic and has heart and high blood pressure problems.  When I arrived at 2 in the afternoon, his blood pressure was 177/87, and he had not been offered anything to eat. I told the staff that he needed his afternoon blood pressure meds.  They said I had to wait for the doctor.  The doctor came and examined him and ordered a nitroglycerine patch and left.  An hour later, he had still not received blood pressure medication and his blood pressure was still alarmingly elevated.  I talked to the nurse who told me that the doctor hadn't ordered any medication as he was afraid that with the nitro it would lower his blood pressure too low.  I had to leave at 4 p.m. for a doctor's appointment.  My husband called me at 6:45 p.m. to say that he had been given a room.  His blood pressure was still sky high; he had still not been offered food or blood pressure lowering meds.  I talked to him several more times trying to calm him down.  At 9:30 p.m. he still hadn't received medication or food.   The next morning when I went to visit, they had him on a sodium chloride drip.  I don't understand why they would be giving sodium when his blood pressure often shoots up to over 200.  He could have been given water to drink if he was dehydrated.    We will never go to this hospital again
03/18/2022 15:28:22 - INFO - __main__ - ['negative']
03/18/2022 15:28:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/18/2022 15:28:29 - INFO - __main__ - Tokenizing Output ...
03/18/2022 15:28:36 - INFO - __main__ - Loaded 7600 examples from test data
03/18/2022 15:28:37 - INFO - __main__ - load prompt embedding from ckpt
03/18/2022 15:28:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/18/2022 15:28:38 - INFO - __main__ - Starting training!
