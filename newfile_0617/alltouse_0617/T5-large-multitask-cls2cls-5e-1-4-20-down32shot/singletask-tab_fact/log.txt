05/23/2022 18:53:35 - INFO - __main__ - Namespace(task_dir='data_32/tab_fact/', task_name='tab_fact', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/23/2022 18:53:35 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact
05/23/2022 18:53:35 - INFO - __main__ - Namespace(task_dir='data_32/tab_fact/', task_name='tab_fact', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/23/2022 18:53:35 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact
05/23/2022 18:53:37 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/23/2022 18:53:37 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/23/2022 18:53:37 - INFO - __main__ - args.device: cuda:0
05/23/2022 18:53:37 - INFO - __main__ - Using 2 gpus
05/23/2022 18:53:37 - INFO - __main__ - args.device: cuda:1
05/23/2022 18:53:37 - INFO - __main__ - Using 2 gpus
05/23/2022 18:53:37 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_32_100', 'tab_fact_32_13', 'tab_fact_32_21', 'tab_fact_32_42', 'tab_fact_32_87']
05/23/2022 18:53:37 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_32_100', 'tab_fact_32_13', 'tab_fact_32_21', 'tab_fact_32_42', 'tab_fact_32_87']
05/23/2022 18:53:41 - INFO - __main__ - Running ... prefix=tab_fact_32_100, lr=0.5, bsz=8 ...
05/23/2022 18:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 18:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 18:53:42 - INFO - __main__ - Printing 3 examples
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 18:53:42 - INFO - __main__ - Printing 3 examples
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 18:53:42 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 18:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 18:53:42 - INFO - __main__ - Printing 3 examples
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 18:53:42 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 18:53:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 18:53:42 - INFO - __main__ - Printing 3 examples
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 18:53:42 - INFO - __main__ - ['refuted']
05/23/2022 18:53:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 18:53:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 18:53:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 18:53:43 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 18:53:43 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 18:54:00 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 18:54:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 18:54:01 - INFO - __main__ - Starting training!
05/23/2022 18:54:02 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 18:54:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 18:54:08 - INFO - __main__ - Starting training!
05/23/2022 18:54:14 - INFO - __main__ - Step 10 Global step 10 Train loss 4.31 on epoch=2
05/23/2022 18:54:18 - INFO - __main__ - Step 20 Global step 20 Train loss 2.49 on epoch=4
05/23/2022 18:54:23 - INFO - __main__ - Step 30 Global step 30 Train loss 1.59 on epoch=7
05/23/2022 18:54:27 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=9
05/23/2022 18:54:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=12
05/23/2022 18:54:34 - INFO - __main__ - Global step 50 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=12
05/23/2022 18:54:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/23/2022 18:54:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=14
05/23/2022 18:54:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=17
05/23/2022 18:54:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.33 on epoch=19
05/23/2022 18:54:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=22
05/23/2022 18:54:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=24
05/23/2022 18:54:59 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 18:55:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=27
05/23/2022 18:55:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=29
05/23/2022 18:55:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=32
05/23/2022 18:55:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=34
05/23/2022 18:55:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=37
05/23/2022 18:55:24 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 18:55:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=39
05/23/2022 18:55:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=42
05/23/2022 18:55:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=44
05/23/2022 18:55:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=47
05/23/2022 18:55:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=49
05/23/2022 18:55:49 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 18:55:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=52
05/23/2022 18:55:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=54
05/23/2022 18:56:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=57
05/23/2022 18:56:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=59
05/23/2022 18:56:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=62
05/23/2022 18:56:14 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 18:56:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=64
05/23/2022 18:56:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=67
05/23/2022 18:56:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=69
05/23/2022 18:56:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=72
05/23/2022 18:56:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=74
05/23/2022 18:56:40 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 18:56:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=77
05/23/2022 18:56:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=79
05/23/2022 18:56:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=82
05/23/2022 18:56:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=84
05/23/2022 18:57:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
05/23/2022 18:57:05 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 18:57:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.19 on epoch=89
05/23/2022 18:57:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=92
05/23/2022 18:57:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=94
05/23/2022 18:57:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/23/2022 18:57:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
05/23/2022 18:57:30 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 18:57:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/23/2022 18:57:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/23/2022 18:57:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=107
05/23/2022 18:57:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/23/2022 18:57:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
05/23/2022 18:57:55 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
05/23/2022 18:57:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
05/23/2022 18:58:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/23/2022 18:58:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
05/23/2022 18:58:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
05/23/2022 18:58:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/23/2022 18:58:20 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.32631578947368417 on epoch=124
05/23/2022 18:58:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/23/2022 18:58:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/23/2022 18:58:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=132
05/23/2022 18:58:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/23/2022 18:58:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/23/2022 18:58:45 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.4330011074197121 on epoch=137
05/23/2022 18:58:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4330011074197121 on epoch=137, global_step=550
05/23/2022 18:58:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=139
05/23/2022 18:58:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
05/23/2022 18:58:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
05/23/2022 18:59:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=147
05/23/2022 18:59:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
05/23/2022 18:59:10 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.51417004048583 on epoch=149
05/23/2022 18:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4330011074197121 -> 0.51417004048583 on epoch=149, global_step=600
05/23/2022 18:59:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/23/2022 18:59:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
05/23/2022 18:59:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/23/2022 18:59:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
05/23/2022 18:59:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=162
05/23/2022 18:59:35 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.3915298184961106 on epoch=162
05/23/2022 18:59:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/23/2022 18:59:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=167
05/23/2022 18:59:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=169
05/23/2022 18:59:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
05/23/2022 18:59:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/23/2022 19:00:00 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.51417004048583 on epoch=174
05/23/2022 19:00:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/23/2022 19:00:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
05/23/2022 19:00:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=182
05/23/2022 19:00:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
05/23/2022 19:00:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
05/23/2022 19:00:25 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.21833508956796627 on epoch=187
05/23/2022 19:00:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/23/2022 19:00:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/23/2022 19:00:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=194
05/23/2022 19:00:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=197
05/23/2022 19:00:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=199
05/23/2022 19:00:50 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.34575260804769004 on epoch=199
05/23/2022 19:00:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/23/2022 19:00:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
05/23/2022 19:01:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
05/23/2022 19:01:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=209
05/23/2022 19:01:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
05/23/2022 19:01:15 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.2168464304057524 on epoch=212
05/23/2022 19:01:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/23/2022 19:01:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
05/23/2022 19:01:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
05/23/2022 19:01:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/23/2022 19:01:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/23/2022 19:01:40 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.14308498519024834 on epoch=224
05/23/2022 19:01:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
05/23/2022 19:01:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/23/2022 19:01:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/23/2022 19:01:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
05/23/2022 19:02:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/23/2022 19:02:05 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.3777115416459678 on epoch=237
05/23/2022 19:02:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
05/23/2022 19:02:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/23/2022 19:02:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/23/2022 19:02:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
05/23/2022 19:02:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/23/2022 19:02:31 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.5755342667649226 on epoch=249
05/23/2022 19:02:31 - INFO - __main__ - Saving model with best Classification-F1: 0.51417004048583 -> 0.5755342667649226 on epoch=249, global_step=1000
05/23/2022 19:02:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/23/2022 19:02:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
05/23/2022 19:02:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/23/2022 19:02:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
05/23/2022 19:02:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
05/23/2022 19:02:56 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.14391478431668808 on epoch=262
05/23/2022 19:03:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/23/2022 19:03:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
05/23/2022 19:03:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
05/23/2022 19:03:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/23/2022 19:03:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
05/23/2022 19:03:21 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.2175115207373272 on epoch=274
05/23/2022 19:03:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/23/2022 19:03:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
05/23/2022 19:03:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
05/23/2022 19:03:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/23/2022 19:03:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 19:03:46 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.2696572580645161 on epoch=287
05/23/2022 19:03:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 19:03:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/23/2022 19:03:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/23/2022 19:04:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/23/2022 19:04:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 19:04:11 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.160625814863103 on epoch=299
05/23/2022 19:04:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 19:04:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/23/2022 19:04:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/23/2022 19:04:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/23/2022 19:04:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/23/2022 19:04:36 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.20308800823468864 on epoch=312
05/23/2022 19:04:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/23/2022 19:04:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/23/2022 19:04:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/23/2022 19:04:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/23/2022 19:04:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/23/2022 19:05:01 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.20481310803891448 on epoch=324
05/23/2022 19:05:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
05/23/2022 19:05:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/23/2022 19:05:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 19:05:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/23/2022 19:05:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/23/2022 19:05:26 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.18597131681877443 on epoch=337
05/23/2022 19:05:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/23/2022 19:05:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/23/2022 19:05:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 19:05:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/23/2022 19:05:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/23/2022 19:05:51 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.13900226757369613 on epoch=349
05/23/2022 19:05:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 19:06:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 19:06:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 19:06:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 19:06:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/23/2022 19:06:17 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.1237183868762816 on epoch=362
05/23/2022 19:06:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/23/2022 19:06:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 19:06:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
05/23/2022 19:06:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/23/2022 19:06:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/23/2022 19:06:42 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.18180836707152498 on epoch=374
05/23/2022 19:06:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/23/2022 19:06:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/23/2022 19:06:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/23/2022 19:06:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/23/2022 19:07:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/23/2022 19:07:07 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.18724082934609249 on epoch=387
05/23/2022 19:07:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/23/2022 19:07:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 19:07:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/23/2022 19:07:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 19:07:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 19:07:32 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.12605042016806722 on epoch=399
05/23/2022 19:07:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 19:07:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 19:07:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/23/2022 19:07:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 19:07:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 19:07:57 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.16968864468864467 on epoch=412
05/23/2022 19:08:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 19:08:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/23/2022 19:08:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 19:08:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 19:08:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 19:08:22 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.19014492753623186 on epoch=424
05/23/2022 19:08:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 19:08:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 19:08:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 19:08:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 19:08:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 19:08:47 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.14303959131545338 on epoch=437
05/23/2022 19:08:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 19:08:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 19:09:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/23/2022 19:09:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/23/2022 19:09:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 19:09:12 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.13048004626951995 on epoch=449
05/23/2022 19:09:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 19:09:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 19:09:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/23/2022 19:09:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 19:09:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 19:09:38 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.19953063885267275 on epoch=462
05/23/2022 19:09:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 19:09:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 19:09:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 19:09:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 19:10:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 19:10:03 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.1451388888888889 on epoch=474
05/23/2022 19:10:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 19:10:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 19:10:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/23/2022 19:10:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 19:10:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 19:10:28 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.17899590163934426 on epoch=487
05/23/2022 19:10:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 19:10:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/23/2022 19:10:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 19:10:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 19:10:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 19:10:53 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.10683936403508773 on epoch=499
05/23/2022 19:10:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/23/2022 19:11:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 19:11:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 19:11:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/23/2022 19:11:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 19:11:18 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.10606060606060606 on epoch=512
05/23/2022 19:11:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 19:11:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 19:11:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 19:11:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 19:11:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 19:11:43 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.15223665223665225 on epoch=524
05/23/2022 19:11:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 19:11:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/23/2022 19:11:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 19:12:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/23/2022 19:12:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 19:12:08 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.18434782608695655 on epoch=537
05/23/2022 19:12:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 19:12:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 19:12:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 19:12:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 19:12:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/23/2022 19:12:33 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.18149253731343282 on epoch=549
05/23/2022 19:12:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 19:12:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 19:12:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 19:12:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 19:12:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/23/2022 19:12:58 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.1041261461517088 on epoch=562
05/23/2022 19:13:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 19:13:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 19:13:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 19:13:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 19:13:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/23/2022 19:13:24 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.10817575083426029 on epoch=574
05/23/2022 19:13:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 19:13:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 19:13:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 19:13:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/23/2022 19:13:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 19:13:49 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.10509950248756217 on epoch=587
05/23/2022 19:13:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 19:13:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 19:14:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 19:14:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 19:14:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 19:14:14 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.1146414423154661 on epoch=599
05/23/2022 19:14:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 19:14:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 19:14:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/23/2022 19:14:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 19:14:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/23/2022 19:14:39 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.09659412097230075 on epoch=612
05/23/2022 19:14:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 19:14:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 19:14:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 19:14:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/23/2022 19:15:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 19:15:04 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.10761278195488722 on epoch=624
05/23/2022 19:15:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 19:15:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 19:15:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
05/23/2022 19:15:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 19:15:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/23/2022 19:15:29 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.12181259600614439 on epoch=637
05/23/2022 19:15:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 19:15:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 19:15:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 19:15:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 19:15:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/23/2022 19:15:54 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.14592161016949154 on epoch=649
05/23/2022 19:15:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 19:16:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/23/2022 19:16:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 19:16:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 19:16:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 19:16:19 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.10984848484848485 on epoch=662
05/23/2022 19:16:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 19:16:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=667
05/23/2022 19:16:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 19:16:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 19:16:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 19:16:44 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.14588859416445624 on epoch=674
05/23/2022 19:16:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 19:16:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 19:16:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 19:17:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 19:17:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/23/2022 19:17:09 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.09510489510489512 on epoch=687
05/23/2022 19:17:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 19:17:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 19:17:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 19:17:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 19:17:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 19:17:34 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.1279060665362035 on epoch=699
05/23/2022 19:17:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/23/2022 19:17:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 19:17:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/23/2022 19:17:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 19:17:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 19:17:58 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.15220458553791885 on epoch=712
05/23/2022 19:18:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/23/2022 19:18:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 19:18:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/23/2022 19:18:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 19:18:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 19:18:23 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.1714285714285714 on epoch=724
05/23/2022 19:18:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 19:18:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 19:18:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 19:18:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/23/2022 19:18:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 19:18:49 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2214912280701754 on epoch=737
05/23/2022 19:18:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 19:18:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 19:19:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 19:19:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
05/23/2022 19:19:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/23/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:19:12 - INFO - __main__ - Printing 3 examples
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:19:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:19:12 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 19:19:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:19:12 - INFO - __main__ - Printing 3 examples
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 19:19:12 - INFO - __main__ - ['refuted']
05/23/2022 19:19:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:19:13 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:19:13 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 19:19:14 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.11342701014832161 on epoch=749
05/23/2022 19:19:14 - INFO - __main__ - save last model!
05/23/2022 19:19:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 19:19:14 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 19:19:14 - INFO - __main__ - Printing 3 examples
05/23/2022 19:19:14 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:19:14 - INFO - __main__ - ['entailed']
05/23/2022 19:19:14 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:19:14 - INFO - __main__ - ['entailed']
05/23/2022 19:19:14 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:19:14 - INFO - __main__ - ['entailed']
05/23/2022 19:19:14 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:19:31 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 19:19:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 19:19:32 - INFO - __main__ - Starting training!
05/23/2022 19:19:38 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:19:50 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 19:28:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_100_0.5_8_predictions.txt
05/23/2022 19:28:35 - INFO - __main__ - Classification-F1 on test data: 0.0048
05/23/2022 19:28:36 - INFO - __main__ - prefix=tab_fact_32_100, lr=0.5, bsz=8, dev_performance=0.5755342667649226, test_performance=0.004767432371835323
05/23/2022 19:28:36 - INFO - __main__ - Running ... prefix=tab_fact_32_100, lr=0.4, bsz=8 ...
05/23/2022 19:28:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:28:36 - INFO - __main__ - Printing 3 examples
05/23/2022 19:28:36 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 19:28:36 - INFO - __main__ - ['refuted']
05/23/2022 19:28:36 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 19:28:36 - INFO - __main__ - ['refuted']
05/23/2022 19:28:36 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 19:28:36 - INFO - __main__ - ['refuted']
05/23/2022 19:28:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:28:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:28:37 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 19:28:37 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:28:37 - INFO - __main__ - Printing 3 examples
05/23/2022 19:28:37 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 19:28:37 - INFO - __main__ - ['refuted']
05/23/2022 19:28:37 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 19:28:37 - INFO - __main__ - ['refuted']
05/23/2022 19:28:37 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 19:28:37 - INFO - __main__ - ['refuted']
05/23/2022 19:28:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:28:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:28:37 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 19:28:51 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 19:28:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 19:28:52 - INFO - __main__ - Starting training!
05/23/2022 19:28:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.94 on epoch=2
05/23/2022 19:29:01 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=4
05/23/2022 19:29:06 - INFO - __main__ - Step 30 Global step 30 Train loss 2.09 on epoch=7
05/23/2022 19:29:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.48 on epoch=9
05/23/2022 19:29:15 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=12
05/23/2022 19:29:18 - INFO - __main__ - Global step 50 Train loss 2.53 Classification-F1 0.3333333333333333 on epoch=12
05/23/2022 19:29:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/23/2022 19:29:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=14
05/23/2022 19:29:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=17
05/23/2022 19:29:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=19
05/23/2022 19:29:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=22
05/23/2022 19:29:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=24
05/23/2022 19:29:43 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.3671451355661882 on epoch=24
05/23/2022 19:29:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3671451355661882 on epoch=24, global_step=100
05/23/2022 19:29:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=27
05/23/2022 19:29:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=29
05/23/2022 19:29:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=32
05/23/2022 19:30:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=34
05/23/2022 19:30:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=37
05/23/2022 19:30:08 - INFO - __main__ - Global step 150 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 19:30:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=39
05/23/2022 19:30:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=42
05/23/2022 19:30:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=44
05/23/2022 19:30:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=47
05/23/2022 19:30:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=49
05/23/2022 19:30:33 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 19:30:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=52
05/23/2022 19:30:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=54
05/23/2022 19:30:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=57
05/23/2022 19:30:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=59
05/23/2022 19:30:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=62
05/23/2022 19:30:58 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 19:31:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=64
05/23/2022 19:31:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=67
05/23/2022 19:31:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=69
05/23/2022 19:31:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=72
05/23/2022 19:31:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/23/2022 19:31:24 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3671451355661882 on epoch=74
05/23/2022 19:31:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=77
05/23/2022 19:31:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/23/2022 19:31:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=82
05/23/2022 19:31:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=84
05/23/2022 19:31:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
05/23/2022 19:31:49 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3671451355661882 on epoch=87
05/23/2022 19:31:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=89
05/23/2022 19:31:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=92
05/23/2022 19:32:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=94
05/23/2022 19:32:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=97
05/23/2022 19:32:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/23/2022 19:32:14 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 19:32:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/23/2022 19:32:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
05/23/2022 19:32:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/23/2022 19:32:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=109
05/23/2022 19:32:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
05/23/2022 19:32:39 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3992490613266583 on epoch=112
05/23/2022 19:32:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.3992490613266583 on epoch=112, global_step=450
05/23/2022 19:32:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=114
05/23/2022 19:32:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=117
05/23/2022 19:32:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/23/2022 19:32:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=122
05/23/2022 19:33:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/23/2022 19:33:04 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.4589371980676329 on epoch=124
05/23/2022 19:33:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4589371980676329 on epoch=124, global_step=500
05/23/2022 19:33:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/23/2022 19:33:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=129
05/23/2022 19:33:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=132
05/23/2022 19:33:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
05/23/2022 19:33:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=137
05/23/2022 19:33:29 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.5733333333333335 on epoch=137
05/23/2022 19:33:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5733333333333335 on epoch=137, global_step=550
05/23/2022 19:33:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=139
05/23/2022 19:33:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
05/23/2022 19:33:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=144
05/23/2022 19:33:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
05/23/2022 19:33:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=149
05/23/2022 19:33:54 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.5405128205128205 on epoch=149
05/23/2022 19:33:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/23/2022 19:34:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=154
05/23/2022 19:34:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=157
05/23/2022 19:34:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
05/23/2022 19:34:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=162
05/23/2022 19:34:20 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.5333333333333333 on epoch=162
05/23/2022 19:34:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
05/23/2022 19:34:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/23/2022 19:34:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=169
05/23/2022 19:34:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
05/23/2022 19:34:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/23/2022 19:34:45 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.5755342667649226 on epoch=174
05/23/2022 19:34:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.5755342667649226 on epoch=174, global_step=700
05/23/2022 19:34:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=177
05/23/2022 19:34:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=179
05/23/2022 19:34:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=182
05/23/2022 19:35:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
05/23/2022 19:35:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=187
05/23/2022 19:35:10 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.38496240601503756 on epoch=187
05/23/2022 19:35:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
05/23/2022 19:35:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
05/23/2022 19:35:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/23/2022 19:35:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=197
05/23/2022 19:35:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
05/23/2022 19:35:35 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.269609079445145 on epoch=199
05/23/2022 19:35:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
05/23/2022 19:35:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
05/23/2022 19:35:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
05/23/2022 19:35:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=209
05/23/2022 19:35:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
05/23/2022 19:36:00 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.3776674937965261 on epoch=212
05/23/2022 19:36:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/23/2022 19:36:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=217
05/23/2022 19:36:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/23/2022 19:36:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/23/2022 19:36:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
05/23/2022 19:36:25 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.2537200504413619 on epoch=224
05/23/2022 19:36:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/23/2022 19:36:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/23/2022 19:36:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/23/2022 19:36:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
05/23/2022 19:36:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
05/23/2022 19:36:51 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.26395289298515107 on epoch=237
05/23/2022 19:36:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
05/23/2022 19:37:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
05/23/2022 19:37:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
05/23/2022 19:37:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
05/23/2022 19:37:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
05/23/2022 19:37:16 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.27336999214454044 on epoch=249
05/23/2022 19:37:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=252
05/23/2022 19:37:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/23/2022 19:37:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
05/23/2022 19:37:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
05/23/2022 19:37:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
05/23/2022 19:37:41 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.2765064836003051 on epoch=262
05/23/2022 19:37:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
05/23/2022 19:37:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/23/2022 19:37:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
05/23/2022 19:37:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
05/23/2022 19:38:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/23/2022 19:38:06 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.36739417989417983 on epoch=274
05/23/2022 19:38:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
05/23/2022 19:38:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
05/23/2022 19:38:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
05/23/2022 19:38:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
05/23/2022 19:38:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/23/2022 19:38:31 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.268909688843916 on epoch=287
05/23/2022 19:38:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/23/2022 19:38:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/23/2022 19:38:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/23/2022 19:38:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
05/23/2022 19:38:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
05/23/2022 19:38:57 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.3387096774193548 on epoch=299
05/23/2022 19:39:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/23/2022 19:39:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/23/2022 19:39:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/23/2022 19:39:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/23/2022 19:39:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=312
05/23/2022 19:39:22 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.3356216628527841 on epoch=312
05/23/2022 19:39:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/23/2022 19:39:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/23/2022 19:39:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/23/2022 19:39:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/23/2022 19:39:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/23/2022 19:39:47 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.27972207925887804 on epoch=324
05/23/2022 19:39:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
05/23/2022 19:39:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
05/23/2022 19:40:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
05/23/2022 19:40:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/23/2022 19:40:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/23/2022 19:40:12 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=337
05/23/2022 19:40:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/23/2022 19:40:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/23/2022 19:40:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/23/2022 19:40:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 19:40:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/23/2022 19:40:37 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.3519450033534541 on epoch=349
05/23/2022 19:40:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/23/2022 19:40:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/23/2022 19:40:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/23/2022 19:40:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/23/2022 19:41:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/23/2022 19:41:02 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.25396825396825395 on epoch=362
05/23/2022 19:41:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/23/2022 19:41:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 19:41:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/23/2022 19:41:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/23/2022 19:41:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/23/2022 19:41:27 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.3328092243186583 on epoch=374
05/23/2022 19:41:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/23/2022 19:41:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/23/2022 19:41:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
05/23/2022 19:41:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/23/2022 19:41:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 19:41:53 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.18404312668463613 on epoch=387
05/23/2022 19:41:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/23/2022 19:42:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/23/2022 19:42:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/23/2022 19:42:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/23/2022 19:42:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 19:42:18 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.31746031746031744 on epoch=399
05/23/2022 19:42:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/23/2022 19:42:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 19:42:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/23/2022 19:42:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/23/2022 19:42:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/23/2022 19:42:43 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.3247097844112769 on epoch=412
05/23/2022 19:42:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/23/2022 19:42:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/23/2022 19:42:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/23/2022 19:43:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/23/2022 19:43:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/23/2022 19:43:08 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.2463973237261966 on epoch=424
05/23/2022 19:43:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/23/2022 19:43:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 19:43:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/23/2022 19:43:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/23/2022 19:43:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 19:43:33 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.1768834399431414 on epoch=437
05/23/2022 19:43:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/23/2022 19:43:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 19:43:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/23/2022 19:43:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 19:43:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/23/2022 19:43:58 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.15586781160551652 on epoch=449
05/23/2022 19:44:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/23/2022 19:44:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/23/2022 19:44:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/23/2022 19:44:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 19:44:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/23/2022 19:44:23 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.3253968253968254 on epoch=462
05/23/2022 19:44:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/23/2022 19:44:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/23/2022 19:44:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/23/2022 19:44:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 19:44:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
05/23/2022 19:44:48 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.15586781160551652 on epoch=474
05/23/2022 19:44:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 19:44:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 19:45:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/23/2022 19:45:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 19:45:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 19:45:14 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5126504544338 on epoch=487
05/23/2022 19:45:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/23/2022 19:45:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 19:45:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/23/2022 19:45:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/23/2022 19:45:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/23/2022 19:45:39 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.1395833333333333 on epoch=499
05/23/2022 19:45:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 19:45:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/23/2022 19:45:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/23/2022 19:45:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 19:46:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/23/2022 19:46:04 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.22954545454545455 on epoch=512
05/23/2022 19:46:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/23/2022 19:46:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/23/2022 19:46:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/23/2022 19:46:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/23/2022 19:46:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 19:46:29 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.22560499609679935 on epoch=524
05/23/2022 19:46:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 19:46:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 19:46:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/23/2022 19:46:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/23/2022 19:46:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/23/2022 19:46:54 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.14629629629629629 on epoch=537
05/23/2022 19:46:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/23/2022 19:47:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 19:47:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 19:47:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 19:47:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 19:47:19 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1441009239516702 on epoch=549
05/23/2022 19:47:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 19:47:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 19:47:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 19:47:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 19:47:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 19:47:45 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.14516129032258066 on epoch=562
05/23/2022 19:47:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/23/2022 19:47:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 19:47:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 19:48:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 19:48:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 19:48:10 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2196796338672769 on epoch=574
05/23/2022 19:48:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/23/2022 19:48:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 19:48:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 19:48:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/23/2022 19:48:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 19:48:35 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.12028447742733459 on epoch=587
05/23/2022 19:48:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 19:48:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 19:48:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/23/2022 19:48:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/23/2022 19:48:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/23/2022 19:49:00 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.11676082862523539 on epoch=599
05/23/2022 19:49:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 19:49:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 19:49:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/23/2022 19:49:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/23/2022 19:49:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 19:49:25 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.19114770972722592 on epoch=612
05/23/2022 19:49:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/23/2022 19:49:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 19:49:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 19:49:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 19:49:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 19:49:50 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.1294144452039189 on epoch=624
05/23/2022 19:49:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/23/2022 19:49:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 19:50:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 19:50:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 19:50:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/23/2022 19:50:16 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.32333833083458274 on epoch=637
05/23/2022 19:50:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/23/2022 19:50:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 19:50:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 19:50:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
05/23/2022 19:50:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 19:50:41 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3118440779610195 on epoch=649
05/23/2022 19:50:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 19:50:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 19:50:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 19:50:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/23/2022 19:51:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/23/2022 19:51:06 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.16545454545454544 on epoch=662
05/23/2022 19:51:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 19:51:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 19:51:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 19:51:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 19:51:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/23/2022 19:51:31 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.14511232544019428 on epoch=674
05/23/2022 19:51:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 19:51:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 19:51:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 19:51:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 19:51:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/23/2022 19:51:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.20773565573770492 on epoch=687
05/23/2022 19:52:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 19:52:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 19:52:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 19:52:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 19:52:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 19:52:21 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.3118440779610195 on epoch=699
05/23/2022 19:52:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 19:52:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 19:52:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 19:52:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 19:52:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 19:52:47 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.23712121212121212 on epoch=712
05/23/2022 19:52:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 19:52:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 19:53:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 19:53:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 19:53:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 19:53:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.32130325814536337 on epoch=724
05/23/2022 19:53:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/23/2022 19:53:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 19:53:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 19:53:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/23/2022 19:53:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 19:53:37 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3233632436025258 on epoch=737
05/23/2022 19:53:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 19:53:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 19:53:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 19:53:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 19:54:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 19:54:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:54:01 - INFO - __main__ - Printing 3 examples
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:54:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:54:01 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 19:54:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 19:54:01 - INFO - __main__ - Printing 3 examples
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 19:54:01 - INFO - __main__ - ['refuted']
05/23/2022 19:54:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:54:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:54:01 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 19:54:02 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.3146216260970359 on epoch=749
05/23/2022 19:54:02 - INFO - __main__ - save last model!
05/23/2022 19:54:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 19:54:03 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 19:54:03 - INFO - __main__ - Printing 3 examples
05/23/2022 19:54:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:54:03 - INFO - __main__ - ['entailed']
05/23/2022 19:54:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:54:03 - INFO - __main__ - ['entailed']
05/23/2022 19:54:03 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 19:54:03 - INFO - __main__ - ['entailed']
05/23/2022 19:54:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 19:54:20 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 19:54:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 19:54:21 - INFO - __main__ - Starting training!
05/23/2022 19:54:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 19:54:39 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 20:03:27 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_100_0.4_8_predictions.txt
05/23/2022 20:03:27 - INFO - __main__ - Classification-F1 on test data: 0.0140
05/23/2022 20:03:27 - INFO - __main__ - prefix=tab_fact_32_100, lr=0.4, bsz=8, dev_performance=0.5755342667649226, test_performance=0.01402595523071324
05/23/2022 20:03:27 - INFO - __main__ - Running ... prefix=tab_fact_32_100, lr=0.3, bsz=8 ...
05/23/2022 20:03:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:03:28 - INFO - __main__ - Printing 3 examples
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:03:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:03:28 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 20:03:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:03:28 - INFO - __main__ - Printing 3 examples
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 20:03:28 - INFO - __main__ - ['refuted']
05/23/2022 20:03:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:03:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:03:28 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 20:03:47 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 20:03:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 20:03:48 - INFO - __main__ - Starting training!
05/23/2022 20:03:53 - INFO - __main__ - Step 10 Global step 10 Train loss 4.84 on epoch=2
05/23/2022 20:03:58 - INFO - __main__ - Step 20 Global step 20 Train loss 3.37 on epoch=4
05/23/2022 20:04:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
05/23/2022 20:04:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.90 on epoch=9
05/23/2022 20:04:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
05/23/2022 20:04:14 - INFO - __main__ - Global step 50 Train loss 2.85 Classification-F1 0.07094594594594594 on epoch=12
05/23/2022 20:04:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07094594594594594 on epoch=12, global_step=50
05/23/2022 20:04:18 - INFO - __main__ - Step 60 Global step 60 Train loss 1.08 on epoch=14
05/23/2022 20:04:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.78 on epoch=17
05/23/2022 20:04:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=19
05/23/2022 20:04:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=22
05/23/2022 20:04:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=24
05/23/2022 20:04:39 - INFO - __main__ - Global step 100 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 20:04:39 - INFO - __main__ - Saving model with best Classification-F1: 0.07094594594594594 -> 0.3333333333333333 on epoch=24, global_step=100
05/23/2022 20:04:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=27
05/23/2022 20:04:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=29
05/23/2022 20:04:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=32
05/23/2022 20:04:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=34
05/23/2022 20:05:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=37
05/23/2022 20:05:04 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 20:05:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=39
05/23/2022 20:05:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=42
05/23/2022 20:05:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=44
05/23/2022 20:05:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=47
05/23/2022 20:05:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=49
05/23/2022 20:05:29 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 20:05:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=52
05/23/2022 20:05:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=54
05/23/2022 20:05:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=57
05/23/2022 20:05:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=59
05/23/2022 20:05:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=62
05/23/2022 20:05:54 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 20:05:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=64
05/23/2022 20:06:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=67
05/23/2022 20:06:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=69
05/23/2022 20:06:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=72
05/23/2022 20:06:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=74
05/23/2022 20:06:19 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 20:06:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=77
05/23/2022 20:06:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=79
05/23/2022 20:06:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=82
05/23/2022 20:06:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=84
05/23/2022 20:06:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=87
05/23/2022 20:06:45 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 20:06:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=89
05/23/2022 20:06:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/23/2022 20:06:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=94
05/23/2022 20:07:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/23/2022 20:07:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=99
05/23/2022 20:07:10 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 20:07:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/23/2022 20:07:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=104
05/23/2022 20:07:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/23/2022 20:07:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/23/2022 20:07:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/23/2022 20:07:35 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=112
05/23/2022 20:07:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
05/23/2022 20:07:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=117
05/23/2022 20:07:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/23/2022 20:07:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=122
05/23/2022 20:07:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/23/2022 20:08:00 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=124
05/23/2022 20:08:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
05/23/2022 20:08:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/23/2022 20:08:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
05/23/2022 20:08:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
05/23/2022 20:08:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
05/23/2022 20:08:25 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=137
05/23/2022 20:08:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/23/2022 20:08:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
05/23/2022 20:08:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=144
05/23/2022 20:08:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=147
05/23/2022 20:08:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=149
05/23/2022 20:08:50 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=149
05/23/2022 20:08:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
05/23/2022 20:08:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
05/23/2022 20:09:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=157
05/23/2022 20:09:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
05/23/2022 20:09:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
05/23/2022 20:09:15 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.32631578947368417 on epoch=162
05/23/2022 20:09:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
05/23/2022 20:09:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
05/23/2022 20:09:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/23/2022 20:09:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/23/2022 20:09:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/23/2022 20:09:41 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.32631578947368417 on epoch=174
05/23/2022 20:09:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=177
05/23/2022 20:09:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=179
05/23/2022 20:09:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
05/23/2022 20:09:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=184
05/23/2022 20:10:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=187
05/23/2022 20:10:06 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=187
05/23/2022 20:10:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=189
05/23/2022 20:10:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
05/23/2022 20:10:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
05/23/2022 20:10:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=197
05/23/2022 20:10:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
05/23/2022 20:10:31 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.3511520737327189 on epoch=199
05/23/2022 20:10:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3511520737327189 on epoch=199, global_step=800
05/23/2022 20:10:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
05/23/2022 20:10:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=204
05/23/2022 20:10:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
05/23/2022 20:10:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=209
05/23/2022 20:10:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
05/23/2022 20:10:56 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.36374269005847953 on epoch=212
05/23/2022 20:10:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3511520737327189 -> 0.36374269005847953 on epoch=212, global_step=850
05/23/2022 20:11:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/23/2022 20:11:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=217
05/23/2022 20:11:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=219
05/23/2022 20:11:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=222
05/23/2022 20:11:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=224
05/23/2022 20:11:21 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3591989987484355 on epoch=224
05/23/2022 20:11:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
05/23/2022 20:11:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
05/23/2022 20:11:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
05/23/2022 20:11:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
05/23/2022 20:11:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
05/23/2022 20:11:47 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.40116959064327484 on epoch=237
05/23/2022 20:11:47 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.40116959064327484 on epoch=237, global_step=950
05/23/2022 20:11:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/23/2022 20:11:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=242
05/23/2022 20:12:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
05/23/2022 20:12:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=247
05/23/2022 20:12:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=249
05/23/2022 20:12:12 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.5873015873015872 on epoch=249
05/23/2022 20:12:12 - INFO - __main__ - Saving model with best Classification-F1: 0.40116959064327484 -> 0.5873015873015872 on epoch=249, global_step=1000
05/23/2022 20:12:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
05/23/2022 20:12:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=254
05/23/2022 20:12:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
05/23/2022 20:12:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
05/23/2022 20:12:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
05/23/2022 20:12:37 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.500880503144654 on epoch=262
05/23/2022 20:12:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
05/23/2022 20:12:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
05/23/2022 20:12:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/23/2022 20:12:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=272
05/23/2022 20:13:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
05/23/2022 20:13:02 - INFO - __main__ - Global step 1100 Train loss 0.15 Classification-F1 0.5330817610062892 on epoch=274
05/23/2022 20:13:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
05/23/2022 20:13:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
05/23/2022 20:13:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=282
05/23/2022 20:13:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
05/23/2022 20:13:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
05/23/2022 20:13:28 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.5933528836754642 on epoch=287
05/23/2022 20:13:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5873015873015872 -> 0.5933528836754642 on epoch=287, global_step=1150
05/23/2022 20:13:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
05/23/2022 20:13:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
05/23/2022 20:13:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
05/23/2022 20:13:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/23/2022 20:13:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/23/2022 20:13:53 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.5730170496664195 on epoch=299
05/23/2022 20:13:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=302
05/23/2022 20:14:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/23/2022 20:14:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
05/23/2022 20:14:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
05/23/2022 20:14:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/23/2022 20:14:18 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.6046454163577959 on epoch=312
05/23/2022 20:14:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6046454163577959 on epoch=312, global_step=1250
05/23/2022 20:14:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
05/23/2022 20:14:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=317
05/23/2022 20:14:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=319
05/23/2022 20:14:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/23/2022 20:14:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=324
05/23/2022 20:14:43 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.6092796092796093 on epoch=324
05/23/2022 20:14:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6046454163577959 -> 0.6092796092796093 on epoch=324, global_step=1300
05/23/2022 20:14:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=327
05/23/2022 20:14:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/23/2022 20:14:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=332
05/23/2022 20:15:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
05/23/2022 20:15:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=337
05/23/2022 20:15:09 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.3986765922249793 on epoch=337
05/23/2022 20:15:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
05/23/2022 20:15:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
05/23/2022 20:15:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
05/23/2022 20:15:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/23/2022 20:15:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
05/23/2022 20:15:34 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.408842523596622 on epoch=349
05/23/2022 20:15:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/23/2022 20:15:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
05/23/2022 20:15:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=357
05/23/2022 20:15:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/23/2022 20:15:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
05/23/2022 20:15:59 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.2854981084489281 on epoch=362
05/23/2022 20:16:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/23/2022 20:16:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/23/2022 20:16:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/23/2022 20:16:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/23/2022 20:16:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/23/2022 20:16:24 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.6085148030340103 on epoch=374
05/23/2022 20:16:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/23/2022 20:16:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/23/2022 20:16:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/23/2022 20:16:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
05/23/2022 20:16:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/23/2022 20:16:49 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.5933528836754642 on epoch=387
05/23/2022 20:16:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/23/2022 20:16:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/23/2022 20:17:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/23/2022 20:17:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/23/2022 20:17:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
05/23/2022 20:17:15 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.2923427991886409 on epoch=399
05/23/2022 20:17:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/23/2022 20:17:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
05/23/2022 20:17:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
05/23/2022 20:17:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
05/23/2022 20:17:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
05/23/2022 20:17:40 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.3009634888438134 on epoch=412
05/23/2022 20:17:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
05/23/2022 20:17:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/23/2022 20:17:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/23/2022 20:17:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/23/2022 20:18:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/23/2022 20:18:05 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.2364663585002568 on epoch=424
05/23/2022 20:18:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/23/2022 20:18:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/23/2022 20:18:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=432
05/23/2022 20:18:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/23/2022 20:18:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/23/2022 20:18:30 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.13075602060785174 on epoch=437
05/23/2022 20:18:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/23/2022 20:18:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/23/2022 20:18:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/23/2022 20:18:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/23/2022 20:18:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/23/2022 20:18:56 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.2185997910135841 on epoch=449
05/23/2022 20:19:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
05/23/2022 20:19:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/23/2022 20:19:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/23/2022 20:19:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 20:19:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 20:19:21 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.2696572580645161 on epoch=462
05/23/2022 20:19:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/23/2022 20:19:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/23/2022 20:19:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
05/23/2022 20:19:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/23/2022 20:19:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/23/2022 20:19:46 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.22291021671826625 on epoch=474
05/23/2022 20:19:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/23/2022 20:19:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 20:20:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/23/2022 20:20:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/23/2022 20:20:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/23/2022 20:20:11 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.2696572580645161 on epoch=487
05/23/2022 20:20:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/23/2022 20:20:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/23/2022 20:20:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/23/2022 20:20:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/23/2022 20:20:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/23/2022 20:20:37 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.1999479573250065 on epoch=499
05/23/2022 20:20:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/23/2022 20:20:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/23/2022 20:20:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 20:20:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 20:20:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/23/2022 20:21:02 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.167989417989418 on epoch=512
05/23/2022 20:21:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/23/2022 20:21:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/23/2022 20:21:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/23/2022 20:21:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
05/23/2022 20:21:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/23/2022 20:21:27 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.14988290398126464 on epoch=524
05/23/2022 20:21:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 20:21:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 20:21:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/23/2022 20:21:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 20:21:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/23/2022 20:21:53 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.15714285714285714 on epoch=537
05/23/2022 20:21:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/23/2022 20:22:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/23/2022 20:22:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 20:22:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/23/2022 20:22:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/23/2022 20:22:18 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.22362609142270157 on epoch=549
05/23/2022 20:22:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/23/2022 20:22:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/23/2022 20:22:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/23/2022 20:22:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/23/2022 20:22:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/23/2022 20:22:43 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.2433401639344262 on epoch=562
05/23/2022 20:22:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
05/23/2022 20:22:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 20:22:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/23/2022 20:23:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 20:23:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/23/2022 20:23:09 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.13249607535321822 on epoch=574
05/23/2022 20:23:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 20:23:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/23/2022 20:23:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 20:23:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 20:23:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 20:23:34 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.14852607709750565 on epoch=587
05/23/2022 20:23:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
05/23/2022 20:23:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/23/2022 20:23:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/23/2022 20:23:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/23/2022 20:23:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/23/2022 20:23:59 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.1450781805259417 on epoch=599
05/23/2022 20:24:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 20:24:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/23/2022 20:24:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 20:24:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 20:24:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/23/2022 20:24:25 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.14035896844613552 on epoch=612
05/23/2022 20:24:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/23/2022 20:24:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/23/2022 20:24:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 20:24:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/23/2022 20:24:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 20:24:50 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.13934426229508196 on epoch=624
05/23/2022 20:24:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 20:24:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 20:25:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/23/2022 20:25:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/23/2022 20:25:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 20:25:15 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.22397540983606556 on epoch=637
05/23/2022 20:25:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 20:25:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 20:25:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 20:25:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/23/2022 20:25:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/23/2022 20:25:41 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.20401441070509524 on epoch=649
05/23/2022 20:25:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 20:25:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/23/2022 20:25:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 20:25:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 20:26:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 20:26:06 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.13371915930551323 on epoch=662
05/23/2022 20:26:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 20:26:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/23/2022 20:26:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/23/2022 20:26:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 20:26:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 20:26:31 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.2046153846153846 on epoch=674
05/23/2022 20:26:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 20:26:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/23/2022 20:26:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 20:26:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 20:26:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/23/2022 20:26:57 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.11126373626373626 on epoch=687
05/23/2022 20:27:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 20:27:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/23/2022 20:27:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 20:27:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 20:27:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 20:27:22 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2772727272727272 on epoch=699
05/23/2022 20:27:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 20:27:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 20:27:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 20:27:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 20:27:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/23/2022 20:27:47 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.09499671454050503 on epoch=712
05/23/2022 20:27:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 20:27:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 20:28:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 20:28:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 20:28:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/23/2022 20:28:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.20401441070509524 on epoch=724
05/23/2022 20:28:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/23/2022 20:28:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 20:28:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 20:28:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 20:28:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 20:28:38 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.1663183559735284 on epoch=737
05/23/2022 20:28:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 20:28:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 20:28:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 20:28:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/23/2022 20:29:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/23/2022 20:29:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:29:01 - INFO - __main__ - Printing 3 examples
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:29:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:29:01 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 20:29:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:29:01 - INFO - __main__ - Printing 3 examples
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 20:29:01 - INFO - __main__ - ['refuted']
05/23/2022 20:29:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:29:02 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:29:02 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 20:29:03 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.12436974789915967 on epoch=749
05/23/2022 20:29:03 - INFO - __main__ - save last model!
05/23/2022 20:29:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 20:29:03 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 20:29:03 - INFO - __main__ - Printing 3 examples
05/23/2022 20:29:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 20:29:03 - INFO - __main__ - ['entailed']
05/23/2022 20:29:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 20:29:03 - INFO - __main__ - ['entailed']
05/23/2022 20:29:03 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 20:29:03 - INFO - __main__ - ['entailed']
05/23/2022 20:29:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:29:17 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 20:29:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 20:29:18 - INFO - __main__ - Starting training!
05/23/2022 20:29:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:29:40 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 20:38:36 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_100_0.3_8_predictions.txt
05/23/2022 20:38:36 - INFO - __main__ - Classification-F1 on test data: 0.0045
05/23/2022 20:38:36 - INFO - __main__ - prefix=tab_fact_32_100, lr=0.3, bsz=8, dev_performance=0.6092796092796093, test_performance=0.004504504667875662
05/23/2022 20:38:36 - INFO - __main__ - Running ... prefix=tab_fact_32_100, lr=0.2, bsz=8 ...
05/23/2022 20:38:37 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:38:37 - INFO - __main__ - Printing 3 examples
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:38:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:38:37 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 20:38:37 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 20:38:37 - INFO - __main__ - Printing 3 examples
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: the steam torpedo boat be the only type to be construct in a single year [SEP] table_caption: guangdong fleet [SEP] table_text: name (pinyin)#name (wade giles)#characters#type#construction [n] leihu#lei - hu#雷虎#steam torpedo boat#1884 , stettin , germany [n] leilong#lei - lung#雷龍#steam torpedo boat#1884 , stettin , germany [n] leidui#lei - tui#雷兑#steam torpedo boat#1885 , schichau , germany [n] leigan#lei - kan#雷乾#steam torpedo boat#1885 , schichau , germany [n] leiqian#lei - ch'ien#雷坎#steam torpedo boat#1885 , schichau , germany [n] leikun#lei - k'un#雷坤#steam torpedo boat#1885 , schichau , germany [n] leili#lei - li#雷離#steam torpedo boat#1885 , schichau , germany [n] leigen#lei - gen#雷艮#steam torpedo boat#1885 , schichau , germany [n] leixun#lei - hsun#雷巽#steam torpedo boat#1885 , schichau , germany [n] leizhen#lei - chen#雷震#steam torpedo boat#1885 , schichau , germany [n] leizhong#lei - chung#雷中#steam torpedo boat#1885 , schichau , germany [n] guangheng#kuang - heng#廣亨#composite shallow - draft gunboat#1886 , canton [n] guangli#kuang - li#廣利#composite shallow - draft gunboat#1886 , canton [n] guangyuan#kuang - yuan#廣元#composite shallow - draft gunboat#1886 , canton [n] guangzhen#kuang - chen#廣貞#composite shallow - draft gunboat#1886 , canton [n] guanggeng#kuang - keng#廣庚#wooden gunboat#c1887 , foochow navy yard [n] guangxing#kuang - hsing#廣興#wooden gunboat#c1887 , foochow navy yard [n] guangzhen#kuang - chen#廣鎮#wooden gunboat#c1887 , foochow navy yard [n] guangkui#kuang - k'uei#廣癸#wooden gunboat#c1887 , foochow navy yard [n] guangjia#kuang - chia#廣甲#composite cruiser#1887 , foochow navy yard [n] guangyi#kuang - i#廣乙#steel torpedo gunboat#1892 , foochow navy yard [n] guangbing#kuang - ping#廣丙#steel torpedo gunboat#1892 , foochow navy yard [n] guangding#kuang - ting#廣丁#steel torpedo gunboat#1892 , foochow navy yard [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: xle02008 be the production code for number 9 [SEP] table_caption: list of the league episodes [SEP] table_text: no#-#title#directed by#written by#original air date#production code#us viewers (million) [n] 7#1#vegas draft#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 16 , 2010#xle02001#1.71 [n] 8#2#bro - lo el cuã±ado#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 23 , 2010#xle02002#1.05 [n] 9#3#the white knuckler#jeff schaffer#jeff schaffer & jackie marcus schaffer#september 30 , 2010#xle02003#0.89 [n] 10#4#the kluneberg#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 7 , 2010#xle02004#0.82 [n] 11#5#the marathon#jackie marcus schaffer#craig digregorio#october 14 , 2010#xle02005#0.86 [n] 12#6#the anniversary party#jeff schaffer#nick kroll & paul scheer#october 21 , 2010#xle02006#0.62 [n] 13#7#ghost monkey#jeff schaffer#jeff schaffer & jackie marcus schaffer#october 28 , 2010#xle02007#0.67 [n] 14#8#the tie#jackie marcus schaffer#dan o'keefe#november 4 , 2010#xle02008#1.01 [n] 15#9#the expert witness#jeff schaffer#nick kroll & paul scheer#november 11 , 2010#xle02009#1.00 [n] 16#10#high school reunion#jeff schaffer#jeff schaffer & jackie marcus schaffer#november 18 , 2010#xle02010#1.04 [n] 18#12#kegel the elf#jeff schaffer#jeff schaffer & jackie marcus schaffer#december 9 , 2010#xle02012#1.05 [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ -  [tab_fact] statement: helpful tracy receive fewer than 145000 view [SEP] table_caption: none [SEP] table_text: #original title#directed by#written by#original airdate#prod code#viewers [n] 1 - 1#tracy returns#susan tully#elly brewer#8 july 2002#1.1#267000 [n] 1 - 2#dares#david skynner#elly brewer#15 july 2002#1.2#363000 [n] 1 - 3#sneaking in ben#susan tully#andy walker#22 july 2002#1.3#499000 [n] 1 - 4#cam 's first visit#susan tully#elly brewer#29 july 2002#1.4#233000 [n] 1 - 5#child of the week#susan tully#elly brewer#29 july 2002#1.5#278000 [n] 1 - 6#the truth is revealed#susan tully#carol russell#5 august 2002#1.6#315000 [n] 1 - 7#never ever wanna see him again#susan tully#mary morris#5 august 2002#1.7#221000 [n] 1 - 8#1000 words about tracy#susan tully#arnold evans#12 august 2002#1.8#261000 [n] 1 - 9#bad peter#susan tully#mary morris#12 august 2002#1.9#242000 [n] 1 - 10#cam 's place#susan tully#andy walker#19 august 2002#1.10#423000 [n] 1 - 11#dumping ground virus#david skynner#elly brewer#19 august 2002#1.11#444000 [n] 1 - 12#justine 's tv#susan tully#elly brewer#26 august 2002#1.12#417000 [n] 1 - 13#tracy and cam row#susan tully#elly brewer#26 august 2002#1.13#526000 [n] 1 - 14#sleepover#david skynner#mary morris#3 september 2002#1.14#658000 [n] 1 - 15#parent 's evening#susan tully#laura summers#3 september 2002#1.15#215000 [n] 1 - 16#the postcard#susan tully#roger griffiths#10 september 2002#1.16#215000 [n] 1 - 18#helpful tracy#david skynner#mary morris#10 september 2002#1.17#145000 [n] 1 - 19#friend#david skynner#laura summers#17 september 2002#1.19#815000 [n] 1 - 20#treasure hunt#david skynner#laura summers#24 september 2002#1.20#234000 [n] 1 - 21#romance#david skynner#elly brewer#24 september 2002#1.21#415000 [n] 1 - 22#temporary care worker#david skynner#othniel smith#1 october 2002#1.22#615000 [n] 1 - 23#cut the weed#david skynner#graham alborough#1 october 2002#1.23#522000 [n] 1 - 24#need armbands#david skynner#laura summers#8 october 2002#1.24#195000 [n] 1 - 25#miss you#david skynner#mary morris#8 october 2002#1.25#395000 [n] 
05/23/2022 20:38:37 - INFO - __main__ - ['refuted']
05/23/2022 20:38:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 20:38:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 20:38:38 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 20:38:53 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 20:38:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 20:38:53 - INFO - __main__ - Starting training!
05/23/2022 20:38:58 - INFO - __main__ - Step 10 Global step 10 Train loss 5.33 on epoch=2
05/23/2022 20:39:03 - INFO - __main__ - Step 20 Global step 20 Train loss 3.90 on epoch=4
05/23/2022 20:39:07 - INFO - __main__ - Step 30 Global step 30 Train loss 3.24 on epoch=7
05/23/2022 20:39:11 - INFO - __main__ - Step 40 Global step 40 Train loss 2.49 on epoch=9
05/23/2022 20:39:16 - INFO - __main__ - Step 50 Global step 50 Train loss 2.11 on epoch=12
05/23/2022 20:39:19 - INFO - __main__ - Global step 50 Train loss 3.42 Classification-F1 0.0 on epoch=12
05/23/2022 20:39:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/23/2022 20:39:23 - INFO - __main__ - Step 60 Global step 60 Train loss 1.79 on epoch=14
05/23/2022 20:39:28 - INFO - __main__ - Step 70 Global step 70 Train loss 1.50 on epoch=17
05/23/2022 20:39:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.26 on epoch=19
05/23/2022 20:39:37 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=22
05/23/2022 20:39:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/23/2022 20:39:44 - INFO - __main__ - Global step 100 Train loss 1.30 Classification-F1 0.3671451355661882 on epoch=24
05/23/2022 20:39:44 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.3671451355661882 on epoch=24, global_step=100
05/23/2022 20:39:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=27
05/23/2022 20:39:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=29
05/23/2022 20:39:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=32
05/23/2022 20:40:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=34
05/23/2022 20:40:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=37
05/23/2022 20:40:09 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 20:40:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=39
05/23/2022 20:40:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=42
05/23/2022 20:40:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=44
05/23/2022 20:40:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=47
05/23/2022 20:40:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=49
05/23/2022 20:40:34 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 20:40:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=52
05/23/2022 20:40:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=54
05/23/2022 20:40:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.32 on epoch=57
05/23/2022 20:40:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=59
05/23/2022 20:40:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=62
05/23/2022 20:40:59 - INFO - __main__ - Global step 250 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 20:41:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=64
05/23/2022 20:41:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=67
05/23/2022 20:41:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=69
05/23/2022 20:41:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=72
05/23/2022 20:41:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=74
05/23/2022 20:41:24 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 20:41:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=77
05/23/2022 20:41:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=79
05/23/2022 20:41:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=82
05/23/2022 20:41:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=84
05/23/2022 20:41:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=87
05/23/2022 20:41:49 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 20:41:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=89
05/23/2022 20:41:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=92
05/23/2022 20:42:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=94
05/23/2022 20:42:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=97
05/23/2022 20:42:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
05/23/2022 20:42:14 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 20:42:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=102
05/23/2022 20:42:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=104
05/23/2022 20:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=107
05/23/2022 20:42:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=109
05/23/2022 20:42:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/23/2022 20:42:39 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=112
05/23/2022 20:42:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=114
05/23/2022 20:42:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
05/23/2022 20:42:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=119
05/23/2022 20:42:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/23/2022 20:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=124
05/23/2022 20:43:04 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=124
05/23/2022 20:43:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/23/2022 20:43:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=129
05/23/2022 20:43:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
05/23/2022 20:43:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=134
05/23/2022 20:43:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/23/2022 20:43:29 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=137
05/23/2022 20:43:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/23/2022 20:43:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=142
05/23/2022 20:43:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=144
05/23/2022 20:43:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=147
05/23/2022 20:43:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=149
05/23/2022 20:43:54 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=149
05/23/2022 20:43:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
05/23/2022 20:44:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=154
05/23/2022 20:44:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/23/2022 20:44:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=159
05/23/2022 20:44:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/23/2022 20:44:19 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=162
05/23/2022 20:44:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=164
05/23/2022 20:44:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=167
05/23/2022 20:44:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=169
05/23/2022 20:44:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/23/2022 20:44:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=174
05/23/2022 20:44:44 - INFO - __main__ - Global step 700 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=174
05/23/2022 20:44:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=177
05/23/2022 20:44:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
05/23/2022 20:44:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=182
05/23/2022 20:45:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=184
05/23/2022 20:45:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=187
05/23/2022 20:45:09 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=187
05/23/2022 20:45:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=189
05/23/2022 20:45:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/23/2022 20:45:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
05/23/2022 20:45:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=197
05/23/2022 20:45:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=199
05/23/2022 20:45:34 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=199
05/23/2022 20:45:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=202
05/23/2022 20:45:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=204
05/23/2022 20:45:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=207
05/23/2022 20:45:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=209
05/23/2022 20:45:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/23/2022 20:45:59 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=212
05/23/2022 20:46:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
05/23/2022 20:46:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
05/23/2022 20:46:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=219
05/23/2022 20:46:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
05/23/2022 20:46:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=224
05/23/2022 20:46:24 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=224
05/23/2022 20:46:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=227
05/23/2022 20:46:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=229
05/23/2022 20:46:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=232
05/23/2022 20:46:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=234
05/23/2022 20:46:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=237
05/23/2022 20:46:49 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=237
05/23/2022 20:46:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=239
05/23/2022 20:46:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=242
05/23/2022 20:47:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=244
05/23/2022 20:47:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
05/23/2022 20:47:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
05/23/2022 20:47:14 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=249
05/23/2022 20:47:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=252
05/23/2022 20:47:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=254
05/23/2022 20:47:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=257
05/23/2022 20:47:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=259
05/23/2022 20:47:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=262
05/23/2022 20:47:39 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=262
05/23/2022 20:47:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=264
05/23/2022 20:47:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=267
05/23/2022 20:47:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=269
05/23/2022 20:47:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=272
05/23/2022 20:48:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
05/23/2022 20:48:04 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3671451355661882 on epoch=274
05/23/2022 20:48:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=277
05/23/2022 20:48:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=279
05/23/2022 20:48:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
05/23/2022 20:48:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=284
05/23/2022 20:48:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
05/23/2022 20:48:29 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.429800307219662 on epoch=287
05/23/2022 20:48:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.429800307219662 on epoch=287, global_step=1150
05/23/2022 20:48:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=289
05/23/2022 20:48:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=292
05/23/2022 20:48:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=294
05/23/2022 20:48:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
05/23/2022 20:48:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=299
05/23/2022 20:48:54 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.5588547189819725 on epoch=299
05/23/2022 20:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.429800307219662 -> 0.5588547189819725 on epoch=299, global_step=1200
05/23/2022 20:48:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
05/23/2022 20:49:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=304
05/23/2022 20:49:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=307
05/23/2022 20:49:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=309
05/23/2022 20:49:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=312
05/23/2022 20:49:19 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.5460992907801419 on epoch=312
05/23/2022 20:49:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=314
05/23/2022 20:49:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=317
05/23/2022 20:49:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=319
05/23/2022 20:49:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=322
05/23/2022 20:49:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=324
05/23/2022 20:49:44 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.5249204665959704 on epoch=324
05/23/2022 20:49:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=327
05/23/2022 20:49:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=329
05/23/2022 20:49:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=332
05/23/2022 20:50:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=334
05/23/2022 20:50:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=337
05/23/2022 20:50:09 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.5405128205128205 on epoch=337
05/23/2022 20:50:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=339
05/23/2022 20:50:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=342
05/23/2022 20:50:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
05/23/2022 20:50:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=347
05/23/2022 20:50:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=349
05/23/2022 20:50:34 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.6046454163577959 on epoch=349
05/23/2022 20:50:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.6046454163577959 on epoch=349, global_step=1400
05/23/2022 20:50:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
05/23/2022 20:50:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=354
05/23/2022 20:50:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=357
05/23/2022 20:50:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=359
05/23/2022 20:50:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/23/2022 20:50:59 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.5901477832512315 on epoch=362
05/23/2022 20:51:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=364
05/23/2022 20:51:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=367
05/23/2022 20:51:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=369
05/23/2022 20:51:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=372
05/23/2022 20:51:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=374
05/23/2022 20:51:24 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.5696139476961395 on epoch=374
05/23/2022 20:51:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=377
05/23/2022 20:51:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/23/2022 20:51:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=382
05/23/2022 20:51:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
05/23/2022 20:51:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=387
05/23/2022 20:51:49 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.5586206896551724 on epoch=387
05/23/2022 20:51:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=389
05/23/2022 20:51:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=392
05/23/2022 20:52:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=394
05/23/2022 20:52:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=397
05/23/2022 20:52:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
05/23/2022 20:52:14 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.5755342667649226 on epoch=399
05/23/2022 20:52:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=402
05/23/2022 20:52:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=404
05/23/2022 20:52:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=407
05/23/2022 20:52:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/23/2022 20:52:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=412
05/23/2022 20:52:39 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.5921568627450979 on epoch=412
05/23/2022 20:52:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=414
05/23/2022 20:52:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=417
05/23/2022 20:52:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=419
05/23/2022 20:52:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=422
05/23/2022 20:53:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=424
05/23/2022 20:53:04 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.5625 on epoch=424
05/23/2022 20:53:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=427
05/23/2022 20:53:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=429
05/23/2022 20:53:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
05/23/2022 20:53:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
05/23/2022 20:53:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=437
05/23/2022 20:53:29 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.5933528836754642 on epoch=437
05/23/2022 20:53:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
05/23/2022 20:53:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=442
05/23/2022 20:53:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=444
05/23/2022 20:53:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=447
05/23/2022 20:53:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
05/23/2022 20:53:54 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.6085148030340103 on epoch=449
05/23/2022 20:53:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6046454163577959 -> 0.6085148030340103 on epoch=449, global_step=1800
05/23/2022 20:53:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=452
05/23/2022 20:54:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=454
05/23/2022 20:54:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=457
05/23/2022 20:54:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=459
05/23/2022 20:54:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=462
05/23/2022 20:54:19 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.5755342667649226 on epoch=462
05/23/2022 20:54:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=464
05/23/2022 20:54:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=467
05/23/2022 20:54:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=469
05/23/2022 20:54:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=472
05/23/2022 20:54:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=474
05/23/2022 20:54:44 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.36739417989417983 on epoch=474
05/23/2022 20:54:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=477
05/23/2022 20:54:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=479
05/23/2022 20:54:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=482
05/23/2022 20:55:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/23/2022 20:55:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
05/23/2022 20:55:09 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.6069761729304839 on epoch=487
05/23/2022 20:55:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=489
05/23/2022 20:55:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=492
05/23/2022 20:55:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
05/23/2022 20:55:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=497
05/23/2022 20:55:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=499
05/23/2022 20:55:34 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.3884201819685691 on epoch=499
05/23/2022 20:55:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
05/23/2022 20:55:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
05/23/2022 20:55:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=507
05/23/2022 20:55:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=509
05/23/2022 20:55:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=512
05/23/2022 20:55:59 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.5625 on epoch=512
05/23/2022 20:56:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
05/23/2022 20:56:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/23/2022 20:56:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
05/23/2022 20:56:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/23/2022 20:56:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/23/2022 20:56:24 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.34575260804769004 on epoch=524
05/23/2022 20:56:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
05/23/2022 20:56:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=529
05/23/2022 20:56:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
05/23/2022 20:56:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/23/2022 20:56:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
05/23/2022 20:56:49 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.36678257989733404 on epoch=537
05/23/2022 20:56:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=539
05/23/2022 20:56:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/23/2022 20:57:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/23/2022 20:57:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
05/23/2022 20:57:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=549
05/23/2022 20:57:14 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.26136363636363635 on epoch=549
05/23/2022 20:57:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
05/23/2022 20:57:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=554
05/23/2022 20:57:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=557
05/23/2022 20:57:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
05/23/2022 20:57:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=562
05/23/2022 20:57:38 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5307917888563051 on epoch=562
05/23/2022 20:57:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
05/23/2022 20:57:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=567
05/23/2022 20:57:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
05/23/2022 20:57:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=572
05/23/2022 20:58:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/23/2022 20:58:03 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.23787878787878788 on epoch=574
05/23/2022 20:58:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=577
05/23/2022 20:58:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/23/2022 20:58:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=582
05/23/2022 20:58:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/23/2022 20:58:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
05/23/2022 20:58:28 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.23706896551724135 on epoch=587
05/23/2022 20:58:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=589
05/23/2022 20:58:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
05/23/2022 20:58:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/23/2022 20:58:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/23/2022 20:58:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
05/23/2022 20:58:53 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.2696572580645161 on epoch=599
05/23/2022 20:58:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
05/23/2022 20:59:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
05/23/2022 20:59:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/23/2022 20:59:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=609
05/23/2022 20:59:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
05/23/2022 20:59:18 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.15552072448624174 on epoch=612
05/23/2022 20:59:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
05/23/2022 20:59:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/23/2022 20:59:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/23/2022 20:59:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
05/23/2022 20:59:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=624
05/23/2022 20:59:43 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.22396313364055304 on epoch=624
05/23/2022 20:59:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/23/2022 20:59:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
05/23/2022 20:59:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=632
05/23/2022 21:00:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/23/2022 21:00:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=637
05/23/2022 21:00:08 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.21741803278688526 on epoch=637
05/23/2022 21:00:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/23/2022 21:00:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=642
05/23/2022 21:00:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
05/23/2022 21:00:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/23/2022 21:00:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/23/2022 21:00:32 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.25297242600556535 on epoch=649
05/23/2022 21:00:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
05/23/2022 21:00:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/23/2022 21:00:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/23/2022 21:00:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=659
05/23/2022 21:00:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/23/2022 21:00:57 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.26191677175283734 on epoch=662
05/23/2022 21:01:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
05/23/2022 21:01:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/23/2022 21:01:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
05/23/2022 21:01:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/23/2022 21:01:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/23/2022 21:01:22 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.1663183559735284 on epoch=674
05/23/2022 21:01:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=677
05/23/2022 21:01:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/23/2022 21:01:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/23/2022 21:01:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 21:01:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/23/2022 21:01:47 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.1663183559735284 on epoch=687
05/23/2022 21:01:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
05/23/2022 21:01:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/23/2022 21:02:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/23/2022 21:02:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/23/2022 21:02:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=699
05/23/2022 21:02:12 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.14050510892616155 on epoch=699
05/23/2022 21:02:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/23/2022 21:02:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
05/23/2022 21:02:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/23/2022 21:02:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/23/2022 21:02:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
05/23/2022 21:02:38 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.53125 on epoch=712
05/23/2022 21:02:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/23/2022 21:02:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=717
05/23/2022 21:02:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/23/2022 21:02:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
05/23/2022 21:03:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 21:03:02 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.335978835978836 on epoch=724
05/23/2022 21:03:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/23/2022 21:03:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/23/2022 21:03:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=732
05/23/2022 21:03:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/23/2022 21:03:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/23/2022 21:03:27 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.2210953346855984 on epoch=737
05/23/2022 21:03:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
05/23/2022 21:03:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=742
05/23/2022 21:03:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/23/2022 21:03:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
05/23/2022 21:03:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/23/2022 21:03:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:03:51 - INFO - __main__ - Printing 3 examples
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:03:51 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:03:51 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 21:03:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:03:51 - INFO - __main__ - Printing 3 examples
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 21:03:51 - INFO - __main__ - ['refuted']
05/23/2022 21:03:51 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:03:51 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:03:52 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 21:03:53 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.3135593220338983 on epoch=749
05/23/2022 21:03:53 - INFO - __main__ - save last model!
05/23/2022 21:03:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 21:03:53 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 21:03:53 - INFO - __main__ - Printing 3 examples
05/23/2022 21:03:53 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:03:53 - INFO - __main__ - ['entailed']
05/23/2022 21:03:53 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:03:53 - INFO - __main__ - ['entailed']
05/23/2022 21:03:53 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:03:53 - INFO - __main__ - ['entailed']
05/23/2022 21:03:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:04:10 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 21:04:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 21:04:11 - INFO - __main__ - Starting training!
05/23/2022 21:04:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:04:29 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 21:13:09 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_100_0.2_8_predictions.txt
05/23/2022 21:13:09 - INFO - __main__ - Classification-F1 on test data: 0.0128
05/23/2022 21:13:09 - INFO - __main__ - prefix=tab_fact_32_100, lr=0.2, bsz=8, dev_performance=0.6085148030340103, test_performance=0.012807267937021988
05/23/2022 21:13:09 - INFO - __main__ - Running ... prefix=tab_fact_32_13, lr=0.5, bsz=8 ...
05/23/2022 21:13:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:13:10 - INFO - __main__ - Printing 3 examples
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:13:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:13:10 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 21:13:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:13:10 - INFO - __main__ - Printing 3 examples
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 21:13:10 - INFO - __main__ - ['refuted']
05/23/2022 21:13:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:13:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:13:10 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 21:13:26 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 21:13:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 21:13:26 - INFO - __main__ - Starting training!
05/23/2022 21:13:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.74 on epoch=2
05/23/2022 21:13:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.77 on epoch=4
05/23/2022 21:13:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.71 on epoch=7
05/23/2022 21:13:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=9
05/23/2022 21:13:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.63 on epoch=12
05/23/2022 21:13:52 - INFO - __main__ - Global step 50 Train loss 2.19 Classification-F1 0.3671451355661882 on epoch=12
05/23/2022 21:13:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3671451355661882 on epoch=12, global_step=50
05/23/2022 21:13:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=14
05/23/2022 21:14:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=17
05/23/2022 21:14:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=19
05/23/2022 21:14:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=22
05/23/2022 21:14:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=24
05/23/2022 21:14:17 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 21:14:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=27
05/23/2022 21:14:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=29
05/23/2022 21:14:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=32
05/23/2022 21:14:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=34
05/23/2022 21:14:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=37
05/23/2022 21:14:42 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 21:14:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=39
05/23/2022 21:14:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=42
05/23/2022 21:14:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=44
05/23/2022 21:15:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=47
05/23/2022 21:15:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=49
05/23/2022 21:15:07 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 21:15:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=52
05/23/2022 21:15:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=54
05/23/2022 21:15:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=57
05/23/2022 21:15:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=59
05/23/2022 21:15:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=62
05/23/2022 21:15:32 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.32631578947368417 on epoch=62
05/23/2022 21:15:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=64
05/23/2022 21:15:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=67
05/23/2022 21:15:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=69
05/23/2022 21:15:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=72
05/23/2022 21:15:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/23/2022 21:15:57 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.32631578947368417 on epoch=74
05/23/2022 21:16:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=77
05/23/2022 21:16:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=79
05/23/2022 21:16:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=82
05/23/2022 21:16:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=84
05/23/2022 21:16:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=87
05/23/2022 21:16:23 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.32631578947368417 on epoch=87
05/23/2022 21:16:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.18 on epoch=89
05/23/2022 21:16:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=92
05/23/2022 21:16:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=94
05/23/2022 21:16:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=97
05/23/2022 21:16:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
05/23/2022 21:16:48 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.2554865424430642 on epoch=99
05/23/2022 21:16:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=102
05/23/2022 21:16:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/23/2022 21:17:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
05/23/2022 21:17:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/23/2022 21:17:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=112
05/23/2022 21:17:13 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.21276595744680848 on epoch=112
05/23/2022 21:17:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=114
05/23/2022 21:17:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
05/23/2022 21:17:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=119
05/23/2022 21:17:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.17 on epoch=122
05/23/2022 21:17:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/23/2022 21:17:38 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.3511520737327189 on epoch=124
05/23/2022 21:17:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.18 on epoch=127
05/23/2022 21:17:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.16 on epoch=129
05/23/2022 21:17:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=132
05/23/2022 21:17:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=134
05/23/2022 21:18:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=137
05/23/2022 21:18:03 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.4545454545454546 on epoch=137
05/23/2022 21:18:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.4545454545454546 on epoch=137, global_step=550
05/23/2022 21:18:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=139
05/23/2022 21:18:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=142
05/23/2022 21:18:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
05/23/2022 21:18:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=147
05/23/2022 21:18:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=149
05/23/2022 21:18:28 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.5599694423223835 on epoch=149
05/23/2022 21:18:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4545454545454546 -> 0.5599694423223835 on epoch=149, global_step=600
05/23/2022 21:18:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=152
05/23/2022 21:18:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=154
05/23/2022 21:18:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.14 on epoch=157
05/23/2022 21:18:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=159
05/23/2022 21:18:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/23/2022 21:18:54 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.387893864013267 on epoch=162
05/23/2022 21:18:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
05/23/2022 21:19:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=167
05/23/2022 21:19:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=169
05/23/2022 21:19:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=172
05/23/2022 21:19:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/23/2022 21:19:19 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.2854981084489281 on epoch=174
05/23/2022 21:19:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
05/23/2022 21:19:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=179
05/23/2022 21:19:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
05/23/2022 21:19:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=184
05/23/2022 21:19:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
05/23/2022 21:19:44 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.38000000000000006 on epoch=187
05/23/2022 21:19:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
05/23/2022 21:19:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/23/2022 21:19:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
05/23/2022 21:20:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/23/2022 21:20:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
05/23/2022 21:20:09 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.2632485426603074 on epoch=199
05/23/2022 21:20:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/23/2022 21:20:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=204
05/23/2022 21:20:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
05/23/2022 21:20:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
05/23/2022 21:20:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/23/2022 21:20:34 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.39969488939740655 on epoch=212
05/23/2022 21:20:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/23/2022 21:20:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/23/2022 21:20:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/23/2022 21:20:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=222
05/23/2022 21:20:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/23/2022 21:20:59 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.3040194572452637 on epoch=224
05/23/2022 21:21:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/23/2022 21:21:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/23/2022 21:21:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=232
05/23/2022 21:21:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=234
05/23/2022 21:21:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/23/2022 21:21:24 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.2287925696594427 on epoch=237
05/23/2022 21:21:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/23/2022 21:21:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/23/2022 21:21:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/23/2022 21:21:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/23/2022 21:21:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/23/2022 21:21:50 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.619736502195815 on epoch=249
05/23/2022 21:21:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5599694423223835 -> 0.619736502195815 on epoch=249, global_step=1000
05/23/2022 21:21:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
05/23/2022 21:21:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/23/2022 21:22:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
05/23/2022 21:22:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/23/2022 21:22:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/23/2022 21:22:15 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.2217391304347826 on epoch=262
05/23/2022 21:22:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/23/2022 21:22:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
05/23/2022 21:22:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/23/2022 21:22:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 21:22:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/23/2022 21:22:40 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.22722457627118642 on epoch=274
05/23/2022 21:22:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
05/23/2022 21:22:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/23/2022 21:22:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/23/2022 21:22:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/23/2022 21:23:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/23/2022 21:23:05 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.23025641025641025 on epoch=287
05/23/2022 21:23:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 21:23:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/23/2022 21:23:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/23/2022 21:23:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/23/2022 21:23:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 21:23:30 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.3142857142857143 on epoch=299
05/23/2022 21:23:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 21:23:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/23/2022 21:23:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/23/2022 21:23:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 21:23:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
05/23/2022 21:23:55 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.2997711670480549 on epoch=312
05/23/2022 21:24:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 21:24:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/23/2022 21:24:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/23/2022 21:24:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/23/2022 21:24:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 21:24:20 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.30625 on epoch=324
05/23/2022 21:24:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/23/2022 21:24:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/23/2022 21:24:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
05/23/2022 21:24:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 21:24:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/23/2022 21:24:46 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.38376068376068373 on epoch=337
05/23/2022 21:24:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/23/2022 21:24:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/23/2022 21:24:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 21:25:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 21:25:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 21:25:11 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.36389181594661046 on epoch=349
05/23/2022 21:25:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 21:25:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 21:25:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 21:25:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/23/2022 21:25:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/23/2022 21:25:36 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3829130211013896 on epoch=362
05/23/2022 21:25:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 21:25:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/23/2022 21:25:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 21:25:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 21:25:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/23/2022 21:26:01 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.28710837185413457 on epoch=374
05/23/2022 21:26:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/23/2022 21:26:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 21:26:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 21:26:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/23/2022 21:26:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/23/2022 21:26:26 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.36969696969696964 on epoch=387
05/23/2022 21:26:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/23/2022 21:26:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 21:26:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/23/2022 21:26:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 21:26:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 21:26:51 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2854981084489281 on epoch=399
05/23/2022 21:26:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 21:27:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/23/2022 21:27:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/23/2022 21:27:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 21:27:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 21:27:16 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.29107142857142854 on epoch=412
05/23/2022 21:27:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/23/2022 21:27:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 21:27:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 21:27:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 21:27:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/23/2022 21:27:41 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.387812551746978 on epoch=424
05/23/2022 21:27:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/23/2022 21:27:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/23/2022 21:27:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/23/2022 21:27:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/23/2022 21:28:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/23/2022 21:28:07 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4061821219715956 on epoch=437
05/23/2022 21:28:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/23/2022 21:28:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/23/2022 21:28:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 21:28:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/23/2022 21:28:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 21:28:32 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.36739417989417983 on epoch=449
05/23/2022 21:28:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 21:28:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 21:28:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/23/2022 21:28:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 21:28:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/23/2022 21:28:57 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4022177419354838 on epoch=462
05/23/2022 21:29:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 21:29:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 21:29:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/23/2022 21:29:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/23/2022 21:29:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 21:29:22 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.3727101523711693 on epoch=474
05/23/2022 21:29:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 21:29:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/23/2022 21:29:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 21:29:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/23/2022 21:29:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 21:29:47 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.4022177419354838 on epoch=487
05/23/2022 21:29:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 21:29:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 21:30:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/23/2022 21:30:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 21:30:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 21:30:13 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4166666666666667 on epoch=499
05/23/2022 21:30:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/23/2022 21:30:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 21:30:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 21:30:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/23/2022 21:30:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 21:30:38 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.37857142857142856 on epoch=512
05/23/2022 21:30:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 21:30:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 21:30:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/23/2022 21:30:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/23/2022 21:31:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 21:31:03 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.3827000947703971 on epoch=524
05/23/2022 21:31:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 21:31:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/23/2022 21:31:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 21:31:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 21:31:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 21:31:28 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.4012846517917512 on epoch=537
05/23/2022 21:31:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 21:31:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 21:31:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 21:31:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 21:31:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 21:31:53 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.4123450543890716 on epoch=549
05/23/2022 21:31:58 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 21:32:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 21:32:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 21:32:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 21:32:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 21:32:18 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.4225828262339419 on epoch=562
05/23/2022 21:32:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 21:32:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 21:32:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 21:32:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 21:32:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 21:32:43 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.4000000000000001 on epoch=574
05/23/2022 21:32:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 21:32:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 21:32:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 21:33:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 21:33:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 21:33:09 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.4012846517917512 on epoch=587
05/23/2022 21:33:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 21:33:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 21:33:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 21:33:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 21:33:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 21:33:34 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.4127784783522488 on epoch=599
05/23/2022 21:33:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 21:33:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 21:33:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 21:33:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 21:33:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 21:33:59 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.4230510752688172 on epoch=612
05/23/2022 21:34:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 21:34:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 21:34:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 21:34:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 21:34:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 21:34:24 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.39109537060460403 on epoch=624
05/23/2022 21:34:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 21:34:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 21:34:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 21:34:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 21:34:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 21:34:49 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.4012846517917512 on epoch=637
05/23/2022 21:34:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 21:34:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 21:35:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 21:35:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 21:35:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/23/2022 21:35:14 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.38080808080808076 on epoch=649
05/23/2022 21:35:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 21:35:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 21:35:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 21:35:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 21:35:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 21:35:39 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.40777666999002987 on epoch=662
05/23/2022 21:35:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 21:35:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 21:35:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 21:35:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 21:36:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 21:36:05 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.387998982964658 on epoch=674
05/23/2022 21:36:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 21:36:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 21:36:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 21:36:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 21:36:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 21:36:30 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.39109537060460403 on epoch=687
05/23/2022 21:36:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 21:36:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 21:36:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 21:36:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 21:36:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 21:36:55 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3986765922249793 on epoch=699
05/23/2022 21:36:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 21:37:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 21:37:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 21:37:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 21:37:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 21:37:20 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.404040404040404 on epoch=712
05/23/2022 21:37:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 21:37:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 21:37:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 21:37:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 21:37:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 21:37:45 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.4401794616151545 on epoch=724
05/23/2022 21:37:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 21:37:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 21:37:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 21:38:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 21:38:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 21:38:10 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.41059829059829056 on epoch=737
05/23/2022 21:38:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/23/2022 21:38:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 21:38:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 21:38:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 21:38:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 21:38:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:38:34 - INFO - __main__ - Printing 3 examples
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:38:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:38:34 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 21:38:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:38:34 - INFO - __main__ - Printing 3 examples
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 21:38:34 - INFO - __main__ - ['refuted']
05/23/2022 21:38:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:38:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:38:34 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 21:38:36 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.4254208754208754 on epoch=749
05/23/2022 21:38:36 - INFO - __main__ - save last model!
05/23/2022 21:38:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 21:38:36 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 21:38:36 - INFO - __main__ - Printing 3 examples
05/23/2022 21:38:36 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:38:36 - INFO - __main__ - ['entailed']
05/23/2022 21:38:36 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:38:36 - INFO - __main__ - ['entailed']
05/23/2022 21:38:36 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 21:38:36 - INFO - __main__ - ['entailed']
05/23/2022 21:38:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:38:50 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 21:38:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 21:38:51 - INFO - __main__ - Starting training!
05/23/2022 21:39:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:39:13 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 21:48:05 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_13_0.5_8_predictions.txt
05/23/2022 21:48:05 - INFO - __main__ - Classification-F1 on test data: 0.0214
05/23/2022 21:48:05 - INFO - __main__ - prefix=tab_fact_32_13, lr=0.5, bsz=8, dev_performance=0.619736502195815, test_performance=0.02144844652993435
05/23/2022 21:48:05 - INFO - __main__ - Running ... prefix=tab_fact_32_13, lr=0.4, bsz=8 ...
05/23/2022 21:48:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:48:06 - INFO - __main__ - Printing 3 examples
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:48:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:48:06 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 21:48:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 21:48:06 - INFO - __main__ - Printing 3 examples
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 21:48:06 - INFO - __main__ - ['refuted']
05/23/2022 21:48:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 21:48:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 21:48:07 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 21:48:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 21:48:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 21:48:22 - INFO - __main__ - Starting training!
05/23/2022 21:48:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.79 on epoch=2
05/23/2022 21:48:32 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=4
05/23/2022 21:48:36 - INFO - __main__ - Step 30 Global step 30 Train loss 2.15 on epoch=7
05/23/2022 21:48:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.44 on epoch=9
05/23/2022 21:48:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=12
05/23/2022 21:48:48 - INFO - __main__ - Global step 50 Train loss 2.46 Classification-F1 0.15517241379310343 on epoch=12
05/23/2022 21:48:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15517241379310343 on epoch=12, global_step=50
05/23/2022 21:48:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=14
05/23/2022 21:48:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=17
05/23/2022 21:49:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=19
05/23/2022 21:49:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=22
05/23/2022 21:49:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=24
05/23/2022 21:49:13 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 21:49:13 - INFO - __main__ - Saving model with best Classification-F1: 0.15517241379310343 -> 0.3333333333333333 on epoch=24, global_step=100
05/23/2022 21:49:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=27
05/23/2022 21:49:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=29
05/23/2022 21:49:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=32
05/23/2022 21:49:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=34
05/23/2022 21:49:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=37
05/23/2022 21:49:38 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 21:49:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=39
05/23/2022 21:49:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=42
05/23/2022 21:49:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=44
05/23/2022 21:49:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=47
05/23/2022 21:50:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=49
05/23/2022 21:50:03 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 21:50:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=52
05/23/2022 21:50:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=54
05/23/2022 21:50:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=57
05/23/2022 21:50:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=59
05/23/2022 21:50:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=62
05/23/2022 21:50:28 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 21:50:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=64
05/23/2022 21:50:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=67
05/23/2022 21:50:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=69
05/23/2022 21:50:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=72
05/23/2022 21:50:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/23/2022 21:50:53 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 21:50:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/23/2022 21:51:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=79
05/23/2022 21:51:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=82
05/23/2022 21:51:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=84
05/23/2022 21:51:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=87
05/23/2022 21:51:18 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.32631578947368417 on epoch=87
05/23/2022 21:51:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/23/2022 21:51:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=92
05/23/2022 21:51:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=94
05/23/2022 21:51:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/23/2022 21:51:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
05/23/2022 21:51:43 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.2175438596491228 on epoch=99
05/23/2022 21:51:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/23/2022 21:51:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
05/23/2022 21:51:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=107
05/23/2022 21:52:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/23/2022 21:52:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
05/23/2022 21:52:08 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.2346616065781151 on epoch=112
05/23/2022 21:52:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=114
05/23/2022 21:52:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/23/2022 21:52:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
05/23/2022 21:52:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=122
05/23/2022 21:52:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=124
05/23/2022 21:52:34 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.2554865424430642 on epoch=124
05/23/2022 21:52:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
05/23/2022 21:52:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=129
05/23/2022 21:52:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
05/23/2022 21:52:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
05/23/2022 21:52:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=137
05/23/2022 21:52:59 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.2554865424430642 on epoch=137
05/23/2022 21:53:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/23/2022 21:53:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
05/23/2022 21:53:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=144
05/23/2022 21:53:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=147
05/23/2022 21:53:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
05/23/2022 21:53:24 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.36126323475721067 on epoch=149
05/23/2022 21:53:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36126323475721067 on epoch=149, global_step=600
05/23/2022 21:53:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=152
05/23/2022 21:53:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=154
05/23/2022 21:53:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/23/2022 21:53:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
05/23/2022 21:53:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/23/2022 21:53:49 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.2874039030159669 on epoch=162
05/23/2022 21:53:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=164
05/23/2022 21:53:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=167
05/23/2022 21:54:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
05/23/2022 21:54:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=172
05/23/2022 21:54:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
05/23/2022 21:54:14 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.36126323475721067 on epoch=174
05/23/2022 21:54:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/23/2022 21:54:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/23/2022 21:54:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
05/23/2022 21:54:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/23/2022 21:54:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=187
05/23/2022 21:54:39 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.3671497584541063 on epoch=187
05/23/2022 21:54:39 - INFO - __main__ - Saving model with best Classification-F1: 0.36126323475721067 -> 0.3671497584541063 on epoch=187, global_step=750
05/23/2022 21:54:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
05/23/2022 21:54:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
05/23/2022 21:54:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
05/23/2022 21:54:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=197
05/23/2022 21:55:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/23/2022 21:55:04 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.3525641025641026 on epoch=199
05/23/2022 21:55:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/23/2022 21:55:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=204
05/23/2022 21:55:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
05/23/2022 21:55:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=209
05/23/2022 21:55:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
05/23/2022 21:55:29 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.3868065967016492 on epoch=212
05/23/2022 21:55:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3671497584541063 -> 0.3868065967016492 on epoch=212, global_step=850
05/23/2022 21:55:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
05/23/2022 21:55:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=217
05/23/2022 21:55:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
05/23/2022 21:55:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/23/2022 21:55:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=224
05/23/2022 21:55:54 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.3732394366197183 on epoch=224
05/23/2022 21:55:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/23/2022 21:56:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=229
05/23/2022 21:56:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/23/2022 21:56:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=234
05/23/2022 21:56:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
05/23/2022 21:56:19 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.39478114478114473 on epoch=237
05/23/2022 21:56:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3868065967016492 -> 0.39478114478114473 on epoch=237, global_step=950
05/23/2022 21:56:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
05/23/2022 21:56:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
05/23/2022 21:56:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
05/23/2022 21:56:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/23/2022 21:56:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/23/2022 21:56:45 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.36556995679627785 on epoch=249
05/23/2022 21:56:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/23/2022 21:56:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/23/2022 21:56:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/23/2022 21:57:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
05/23/2022 21:57:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/23/2022 21:57:10 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.2925247902364607 on epoch=262
05/23/2022 21:57:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/23/2022 21:57:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/23/2022 21:57:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
05/23/2022 21:57:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/23/2022 21:57:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/23/2022 21:57:35 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.3398989898989899 on epoch=274
05/23/2022 21:57:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/23/2022 21:57:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
05/23/2022 21:57:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 21:57:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/23/2022 21:57:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=287
05/23/2022 21:58:01 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.5730170496664195 on epoch=287
05/23/2022 21:58:01 - INFO - __main__ - Saving model with best Classification-F1: 0.39478114478114473 -> 0.5730170496664195 on epoch=287, global_step=1150
05/23/2022 21:58:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/23/2022 21:58:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/23/2022 21:58:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 21:58:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/23/2022 21:58:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/23/2022 21:58:26 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.3566815697963239 on epoch=299
05/23/2022 21:58:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/23/2022 21:58:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/23/2022 21:58:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/23/2022 21:58:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/23/2022 21:58:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/23/2022 21:58:51 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.3753123438280859 on epoch=312
05/23/2022 21:58:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/23/2022 21:59:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/23/2022 21:59:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/23/2022 21:59:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/23/2022 21:59:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 21:59:17 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5780219780219781 on epoch=324
05/23/2022 21:59:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5730170496664195 -> 0.5780219780219781 on epoch=324, global_step=1300
05/23/2022 21:59:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
05/23/2022 21:59:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
05/23/2022 21:59:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/23/2022 21:59:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/23/2022 21:59:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/23/2022 21:59:42 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5625 on epoch=337
05/23/2022 21:59:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/23/2022 21:59:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 21:59:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 22:00:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/23/2022 22:00:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/23/2022 22:00:08 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.4190761050182785 on epoch=349
05/23/2022 22:00:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 22:00:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/23/2022 22:00:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 22:00:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 22:00:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 22:00:33 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.35415625520573046 on epoch=362
05/23/2022 22:00:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 22:00:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/23/2022 22:00:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 22:00:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/23/2022 22:00:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/23/2022 22:00:58 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.34547008547008545 on epoch=374
05/23/2022 22:01:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/23/2022 22:01:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 22:01:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/23/2022 22:01:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/23/2022 22:01:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 22:01:24 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=387
05/23/2022 22:01:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/23/2022 22:01:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/23/2022 22:01:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 22:01:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 22:01:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 22:01:49 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5933528836754642 on epoch=399
05/23/2022 22:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5780219780219781 -> 0.5933528836754642 on epoch=399, global_step=1600
05/23/2022 22:01:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/23/2022 22:01:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/23/2022 22:02:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 22:02:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 22:02:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 22:02:14 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5413886829750433 on epoch=412
05/23/2022 22:02:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/23/2022 22:02:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 22:02:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 22:02:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/23/2022 22:02:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/23/2022 22:02:39 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.577195987276731 on epoch=424
05/23/2022 22:02:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 22:02:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 22:02:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/23/2022 22:02:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 22:03:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 22:03:05 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=437
05/23/2022 22:03:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 22:03:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 22:03:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 22:03:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 22:03:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 22:03:30 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.35421888053467 on epoch=449
05/23/2022 22:03:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 22:03:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/23/2022 22:03:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 22:03:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/23/2022 22:03:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 22:03:55 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.408842523596622 on epoch=462
05/23/2022 22:04:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 22:04:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 22:04:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/23/2022 22:04:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 22:04:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/23/2022 22:04:20 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6717948717948719 on epoch=474
05/23/2022 22:04:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6717948717948719 on epoch=474, global_step=1900
05/23/2022 22:04:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 22:04:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 22:04:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/23/2022 22:04:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 22:04:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/23/2022 22:04:45 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5755342667649226 on epoch=487
05/23/2022 22:04:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/23/2022 22:04:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 22:04:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 22:05:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 22:05:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 22:05:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5620723362658846 on epoch=499
05/23/2022 22:05:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 22:05:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 22:05:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 22:05:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 22:05:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 22:05:36 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6085148030340103 on epoch=512
05/23/2022 22:05:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/23/2022 22:05:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/23/2022 22:05:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 22:05:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 22:05:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 22:06:02 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5586206896551724 on epoch=524
05/23/2022 22:06:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 22:06:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/23/2022 22:06:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/23/2022 22:06:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 22:06:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 22:06:27 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5755342667649226 on epoch=537
05/23/2022 22:06:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 22:06:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 22:06:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/23/2022 22:06:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 22:06:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 22:06:52 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6092796092796093 on epoch=549
05/23/2022 22:06:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 22:07:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 22:07:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 22:07:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 22:07:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 22:07:18 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6092796092796093 on epoch=562
05/23/2022 22:07:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 22:07:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 22:07:31 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 22:07:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/23/2022 22:07:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 22:07:43 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6085148030340103 on epoch=574
05/23/2022 22:07:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 22:07:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 22:07:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 22:08:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 22:08:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 22:08:08 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.5586206896551724 on epoch=587
05/23/2022 22:08:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 22:08:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 22:08:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 22:08:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/23/2022 22:08:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 22:08:34 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6405372405372406 on epoch=599
05/23/2022 22:08:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 22:08:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 22:08:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 22:08:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 22:08:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 22:08:59 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.5730170496664195 on epoch=612
05/23/2022 22:09:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 22:09:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/23/2022 22:09:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 22:09:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 22:09:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 22:09:24 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5586206896551724 on epoch=624
05/23/2022 22:09:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 22:09:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 22:09:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 22:09:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/23/2022 22:09:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 22:09:50 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5238095238095238 on epoch=637
05/23/2022 22:09:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 22:09:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 22:10:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 22:10:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 22:10:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 22:10:15 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.5238095238095238 on epoch=649
05/23/2022 22:10:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 22:10:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/23/2022 22:10:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 22:10:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 22:10:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 22:10:40 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.624633431085044 on epoch=662
05/23/2022 22:10:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 22:10:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 22:10:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/23/2022 22:10:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 22:11:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 22:11:06 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6405372405372406 on epoch=674
05/23/2022 22:11:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 22:11:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 22:11:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 22:11:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 22:11:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 22:11:31 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.624633431085044 on epoch=687
05/23/2022 22:11:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 22:11:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 22:11:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 22:11:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 22:11:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 22:11:57 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.5921568627450979 on epoch=699
05/23/2022 22:12:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 22:12:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 22:12:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 22:12:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 22:12:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 22:12:22 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6085148030340103 on epoch=712
05/23/2022 22:12:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 22:12:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 22:12:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/23/2022 22:12:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 22:12:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 22:12:47 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6085148030340103 on epoch=724
05/23/2022 22:12:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 22:12:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 22:13:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 22:13:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 22:13:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 22:13:12 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.624633431085044 on epoch=737
05/23/2022 22:13:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/23/2022 22:13:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 22:13:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 22:13:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 22:13:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 22:13:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:13:36 - INFO - __main__ - Printing 3 examples
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:13:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:13:36 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 22:13:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:13:36 - INFO - __main__ - Printing 3 examples
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 22:13:36 - INFO - __main__ - ['refuted']
05/23/2022 22:13:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:13:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:13:36 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 22:13:37 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5730170496664195 on epoch=749
05/23/2022 22:13:37 - INFO - __main__ - save last model!
05/23/2022 22:13:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 22:13:38 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 22:13:38 - INFO - __main__ - Printing 3 examples
05/23/2022 22:13:38 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:13:38 - INFO - __main__ - ['entailed']
05/23/2022 22:13:38 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:13:38 - INFO - __main__ - ['entailed']
05/23/2022 22:13:38 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:13:38 - INFO - __main__ - ['entailed']
05/23/2022 22:13:38 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:13:55 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 22:13:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 22:13:55 - INFO - __main__ - Starting training!
05/23/2022 22:14:02 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:14:14 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 22:23:18 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_13_0.4_8_predictions.txt
05/23/2022 22:23:18 - INFO - __main__ - Classification-F1 on test data: 0.0190
05/23/2022 22:23:18 - INFO - __main__ - prefix=tab_fact_32_13, lr=0.4, bsz=8, dev_performance=0.6717948717948719, test_performance=0.019018798307891868
05/23/2022 22:23:18 - INFO - __main__ - Running ... prefix=tab_fact_32_13, lr=0.3, bsz=8 ...
05/23/2022 22:23:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:23:19 - INFO - __main__ - Printing 3 examples
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:23:19 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:23:19 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 22:23:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:23:19 - INFO - __main__ - Printing 3 examples
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 22:23:19 - INFO - __main__ - ['refuted']
05/23/2022 22:23:19 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:23:19 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:23:19 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 22:23:34 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 22:23:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 22:23:35 - INFO - __main__ - Starting training!
05/23/2022 22:23:40 - INFO - __main__ - Step 10 Global step 10 Train loss 5.06 on epoch=2
05/23/2022 22:23:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.50 on epoch=4
05/23/2022 22:23:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.58 on epoch=7
05/23/2022 22:23:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.95 on epoch=9
05/23/2022 22:23:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.47 on epoch=12
05/23/2022 22:24:01 - INFO - __main__ - Global step 50 Train loss 2.91 Classification-F1 0.03841229193341869 on epoch=12
05/23/2022 22:24:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.03841229193341869 on epoch=12, global_step=50
05/23/2022 22:24:05 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=14
05/23/2022 22:24:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=17
05/23/2022 22:24:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=19
05/23/2022 22:24:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=22
05/23/2022 22:24:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=24
05/23/2022 22:24:26 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 22:24:26 - INFO - __main__ - Saving model with best Classification-F1: 0.03841229193341869 -> 0.3333333333333333 on epoch=24, global_step=100
05/23/2022 22:24:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=27
05/23/2022 22:24:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=29
05/23/2022 22:24:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=32
05/23/2022 22:24:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=34
05/23/2022 22:24:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=37
05/23/2022 22:24:51 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 22:24:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=39
05/23/2022 22:25:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=42
05/23/2022 22:25:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=44
05/23/2022 22:25:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=47
05/23/2022 22:25:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=49
05/23/2022 22:25:16 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 22:25:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=52
05/23/2022 22:25:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=54
05/23/2022 22:25:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=57
05/23/2022 22:25:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=59
05/23/2022 22:25:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=62
05/23/2022 22:25:41 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 22:25:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=64
05/23/2022 22:25:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=67
05/23/2022 22:25:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=69
05/23/2022 22:25:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=72
05/23/2022 22:26:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
05/23/2022 22:26:06 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 22:26:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=77
05/23/2022 22:26:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=79
05/23/2022 22:26:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=82
05/23/2022 22:26:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=84
05/23/2022 22:26:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
05/23/2022 22:26:31 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 22:26:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/23/2022 22:26:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=92
05/23/2022 22:26:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
05/23/2022 22:26:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/23/2022 22:26:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/23/2022 22:26:55 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 22:27:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=102
05/23/2022 22:27:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=104
05/23/2022 22:27:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/23/2022 22:27:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/23/2022 22:27:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
05/23/2022 22:27:20 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.2175438596491228 on epoch=112
05/23/2022 22:27:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/23/2022 22:27:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
05/23/2022 22:27:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=119
05/23/2022 22:27:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
05/23/2022 22:27:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
05/23/2022 22:27:45 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.2175438596491228 on epoch=124
05/23/2022 22:27:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
05/23/2022 22:27:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=129
05/23/2022 22:27:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=132
05/23/2022 22:28:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/23/2022 22:28:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=137
05/23/2022 22:28:10 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.2175438596491228 on epoch=137
05/23/2022 22:28:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/23/2022 22:28:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=142
05/23/2022 22:28:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=144
05/23/2022 22:28:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
05/23/2022 22:28:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=149
05/23/2022 22:28:35 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.2346616065781151 on epoch=149
05/23/2022 22:28:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/23/2022 22:28:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=154
05/23/2022 22:28:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/23/2022 22:28:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=159
05/23/2022 22:28:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
05/23/2022 22:29:00 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.2554865424430642 on epoch=162
05/23/2022 22:29:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/23/2022 22:29:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
05/23/2022 22:29:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=169
05/23/2022 22:29:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=172
05/23/2022 22:29:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/23/2022 22:29:25 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.2554865424430642 on epoch=174
05/23/2022 22:29:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=177
05/23/2022 22:29:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/23/2022 22:29:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=182
05/23/2022 22:29:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
05/23/2022 22:29:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
05/23/2022 22:29:50 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.30516705516705517 on epoch=187
05/23/2022 22:29:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=189
05/23/2022 22:29:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=192
05/23/2022 22:30:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
05/23/2022 22:30:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
05/23/2022 22:30:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
05/23/2022 22:30:15 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.2753357753357753 on epoch=199
05/23/2022 22:30:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
05/23/2022 22:30:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
05/23/2022 22:30:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
05/23/2022 22:30:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/23/2022 22:30:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=212
05/23/2022 22:30:40 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.3810283687943263 on epoch=212
05/23/2022 22:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3810283687943263 on epoch=212, global_step=850
05/23/2022 22:30:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/23/2022 22:30:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
05/23/2022 22:30:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
05/23/2022 22:30:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
05/23/2022 22:31:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=224
05/23/2022 22:31:05 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.37218706047819966 on epoch=224
05/23/2022 22:31:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
05/23/2022 22:31:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
05/23/2022 22:31:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=232
05/23/2022 22:31:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=234
05/23/2022 22:31:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=237
05/23/2022 22:31:30 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.38987296475219174 on epoch=237
05/23/2022 22:31:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3810283687943263 -> 0.38987296475219174 on epoch=237, global_step=950
05/23/2022 22:31:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/23/2022 22:31:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=242
05/23/2022 22:31:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
05/23/2022 22:31:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
05/23/2022 22:31:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=249
05/23/2022 22:31:55 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.37340067340067346 on epoch=249
05/23/2022 22:32:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
05/23/2022 22:32:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=254
05/23/2022 22:32:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
05/23/2022 22:32:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
05/23/2022 22:32:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/23/2022 22:32:20 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.3923168451470338 on epoch=262
05/23/2022 22:32:20 - INFO - __main__ - Saving model with best Classification-F1: 0.38987296475219174 -> 0.3923168451470338 on epoch=262, global_step=1050
05/23/2022 22:32:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
05/23/2022 22:32:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
05/23/2022 22:32:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=269
05/23/2022 22:32:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
05/23/2022 22:32:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=274
05/23/2022 22:32:45 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.35978835978835977 on epoch=274
05/23/2022 22:32:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=277
05/23/2022 22:32:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=279
05/23/2022 22:32:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
05/23/2022 22:33:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
05/23/2022 22:33:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
05/23/2022 22:33:10 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.3702956989247312 on epoch=287
05/23/2022 22:33:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=289
05/23/2022 22:33:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/23/2022 22:33:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=294
05/23/2022 22:33:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/23/2022 22:33:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=299
05/23/2022 22:33:36 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.35978835978835977 on epoch=299
05/23/2022 22:33:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
05/23/2022 22:33:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/23/2022 22:33:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/23/2022 22:33:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
05/23/2022 22:33:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=312
05/23/2022 22:34:01 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.3776674937965261 on epoch=312
05/23/2022 22:34:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=314
05/23/2022 22:34:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
05/23/2022 22:34:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/23/2022 22:34:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=322
05/23/2022 22:34:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=324
05/23/2022 22:34:26 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.3776674937965261 on epoch=324
05/23/2022 22:34:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
05/23/2022 22:34:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
05/23/2022 22:34:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/23/2022 22:34:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/23/2022 22:34:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/23/2022 22:34:51 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.26607142857142857 on epoch=337
05/23/2022 22:34:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/23/2022 22:35:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/23/2022 22:35:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/23/2022 22:35:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/23/2022 22:35:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/23/2022 22:35:16 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.24760920134401654 on epoch=349
05/23/2022 22:35:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
05/23/2022 22:35:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
05/23/2022 22:35:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
05/23/2022 22:35:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=359
05/23/2022 22:35:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
05/23/2022 22:35:41 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.26774847870182555 on epoch=362
05/23/2022 22:35:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/23/2022 22:35:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/23/2022 22:35:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/23/2022 22:35:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/23/2022 22:36:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/23/2022 22:36:06 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.35815060343362237 on epoch=374
05/23/2022 22:36:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/23/2022 22:36:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/23/2022 22:36:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/23/2022 22:36:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/23/2022 22:36:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
05/23/2022 22:36:31 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.39201758836462025 on epoch=387
05/23/2022 22:36:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
05/23/2022 22:36:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/23/2022 22:36:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/23/2022 22:36:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/23/2022 22:36:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/23/2022 22:36:56 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.21833508956796627 on epoch=399
05/23/2022 22:37:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/23/2022 22:37:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/23/2022 22:37:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/23/2022 22:37:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/23/2022 22:37:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
05/23/2022 22:37:22 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.2801757560093048 on epoch=412
05/23/2022 22:37:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/23/2022 22:37:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/23/2022 22:37:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/23/2022 22:37:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/23/2022 22:37:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/23/2022 22:37:47 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.22181818181818183 on epoch=424
05/23/2022 22:37:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 22:37:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/23/2022 22:38:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/23/2022 22:38:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/23/2022 22:38:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/23/2022 22:38:12 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.2100103199174407 on epoch=437
05/23/2022 22:38:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/23/2022 22:38:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/23/2022 22:38:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/23/2022 22:38:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/23/2022 22:38:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/23/2022 22:38:37 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.39201758836462025 on epoch=449
05/23/2022 22:38:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/23/2022 22:38:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/23/2022 22:38:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/23/2022 22:38:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/23/2022 22:39:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 22:39:02 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.3888888888888889 on epoch=462
05/23/2022 22:39:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/23/2022 22:39:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 22:39:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/23/2022 22:39:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/23/2022 22:39:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/23/2022 22:39:28 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.2854981084489281 on epoch=474
05/23/2022 22:39:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 22:39:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/23/2022 22:39:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/23/2022 22:39:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 22:39:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/23/2022 22:39:53 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.3671794871794871 on epoch=487
05/23/2022 22:39:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/23/2022 22:40:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/23/2022 22:40:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 22:40:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 22:40:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/23/2022 22:40:18 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.3826599326599327 on epoch=499
05/23/2022 22:40:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 22:40:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/23/2022 22:40:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/23/2022 22:40:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=509
05/23/2022 22:40:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 22:40:43 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.37053948926095054 on epoch=512
05/23/2022 22:40:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 22:40:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/23/2022 22:40:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 22:41:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/23/2022 22:41:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/23/2022 22:41:08 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.3826599326599327 on epoch=524
05/23/2022 22:41:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 22:41:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 22:41:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/23/2022 22:41:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 22:41:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 22:41:34 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.38496240601503756 on epoch=537
05/23/2022 22:41:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 22:41:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 22:41:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 22:41:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 22:41:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/23/2022 22:41:59 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.38667331339315386 on epoch=549
05/23/2022 22:42:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/23/2022 22:42:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/23/2022 22:42:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 22:42:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/23/2022 22:42:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 22:42:24 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.38496240601503756 on epoch=562
05/23/2022 22:42:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 22:42:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 22:42:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 22:42:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 22:42:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 22:42:49 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.3753123438280859 on epoch=574
05/23/2022 22:42:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/23/2022 22:42:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/23/2022 22:43:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/23/2022 22:43:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/23/2022 22:43:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 22:43:14 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.36556995679627785 on epoch=587
05/23/2022 22:43:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/23/2022 22:43:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 22:43:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 22:43:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 22:43:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/23/2022 22:43:40 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2964590333419488 on epoch=599
05/23/2022 22:43:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 22:43:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 22:43:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 22:43:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 22:44:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 22:44:05 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.29411764705882354 on epoch=612
05/23/2022 22:44:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 22:44:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/23/2022 22:44:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=619
05/23/2022 22:44:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 22:44:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 22:44:30 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.38667331339315386 on epoch=624
05/23/2022 22:44:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 22:44:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 22:44:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 22:44:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 22:44:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 22:44:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.41618698761555906 on epoch=637
05/23/2022 22:44:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3923168451470338 -> 0.41618698761555906 on epoch=637, global_step=2550
05/23/2022 22:44:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 22:45:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 22:45:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 22:45:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 22:45:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 22:45:20 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.39784411276948584 on epoch=649
05/23/2022 22:45:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 22:45:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 22:45:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 22:45:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 22:45:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 22:45:45 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.38667331339315386 on epoch=662
05/23/2022 22:45:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 22:45:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 22:45:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/23/2022 22:46:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/23/2022 22:46:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 22:46:11 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.4093915343915344 on epoch=674
05/23/2022 22:46:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 22:46:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/23/2022 22:46:24 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 22:46:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
05/23/2022 22:46:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 22:46:36 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.387812551746978 on epoch=687
05/23/2022 22:46:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 22:46:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 22:46:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 22:46:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 22:46:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/23/2022 22:47:01 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.3883928571428572 on epoch=699
05/23/2022 22:47:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 22:47:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 22:47:14 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 22:47:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 22:47:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 22:47:26 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3976190476190476 on epoch=712
05/23/2022 22:47:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 22:47:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 22:47:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 22:47:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 22:47:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 22:47:51 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.38667331339315386 on epoch=724
05/23/2022 22:47:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 22:48:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 22:48:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/23/2022 22:48:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 22:48:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 22:48:16 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3753123438280859 on epoch=737
05/23/2022 22:48:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/23/2022 22:48:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 22:48:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 22:48:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 22:48:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 22:48:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:48:39 - INFO - __main__ - Printing 3 examples
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:48:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:48:39 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 22:48:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:48:39 - INFO - __main__ - Printing 3 examples
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 22:48:39 - INFO - __main__ - ['refuted']
05/23/2022 22:48:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:48:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:48:39 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 22:48:41 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2848469516822666 on epoch=749
05/23/2022 22:48:41 - INFO - __main__ - save last model!
05/23/2022 22:48:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 22:48:41 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 22:48:41 - INFO - __main__ - Printing 3 examples
05/23/2022 22:48:41 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:48:41 - INFO - __main__ - ['entailed']
05/23/2022 22:48:41 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:48:41 - INFO - __main__ - ['entailed']
05/23/2022 22:48:41 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 22:48:41 - INFO - __main__ - ['entailed']
05/23/2022 22:48:41 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:48:55 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 22:48:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 22:48:56 - INFO - __main__ - Starting training!
05/23/2022 22:49:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:49:18 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 22:58:28 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_13_0.3_8_predictions.txt
05/23/2022 22:58:28 - INFO - __main__ - Classification-F1 on test data: 0.0125
05/23/2022 22:58:29 - INFO - __main__ - prefix=tab_fact_32_13, lr=0.3, bsz=8, dev_performance=0.41618698761555906, test_performance=0.012537279273346417
05/23/2022 22:58:29 - INFO - __main__ - Running ... prefix=tab_fact_32_13, lr=0.2, bsz=8 ...
05/23/2022 22:58:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:58:30 - INFO - __main__ - Printing 3 examples
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:58:30 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:58:30 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 22:58:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 22:58:30 - INFO - __main__ - Printing 3 examples
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: track 10.0 be by morningwood [SEP] table_caption: list of songs from burnout revenge and burnout legends [SEP] table_text: track#song title#artist#album#legends track [n] 1#lights and sounds#yellowcard#5.4 lights and sounds#yes [n] 2#break on through (to the other side)#the doors#5.4 the doors#no [n] 3#shotgun#the outline#5.4 you smash it , we'll build around it#yes [n] 4#life burns!#apocalyptica#5.4 apocalyptica#no [n] 5#top of the world#the all - american rejects#5.4 move along#no [n] 6#the big jump#the chemical brothers#5.4 push the button#no [n] 7#stand up#pennywise#5.4 the fuse#yes [n] 8#tuned to a different station#dogs#5.4 turn against this land#yes [n] 9#the world#the starting line#5.4 based on a true story#no [n] 10#daft punk is playing at my house#lcd soundsystem#5.4 lcd soundsystem#no [n] 11#do what you want#ok go#5.4 oh no#yes [n] 12#band - girls - money#tsar#5.4 band - girls - money#yes [n] 13#come on#andy hunter degree#5.4 life#no [n] 14#almost here#the academy is#5.4 almost here#yes [n] 15#riot radio#the dead 60s#5.4 the dead 60s#yes [n] 16#nü rock#morningwood#5.4 morningwood#yes [n] 17#today#junkie xl#4.2 today#yes [n] 18#heard that sound#mxpx#5.4 panic#yes [n] 19#bundy#animal alpha#5.4 pheromones#yes [n] 20#i want#goldfinger#5.4 disconnection notice#yes [n] 21#hand of blood#bullet for my valentine#5.4 hand of blood#no [n] 22#dance , dance#fall out boy#5.4 from under the cork tree#no [n] 23#all the rage#funeral for a friend#5.4 hours#no [n] 24#apply some pressure#maxïmo park#5.4 a certain trigger#no [n] 25#better world#infusion#5.4 six feet above yesterday#no [n] 26#an honest mistake#the bravery#5.4 the bravery#no [n] 27#ink#finch#5.4 say hello to sunshine#yes [n] 28#the hey man!#emanuel#5.4 soundtrack to a headrush#yes [n] 29#as the tables turn#cky#5.4 an answer can be found#no [n] 30#red flag#billy talent#5.4 billy talent ii#yes [n] 31#wake the dead#comeback kid#5.4 wake the dead#yes [n] 32#helicopter#bloc party#5.4 silent alarm#no [n] 33#flyover#asian dub foundation#5.4 tank#no [n] 34#straight to video#mindless self indulgence#5.4 you'll rebel to anything#yes [n] 35#beast and the harlot#avenged sevenfold#5.4 city of evil#no [n] 36#first day#timo maas#5.4 pictures#no [n] 37#the great escape#we are scientists#5.4 with love and squalor#yes [n] 38#fight#unwritten law#5.4 here 's to the mourning#no [n] 39#fear and loathing#the black velvets#5.4 the black velvets#yes [n] 40#lullaby#thrice#5.4 vheissu#no [n] 41#shot down#nine black alps#5.4 everything is#yes [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: all the match list in the table be win by josé acasuso and play on a clay surface [SEP] table_caption: sebastián prieto [SEP] table_text: outcome#date#tournament#surface#partnering#opponent in the final#score [n] winner#november 9 , 1998#santiago , chile#clay#mariano hood#massimo bertolini devin bowen#7 - 6 , 6 - 7 , 7 - 6 [n] winner#october 4 , 1999#palermo , italy#clay#mariano hood#lan bale alberto martín#6 - 3 , 6 - 1 [n] winner#january 28 , 2001#bogotá , colombia#clay#mariano hood#martín rodríguez andré sá#6 - 2 , 6 - 4 [n] winner#february 17 , 2003#buenos aires , argentina#clay#mariano hood#lucas arnold ker david nalbandian#6 - 2 , 6 - 2 [n] winner#july 18 , 2005#stuttgart , germany#clay#josé acasuso#mariano hood tommy robredo#7 - 6 (4) , 6 - 3 [n] winner#september 12 , 2005#bucharest , romania#clay#josé acasuso#victor hănescu andrei pavel#6 - 3 , 4 - 6 , 6 - 3 [n] winner#january 30 , 2006#viña del mar , chile#clay#josé acasuso#františek čermák leoš friedl#7 - 6 (2) , 6 - 4 [n] winner#february 19 , 2007#buenos aires , argentina#clay#martín garcía#albert montañés rubén ramírez hidalgo#6 - 4 , 6 - 2 [n] winner#february 2 , 2008#viña del mar , chile#clay#josé acasuso#máximo gonzález juan mónaco#6 - 1 , 3 - 0 , ret [n] winner#february 21 , 2010#buenos aires , argentina#clay#horacio zeballos#simon greul peter luczak#7 - 6 (4) , 6 - 3 [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ -  [tab_fact] statement: waverley park be the venue for 5 of the game [SEP] table_caption: 1996 ansett australia cup [SEP] table_text: home team#home team score#away team#away team score#ground#crowd#date#time [n] adelaide#18.16 (124)#melbourne#10.5 (65)#football park#24143#friday 23 february 1996#8:00 pm [n] hawthorn#9.19 (73)#st kilda#19.13 (127)#waverley park#16061#saturday , 23 february 1996#8:00 pm [n] fremantle#7.15 (57)#west coast#10.11 (71)#marrara stadium#9078#sunday , 25 february 1996#7:05 pm [n] fitzroy#12.15 (87)#footscray#16.15 (111)#waverley park#4818#monday , 26 february 1996#8:00 pm [n] collingwood#14.10 (94)#richmond#8.14 (62)#waverley park#13307#wednesday 25 february 1996#8:00 pm [n] sydney#20.8 (128)#north melbourne#22.18 (150)#bruce stadium#9405#sunday , 2 march 1996#2:00 pm [n] carlton#14.12 (96)#essendon#8.14 (62)#waverley park#23837#saturday , 2 march 1996#8:00 pm [n] brisbane#14. 25 (109)#geelong#9.9 (63)#the gabba#18325#monday , 4 march 1996#7:00 pm [n] 
05/23/2022 22:58:30 - INFO - __main__ - ['refuted']
05/23/2022 22:58:30 - INFO - __main__ - Tokenizing Input ...
05/23/2022 22:58:30 - INFO - __main__ - Tokenizing Output ...
05/23/2022 22:58:30 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 22:58:45 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 22:58:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 22:58:45 - INFO - __main__ - Starting training!
05/23/2022 22:58:51 - INFO - __main__ - Step 10 Global step 10 Train loss 5.83 on epoch=2
05/23/2022 22:58:55 - INFO - __main__ - Step 20 Global step 20 Train loss 3.82 on epoch=4
05/23/2022 22:59:00 - INFO - __main__ - Step 30 Global step 30 Train loss 3.16 on epoch=7
05/23/2022 22:59:04 - INFO - __main__ - Step 40 Global step 40 Train loss 2.67 on epoch=9
05/23/2022 22:59:08 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=12
05/23/2022 22:59:11 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.0 on epoch=12
05/23/2022 22:59:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/23/2022 22:59:16 - INFO - __main__ - Step 60 Global step 60 Train loss 1.76 on epoch=14
05/23/2022 22:59:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.55 on epoch=17
05/23/2022 22:59:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.34 on epoch=19
05/23/2022 22:59:29 - INFO - __main__ - Step 90 Global step 90 Train loss 1.03 on epoch=22
05/23/2022 22:59:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=24
05/23/2022 22:59:36 - INFO - __main__ - Global step 100 Train loss 1.32 Classification-F1 0.22695035460992907 on epoch=24
05/23/2022 22:59:36 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.22695035460992907 on epoch=24, global_step=100
05/23/2022 22:59:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.67 on epoch=27
05/23/2022 22:59:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=29
05/23/2022 22:59:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=32
05/23/2022 22:59:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=34
05/23/2022 22:59:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=37
05/23/2022 23:00:01 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 23:00:01 - INFO - __main__ - Saving model with best Classification-F1: 0.22695035460992907 -> 0.3333333333333333 on epoch=37, global_step=150
05/23/2022 23:00:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=39
05/23/2022 23:00:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=42
05/23/2022 23:00:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=44
05/23/2022 23:00:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=47
05/23/2022 23:00:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=49
05/23/2022 23:00:26 - INFO - __main__ - Global step 200 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 23:00:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=52
05/23/2022 23:00:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=54
05/23/2022 23:00:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=57
05/23/2022 23:00:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=59
05/23/2022 23:00:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=62
05/23/2022 23:00:51 - INFO - __main__ - Global step 250 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 23:00:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
05/23/2022 23:01:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=67
05/23/2022 23:01:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=69
05/23/2022 23:01:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=72
05/23/2022 23:01:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
05/23/2022 23:01:16 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 23:01:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/23/2022 23:01:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
05/23/2022 23:01:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=82
05/23/2022 23:01:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=84
05/23/2022 23:01:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/23/2022 23:01:41 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 23:01:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=89
05/23/2022 23:01:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=92
05/23/2022 23:01:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=94
05/23/2022 23:01:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=97
05/23/2022 23:02:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
05/23/2022 23:02:07 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 23:02:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/23/2022 23:02:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
05/23/2022 23:02:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
05/23/2022 23:02:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/23/2022 23:02:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/23/2022 23:02:32 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=112
05/23/2022 23:02:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=114
05/23/2022 23:02:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
05/23/2022 23:02:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
05/23/2022 23:02:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=122
05/23/2022 23:02:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/23/2022 23:02:57 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=124
05/23/2022 23:03:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/23/2022 23:03:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/23/2022 23:03:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=132
05/23/2022 23:03:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
05/23/2022 23:03:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=137
05/23/2022 23:03:22 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=137
05/23/2022 23:03:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=139
05/23/2022 23:03:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
05/23/2022 23:03:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
05/23/2022 23:03:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=147
05/23/2022 23:03:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=149
05/23/2022 23:03:48 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=149
05/23/2022 23:03:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
05/23/2022 23:03:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
05/23/2022 23:04:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/23/2022 23:04:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
05/23/2022 23:04:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
05/23/2022 23:04:13 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=162
05/23/2022 23:04:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=164
05/23/2022 23:04:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
05/23/2022 23:04:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=169
05/23/2022 23:04:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=172
05/23/2022 23:04:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/23/2022 23:04:38 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=174
05/23/2022 23:04:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=177
05/23/2022 23:04:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=179
05/23/2022 23:04:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/23/2022 23:04:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=184
05/23/2022 23:05:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=187
05/23/2022 23:05:04 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.32631578947368417 on epoch=187
05/23/2022 23:05:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=189
05/23/2022 23:05:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=192
05/23/2022 23:05:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=194
05/23/2022 23:05:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
05/23/2022 23:05:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
05/23/2022 23:05:29 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.2175438596491228 on epoch=199
05/23/2022 23:05:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=202
05/23/2022 23:05:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=204
05/23/2022 23:05:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
05/23/2022 23:05:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=209
05/23/2022 23:05:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
05/23/2022 23:05:54 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.2175438596491228 on epoch=212
05/23/2022 23:05:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=214
05/23/2022 23:06:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=217
05/23/2022 23:06:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=219
05/23/2022 23:06:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=222
05/23/2022 23:06:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=224
05/23/2022 23:06:19 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.2175438596491228 on epoch=224
05/23/2022 23:06:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=227
05/23/2022 23:06:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=229
05/23/2022 23:06:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=232
05/23/2022 23:06:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
05/23/2022 23:06:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=237
05/23/2022 23:06:45 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.2175438596491228 on epoch=237
05/23/2022 23:06:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=239
05/23/2022 23:06:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
05/23/2022 23:06:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=244
05/23/2022 23:07:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=247
05/23/2022 23:07:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=249
05/23/2022 23:07:10 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.2175438596491228 on epoch=249
05/23/2022 23:07:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=252
05/23/2022 23:07:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
05/23/2022 23:07:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=257
05/23/2022 23:07:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=259
05/23/2022 23:07:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=262
05/23/2022 23:07:36 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.2346616065781151 on epoch=262
05/23/2022 23:07:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
05/23/2022 23:07:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
05/23/2022 23:07:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=269
05/23/2022 23:07:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/23/2022 23:07:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
05/23/2022 23:08:01 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.2346616065781151 on epoch=274
05/23/2022 23:08:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
05/23/2022 23:08:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/23/2022 23:08:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=282
05/23/2022 23:08:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=284
05/23/2022 23:08:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=287
05/23/2022 23:08:26 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.2688688688688689 on epoch=287
05/23/2022 23:08:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=289
05/23/2022 23:08:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=292
05/23/2022 23:08:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
05/23/2022 23:08:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
05/23/2022 23:08:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=299
05/23/2022 23:08:51 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.2688688688688689 on epoch=299
05/23/2022 23:08:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=302
05/23/2022 23:09:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=304
05/23/2022 23:09:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=307
05/23/2022 23:09:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=309
05/23/2022 23:09:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=312
05/23/2022 23:09:16 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.2804972804972805 on epoch=312
05/23/2022 23:09:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=314
05/23/2022 23:09:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=317
05/23/2022 23:09:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=319
05/23/2022 23:09:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/23/2022 23:09:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=324
05/23/2022 23:09:42 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.3380795910916394 on epoch=324
05/23/2022 23:09:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3380795910916394 on epoch=324, global_step=1300
05/23/2022 23:09:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=327
05/23/2022 23:09:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
05/23/2022 23:09:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
05/23/2022 23:10:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
05/23/2022 23:10:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=337
05/23/2022 23:10:07 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.36992753623188407 on epoch=337
05/23/2022 23:10:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3380795910916394 -> 0.36992753623188407 on epoch=337, global_step=1350
05/23/2022 23:10:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=339
05/23/2022 23:10:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=342
05/23/2022 23:10:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
05/23/2022 23:10:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=347
05/23/2022 23:10:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=349
05/23/2022 23:10:32 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.37218706047819966 on epoch=349
05/23/2022 23:10:32 - INFO - __main__ - Saving model with best Classification-F1: 0.36992753623188407 -> 0.37218706047819966 on epoch=349, global_step=1400
05/23/2022 23:10:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=352
05/23/2022 23:10:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=354
05/23/2022 23:10:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=357
05/23/2022 23:10:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=359
05/23/2022 23:10:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=362
05/23/2022 23:10:58 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.35284552845528455 on epoch=362
05/23/2022 23:11:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=364
05/23/2022 23:11:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=367
05/23/2022 23:11:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
05/23/2022 23:11:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=372
05/23/2022 23:11:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=374
05/23/2022 23:11:23 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.3525641025641026 on epoch=374
05/23/2022 23:11:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=377
05/23/2022 23:11:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/23/2022 23:11:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
05/23/2022 23:11:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=384
05/23/2022 23:11:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=387
05/23/2022 23:11:48 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.36738906088751283 on epoch=387
05/23/2022 23:11:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=389
05/23/2022 23:11:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=392
05/23/2022 23:12:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=394
05/23/2022 23:12:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=397
05/23/2022 23:12:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=399
05/23/2022 23:12:14 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.41390447050824414 on epoch=399
05/23/2022 23:12:14 - INFO - __main__ - Saving model with best Classification-F1: 0.37218706047819966 -> 0.41390447050824414 on epoch=399, global_step=1600
05/23/2022 23:12:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=402
05/23/2022 23:12:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=404
05/23/2022 23:12:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
05/23/2022 23:12:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/23/2022 23:12:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
05/23/2022 23:12:39 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.3923168451470338 on epoch=412
05/23/2022 23:12:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
05/23/2022 23:12:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=417
05/23/2022 23:12:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/23/2022 23:12:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
05/23/2022 23:13:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=424
05/23/2022 23:13:05 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.3966583124477861 on epoch=424
05/23/2022 23:13:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
05/23/2022 23:13:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=429
05/23/2022 23:13:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=432
05/23/2022 23:13:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
05/23/2022 23:13:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
05/23/2022 23:13:30 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.29369482976040356 on epoch=437
05/23/2022 23:13:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=439
05/23/2022 23:13:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=442
05/23/2022 23:13:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
05/23/2022 23:13:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
05/23/2022 23:13:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
05/23/2022 23:13:55 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.4190761050182785 on epoch=449
05/23/2022 23:13:55 - INFO - __main__ - Saving model with best Classification-F1: 0.41390447050824414 -> 0.4190761050182785 on epoch=449, global_step=1800
05/23/2022 23:14:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=452
05/23/2022 23:14:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
05/23/2022 23:14:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
05/23/2022 23:14:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/23/2022 23:14:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/23/2022 23:14:21 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.3987415134956119 on epoch=462
05/23/2022 23:14:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=464
05/23/2022 23:14:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/23/2022 23:14:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/23/2022 23:14:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
05/23/2022 23:14:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
05/23/2022 23:14:46 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.3884201819685691 on epoch=474
05/23/2022 23:14:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/23/2022 23:14:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/23/2022 23:14:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/23/2022 23:15:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=484
05/23/2022 23:15:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/23/2022 23:15:11 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.28484848484848485 on epoch=487
05/23/2022 23:15:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=489
05/23/2022 23:15:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/23/2022 23:15:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/23/2022 23:15:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/23/2022 23:15:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=499
05/23/2022 23:15:37 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.41997354497354494 on epoch=499
05/23/2022 23:15:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4190761050182785 -> 0.41997354497354494 on epoch=499, global_step=2000
05/23/2022 23:15:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/23/2022 23:15:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
05/23/2022 23:15:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/23/2022 23:15:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/23/2022 23:15:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
05/23/2022 23:16:02 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.41997354497354494 on epoch=512
05/23/2022 23:16:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
05/23/2022 23:16:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/23/2022 23:16:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
05/23/2022 23:16:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
05/23/2022 23:16:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/23/2022 23:16:27 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.3986765922249793 on epoch=524
05/23/2022 23:16:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
05/23/2022 23:16:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/23/2022 23:16:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/23/2022 23:16:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 23:16:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/23/2022 23:16:53 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.3013871374527113 on epoch=537
05/23/2022 23:16:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/23/2022 23:17:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/23/2022 23:17:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/23/2022 23:17:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/23/2022 23:17:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/23/2022 23:17:18 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.3986765922249793 on epoch=549
05/23/2022 23:17:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/23/2022 23:17:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/23/2022 23:17:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/23/2022 23:17:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/23/2022 23:17:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/23/2022 23:17:43 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.4061821219715956 on epoch=562
05/23/2022 23:17:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/23/2022 23:17:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/23/2022 23:17:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
05/23/2022 23:18:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/23/2022 23:18:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/23/2022 23:18:09 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.39784411276948584 on epoch=574
05/23/2022 23:18:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/23/2022 23:18:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/23/2022 23:18:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
05/23/2022 23:18:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/23/2022 23:18:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 23:18:34 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.41997354497354494 on epoch=587
05/23/2022 23:18:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/23/2022 23:18:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/23/2022 23:18:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/23/2022 23:18:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/23/2022 23:18:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
05/23/2022 23:19:00 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.41997354497354494 on epoch=599
05/23/2022 23:19:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
05/23/2022 23:19:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/23/2022 23:19:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/23/2022 23:19:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/23/2022 23:19:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/23/2022 23:19:25 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.28484848484848485 on epoch=612
05/23/2022 23:19:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 23:19:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/23/2022 23:19:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 23:19:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
05/23/2022 23:19:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 23:19:50 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.28484848484848485 on epoch=624
05/23/2022 23:19:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/23/2022 23:19:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 23:20:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/23/2022 23:20:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/23/2022 23:20:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/23/2022 23:20:16 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3851441985244802 on epoch=637
05/23/2022 23:20:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/23/2022 23:20:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/23/2022 23:20:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 23:20:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 23:20:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/23/2022 23:20:41 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.38667331339315386 on epoch=649
05/23/2022 23:20:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 23:20:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/23/2022 23:20:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/23/2022 23:20:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 23:21:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 23:21:07 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.3777115416459678 on epoch=662
05/23/2022 23:21:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 23:21:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/23/2022 23:21:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/23/2022 23:21:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/23/2022 23:21:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/23/2022 23:21:32 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.3779761904761905 on epoch=674
05/23/2022 23:21:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/23/2022 23:21:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/23/2022 23:21:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 23:21:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
05/23/2022 23:21:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 23:21:58 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.3779761904761905 on epoch=687
05/23/2022 23:22:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/23/2022 23:22:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 23:22:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/23/2022 23:22:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 23:22:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 23:22:23 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.4178780284043442 on epoch=699
05/23/2022 23:22:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 23:22:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/23/2022 23:22:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 23:22:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/23/2022 23:22:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 23:22:49 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.4093915343915344 on epoch=712
05/23/2022 23:22:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 23:22:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 23:23:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 23:23:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 23:23:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 23:23:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3014112903225806 on epoch=724
05/23/2022 23:23:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=727
05/23/2022 23:23:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/23/2022 23:23:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/23/2022 23:23:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 23:23:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/23/2022 23:23:40 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.40436326737696593 on epoch=737
05/23/2022 23:23:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/23/2022 23:23:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 23:23:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 23:23:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 23:24:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 23:24:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:24:04 - INFO - __main__ - Printing 3 examples
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:24:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:24:04 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 23:24:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:24:04 - INFO - __main__ - Printing 3 examples
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/23/2022 23:24:04 - INFO - __main__ - ['entailed']
05/23/2022 23:24:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:24:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:24:04 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 23:24:05 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2909992372234935 on epoch=749
05/23/2022 23:24:05 - INFO - __main__ - save last model!
05/23/2022 23:24:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 23:24:05 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 23:24:05 - INFO - __main__ - Printing 3 examples
05/23/2022 23:24:05 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:24:05 - INFO - __main__ - ['entailed']
05/23/2022 23:24:05 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:24:05 - INFO - __main__ - ['entailed']
05/23/2022 23:24:05 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:24:05 - INFO - __main__ - ['entailed']
05/23/2022 23:24:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:24:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 23:24:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 23:24:23 - INFO - __main__ - Starting training!
05/23/2022 23:24:30 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:24:42 - INFO - __main__ - Loaded 12792 examples from test data
05/23/2022 23:33:55 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_13_0.2_8_predictions.txt
05/23/2022 23:33:55 - INFO - __main__ - Classification-F1 on test data: 0.0109
05/23/2022 23:33:55 - INFO - __main__ - prefix=tab_fact_32_13, lr=0.2, bsz=8, dev_performance=0.41997354497354494, test_performance=0.010901457251077929
05/23/2022 23:33:55 - INFO - __main__ - Running ... prefix=tab_fact_32_21, lr=0.5, bsz=8 ...
05/23/2022 23:33:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:33:56 - INFO - __main__ - Printing 3 examples
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:33:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:33:56 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 23:33:56 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:33:56 - INFO - __main__ - Printing 3 examples
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/23/2022 23:33:56 - INFO - __main__ - ['entailed']
05/23/2022 23:33:56 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:33:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:33:56 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 23:34:11 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 23:34:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 23:34:12 - INFO - __main__ - Starting training!
05/23/2022 23:34:17 - INFO - __main__ - Step 10 Global step 10 Train loss 5.04 on epoch=2
05/23/2022 23:34:22 - INFO - __main__ - Step 20 Global step 20 Train loss 2.58 on epoch=4
05/23/2022 23:34:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.90 on epoch=7
05/23/2022 23:34:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=9
05/23/2022 23:34:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=12
05/23/2022 23:34:38 - INFO - __main__ - Global step 50 Train loss 2.27 Classification-F1 0.3333333333333333 on epoch=12
05/23/2022 23:34:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/23/2022 23:34:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=14
05/23/2022 23:34:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=17
05/23/2022 23:34:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.34 on epoch=19
05/23/2022 23:34:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=22
05/23/2022 23:35:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=24
05/23/2022 23:35:03 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=24
05/23/2022 23:35:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=27
05/23/2022 23:35:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=29
05/23/2022 23:35:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=32
05/23/2022 23:35:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=34
05/23/2022 23:35:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=37
05/23/2022 23:35:28 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=37
05/23/2022 23:35:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=39
05/23/2022 23:35:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=42
05/23/2022 23:35:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=44
05/23/2022 23:35:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=47
05/23/2022 23:35:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=49
05/23/2022 23:35:54 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/23/2022 23:35:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=52
05/23/2022 23:36:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=54
05/23/2022 23:36:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=57
05/23/2022 23:36:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=59
05/23/2022 23:36:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=62
05/23/2022 23:36:19 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/23/2022 23:36:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=64
05/23/2022 23:36:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=67
05/23/2022 23:36:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=69
05/23/2022 23:36:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=72
05/23/2022 23:36:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
05/23/2022 23:36:44 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/23/2022 23:36:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=77
05/23/2022 23:36:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=79
05/23/2022 23:36:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=82
05/23/2022 23:37:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=84
05/23/2022 23:37:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/23/2022 23:37:09 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=87
05/23/2022 23:37:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=89
05/23/2022 23:37:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=92
05/23/2022 23:37:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=94
05/23/2022 23:37:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=97
05/23/2022 23:37:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=99
05/23/2022 23:37:35 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
05/23/2022 23:37:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/23/2022 23:37:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/23/2022 23:37:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=107
05/23/2022 23:37:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/23/2022 23:37:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=112
05/23/2022 23:38:00 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/23/2022 23:38:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/23/2022 23:38:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/23/2022 23:38:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/23/2022 23:38:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
05/23/2022 23:38:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/23/2022 23:38:25 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
05/23/2022 23:38:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
05/23/2022 23:38:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/23/2022 23:38:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
05/23/2022 23:38:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
05/23/2022 23:38:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/23/2022 23:38:50 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=137
05/23/2022 23:38:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/23/2022 23:38:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=142
05/23/2022 23:39:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=144
05/23/2022 23:39:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=147
05/23/2022 23:39:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
05/23/2022 23:39:15 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=149
05/23/2022 23:39:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/23/2022 23:39:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/23/2022 23:39:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/23/2022 23:39:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
05/23/2022 23:39:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/23/2022 23:39:40 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
05/23/2022 23:39:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=164
05/23/2022 23:39:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=167
05/23/2022 23:39:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=169
05/23/2022 23:39:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
05/23/2022 23:40:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/23/2022 23:40:05 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.39047619047619053 on epoch=174
05/23/2022 23:40:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39047619047619053 on epoch=174, global_step=700
05/23/2022 23:40:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
05/23/2022 23:40:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
05/23/2022 23:40:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/23/2022 23:40:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/23/2022 23:40:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/23/2022 23:40:30 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.4796747967479674 on epoch=187
05/23/2022 23:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.39047619047619053 -> 0.4796747967479674 on epoch=187, global_step=750
05/23/2022 23:40:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
05/23/2022 23:40:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=192
05/23/2022 23:40:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
05/23/2022 23:40:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
05/23/2022 23:40:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/23/2022 23:40:55 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.2874039030159669 on epoch=199
05/23/2022 23:41:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=202
05/23/2022 23:41:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=204
05/23/2022 23:41:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=207
05/23/2022 23:41:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/23/2022 23:41:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/23/2022 23:41:21 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.41075141075141075 on epoch=212
05/23/2022 23:41:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=214
05/23/2022 23:41:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=217
05/23/2022 23:41:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
05/23/2022 23:41:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
05/23/2022 23:41:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=224
05/23/2022 23:41:46 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.17292025243832473 on epoch=224
05/23/2022 23:41:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=227
05/23/2022 23:41:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=229
05/23/2022 23:41:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=232
05/23/2022 23:42:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=234
05/23/2022 23:42:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
05/23/2022 23:42:11 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.3359929078014184 on epoch=237
05/23/2022 23:42:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/23/2022 23:42:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
05/23/2022 23:42:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
05/23/2022 23:42:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=247
05/23/2022 23:42:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/23/2022 23:42:36 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.5555555555555556 on epoch=249
05/23/2022 23:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4796747967479674 -> 0.5555555555555556 on epoch=249, global_step=1000
05/23/2022 23:42:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
05/23/2022 23:42:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
05/23/2022 23:42:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/23/2022 23:42:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
05/23/2022 23:42:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=262
05/23/2022 23:43:01 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.22815789473684212 on epoch=262
05/23/2022 23:43:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
05/23/2022 23:43:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/23/2022 23:43:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=269
05/23/2022 23:43:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
05/23/2022 23:43:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
05/23/2022 23:43:26 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.44209215442092153 on epoch=274
05/23/2022 23:43:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=277
05/23/2022 23:43:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
05/23/2022 23:43:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=282
05/23/2022 23:43:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
05/23/2022 23:43:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=287
05/23/2022 23:43:51 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.2887856897144823 on epoch=287
05/23/2022 23:43:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/23/2022 23:44:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
05/23/2022 23:44:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
05/23/2022 23:44:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/23/2022 23:44:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/23/2022 23:44:16 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.17712215320910976 on epoch=299
05/23/2022 23:44:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/23/2022 23:44:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/23/2022 23:44:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/23/2022 23:44:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/23/2022 23:44:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/23/2022 23:44:42 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5780219780219781 on epoch=312
05/23/2022 23:44:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5555555555555556 -> 0.5780219780219781 on epoch=312, global_step=1250
05/23/2022 23:44:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
05/23/2022 23:44:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
05/23/2022 23:44:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/23/2022 23:44:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/23/2022 23:45:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 23:45:07 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.19979134063641105 on epoch=324
05/23/2022 23:45:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/23/2022 23:45:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
05/23/2022 23:45:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
05/23/2022 23:45:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/23/2022 23:45:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/23/2022 23:45:32 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.09919114516815666 on epoch=337
05/23/2022 23:45:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/23/2022 23:45:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/23/2022 23:45:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 23:45:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 23:45:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/23/2022 23:45:57 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.14565527065527065 on epoch=349
05/23/2022 23:46:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 23:46:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/23/2022 23:46:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/23/2022 23:46:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/23/2022 23:46:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/23/2022 23:46:22 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.1421670117322291 on epoch=362
05/23/2022 23:46:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/23/2022 23:46:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/23/2022 23:46:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/23/2022 23:46:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/23/2022 23:46:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/23/2022 23:46:47 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.11601039103089963 on epoch=374
05/23/2022 23:46:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/23/2022 23:46:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/23/2022 23:47:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 23:47:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/23/2022 23:47:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/23/2022 23:47:12 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.14756258234519107 on epoch=387
05/23/2022 23:47:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/23/2022 23:47:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/23/2022 23:47:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 23:47:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/23/2022 23:47:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
05/23/2022 23:47:37 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.1762586115527292 on epoch=399
05/23/2022 23:47:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/23/2022 23:47:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/23/2022 23:47:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 23:47:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 23:48:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/23/2022 23:48:02 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.2772727272727272 on epoch=412
05/23/2022 23:48:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 23:48:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/23/2022 23:48:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 23:48:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/23/2022 23:48:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/23/2022 23:48:27 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.26607142857142857 on epoch=424
05/23/2022 23:48:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 23:48:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 23:48:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/23/2022 23:48:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/23/2022 23:48:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/23/2022 23:48:52 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.21116231438812086 on epoch=437
05/23/2022 23:48:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 23:49:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 23:49:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 23:49:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/23/2022 23:49:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/23/2022 23:49:18 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.1448834853090172 on epoch=449
05/23/2022 23:49:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 23:49:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 23:49:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/23/2022 23:49:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 23:49:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 23:49:42 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.1536231884057971 on epoch=462
05/23/2022 23:49:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 23:49:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 23:49:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 23:50:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 23:50:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 23:50:08 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.20462184873949582 on epoch=474
05/23/2022 23:50:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 23:50:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 23:50:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/23/2022 23:50:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 23:50:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 23:50:33 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.1488095238095238 on epoch=487
05/23/2022 23:50:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 23:50:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 23:50:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 23:50:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 23:50:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 23:50:58 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.1778309409888357 on epoch=499
05/23/2022 23:51:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 23:51:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 23:51:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 23:51:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 23:51:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/23/2022 23:51:22 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.12173202614379085 on epoch=512
05/23/2022 23:51:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 23:51:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 23:51:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/23/2022 23:51:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 23:51:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/23/2022 23:51:47 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.23041474654377883 on epoch=524
05/23/2022 23:51:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 23:51:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 23:52:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 23:52:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 23:52:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 23:52:12 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.17055075499694508 on epoch=537
05/23/2022 23:52:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 23:52:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 23:52:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/23/2022 23:52:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/23/2022 23:52:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/23/2022 23:52:37 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1720877786451557 on epoch=549
05/23/2022 23:52:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 23:52:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 23:52:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/23/2022 23:52:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 23:52:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 23:53:01 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.18263888888888888 on epoch=562
05/23/2022 23:53:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 23:53:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 23:53:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 23:53:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 23:53:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 23:53:26 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.11708089668615984 on epoch=574
05/23/2022 23:53:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 23:53:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 23:53:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 23:53:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 23:53:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 23:53:51 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.11708089668615984 on epoch=587
05/23/2022 23:53:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 23:54:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/23/2022 23:54:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 23:54:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 23:54:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 23:54:16 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.1630791035521304 on epoch=599
05/23/2022 23:54:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/23/2022 23:54:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 23:54:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 23:54:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 23:54:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 23:54:40 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.14332399626517273 on epoch=612
05/23/2022 23:54:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 23:54:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 23:54:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 23:54:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 23:55:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 23:55:05 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.11894366197183098 on epoch=624
05/23/2022 23:55:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 23:55:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 23:55:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 23:55:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 23:55:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 23:55:30 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.1343825665859564 on epoch=637
05/23/2022 23:55:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 23:55:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 23:55:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 23:55:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 23:55:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 23:55:54 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.18658280922431866 on epoch=649
05/23/2022 23:55:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 23:56:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 23:56:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 23:56:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 23:56:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 23:56:19 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.09400113186191285 on epoch=662
05/23/2022 23:56:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 23:56:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 23:56:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 23:56:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 23:56:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 23:56:44 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.1421695951107716 on epoch=674
05/23/2022 23:56:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/23/2022 23:56:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 23:56:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 23:57:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 23:57:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 23:57:09 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.08556753980482794 on epoch=687
05/23/2022 23:57:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 23:57:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 23:57:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 23:57:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 23:57:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 23:57:34 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.16243386243386243 on epoch=699
05/23/2022 23:57:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 23:57:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 23:57:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 23:57:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 23:57:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 23:57:59 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.13114754098360656 on epoch=712
05/23/2022 23:58:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 23:58:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 23:58:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/23/2022 23:58:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 23:58:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 23:58:23 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.2513157894736842 on epoch=724
05/23/2022 23:58:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 23:58:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 23:58:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 23:58:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 23:58:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 23:58:48 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.09900161030595814 on epoch=737
05/23/2022 23:58:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 23:58:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 23:59:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 23:59:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 23:59:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 23:59:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:59:11 - INFO - __main__ - Printing 3 examples
05/23/2022 23:59:11 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/23/2022 23:59:11 - INFO - __main__ - ['entailed']
05/23/2022 23:59:11 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/23/2022 23:59:11 - INFO - __main__ - ['entailed']
05/23/2022 23:59:11 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/23/2022 23:59:11 - INFO - __main__ - ['entailed']
05/23/2022 23:59:11 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:59:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:59:11 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 23:59:12 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 23:59:12 - INFO - __main__ - Printing 3 examples
05/23/2022 23:59:12 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/23/2022 23:59:12 - INFO - __main__ - ['entailed']
05/23/2022 23:59:12 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/23/2022 23:59:12 - INFO - __main__ - ['entailed']
05/23/2022 23:59:12 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/23/2022 23:59:12 - INFO - __main__ - ['entailed']
05/23/2022 23:59:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:59:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:59:12 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 23:59:13 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.13101963949421574 on epoch=749
05/23/2022 23:59:13 - INFO - __main__ - save last model!
05/23/2022 23:59:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 23:59:13 - INFO - __main__ - Start tokenizing ... 12792 instances
05/23/2022 23:59:13 - INFO - __main__ - Printing 3 examples
05/23/2022 23:59:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:59:13 - INFO - __main__ - ['entailed']
05/23/2022 23:59:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:59:13 - INFO - __main__ - ['entailed']
05/23/2022 23:59:13 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/23/2022 23:59:13 - INFO - __main__ - ['entailed']
05/23/2022 23:59:13 - INFO - __main__ - Tokenizing Input ...
05/23/2022 23:59:27 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 23:59:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 23:59:28 - INFO - __main__ - Starting training!
05/23/2022 23:59:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 23:59:50 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 00:08:15 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_21_0.5_8_predictions.txt
05/24/2022 00:08:15 - INFO - __main__ - Classification-F1 on test data: 0.0040
05/24/2022 00:08:15 - INFO - __main__ - prefix=tab_fact_32_21, lr=0.5, bsz=8, dev_performance=0.5780219780219781, test_performance=0.004028853470663894
05/24/2022 00:08:15 - INFO - __main__ - Running ... prefix=tab_fact_32_21, lr=0.4, bsz=8 ...
05/24/2022 00:08:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:08:16 - INFO - __main__ - Printing 3 examples
05/24/2022 00:08:16 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/24/2022 00:08:16 - INFO - __main__ - ['entailed']
05/24/2022 00:08:16 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/24/2022 00:08:16 - INFO - __main__ - ['entailed']
05/24/2022 00:08:16 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/24/2022 00:08:16 - INFO - __main__ - ['entailed']
05/24/2022 00:08:16 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:08:16 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:08:17 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 00:08:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:08:17 - INFO - __main__ - Printing 3 examples
05/24/2022 00:08:17 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/24/2022 00:08:17 - INFO - __main__ - ['entailed']
05/24/2022 00:08:17 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/24/2022 00:08:17 - INFO - __main__ - ['entailed']
05/24/2022 00:08:17 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/24/2022 00:08:17 - INFO - __main__ - ['entailed']
05/24/2022 00:08:17 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:08:17 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:08:17 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 00:08:36 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 00:08:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 00:08:36 - INFO - __main__ - Starting training!
05/24/2022 00:08:42 - INFO - __main__ - Step 10 Global step 10 Train loss 5.42 on epoch=2
05/24/2022 00:08:46 - INFO - __main__ - Step 20 Global step 20 Train loss 3.27 on epoch=4
05/24/2022 00:08:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.36 on epoch=7
05/24/2022 00:08:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.78 on epoch=9
05/24/2022 00:08:59 - INFO - __main__ - Step 50 Global step 50 Train loss 1.38 on epoch=12
05/24/2022 00:09:02 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.3333333333333333 on epoch=12
05/24/2022 00:09:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/24/2022 00:09:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=14
05/24/2022 00:09:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=17
05/24/2022 00:09:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=19
05/24/2022 00:09:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=22
05/24/2022 00:09:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=24
05/24/2022 00:09:27 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 00:09:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=27
05/24/2022 00:09:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=29
05/24/2022 00:09:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=32
05/24/2022 00:09:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=34
05/24/2022 00:09:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=37
05/24/2022 00:09:52 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 00:09:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=39
05/24/2022 00:10:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=42
05/24/2022 00:10:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=44
05/24/2022 00:10:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=47
05/24/2022 00:10:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=49
05/24/2022 00:10:18 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 00:10:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=52
05/24/2022 00:10:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=54
05/24/2022 00:10:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=57
05/24/2022 00:10:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=59
05/24/2022 00:10:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=62
05/24/2022 00:10:43 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 00:10:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=64
05/24/2022 00:10:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=67
05/24/2022 00:10:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=69
05/24/2022 00:11:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=72
05/24/2022 00:11:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=74
05/24/2022 00:11:08 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 00:11:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=77
05/24/2022 00:11:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=79
05/24/2022 00:11:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=82
05/24/2022 00:11:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=84
05/24/2022 00:11:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
05/24/2022 00:11:33 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 00:11:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=89
05/24/2022 00:11:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=92
05/24/2022 00:11:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 00:11:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/24/2022 00:11:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/24/2022 00:11:58 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 00:12:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/24/2022 00:12:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/24/2022 00:12:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
05/24/2022 00:12:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=109
05/24/2022 00:12:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
05/24/2022 00:12:23 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 00:12:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=114
05/24/2022 00:12:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
05/24/2022 00:12:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/24/2022 00:12:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/24/2022 00:12:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
05/24/2022 00:12:48 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 00:12:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=127
05/24/2022 00:12:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=129
05/24/2022 00:13:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/24/2022 00:13:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
05/24/2022 00:13:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=137
05/24/2022 00:13:13 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 00:13:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=139
05/24/2022 00:13:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=142
05/24/2022 00:13:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=144
05/24/2022 00:13:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=147
05/24/2022 00:13:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=149
05/24/2022 00:13:38 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 00:13:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
05/24/2022 00:13:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=154
05/24/2022 00:13:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/24/2022 00:13:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
05/24/2022 00:14:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/24/2022 00:14:03 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 00:14:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
05/24/2022 00:14:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
05/24/2022 00:14:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/24/2022 00:14:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/24/2022 00:14:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=174
05/24/2022 00:14:28 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 00:14:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=177
05/24/2022 00:14:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
05/24/2022 00:14:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
05/24/2022 00:14:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=184
05/24/2022 00:14:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/24/2022 00:14:53 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=187
05/24/2022 00:14:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
05/24/2022 00:15:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
05/24/2022 00:15:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=194
05/24/2022 00:15:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
05/24/2022 00:15:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
05/24/2022 00:15:18 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 00:15:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
05/24/2022 00:15:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=204
05/24/2022 00:15:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=207
05/24/2022 00:15:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/24/2022 00:15:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=212
05/24/2022 00:15:43 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.4181818181818182 on epoch=212
05/24/2022 00:15:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=212, global_step=850
05/24/2022 00:15:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/24/2022 00:15:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=217
05/24/2022 00:15:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
05/24/2022 00:16:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=222
05/24/2022 00:16:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=224
05/24/2022 00:16:08 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3591989987484355 on epoch=224
05/24/2022 00:16:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=227
05/24/2022 00:16:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/24/2022 00:16:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
05/24/2022 00:16:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
05/24/2022 00:16:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=237
05/24/2022 00:16:33 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.4202898550724638 on epoch=237
05/24/2022 00:16:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4202898550724638 on epoch=237, global_step=950
05/24/2022 00:16:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=239
05/24/2022 00:16:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=242
05/24/2022 00:16:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
05/24/2022 00:16:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=247
05/24/2022 00:16:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/24/2022 00:16:58 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.5151515151515151 on epoch=249
05/24/2022 00:16:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4202898550724638 -> 0.5151515151515151 on epoch=249, global_step=1000
05/24/2022 00:17:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=252
05/24/2022 00:17:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=254
05/24/2022 00:17:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
05/24/2022 00:17:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
05/24/2022 00:17:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
05/24/2022 00:17:24 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.44379029997196523 on epoch=262
05/24/2022 00:17:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
05/24/2022 00:17:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=267
05/24/2022 00:17:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=269
05/24/2022 00:17:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=272
05/24/2022 00:17:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=274
05/24/2022 00:17:49 - INFO - __main__ - Global step 1100 Train loss 0.15 Classification-F1 0.44379029997196523 on epoch=274
05/24/2022 00:17:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
05/24/2022 00:17:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=279
05/24/2022 00:18:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=282
05/24/2022 00:18:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=284
05/24/2022 00:18:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=287
05/24/2022 00:18:14 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.3915298184961106 on epoch=287
05/24/2022 00:18:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=289
05/24/2022 00:18:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=292
05/24/2022 00:18:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
05/24/2022 00:18:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/24/2022 00:18:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
05/24/2022 00:18:39 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.4181818181818182 on epoch=299
05/24/2022 00:18:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/24/2022 00:18:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
05/24/2022 00:18:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
05/24/2022 00:18:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
05/24/2022 00:19:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/24/2022 00:19:04 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.5925642984466514 on epoch=312
05/24/2022 00:19:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5925642984466514 on epoch=312, global_step=1250
05/24/2022 00:19:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
05/24/2022 00:19:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=317
05/24/2022 00:19:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=319
05/24/2022 00:19:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/24/2022 00:19:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=324
05/24/2022 00:19:29 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.5873015873015872 on epoch=324
05/24/2022 00:19:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
05/24/2022 00:19:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
05/24/2022 00:19:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
05/24/2022 00:19:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
05/24/2022 00:19:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
05/24/2022 00:19:54 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.5925642984466514 on epoch=337
05/24/2022 00:19:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/24/2022 00:20:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=342
05/24/2022 00:20:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
05/24/2022 00:20:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/24/2022 00:20:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/24/2022 00:20:19 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.5373493975903615 on epoch=349
05/24/2022 00:20:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
05/24/2022 00:20:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/24/2022 00:20:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/24/2022 00:20:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/24/2022 00:20:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/24/2022 00:20:44 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.5536037199690003 on epoch=362
05/24/2022 00:20:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/24/2022 00:20:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=367
05/24/2022 00:20:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
05/24/2022 00:21:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=372
05/24/2022 00:21:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
05/24/2022 00:21:09 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.5780219780219781 on epoch=374
05/24/2022 00:21:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=377
05/24/2022 00:21:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/24/2022 00:21:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/24/2022 00:21:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/24/2022 00:21:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/24/2022 00:21:34 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.46880856760374834 on epoch=387
05/24/2022 00:21:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/24/2022 00:21:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/24/2022 00:21:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/24/2022 00:21:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
05/24/2022 00:21:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
05/24/2022 00:21:59 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.5124770160231154 on epoch=399
05/24/2022 00:22:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/24/2022 00:22:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/24/2022 00:22:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/24/2022 00:22:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/24/2022 00:22:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/24/2022 00:22:25 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6405372405372406 on epoch=412
05/24/2022 00:22:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5925642984466514 -> 0.6405372405372406 on epoch=412, global_step=1650
05/24/2022 00:22:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/24/2022 00:22:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/24/2022 00:22:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/24/2022 00:22:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/24/2022 00:22:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/24/2022 00:22:50 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6061538461538463 on epoch=424
05/24/2022 00:22:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/24/2022 00:22:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/24/2022 00:23:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
05/24/2022 00:23:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/24/2022 00:23:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/24/2022 00:23:15 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5925642984466514 on epoch=437
05/24/2022 00:23:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/24/2022 00:23:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/24/2022 00:23:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
05/24/2022 00:23:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/24/2022 00:23:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/24/2022 00:23:40 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.41107671138630897 on epoch=449
05/24/2022 00:23:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/24/2022 00:23:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/24/2022 00:23:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/24/2022 00:23:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/24/2022 00:24:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/24/2022 00:24:05 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.5797215655371684 on epoch=462
05/24/2022 00:24:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/24/2022 00:24:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/24/2022 00:24:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/24/2022 00:24:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/24/2022 00:24:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/24/2022 00:24:31 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5873015873015872 on epoch=474
05/24/2022 00:24:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/24/2022 00:24:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/24/2022 00:24:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/24/2022 00:24:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
05/24/2022 00:24:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/24/2022 00:24:56 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6190476190476191 on epoch=487
05/24/2022 00:25:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/24/2022 00:25:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
05/24/2022 00:25:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/24/2022 00:25:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/24/2022 00:25:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/24/2022 00:25:21 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5974842767295597 on epoch=499
05/24/2022 00:25:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/24/2022 00:25:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/24/2022 00:25:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/24/2022 00:25:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/24/2022 00:25:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/24/2022 00:25:46 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6216748768472906 on epoch=512
05/24/2022 00:25:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/24/2022 00:25:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/24/2022 00:25:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/24/2022 00:26:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/24/2022 00:26:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/24/2022 00:26:11 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6014943960149439 on epoch=524
05/24/2022 00:26:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/24/2022 00:26:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
05/24/2022 00:26:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/24/2022 00:26:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/24/2022 00:26:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/24/2022 00:26:36 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6362737830491723 on epoch=537
05/24/2022 00:26:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/24/2022 00:26:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/24/2022 00:26:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/24/2022 00:26:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/24/2022 00:26:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/24/2022 00:27:01 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6190476190476191 on epoch=549
05/24/2022 00:27:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/24/2022 00:27:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/24/2022 00:27:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/24/2022 00:27:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/24/2022 00:27:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/24/2022 00:27:26 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5835835835835835 on epoch=562
05/24/2022 00:27:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/24/2022 00:27:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/24/2022 00:27:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/24/2022 00:27:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/24/2022 00:27:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/24/2022 00:27:52 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6069761729304839 on epoch=574
05/24/2022 00:27:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/24/2022 00:28:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/24/2022 00:28:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/24/2022 00:28:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/24/2022 00:28:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/24/2022 00:28:17 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.39784411276948584 on epoch=587
05/24/2022 00:28:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/24/2022 00:28:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/24/2022 00:28:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/24/2022 00:28:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/24/2022 00:28:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/24/2022 00:28:42 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.3868065967016492 on epoch=599
05/24/2022 00:28:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/24/2022 00:28:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/24/2022 00:28:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/24/2022 00:29:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/24/2022 00:29:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/24/2022 00:29:07 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.39646843245044144 on epoch=612
05/24/2022 00:29:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/24/2022 00:29:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/24/2022 00:29:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/24/2022 00:29:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/24/2022 00:29:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/24/2022 00:29:32 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5873015873015872 on epoch=624
05/24/2022 00:29:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/24/2022 00:29:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/24/2022 00:29:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/24/2022 00:29:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/24/2022 00:29:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/24/2022 00:29:57 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.387812551746978 on epoch=637
05/24/2022 00:30:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/24/2022 00:30:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/24/2022 00:30:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/24/2022 00:30:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/24/2022 00:30:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/24/2022 00:30:22 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3883928571428572 on epoch=649
05/24/2022 00:30:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/24/2022 00:30:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/24/2022 00:30:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/24/2022 00:30:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/24/2022 00:30:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/24/2022 00:30:47 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6014943960149439 on epoch=662
05/24/2022 00:30:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/24/2022 00:30:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/24/2022 00:31:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/24/2022 00:31:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/24/2022 00:31:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/24/2022 00:31:12 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.28924455825864276 on epoch=674
05/24/2022 00:31:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/24/2022 00:31:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/24/2022 00:31:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/24/2022 00:31:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/24/2022 00:31:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/24/2022 00:31:37 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.39784411276948584 on epoch=687
05/24/2022 00:31:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/24/2022 00:31:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/24/2022 00:31:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/24/2022 00:31:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/24/2022 00:31:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/24/2022 00:32:02 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5873015873015872 on epoch=699
05/24/2022 00:32:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/24/2022 00:32:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/24/2022 00:32:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/24/2022 00:32:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/24/2022 00:32:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/24/2022 00:32:27 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.23031233998975936 on epoch=712
05/24/2022 00:32:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
05/24/2022 00:32:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/24/2022 00:32:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/24/2022 00:32:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/24/2022 00:32:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 00:32:52 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3826599326599327 on epoch=724
05/24/2022 00:32:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 00:33:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 00:33:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/24/2022 00:33:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 00:33:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/24/2022 00:33:17 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.39453386988598255 on epoch=737
05/24/2022 00:33:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/24/2022 00:33:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/24/2022 00:33:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/24/2022 00:33:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/24/2022 00:33:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/24/2022 00:33:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:33:41 - INFO - __main__ - Printing 3 examples
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:33:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:33:41 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 00:33:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:33:41 - INFO - __main__ - Printing 3 examples
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/24/2022 00:33:41 - INFO - __main__ - ['entailed']
05/24/2022 00:33:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:33:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:33:41 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 00:33:43 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6046454163577959 on epoch=749
05/24/2022 00:33:43 - INFO - __main__ - save last model!
05/24/2022 00:33:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 00:33:43 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 00:33:43 - INFO - __main__ - Printing 3 examples
05/24/2022 00:33:43 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 00:33:43 - INFO - __main__ - ['entailed']
05/24/2022 00:33:43 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 00:33:43 - INFO - __main__ - ['entailed']
05/24/2022 00:33:43 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 00:33:43 - INFO - __main__ - ['entailed']
05/24/2022 00:33:43 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:34:00 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 00:34:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 00:34:00 - INFO - __main__ - Starting training!
05/24/2022 00:34:07 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:34:20 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 00:43:19 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_21_0.4_8_predictions.txt
05/24/2022 00:43:19 - INFO - __main__ - Classification-F1 on test data: 0.0158
05/24/2022 00:43:19 - INFO - __main__ - prefix=tab_fact_32_21, lr=0.4, bsz=8, dev_performance=0.6405372405372406, test_performance=0.015823630163686347
05/24/2022 00:43:19 - INFO - __main__ - Running ... prefix=tab_fact_32_21, lr=0.3, bsz=8 ...
05/24/2022 00:43:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:43:20 - INFO - __main__ - Printing 3 examples
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:43:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:43:20 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 00:43:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 00:43:20 - INFO - __main__ - Printing 3 examples
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/24/2022 00:43:20 - INFO - __main__ - ['entailed']
05/24/2022 00:43:20 - INFO - __main__ - Tokenizing Input ...
05/24/2022 00:43:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 00:43:20 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 00:43:36 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 00:43:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 00:43:36 - INFO - __main__ - Starting training!
05/24/2022 00:43:41 - INFO - __main__ - Step 10 Global step 10 Train loss 5.09 on epoch=2
05/24/2022 00:43:46 - INFO - __main__ - Step 20 Global step 20 Train loss 3.45 on epoch=4
05/24/2022 00:43:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=7
05/24/2022 00:43:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.99 on epoch=9
05/24/2022 00:43:59 - INFO - __main__ - Step 50 Global step 50 Train loss 1.58 on epoch=12
05/24/2022 00:44:02 - INFO - __main__ - Global step 50 Train loss 2.92 Classification-F1 0.0784313725490196 on epoch=12
05/24/2022 00:44:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0784313725490196 on epoch=12, global_step=50
05/24/2022 00:44:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.27 on epoch=14
05/24/2022 00:44:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
05/24/2022 00:44:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=19
05/24/2022 00:44:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=22
05/24/2022 00:44:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=24
05/24/2022 00:44:27 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.3671451355661882 on epoch=24
05/24/2022 00:44:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0784313725490196 -> 0.3671451355661882 on epoch=24, global_step=100
05/24/2022 00:44:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=27
05/24/2022 00:44:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=29
05/24/2022 00:44:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=32
05/24/2022 00:44:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=34
05/24/2022 00:44:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=37
05/24/2022 00:44:52 - INFO - __main__ - Global step 150 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 00:44:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=39
05/24/2022 00:45:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=42
05/24/2022 00:45:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=44
05/24/2022 00:45:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=47
05/24/2022 00:45:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=49
05/24/2022 00:45:17 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 00:45:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=52
05/24/2022 00:45:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=54
05/24/2022 00:45:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=57
05/24/2022 00:45:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=59
05/24/2022 00:45:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=62
05/24/2022 00:45:42 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 00:45:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=64
05/24/2022 00:45:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=67
05/24/2022 00:45:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=69
05/24/2022 00:46:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=72
05/24/2022 00:46:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
05/24/2022 00:46:08 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 00:46:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=77
05/24/2022 00:46:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/24/2022 00:46:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=82
05/24/2022 00:46:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=84
05/24/2022 00:46:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=87
05/24/2022 00:46:33 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 00:46:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/24/2022 00:46:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=92
05/24/2022 00:46:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=94
05/24/2022 00:46:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/24/2022 00:46:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
05/24/2022 00:46:58 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 00:47:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/24/2022 00:47:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/24/2022 00:47:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/24/2022 00:47:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
05/24/2022 00:47:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/24/2022 00:47:23 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 00:47:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
05/24/2022 00:47:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
05/24/2022 00:47:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/24/2022 00:47:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=122
05/24/2022 00:47:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/24/2022 00:47:47 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 00:47:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/24/2022 00:47:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=129
05/24/2022 00:48:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/24/2022 00:48:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/24/2022 00:48:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/24/2022 00:48:12 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 00:48:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=139
05/24/2022 00:48:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
05/24/2022 00:48:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=144
05/24/2022 00:48:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
05/24/2022 00:48:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
05/24/2022 00:48:37 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 00:48:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
05/24/2022 00:48:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
05/24/2022 00:48:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=157
05/24/2022 00:48:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=159
05/24/2022 00:48:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/24/2022 00:49:02 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 00:49:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=164
05/24/2022 00:49:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
05/24/2022 00:49:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=169
05/24/2022 00:49:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
05/24/2022 00:49:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/24/2022 00:49:27 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 00:49:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=177
05/24/2022 00:49:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/24/2022 00:49:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=182
05/24/2022 00:49:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
05/24/2022 00:49:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
05/24/2022 00:49:53 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=187
05/24/2022 00:49:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=189
05/24/2022 00:50:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=192
05/24/2022 00:50:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
05/24/2022 00:50:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
05/24/2022 00:50:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=199
05/24/2022 00:50:18 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 00:50:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
05/24/2022 00:50:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=204
05/24/2022 00:50:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
05/24/2022 00:50:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
05/24/2022 00:50:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
05/24/2022 00:50:43 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=212
05/24/2022 00:50:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
05/24/2022 00:50:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=217
05/24/2022 00:50:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
05/24/2022 00:51:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=222
05/24/2022 00:51:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=224
05/24/2022 00:51:08 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=224
05/24/2022 00:51:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=227
05/24/2022 00:51:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=229
05/24/2022 00:51:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=232
05/24/2022 00:51:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=234
05/24/2022 00:51:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=237
05/24/2022 00:51:34 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3591989987484355 on epoch=237
05/24/2022 00:51:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=239
05/24/2022 00:51:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
05/24/2022 00:51:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=244
05/24/2022 00:51:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=247
05/24/2022 00:51:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=249
05/24/2022 00:51:59 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.4545454545454546 on epoch=249
05/24/2022 00:51:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.4545454545454546 on epoch=249, global_step=1000
05/24/2022 00:52:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=252
05/24/2022 00:52:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=254
05/24/2022 00:52:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=257
05/24/2022 00:52:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=259
05/24/2022 00:52:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=262
05/24/2022 00:52:24 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.4385964912280702 on epoch=262
05/24/2022 00:52:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=264
05/24/2022 00:52:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=267
05/24/2022 00:52:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=269
05/24/2022 00:52:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/24/2022 00:52:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=274
05/24/2022 00:52:49 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.5134502923976608 on epoch=274
05/24/2022 00:52:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4545454545454546 -> 0.5134502923976608 on epoch=274, global_step=1100
05/24/2022 00:52:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=277
05/24/2022 00:52:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/24/2022 00:53:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=282
05/24/2022 00:53:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=284
05/24/2022 00:53:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
05/24/2022 00:53:14 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.5272727272727273 on epoch=287
05/24/2022 00:53:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5272727272727273 on epoch=287, global_step=1150
05/24/2022 00:53:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
05/24/2022 00:53:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/24/2022 00:53:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=294
05/24/2022 00:53:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=297
05/24/2022 00:53:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=299
05/24/2022 00:53:40 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.5333333333333333 on epoch=299
05/24/2022 00:53:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5272727272727273 -> 0.5333333333333333 on epoch=299, global_step=1200
05/24/2022 00:53:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
05/24/2022 00:53:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
05/24/2022 00:53:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=307
05/24/2022 00:53:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=309
05/24/2022 00:54:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=312
05/24/2022 00:54:05 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.5588547189819725 on epoch=312
05/24/2022 00:54:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5588547189819725 on epoch=312, global_step=1250
05/24/2022 00:54:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=314
05/24/2022 00:54:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
05/24/2022 00:54:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=319
05/24/2022 00:54:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/24/2022 00:54:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=324
05/24/2022 00:54:30 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.5460992907801419 on epoch=324
05/24/2022 00:54:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=327
05/24/2022 00:54:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
05/24/2022 00:54:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=332
05/24/2022 00:54:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=334
05/24/2022 00:54:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=337
05/24/2022 00:54:55 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.35421888053467 on epoch=337
05/24/2022 00:55:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=339
05/24/2022 00:55:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=342
05/24/2022 00:55:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
05/24/2022 00:55:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=347
05/24/2022 00:55:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
05/24/2022 00:55:20 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.5607843137254902 on epoch=349
05/24/2022 00:55:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5607843137254902 on epoch=349, global_step=1400
05/24/2022 00:55:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
05/24/2022 00:55:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
05/24/2022 00:55:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=357
05/24/2022 00:55:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
05/24/2022 00:55:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
05/24/2022 00:55:46 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.5330817610062892 on epoch=362
05/24/2022 00:55:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=364
05/24/2022 00:55:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=367
05/24/2022 00:55:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=369
05/24/2022 00:56:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=372
05/24/2022 00:56:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=374
05/24/2022 00:56:11 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.5058530510585305 on epoch=374
05/24/2022 00:56:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
05/24/2022 00:56:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=379
05/24/2022 00:56:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
05/24/2022 00:56:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=384
05/24/2022 00:56:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=387
05/24/2022 00:56:36 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.5555555555555556 on epoch=387
05/24/2022 00:56:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=389
05/24/2022 00:56:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
05/24/2022 00:56:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
05/24/2022 00:56:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=397
05/24/2022 00:56:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=399
05/24/2022 00:57:01 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.34446660019940184 on epoch=399
05/24/2022 00:57:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=402
05/24/2022 00:57:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=404
05/24/2022 00:57:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=407
05/24/2022 00:57:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=409
05/24/2022 00:57:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=412
05/24/2022 00:57:26 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.35202020202020207 on epoch=412
05/24/2022 00:57:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=414
05/24/2022 00:57:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=417
05/24/2022 00:57:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=419
05/24/2022 00:57:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
05/24/2022 00:57:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/24/2022 00:57:51 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.26773455377574373 on epoch=424
05/24/2022 00:57:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=427
05/24/2022 00:58:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=429
05/24/2022 00:58:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
05/24/2022 00:58:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
05/24/2022 00:58:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=437
05/24/2022 00:58:16 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.3828851682732961 on epoch=437
05/24/2022 00:58:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/24/2022 00:58:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=442
05/24/2022 00:58:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
05/24/2022 00:58:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/24/2022 00:58:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/24/2022 00:58:42 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.5755342667649226 on epoch=449
05/24/2022 00:58:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5755342667649226 on epoch=449, global_step=1800
05/24/2022 00:58:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/24/2022 00:58:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=454
05/24/2022 00:58:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
05/24/2022 00:59:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
05/24/2022 00:59:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/24/2022 00:59:07 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.5866701110824076 on epoch=462
05/24/2022 00:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5755342667649226 -> 0.5866701110824076 on epoch=462, global_step=1850
05/24/2022 00:59:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/24/2022 00:59:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=467
05/24/2022 00:59:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/24/2022 00:59:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
05/24/2022 00:59:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
05/24/2022 00:59:32 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.37686939182452645 on epoch=474
05/24/2022 00:59:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
05/24/2022 00:59:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/24/2022 00:59:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/24/2022 00:59:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/24/2022 00:59:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/24/2022 00:59:57 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.3674110835401157 on epoch=487
05/24/2022 01:00:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/24/2022 01:00:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/24/2022 01:00:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
05/24/2022 01:00:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
05/24/2022 01:00:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/24/2022 01:00:22 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.21688111168296448 on epoch=499
05/24/2022 01:00:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
05/24/2022 01:00:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/24/2022 01:00:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
05/24/2022 01:00:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/24/2022 01:00:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
05/24/2022 01:00:48 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.39814814814814814 on epoch=512
05/24/2022 01:00:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
05/24/2022 01:00:57 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/24/2022 01:01:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
05/24/2022 01:01:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/24/2022 01:01:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=524
05/24/2022 01:01:13 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.366832504145937 on epoch=524
05/24/2022 01:01:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
05/24/2022 01:01:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/24/2022 01:01:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
05/24/2022 01:01:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/24/2022 01:01:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
05/24/2022 01:01:38 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.363849765258216 on epoch=537
05/24/2022 01:01:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/24/2022 01:01:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/24/2022 01:01:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/24/2022 01:01:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
05/24/2022 01:02:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/24/2022 01:02:03 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.22033542976939202 on epoch=549
05/24/2022 01:02:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/24/2022 01:02:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/24/2022 01:02:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/24/2022 01:02:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/24/2022 01:02:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/24/2022 01:02:28 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.2848469516822666 on epoch=562
05/24/2022 01:02:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/24/2022 01:02:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/24/2022 01:02:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/24/2022 01:02:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/24/2022 01:02:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/24/2022 01:02:54 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.28498985801217036 on epoch=574
05/24/2022 01:02:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/24/2022 01:03:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/24/2022 01:03:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
05/24/2022 01:03:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
05/24/2022 01:03:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/24/2022 01:03:19 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.29833546734955185 on epoch=587
05/24/2022 01:03:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/24/2022 01:03:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/24/2022 01:03:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/24/2022 01:03:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/24/2022 01:03:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/24/2022 01:03:44 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.5755342667649226 on epoch=599
05/24/2022 01:03:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/24/2022 01:03:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/24/2022 01:03:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/24/2022 01:04:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/24/2022 01:04:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/24/2022 01:04:09 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6014943960149439 on epoch=612
05/24/2022 01:04:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5866701110824076 -> 0.6014943960149439 on epoch=612, global_step=2450
05/24/2022 01:04:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/24/2022 01:04:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/24/2022 01:04:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/24/2022 01:04:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/24/2022 01:04:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/24/2022 01:04:35 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.3656505080792937 on epoch=624
05/24/2022 01:04:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/24/2022 01:04:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
05/24/2022 01:04:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/24/2022 01:04:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/24/2022 01:04:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/24/2022 01:05:00 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3851441985244802 on epoch=637
05/24/2022 01:05:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/24/2022 01:05:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/24/2022 01:05:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/24/2022 01:05:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/24/2022 01:05:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/24/2022 01:05:25 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5873015873015872 on epoch=649
05/24/2022 01:05:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/24/2022 01:05:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
05/24/2022 01:05:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/24/2022 01:05:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/24/2022 01:05:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/24/2022 01:05:50 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5873015873015872 on epoch=662
05/24/2022 01:05:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/24/2022 01:05:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/24/2022 01:06:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/24/2022 01:06:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/24/2022 01:06:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/24/2022 01:06:15 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5755342667649226 on epoch=674
05/24/2022 01:06:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/24/2022 01:06:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/24/2022 01:06:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/24/2022 01:06:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/24/2022 01:06:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/24/2022 01:06:41 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.5586206896551724 on epoch=687
05/24/2022 01:06:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/24/2022 01:06:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/24/2022 01:06:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/24/2022 01:06:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/24/2022 01:07:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/24/2022 01:07:06 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.40796268532400465 on epoch=699
05/24/2022 01:07:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/24/2022 01:07:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/24/2022 01:07:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/24/2022 01:07:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/24/2022 01:07:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/24/2022 01:07:31 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6014943960149439 on epoch=712
05/24/2022 01:07:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/24/2022 01:07:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/24/2022 01:07:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/24/2022 01:07:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/24/2022 01:07:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 01:07:56 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6190476190476191 on epoch=724
05/24/2022 01:07:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6014943960149439 -> 0.6190476190476191 on epoch=724, global_step=2900
05/24/2022 01:08:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/24/2022 01:08:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 01:08:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 01:08:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/24/2022 01:08:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/24/2022 01:08:22 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6069761729304839 on epoch=737
05/24/2022 01:08:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/24/2022 01:08:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/24/2022 01:08:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/24/2022 01:08:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/24/2022 01:08:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/24/2022 01:08:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:08:46 - INFO - __main__ - Printing 3 examples
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:08:46 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:08:46 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 01:08:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:08:46 - INFO - __main__ - Printing 3 examples
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/24/2022 01:08:46 - INFO - __main__ - ['entailed']
05/24/2022 01:08:46 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:08:46 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:08:46 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 01:08:47 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.624633431085044 on epoch=749
05/24/2022 01:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.624633431085044 on epoch=749, global_step=3000
05/24/2022 01:08:47 - INFO - __main__ - save last model!
05/24/2022 01:08:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 01:08:47 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 01:08:47 - INFO - __main__ - Printing 3 examples
05/24/2022 01:08:47 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:08:47 - INFO - __main__ - ['entailed']
05/24/2022 01:08:47 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:08:47 - INFO - __main__ - ['entailed']
05/24/2022 01:08:47 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:08:47 - INFO - __main__ - ['entailed']
05/24/2022 01:08:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:09:02 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 01:09:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 01:09:03 - INFO - __main__ - Starting training!
05/24/2022 01:09:13 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:09:26 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 01:18:22 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_21_0.3_8_predictions.txt
05/24/2022 01:18:23 - INFO - __main__ - Classification-F1 on test data: 0.0126
05/24/2022 01:18:23 - INFO - __main__ - prefix=tab_fact_32_21, lr=0.3, bsz=8, dev_performance=0.624633431085044, test_performance=0.012629053816635868
05/24/2022 01:18:23 - INFO - __main__ - Running ... prefix=tab_fact_32_21, lr=0.2, bsz=8 ...
05/24/2022 01:18:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:18:24 - INFO - __main__ - Printing 3 examples
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:18:24 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:18:24 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 01:18:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:18:24 - INFO - __main__ - Printing 3 examples
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: audiogo be the company for pinborough , sarah sarah pinborough [SEP] table_caption: list of doctor who audiobooks [SEP] table_text: title#author#reader#format#company#release date#notes [n] another life#anghelides , peter peter anghelides#barrowman , john john barrowman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] slow decay#lane , andy andy lane#gorman , burn burn gorman#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] border princes#abnett , dan dan abnett#myles , eve eve myles#3 - cd#bbc audio#2007 - 04 - 02 2 april 2007#abridged [n] hidden#saville , steven steven savile#mori , naoko naoko mori#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] everyone says hello#abnett , dan dan abnett#gorman , burn burn gorman#2 - cd#bbc audio#2008 - 02 - 04 4 february 2008#an original audiobook , not published in book form [n] in the shadows#lidster , joseph joseph lidster#myles , eve eve myles#2 - cd#bbc audio#2009 - 05 - 07 7 may 2009#an original audiobook , not published in book form [n] the sin eaters#minchin , brian brian minchin#david - lloyd , gareth gareth david - lloyd#2 - cd#bbc audio#2009 - 06 - 04 4 june 2009#an original audiobook , not published in book form [n] department x#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] ghost train#goss , james james goss#owen , kai kai owen#2 - cd#bbc audio#2011 - 04 - 03 3 march 2011#an original audiobook , not published in book form [n] long time dead#pinborough , sarah sarah pinborough#varma , idria indira varma#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] the men who sold the world#adams , guy guy adams#telfer , john john telfer#download#audiogo#2011 - 10 - 01 october 2011#unabridged [n] army of one#edginton , ian ian edginton#owen , kai kai owen#download / cd#audiogo#2012 - 03 - 08 8 march 2012#an original audiobook , not published in book form [n] fallout#llewellyn , david david llewellyn#price , tom tom price#download / cd#audiogo#2012 - 04 - 05 5 april 2012#an original audiobook , not published in book form [n] red skies#lidster , joseph joseph lidster#telfer , john john telfer#download / cd#audiogo#2012 - 05 - 03 3 may 2012#an original audiobook , not published in book form [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: the majority of àlex corretja 's game take place on clay surface [SEP] table_caption: àlex corretja [SEP] table_text: outcome#date#championship#surface#opponent in the final#score in the final [n] runner - up#2 november 1992#guarujá , brazil#hard#carsten arriens#6 - 7 , 3 - 6 [n] runner - up#3 october 1994#palermo , italy#clay#alberto berasategui#6 - 2 , 6 - 7 (6 - 8) , 4 - 6 [n] winner#14 november 1994#buenos aires , argentina#clay#javier frana#6 - 3 , 5 - 7 , 7 - 6 (7 - 5) [n] runner - up#13 may 1996#hamburg , germany#clay#roberto carretero#6 - 2 , 4 - 6 , 4 - 6 , 4 - 6 [n] runner - up#29 july 1996#kitzbühel , austria#clay#alberto berasategui#2 - 6 , 4 - 6 , 4 - 6 [n] runner - up#7 october 1996#marbella , spain#clay#marc - kevin goellner#6 - 7 (4 - 7) , 6 - 7 (2 - 7) [n] winner#14 april 1997#estoril , portugal#clay#francisco clavet#6 - 3 , 7 - 5 [n] runner - up#28 april 1997#monte carlo , monaco#clay#marcelo ríos#4 - 6 , 4 - 6 , 3 - 6 [n] runner - up#5 may 1997#munich , germany#clay#mark philippoussis#6 - 7 , 6 - 1 , 4 - 6 [n] winner#19 may 1997#rome , italy#clay#marcelo ríos#7 - 5 , 7 - 5 , 6 - 3 [n] winner#21 july 1997#stuttgart outdoor , germany#clay#karol kučera#6 - 2 , 7 - 5 [n] winner#16 february 1998#dubai , uae#hard#félix mantilla botella#7 - 6 (7 - 0) , 6 - 0 [n] runner - up#11 may 1998#hamburg , germany#clay#albert costa#2 - 6 , 0 - 6 , 0 - 1 , ret [n] runner - up#8 june 1998#french open , paris , france#clay#carlos moyà#3 - 6 , 5 - 7 , 3 - 6 [n] winner#13 july 1998#gstaad , switzerland#clay#boris becker#7 - 6 (7 - 5) , 7 - 5 , 6 - 3 [n] winner#24 august 1998#indianapolis , us#hard#andre agassi#2 - 6 , 6 - 2 , 6 - 3 [n] winner#26 october 1998#lyon , france#carpet#tommy haas#2 - 6 , 7 - 6 (8 - 6) , 6 - 1 [n] winner#30 november 1998#tennis masters cup , hanover , germany#hard#carlos moyà#3 - 6 , 3 - 6 , 7 - 5 , 6 - 3 , 7 - 5 [n] runner - up#18 january 1999#sydney , australia#hard#todd martin#3 - 6 , 6 - 7 [n] runner - up#30 august 1999#long island , us#hard#magnus norman#6 - 7 (4 - 7) , 6 - 4 , 3 - 6 [n] runner - up#20 september 1999#mallorca , spain#clay#juan carlos ferrero#6 - 2 , 5 - 7 , 3 - 6 [n] winner#20 march 2000#indian wells , us#hard#thomas enqvist#6 - 4 , 6 - 4 , 6 - 3 [n] winner#17 july 2000#gstaad , switzerland#clay#mariano puerta#6 - 1 , 6 - 3 [n] winner#30 july 2000#kitzbühel , austria#clay#emilio benfele álvarez#6 - 3 , 6 - 1 , 3 - 0 retired [n] winner#21 august 2000#washington , us#hard#andre agassi#6 - 2 , 6 - 3 [n] winner#23 october 2000#toulouse , france#hard#carlos moyà#6 - 3 , 6 - 2 [n] runner - up#11 june 2001#french open , paris , france#clay#gustavo kuerten#7 - 6 (7 - 3) , 5 - 7 , 2 - 6 , 0 - 6 [n] winner#23 july 2001#amsterdam , netherlands#clay#younes el aynaoui#6 - 3 , 5 - 7 , 7 - 6 (7 - 0) , 3 - 6 , 6 - 4 [n] winner#15 july 2002#gstaad , switzerland#clay#gastón gaudio#6 - 3 , 7 - 6 (7 - 3) , 7 - 6 (7 - 3) [n] winner#29 july 2002#kitzbühel , austria#clay#juan carlos ferrero#6 - 4 , 6 - 1 , 6 - 3 [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ -  [tab_fact] statement: 2 pick come from missouri state [SEP] table_caption: atlanta falcons draft history [SEP] table_text: round#pick#overall#name#position#college [n] 1#2#2#tony casillas#defensive tackle#oklahoma [n] 1#17#17#tim green#linebacker#syracuse [n] 6#16#154#floyd dixon#wide receiver#stephen f austin [n] 6#21#159#keith williams#wide receiver#southwest missouri state [n] 8#3#197#kevin hudgens#defensive end#idaho state [n] 9#3#224#kevin starks#tight end#minnesota [n] 10#3#252#tony baker#running back#east carolina [n] 11#3#280#chris hegg#quarterback#northeast missouri state [n] 12#3#308#steve griffin#wide receiver#purdue [n] 
05/24/2022 01:18:24 - INFO - __main__ - ['entailed']
05/24/2022 01:18:24 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:18:24 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:18:24 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 01:18:44 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 01:18:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 01:18:45 - INFO - __main__ - Starting training!
05/24/2022 01:18:50 - INFO - __main__ - Step 10 Global step 10 Train loss 5.62 on epoch=2
05/24/2022 01:18:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=4
05/24/2022 01:18:58 - INFO - __main__ - Step 30 Global step 30 Train loss 3.15 on epoch=7
05/24/2022 01:19:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.69 on epoch=9
05/24/2022 01:19:07 - INFO - __main__ - Step 50 Global step 50 Train loss 2.35 on epoch=12
05/24/2022 01:19:10 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.0 on epoch=12
05/24/2022 01:19:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/24/2022 01:19:14 - INFO - __main__ - Step 60 Global step 60 Train loss 2.09 on epoch=14
05/24/2022 01:19:19 - INFO - __main__ - Step 70 Global step 70 Train loss 1.88 on epoch=17
05/24/2022 01:19:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.51 on epoch=19
05/24/2022 01:19:28 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=22
05/24/2022 01:19:32 - INFO - __main__ - Step 100 Global step 100 Train loss 1.13 on epoch=24
05/24/2022 01:19:35 - INFO - __main__ - Global step 100 Train loss 1.58 Classification-F1 0.2045454545454545 on epoch=24
05/24/2022 01:19:35 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.2045454545454545 on epoch=24, global_step=100
05/24/2022 01:19:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=27
05/24/2022 01:19:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=29
05/24/2022 01:19:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=32
05/24/2022 01:19:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=34
05/24/2022 01:19:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=37
05/24/2022 01:20:00 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 01:20:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2045454545454545 -> 0.3333333333333333 on epoch=37, global_step=150
05/24/2022 01:20:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=39
05/24/2022 01:20:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=42
05/24/2022 01:20:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=44
05/24/2022 01:20:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=47
05/24/2022 01:20:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=49
05/24/2022 01:20:25 - INFO - __main__ - Global step 200 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 01:20:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=52
05/24/2022 01:20:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=54
05/24/2022 01:20:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=57
05/24/2022 01:20:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=59
05/24/2022 01:20:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=62
05/24/2022 01:20:50 - INFO - __main__ - Global step 250 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 01:20:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=64
05/24/2022 01:20:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=67
05/24/2022 01:21:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=69
05/24/2022 01:21:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=72
05/24/2022 01:21:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=74
05/24/2022 01:21:14 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 01:21:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=77
05/24/2022 01:21:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
05/24/2022 01:21:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=82
05/24/2022 01:21:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=84
05/24/2022 01:21:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=87
05/24/2022 01:21:39 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 01:21:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=89
05/24/2022 01:21:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/24/2022 01:21:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 01:21:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=97
05/24/2022 01:22:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
05/24/2022 01:22:04 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 01:22:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/24/2022 01:22:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
05/24/2022 01:22:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=107
05/24/2022 01:22:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=109
05/24/2022 01:22:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/24/2022 01:22:29 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 01:22:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=114
05/24/2022 01:22:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=117
05/24/2022 01:22:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=119
05/24/2022 01:22:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=122
05/24/2022 01:22:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=124
05/24/2022 01:22:54 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 01:22:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=127
05/24/2022 01:23:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=129
05/24/2022 01:23:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/24/2022 01:23:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/24/2022 01:23:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
05/24/2022 01:23:19 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 01:23:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/24/2022 01:23:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
05/24/2022 01:23:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
05/24/2022 01:23:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=147
05/24/2022 01:23:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=149
05/24/2022 01:23:44 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 01:23:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/24/2022 01:23:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
05/24/2022 01:23:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=157
05/24/2022 01:24:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
05/24/2022 01:24:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
05/24/2022 01:24:09 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 01:24:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
05/24/2022 01:24:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=167
05/24/2022 01:24:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/24/2022 01:24:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/24/2022 01:24:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/24/2022 01:24:34 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 01:24:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=177
05/24/2022 01:24:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=179
05/24/2022 01:24:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/24/2022 01:24:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
05/24/2022 01:24:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
05/24/2022 01:24:59 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=187
05/24/2022 01:25:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
05/24/2022 01:25:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
05/24/2022 01:25:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=194
05/24/2022 01:25:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=197
05/24/2022 01:25:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=199
05/24/2022 01:25:24 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 01:25:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=202
05/24/2022 01:25:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=204
05/24/2022 01:25:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
05/24/2022 01:25:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
05/24/2022 01:25:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=212
05/24/2022 01:25:49 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=212
05/24/2022 01:25:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=214
05/24/2022 01:25:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
05/24/2022 01:26:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=219
05/24/2022 01:26:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=222
05/24/2022 01:26:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=224
05/24/2022 01:26:14 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=224
05/24/2022 01:26:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=227
05/24/2022 01:26:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/24/2022 01:26:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=232
05/24/2022 01:26:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=234
05/24/2022 01:26:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=237
05/24/2022 01:26:39 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=237
05/24/2022 01:26:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
05/24/2022 01:26:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
05/24/2022 01:26:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
05/24/2022 01:26:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=247
05/24/2022 01:27:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=249
05/24/2022 01:27:04 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=249
05/24/2022 01:27:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=252
05/24/2022 01:27:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=254
05/24/2022 01:27:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=257
05/24/2022 01:27:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=259
05/24/2022 01:27:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=262
05/24/2022 01:27:30 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=262
05/24/2022 01:27:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
05/24/2022 01:27:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=267
05/24/2022 01:27:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=269
05/24/2022 01:27:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=272
05/24/2022 01:27:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=274
05/24/2022 01:27:55 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=274
05/24/2022 01:27:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=277
05/24/2022 01:28:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=279
05/24/2022 01:28:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
05/24/2022 01:28:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=284
05/24/2022 01:28:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=287
05/24/2022 01:28:20 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=287
05/24/2022 01:28:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=289
05/24/2022 01:28:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=292
05/24/2022 01:28:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=294
05/24/2022 01:28:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=297
05/24/2022 01:28:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=299
05/24/2022 01:28:45 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=299
05/24/2022 01:28:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=302
05/24/2022 01:28:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=304
05/24/2022 01:28:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=307
05/24/2022 01:29:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=309
05/24/2022 01:29:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=312
05/24/2022 01:29:10 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=312
05/24/2022 01:29:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
05/24/2022 01:29:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=317
05/24/2022 01:29:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=319
05/24/2022 01:29:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=322
05/24/2022 01:29:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=324
05/24/2022 01:29:35 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=324
05/24/2022 01:29:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=327
05/24/2022 01:29:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=329
05/24/2022 01:29:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=332
05/24/2022 01:29:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
05/24/2022 01:29:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=337
05/24/2022 01:30:00 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.3671451355661882 on epoch=337
05/24/2022 01:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3671451355661882 on epoch=337, global_step=1350
05/24/2022 01:30:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
05/24/2022 01:30:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=342
05/24/2022 01:30:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=344
05/24/2022 01:30:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=347
05/24/2022 01:30:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=349
05/24/2022 01:30:25 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=349
05/24/2022 01:30:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=352
05/24/2022 01:30:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=354
05/24/2022 01:30:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=357
05/24/2022 01:30:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=359
05/24/2022 01:30:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=362
05/24/2022 01:30:51 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.39047619047619053 on epoch=362
05/24/2022 01:30:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.39047619047619053 on epoch=362, global_step=1450
05/24/2022 01:30:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=364
05/24/2022 01:30:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=367
05/24/2022 01:31:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=369
05/24/2022 01:31:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=372
05/24/2022 01:31:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=374
05/24/2022 01:31:16 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.3591989987484355 on epoch=374
05/24/2022 01:31:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=377
05/24/2022 01:31:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
05/24/2022 01:31:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=382
05/24/2022 01:31:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
05/24/2022 01:31:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=387
05/24/2022 01:31:41 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.39047619047619053 on epoch=387
05/24/2022 01:31:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=389
05/24/2022 01:31:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=392
05/24/2022 01:31:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=394
05/24/2022 01:31:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=397
05/24/2022 01:32:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
05/24/2022 01:32:06 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.4202898550724638 on epoch=399
05/24/2022 01:32:06 - INFO - __main__ - Saving model with best Classification-F1: 0.39047619047619053 -> 0.4202898550724638 on epoch=399, global_step=1600
05/24/2022 01:32:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=402
05/24/2022 01:32:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=404
05/24/2022 01:32:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=407
05/24/2022 01:32:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=409
05/24/2022 01:32:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=412
05/24/2022 01:32:31 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.4202898550724638 on epoch=412
05/24/2022 01:32:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=414
05/24/2022 01:32:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=417
05/24/2022 01:32:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=419
05/24/2022 01:32:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=422
05/24/2022 01:32:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=424
05/24/2022 01:32:56 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.4652837798905215 on epoch=424
05/24/2022 01:32:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4202898550724638 -> 0.4652837798905215 on epoch=424, global_step=1700
05/24/2022 01:33:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=427
05/24/2022 01:33:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=429
05/24/2022 01:33:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=432
05/24/2022 01:33:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=434
05/24/2022 01:33:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=437
05/24/2022 01:33:21 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.4545454545454546 on epoch=437
05/24/2022 01:33:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=439
05/24/2022 01:33:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=442
05/24/2022 01:33:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=444
05/24/2022 01:33:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=447
05/24/2022 01:33:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=449
05/24/2022 01:33:46 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.4909862142099682 on epoch=449
05/24/2022 01:33:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4652837798905215 -> 0.4909862142099682 on epoch=449, global_step=1800
05/24/2022 01:33:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=452
05/24/2022 01:33:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=454
05/24/2022 01:34:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=457
05/24/2022 01:34:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=459
05/24/2022 01:34:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=462
05/24/2022 01:34:12 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.45718194254445965 on epoch=462
05/24/2022 01:34:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=464
05/24/2022 01:34:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=467
05/24/2022 01:34:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=469
05/24/2022 01:34:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=472
05/24/2022 01:34:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=474
05/24/2022 01:34:37 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.46843853820598 on epoch=474
05/24/2022 01:34:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=477
05/24/2022 01:34:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=479
05/24/2022 01:34:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=482
05/24/2022 01:34:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=484
05/24/2022 01:34:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=487
05/24/2022 01:35:02 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.45718194254445965 on epoch=487
05/24/2022 01:35:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=489
05/24/2022 01:35:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=492
05/24/2022 01:35:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=494
05/24/2022 01:35:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=497
05/24/2022 01:35:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=499
05/24/2022 01:35:27 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.6156156156156156 on epoch=499
05/24/2022 01:35:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4909862142099682 -> 0.6156156156156156 on epoch=499, global_step=2000
05/24/2022 01:35:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=502
05/24/2022 01:35:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=504
05/24/2022 01:35:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=507
05/24/2022 01:35:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=509
05/24/2022 01:35:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=512
05/24/2022 01:35:52 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.35284552845528455 on epoch=512
05/24/2022 01:35:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=514
05/24/2022 01:36:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=517
05/24/2022 01:36:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=519
05/24/2022 01:36:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=522
05/24/2022 01:36:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=524
05/24/2022 01:36:17 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.4181818181818182 on epoch=524
05/24/2022 01:36:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=527
05/24/2022 01:36:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=529
05/24/2022 01:36:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=532
05/24/2022 01:36:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=534
05/24/2022 01:36:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=537
05/24/2022 01:36:42 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.6296855345911949 on epoch=537
05/24/2022 01:36:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6156156156156156 -> 0.6296855345911949 on epoch=537, global_step=2150
05/24/2022 01:36:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=539
05/24/2022 01:36:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=542
05/24/2022 01:36:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=544
05/24/2022 01:37:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=547
05/24/2022 01:37:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=549
05/24/2022 01:37:08 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.44379029997196523 on epoch=549
05/24/2022 01:37:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=552
05/24/2022 01:37:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=554
05/24/2022 01:37:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=557
05/24/2022 01:37:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=559
05/24/2022 01:37:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=562
05/24/2022 01:37:33 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.3526570048309179 on epoch=562
05/24/2022 01:37:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=564
05/24/2022 01:37:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=567
05/24/2022 01:37:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=569
05/24/2022 01:37:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=572
05/24/2022 01:37:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=574
05/24/2022 01:37:58 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.5789473684210527 on epoch=574
05/24/2022 01:38:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=577
05/24/2022 01:38:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=579
05/24/2022 01:38:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=582
05/24/2022 01:38:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=584
05/24/2022 01:38:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=587
05/24/2022 01:38:23 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.5373493975903615 on epoch=587
05/24/2022 01:38:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=589
05/24/2022 01:38:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=592
05/24/2022 01:38:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=594
05/24/2022 01:38:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=597
05/24/2022 01:38:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=599
05/24/2022 01:38:49 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.6251591545709192 on epoch=599
05/24/2022 01:38:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=602
05/24/2022 01:38:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=604
05/24/2022 01:39:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=607
05/24/2022 01:39:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=609
05/24/2022 01:39:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=612
05/24/2022 01:39:14 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.6476476476476476 on epoch=612
05/24/2022 01:39:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6296855345911949 -> 0.6476476476476476 on epoch=612, global_step=2450
05/24/2022 01:39:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=614
05/24/2022 01:39:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
05/24/2022 01:39:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=619
05/24/2022 01:39:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=622
05/24/2022 01:39:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=624
05/24/2022 01:39:39 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.5925642984466514 on epoch=624
05/24/2022 01:39:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
05/24/2022 01:39:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=629
05/24/2022 01:39:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=632
05/24/2022 01:39:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=634
05/24/2022 01:40:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=637
05/24/2022 01:40:04 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.6251591545709192 on epoch=637
05/24/2022 01:40:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=639
05/24/2022 01:40:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=642
05/24/2022 01:40:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=644
05/24/2022 01:40:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=647
05/24/2022 01:40:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=649
05/24/2022 01:40:30 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.6384180790960452 on epoch=649
05/24/2022 01:40:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=652
05/24/2022 01:40:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=654
05/24/2022 01:40:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=657
05/24/2022 01:40:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=659
05/24/2022 01:40:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=662
05/24/2022 01:40:55 - INFO - __main__ - Global step 2650 Train loss 0.09 Classification-F1 0.6476476476476476 on epoch=662
05/24/2022 01:41:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=664
05/24/2022 01:41:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=667
05/24/2022 01:41:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=669
05/24/2022 01:41:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=672
05/24/2022 01:41:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=674
05/24/2022 01:41:20 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.6362737830491723 on epoch=674
05/24/2022 01:41:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=677
05/24/2022 01:41:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=679
05/24/2022 01:41:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=682
05/24/2022 01:41:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=684
05/24/2022 01:41:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/24/2022 01:41:46 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.6190476190476191 on epoch=687
05/24/2022 01:41:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
05/24/2022 01:41:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=692
05/24/2022 01:41:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=694
05/24/2022 01:42:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
05/24/2022 01:42:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=699
05/24/2022 01:42:11 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.5536037199690003 on epoch=699
05/24/2022 01:42:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=702
05/24/2022 01:42:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
05/24/2022 01:42:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=707
05/24/2022 01:42:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=709
05/24/2022 01:42:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=712
05/24/2022 01:42:36 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.37973822879483254 on epoch=712
05/24/2022 01:42:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/24/2022 01:42:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=717
05/24/2022 01:42:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=719
05/24/2022 01:42:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=722
05/24/2022 01:42:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=724
05/24/2022 01:43:02 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.6333748443337484 on epoch=724
05/24/2022 01:43:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=727
05/24/2022 01:43:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=729
05/24/2022 01:43:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=732
05/24/2022 01:43:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=734
05/24/2022 01:43:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=737
05/24/2022 01:43:27 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.6156156156156156 on epoch=737
05/24/2022 01:43:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=739
05/24/2022 01:43:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=742
05/24/2022 01:43:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=744
05/24/2022 01:43:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=747
05/24/2022 01:43:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
05/24/2022 01:43:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:43:51 - INFO - __main__ - Printing 3 examples
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:43:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:43:51 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 01:43:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:43:51 - INFO - __main__ - Printing 3 examples
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 01:43:51 - INFO - __main__ - ['refuted']
05/24/2022 01:43:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:43:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:43:51 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 01:43:52 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.6156156156156156 on epoch=749
05/24/2022 01:43:52 - INFO - __main__ - save last model!
05/24/2022 01:43:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 01:43:52 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 01:43:52 - INFO - __main__ - Printing 3 examples
05/24/2022 01:43:52 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:43:52 - INFO - __main__ - ['entailed']
05/24/2022 01:43:52 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:43:52 - INFO - __main__ - ['entailed']
05/24/2022 01:43:52 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 01:43:52 - INFO - __main__ - ['entailed']
05/24/2022 01:43:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:44:09 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 01:44:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 01:44:10 - INFO - __main__ - Starting training!
05/24/2022 01:44:17 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:44:29 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 01:53:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_21_0.2_8_predictions.txt
05/24/2022 01:53:43 - INFO - __main__ - Classification-F1 on test data: 0.0387
05/24/2022 01:53:44 - INFO - __main__ - prefix=tab_fact_32_21, lr=0.2, bsz=8, dev_performance=0.6476476476476476, test_performance=0.03867193030013989
05/24/2022 01:53:44 - INFO - __main__ - Running ... prefix=tab_fact_32_42, lr=0.5, bsz=8 ...
05/24/2022 01:53:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:53:45 - INFO - __main__ - Printing 3 examples
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:53:45 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:53:45 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 01:53:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 01:53:45 - INFO - __main__ - Printing 3 examples
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 01:53:45 - INFO - __main__ - ['refuted']
05/24/2022 01:53:45 - INFO - __main__ - Tokenizing Input ...
05/24/2022 01:53:45 - INFO - __main__ - Tokenizing Output ...
05/24/2022 01:53:45 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 01:54:04 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 01:54:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 01:54:05 - INFO - __main__ - Starting training!
05/24/2022 01:54:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.78 on epoch=2
05/24/2022 01:54:14 - INFO - __main__ - Step 20 Global step 20 Train loss 2.75 on epoch=4
05/24/2022 01:54:19 - INFO - __main__ - Step 30 Global step 30 Train loss 1.72 on epoch=7
05/24/2022 01:54:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.16 on epoch=9
05/24/2022 01:54:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.71 on epoch=12
05/24/2022 01:54:30 - INFO - __main__ - Global step 50 Train loss 2.23 Classification-F1 0.3333333333333333 on epoch=12
05/24/2022 01:54:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/24/2022 01:54:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=14
05/24/2022 01:54:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=17
05/24/2022 01:54:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.35 on epoch=19
05/24/2022 01:54:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=22
05/24/2022 01:54:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=24
05/24/2022 01:54:55 - INFO - __main__ - Global step 100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 01:55:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=27
05/24/2022 01:55:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=29
05/24/2022 01:55:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=32
05/24/2022 01:55:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=34
05/24/2022 01:55:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=37
05/24/2022 01:55:20 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 01:55:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=39
05/24/2022 01:55:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=42
05/24/2022 01:55:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=44
05/24/2022 01:55:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=47
05/24/2022 01:55:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=49
05/24/2022 01:55:45 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 01:55:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=52
05/24/2022 01:55:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=54
05/24/2022 01:55:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=57
05/24/2022 01:56:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=59
05/24/2022 01:56:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=62
05/24/2022 01:56:10 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 01:56:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=64
05/24/2022 01:56:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=67
05/24/2022 01:56:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=69
05/24/2022 01:56:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=72
05/24/2022 01:56:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=74
05/24/2022 01:56:35 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 01:56:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=77
05/24/2022 01:56:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=79
05/24/2022 01:56:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=82
05/24/2022 01:56:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=84
05/24/2022 01:56:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=87
05/24/2022 01:57:00 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 01:57:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/24/2022 01:57:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/24/2022 01:57:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=94
05/24/2022 01:57:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=97
05/24/2022 01:57:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=99
05/24/2022 01:57:25 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.41075141075141075 on epoch=99
05/24/2022 01:57:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.41075141075141075 on epoch=99, global_step=400
05/24/2022 01:57:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/24/2022 01:57:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=104
05/24/2022 01:57:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/24/2022 01:57:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
05/24/2022 01:57:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
05/24/2022 01:57:50 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.2688688688688689 on epoch=112
05/24/2022 01:57:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=114
05/24/2022 01:57:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.15 on epoch=117
05/24/2022 01:58:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=119
05/24/2022 01:58:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/24/2022 01:58:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=124
05/24/2022 01:58:15 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.23342175066312998 on epoch=124
05/24/2022 01:58:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
05/24/2022 01:58:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=129
05/24/2022 01:58:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/24/2022 01:58:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/24/2022 01:58:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=137
05/24/2022 01:58:40 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.23386243386243388 on epoch=137
05/24/2022 01:58:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/24/2022 01:58:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=142
05/24/2022 01:58:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=144
05/24/2022 01:58:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=147
05/24/2022 01:59:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
05/24/2022 01:59:05 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.23488372093023255 on epoch=149
05/24/2022 01:59:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=152
05/24/2022 01:59:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=154
05/24/2022 01:59:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=157
05/24/2022 01:59:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
05/24/2022 01:59:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=162
05/24/2022 01:59:30 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.2519480519480519 on epoch=162
05/24/2022 01:59:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
05/24/2022 01:59:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=167
05/24/2022 01:59:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=169
05/24/2022 01:59:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
05/24/2022 01:59:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=174
05/24/2022 01:59:55 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.14947974730583424 on epoch=174
05/24/2022 01:59:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/24/2022 02:00:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/24/2022 02:00:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/24/2022 02:00:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
05/24/2022 02:00:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=187
05/24/2022 02:00:20 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.15564625850340136 on epoch=187
05/24/2022 02:00:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=189
05/24/2022 02:00:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=192
05/24/2022 02:00:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/24/2022 02:00:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/24/2022 02:00:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
05/24/2022 02:00:45 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.13084772370486658 on epoch=199
05/24/2022 02:00:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=202
05/24/2022 02:00:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
05/24/2022 02:00:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
05/24/2022 02:01:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
05/24/2022 02:01:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
05/24/2022 02:01:10 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.16059757236227823 on epoch=212
05/24/2022 02:01:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
05/24/2022 02:01:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
05/24/2022 02:01:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
05/24/2022 02:01:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/24/2022 02:01:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
05/24/2022 02:01:35 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.14620920832100956 on epoch=224
05/24/2022 02:01:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
05/24/2022 02:01:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
05/24/2022 02:01:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
05/24/2022 02:01:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
05/24/2022 02:01:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
05/24/2022 02:02:01 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.15802611367127495 on epoch=237
05/24/2022 02:02:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=239
05/24/2022 02:02:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/24/2022 02:02:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/24/2022 02:02:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
05/24/2022 02:02:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/24/2022 02:02:26 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.1303966303966304 on epoch=249
05/24/2022 02:02:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/24/2022 02:02:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/24/2022 02:02:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/24/2022 02:02:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
05/24/2022 02:02:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/24/2022 02:02:51 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.1256213704994193 on epoch=262
05/24/2022 02:02:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
05/24/2022 02:03:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/24/2022 02:03:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/24/2022 02:03:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
05/24/2022 02:03:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/24/2022 02:03:16 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.1913186813186813 on epoch=274
05/24/2022 02:03:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/24/2022 02:03:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/24/2022 02:03:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/24/2022 02:03:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/24/2022 02:03:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/24/2022 02:03:41 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.10675185517952814 on epoch=287
05/24/2022 02:03:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/24/2022 02:03:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/24/2022 02:03:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/24/2022 02:03:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/24/2022 02:04:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/24/2022 02:04:06 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.15306122448979595 on epoch=299
05/24/2022 02:04:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/24/2022 02:04:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/24/2022 02:04:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/24/2022 02:04:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/24/2022 02:04:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/24/2022 02:04:32 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.09973174366616991 on epoch=312
05/24/2022 02:04:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/24/2022 02:04:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/24/2022 02:04:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/24/2022 02:04:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/24/2022 02:04:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/24/2022 02:04:57 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.12697727926373312 on epoch=324
05/24/2022 02:05:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/24/2022 02:05:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/24/2022 02:05:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/24/2022 02:05:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/24/2022 02:05:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/24/2022 02:05:22 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.08590163934426229 on epoch=337
05/24/2022 02:05:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/24/2022 02:05:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/24/2022 02:05:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/24/2022 02:05:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/24/2022 02:05:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/24/2022 02:05:47 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.1380018674136321 on epoch=349
05/24/2022 02:05:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/24/2022 02:05:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/24/2022 02:06:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/24/2022 02:06:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/24/2022 02:06:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/24/2022 02:06:13 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.14273806216012383 on epoch=362
05/24/2022 02:06:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/24/2022 02:06:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/24/2022 02:06:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/24/2022 02:06:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/24/2022 02:06:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/24/2022 02:06:38 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.14221938775510204 on epoch=374
05/24/2022 02:06:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/24/2022 02:06:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/24/2022 02:06:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/24/2022 02:06:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/24/2022 02:07:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/24/2022 02:07:03 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.15105908584169456 on epoch=387
05/24/2022 02:07:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/24/2022 02:07:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/24/2022 02:07:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/24/2022 02:07:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/24/2022 02:07:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/24/2022 02:07:28 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.10987509758001562 on epoch=399
05/24/2022 02:07:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/24/2022 02:07:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/24/2022 02:07:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/24/2022 02:07:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/24/2022 02:07:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/24/2022 02:07:53 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.144440384849592 on epoch=412
05/24/2022 02:07:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/24/2022 02:08:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/24/2022 02:08:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/24/2022 02:08:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/24/2022 02:08:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/24/2022 02:08:19 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.11601731601731603 on epoch=424
05/24/2022 02:08:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/24/2022 02:08:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/24/2022 02:08:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/24/2022 02:08:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/24/2022 02:08:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/24/2022 02:08:44 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.12586805555555555 on epoch=437
05/24/2022 02:08:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
05/24/2022 02:08:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/24/2022 02:08:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/24/2022 02:09:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/24/2022 02:09:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/24/2022 02:09:09 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.12244897959183673 on epoch=449
05/24/2022 02:09:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/24/2022 02:09:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/24/2022 02:09:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/24/2022 02:09:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/24/2022 02:09:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/24/2022 02:09:34 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.0755748015259305 on epoch=462
05/24/2022 02:09:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/24/2022 02:09:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/24/2022 02:09:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/24/2022 02:09:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/24/2022 02:09:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/24/2022 02:09:59 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.07828443877551021 on epoch=474
05/24/2022 02:10:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/24/2022 02:10:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/24/2022 02:10:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/24/2022 02:10:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/24/2022 02:10:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/24/2022 02:10:24 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.11341153193894855 on epoch=487
05/24/2022 02:10:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/24/2022 02:10:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/24/2022 02:10:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/24/2022 02:10:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/24/2022 02:10:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/24/2022 02:10:49 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.07509881422924902 on epoch=499
05/24/2022 02:10:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/24/2022 02:10:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/24/2022 02:11:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/24/2022 02:11:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/24/2022 02:11:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/24/2022 02:11:15 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.07133895845120661 on epoch=512
05/24/2022 02:11:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/24/2022 02:11:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/24/2022 02:11:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/24/2022 02:11:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/24/2022 02:11:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/24/2022 02:11:40 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.0869837296620776 on epoch=524
05/24/2022 02:11:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/24/2022 02:11:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/24/2022 02:11:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/24/2022 02:11:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/24/2022 02:12:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/24/2022 02:12:05 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.1360698125404008 on epoch=537
05/24/2022 02:12:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/24/2022 02:12:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/24/2022 02:12:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/24/2022 02:12:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/24/2022 02:12:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/24/2022 02:12:30 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.08223443223443223 on epoch=549
05/24/2022 02:12:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/24/2022 02:12:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/24/2022 02:12:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/24/2022 02:12:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/24/2022 02:12:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/24/2022 02:12:55 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.07578517414582987 on epoch=562
05/24/2022 02:13:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/24/2022 02:13:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/24/2022 02:13:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/24/2022 02:13:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/24/2022 02:13:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/24/2022 02:13:21 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.07578517414582987 on epoch=574
05/24/2022 02:13:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/24/2022 02:13:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/24/2022 02:13:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/24/2022 02:13:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/24/2022 02:13:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/24/2022 02:13:46 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.09313725490196079 on epoch=587
05/24/2022 02:13:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/24/2022 02:13:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/24/2022 02:13:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/24/2022 02:14:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/24/2022 02:14:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/24/2022 02:14:11 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.09551515151515153 on epoch=599
05/24/2022 02:14:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/24/2022 02:14:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/24/2022 02:14:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/24/2022 02:14:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/24/2022 02:14:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/24/2022 02:14:36 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.102263856362217 on epoch=612
05/24/2022 02:14:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/24/2022 02:14:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/24/2022 02:14:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/24/2022 02:14:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/24/2022 02:14:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/24/2022 02:15:01 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.07875802791057028 on epoch=624
05/24/2022 02:15:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/24/2022 02:15:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/24/2022 02:15:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/24/2022 02:15:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/24/2022 02:15:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/24/2022 02:15:27 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.06163934426229509 on epoch=637
05/24/2022 02:15:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/24/2022 02:15:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/24/2022 02:15:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/24/2022 02:15:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/24/2022 02:15:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/24/2022 02:15:52 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.06659540849521331 on epoch=649
05/24/2022 02:15:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/24/2022 02:16:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/24/2022 02:16:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/24/2022 02:16:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/24/2022 02:16:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/24/2022 02:16:17 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.06968837078508064 on epoch=662
05/24/2022 02:16:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/24/2022 02:16:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/24/2022 02:16:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/24/2022 02:16:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/24/2022 02:16:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/24/2022 02:16:42 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.08859784283513096 on epoch=674
05/24/2022 02:16:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/24/2022 02:16:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/24/2022 02:16:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/24/2022 02:17:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/24/2022 02:17:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/24/2022 02:17:07 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.07802159526297457 on epoch=687
05/24/2022 02:17:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/24/2022 02:17:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/24/2022 02:17:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/24/2022 02:17:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/24/2022 02:17:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/24/2022 02:17:32 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.06488479262672811 on epoch=699
05/24/2022 02:17:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/24/2022 02:17:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/24/2022 02:17:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/24/2022 02:17:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/24/2022 02:17:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/24/2022 02:17:57 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.0709353421217828 on epoch=712
05/24/2022 02:18:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/24/2022 02:18:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/24/2022 02:18:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/24/2022 02:18:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/24/2022 02:18:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/24/2022 02:18:22 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.06844181459566076 on epoch=724
05/24/2022 02:18:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/24/2022 02:18:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 02:18:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/24/2022 02:18:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 02:18:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/24/2022 02:18:47 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.06858974358974358 on epoch=737
05/24/2022 02:18:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/24/2022 02:18:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/24/2022 02:19:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/24/2022 02:19:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/24/2022 02:19:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/24/2022 02:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:19:11 - INFO - __main__ - Printing 3 examples
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:19:11 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:19:11 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 02:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:19:11 - INFO - __main__ - Printing 3 examples
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 02:19:11 - INFO - __main__ - ['refuted']
05/24/2022 02:19:11 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:19:11 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:19:12 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 02:19:12 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.09228367964101064 on epoch=749
05/24/2022 02:19:12 - INFO - __main__ - save last model!
05/24/2022 02:19:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 02:19:13 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 02:19:13 - INFO - __main__ - Printing 3 examples
05/24/2022 02:19:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:19:13 - INFO - __main__ - ['entailed']
05/24/2022 02:19:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:19:13 - INFO - __main__ - ['entailed']
05/24/2022 02:19:13 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:19:13 - INFO - __main__ - ['entailed']
05/24/2022 02:19:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:19:27 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 02:19:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 02:19:28 - INFO - __main__ - Starting training!
05/24/2022 02:19:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:19:50 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 02:28:23 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_42_0.5_8_predictions.txt
05/24/2022 02:28:23 - INFO - __main__ - Classification-F1 on test data: 0.0028
05/24/2022 02:28:23 - INFO - __main__ - prefix=tab_fact_32_42, lr=0.5, bsz=8, dev_performance=0.41075141075141075, test_performance=0.002832514321380719
05/24/2022 02:28:23 - INFO - __main__ - Running ... prefix=tab_fact_32_42, lr=0.4, bsz=8 ...
05/24/2022 02:28:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:28:24 - INFO - __main__ - Printing 3 examples
05/24/2022 02:28:24 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 02:28:24 - INFO - __main__ - ['refuted']
05/24/2022 02:28:24 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 02:28:24 - INFO - __main__ - ['refuted']
05/24/2022 02:28:24 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 02:28:24 - INFO - __main__ - ['refuted']
05/24/2022 02:28:24 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:28:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:28:25 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 02:28:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:28:25 - INFO - __main__ - Printing 3 examples
05/24/2022 02:28:25 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 02:28:25 - INFO - __main__ - ['refuted']
05/24/2022 02:28:25 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 02:28:25 - INFO - __main__ - ['refuted']
05/24/2022 02:28:25 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 02:28:25 - INFO - __main__ - ['refuted']
05/24/2022 02:28:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:28:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:28:25 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 02:28:40 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 02:28:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 02:28:41 - INFO - __main__ - Starting training!
05/24/2022 02:28:46 - INFO - __main__ - Step 10 Global step 10 Train loss 4.79 on epoch=2
05/24/2022 02:28:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=4
05/24/2022 02:28:54 - INFO - __main__ - Step 30 Global step 30 Train loss 2.04 on epoch=7
05/24/2022 02:28:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.42 on epoch=9
05/24/2022 02:29:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=12
05/24/2022 02:29:06 - INFO - __main__ - Global step 50 Train loss 2.40 Classification-F1 0.15384615384615383 on epoch=12
05/24/2022 02:29:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15384615384615383 on epoch=12, global_step=50
05/24/2022 02:29:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=14
05/24/2022 02:29:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=17
05/24/2022 02:29:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=19
05/24/2022 02:29:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=22
05/24/2022 02:29:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=24
05/24/2022 02:29:31 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 02:29:31 - INFO - __main__ - Saving model with best Classification-F1: 0.15384615384615383 -> 0.3333333333333333 on epoch=24, global_step=100
05/24/2022 02:29:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=27
05/24/2022 02:29:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=29
05/24/2022 02:29:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=32
05/24/2022 02:29:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=34
05/24/2022 02:29:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=37
05/24/2022 02:29:55 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 02:30:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=39
05/24/2022 02:30:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=42
05/24/2022 02:30:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=44
05/24/2022 02:30:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=47
05/24/2022 02:30:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=49
05/24/2022 02:30:20 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 02:30:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=52
05/24/2022 02:30:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=54
05/24/2022 02:30:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=57
05/24/2022 02:30:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=59
05/24/2022 02:30:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=62
05/24/2022 02:30:45 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 02:30:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=64
05/24/2022 02:30:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=67
05/24/2022 02:30:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=69
05/24/2022 02:31:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=72
05/24/2022 02:31:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
05/24/2022 02:31:10 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 02:31:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=77
05/24/2022 02:31:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=79
05/24/2022 02:31:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=82
05/24/2022 02:31:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=84
05/24/2022 02:31:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=87
05/24/2022 02:31:35 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 02:31:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=89
05/24/2022 02:31:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=92
05/24/2022 02:31:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 02:31:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/24/2022 02:31:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
05/24/2022 02:32:01 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 02:32:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/24/2022 02:32:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/24/2022 02:32:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
05/24/2022 02:32:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/24/2022 02:32:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=112
05/24/2022 02:32:26 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 02:32:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=114
05/24/2022 02:32:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=117
05/24/2022 02:32:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/24/2022 02:32:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=122
05/24/2022 02:32:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/24/2022 02:32:50 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.3511520737327189 on epoch=124
05/24/2022 02:32:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3511520737327189 on epoch=124, global_step=500
05/24/2022 02:32:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/24/2022 02:32:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=129
05/24/2022 02:33:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
05/24/2022 02:33:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=134
05/24/2022 02:33:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/24/2022 02:33:15 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.3591989987484355 on epoch=137
05/24/2022 02:33:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3511520737327189 -> 0.3591989987484355 on epoch=137, global_step=550
05/24/2022 02:33:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
05/24/2022 02:33:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=142
05/24/2022 02:33:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
05/24/2022 02:33:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
05/24/2022 02:33:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
05/24/2022 02:33:41 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.41075141075141075 on epoch=149
05/24/2022 02:33:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3591989987484355 -> 0.41075141075141075 on epoch=149, global_step=600
05/24/2022 02:33:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=152
05/24/2022 02:33:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
05/24/2022 02:33:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/24/2022 02:33:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=159
05/24/2022 02:34:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=162
05/24/2022 02:34:06 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.24183006535947715 on epoch=162
05/24/2022 02:34:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/24/2022 02:34:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=167
05/24/2022 02:34:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=169
05/24/2022 02:34:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
05/24/2022 02:34:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/24/2022 02:34:31 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.2731405710129115 on epoch=174
05/24/2022 02:34:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=177
05/24/2022 02:34:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/24/2022 02:34:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
05/24/2022 02:34:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/24/2022 02:34:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
05/24/2022 02:34:56 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.24166666666666667 on epoch=187
05/24/2022 02:35:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=189
05/24/2022 02:35:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=192
05/24/2022 02:35:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
05/24/2022 02:35:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=197
05/24/2022 02:35:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/24/2022 02:35:21 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.25705329153605017 on epoch=199
05/24/2022 02:35:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/24/2022 02:35:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
05/24/2022 02:35:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
05/24/2022 02:35:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
05/24/2022 02:35:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=212
05/24/2022 02:35:46 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.14882970137207424 on epoch=212
05/24/2022 02:35:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=214
05/24/2022 02:35:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=217
05/24/2022 02:36:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
05/24/2022 02:36:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/24/2022 02:36:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/24/2022 02:36:11 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.18179332968835427 on epoch=224
05/24/2022 02:36:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
05/24/2022 02:36:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/24/2022 02:36:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=232
05/24/2022 02:36:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=234
05/24/2022 02:36:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
05/24/2022 02:36:37 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.20963142319074518 on epoch=237
05/24/2022 02:36:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
05/24/2022 02:36:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
05/24/2022 02:36:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
05/24/2022 02:36:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=247
05/24/2022 02:36:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
05/24/2022 02:37:02 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.16456505847953215 on epoch=249
05/24/2022 02:37:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
05/24/2022 02:37:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
05/24/2022 02:37:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/24/2022 02:37:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/24/2022 02:37:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/24/2022 02:37:27 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.28318230277185497 on epoch=262
05/24/2022 02:37:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
05/24/2022 02:37:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/24/2022 02:37:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/24/2022 02:37:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=272
05/24/2022 02:37:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
05/24/2022 02:37:52 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.29186602870813394 on epoch=274
05/24/2022 02:37:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/24/2022 02:38:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
05/24/2022 02:38:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
05/24/2022 02:38:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/24/2022 02:38:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/24/2022 02:38:17 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.1024561403508772 on epoch=287
05/24/2022 02:38:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/24/2022 02:38:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/24/2022 02:38:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/24/2022 02:38:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
05/24/2022 02:38:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
05/24/2022 02:38:43 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.14273806216012383 on epoch=299
05/24/2022 02:38:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/24/2022 02:38:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/24/2022 02:38:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/24/2022 02:39:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
05/24/2022 02:39:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/24/2022 02:39:08 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.11673052362707535 on epoch=312
05/24/2022 02:39:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/24/2022 02:39:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/24/2022 02:39:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/24/2022 02:39:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
05/24/2022 02:39:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/24/2022 02:39:33 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.13316383283012873 on epoch=324
05/24/2022 02:39:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/24/2022 02:39:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/24/2022 02:39:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/24/2022 02:39:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
05/24/2022 02:39:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/24/2022 02:39:58 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.1343273035613341 on epoch=337
05/24/2022 02:40:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/24/2022 02:40:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/24/2022 02:40:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/24/2022 02:40:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/24/2022 02:40:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/24/2022 02:40:24 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.1610632583124942 on epoch=349
05/24/2022 02:40:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/24/2022 02:40:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/24/2022 02:40:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/24/2022 02:40:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/24/2022 02:40:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/24/2022 02:40:49 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.09912280701754386 on epoch=362
05/24/2022 02:40:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/24/2022 02:40:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/24/2022 02:41:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/24/2022 02:41:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/24/2022 02:41:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/24/2022 02:41:14 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.11403508771929823 on epoch=374
05/24/2022 02:41:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/24/2022 02:41:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/24/2022 02:41:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/24/2022 02:41:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/24/2022 02:41:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/24/2022 02:41:39 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.17632058287795993 on epoch=387
05/24/2022 02:41:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/24/2022 02:41:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/24/2022 02:41:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/24/2022 02:41:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/24/2022 02:42:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/24/2022 02:42:05 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.15242329218433692 on epoch=399
05/24/2022 02:42:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/24/2022 02:42:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/24/2022 02:42:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/24/2022 02:42:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/24/2022 02:42:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/24/2022 02:42:30 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.1531949531949532 on epoch=412
05/24/2022 02:42:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/24/2022 02:42:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/24/2022 02:42:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/24/2022 02:42:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/24/2022 02:42:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/24/2022 02:42:55 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.12041031540600135 on epoch=424
05/24/2022 02:43:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/24/2022 02:43:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/24/2022 02:43:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/24/2022 02:43:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/24/2022 02:43:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/24/2022 02:43:20 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.09912280701754386 on epoch=437
05/24/2022 02:43:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/24/2022 02:43:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/24/2022 02:43:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/24/2022 02:43:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/24/2022 02:43:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/24/2022 02:43:46 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.16464646464646462 on epoch=449
05/24/2022 02:43:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/24/2022 02:43:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/24/2022 02:43:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/24/2022 02:44:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/24/2022 02:44:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/24/2022 02:44:11 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.19672131147540983 on epoch=462
05/24/2022 02:44:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/24/2022 02:44:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/24/2022 02:44:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/24/2022 02:44:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/24/2022 02:44:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/24/2022 02:44:36 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.2032258064516129 on epoch=474
05/24/2022 02:44:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/24/2022 02:44:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/24/2022 02:44:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/24/2022 02:44:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/24/2022 02:44:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/24/2022 02:45:02 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.20328221684153885 on epoch=487
05/24/2022 02:45:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/24/2022 02:45:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/24/2022 02:45:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/24/2022 02:45:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/24/2022 02:45:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/24/2022 02:45:27 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.20328221684153885 on epoch=499
05/24/2022 02:45:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/24/2022 02:45:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/24/2022 02:45:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/24/2022 02:45:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/24/2022 02:45:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/24/2022 02:45:52 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.21515151515151515 on epoch=512
05/24/2022 02:45:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/24/2022 02:46:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/24/2022 02:46:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/24/2022 02:46:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/24/2022 02:46:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/24/2022 02:46:17 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.1344632768361582 on epoch=524
05/24/2022 02:46:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/24/2022 02:46:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/24/2022 02:46:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/24/2022 02:46:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/24/2022 02:46:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/24/2022 02:46:42 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.14240237691001698 on epoch=537
05/24/2022 02:46:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/24/2022 02:46:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/24/2022 02:46:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/24/2022 02:47:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/24/2022 02:47:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/24/2022 02:47:08 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1302289429055964 on epoch=549
05/24/2022 02:47:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/24/2022 02:47:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/24/2022 02:47:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/24/2022 02:47:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/24/2022 02:47:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/24/2022 02:47:33 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.15238095238095237 on epoch=562
05/24/2022 02:47:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/24/2022 02:47:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/24/2022 02:47:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/24/2022 02:47:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/24/2022 02:47:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/24/2022 02:47:58 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.1021662763466042 on epoch=574
05/24/2022 02:48:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/24/2022 02:48:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/24/2022 02:48:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/24/2022 02:48:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/24/2022 02:48:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/24/2022 02:48:23 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.1959051724137931 on epoch=587
05/24/2022 02:48:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/24/2022 02:48:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/24/2022 02:48:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/24/2022 02:48:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/24/2022 02:48:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/24/2022 02:48:49 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.09976190476190476 on epoch=599
05/24/2022 02:48:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/24/2022 02:48:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/24/2022 02:49:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/24/2022 02:49:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/24/2022 02:49:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/24/2022 02:49:14 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.1103448275862069 on epoch=612
05/24/2022 02:49:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/24/2022 02:49:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/24/2022 02:49:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/24/2022 02:49:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/24/2022 02:49:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/24/2022 02:49:39 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.14350834350834352 on epoch=624
05/24/2022 02:49:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/24/2022 02:49:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/24/2022 02:49:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/24/2022 02:49:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/24/2022 02:50:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/24/2022 02:50:04 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.11138014527845037 on epoch=637
05/24/2022 02:50:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/24/2022 02:50:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/24/2022 02:50:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/24/2022 02:50:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/24/2022 02:50:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/24/2022 02:50:29 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.16878858024691357 on epoch=649
05/24/2022 02:50:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/24/2022 02:50:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/24/2022 02:50:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/24/2022 02:50:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/24/2022 02:50:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/24/2022 02:50:55 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.13668907071887784 on epoch=662
05/24/2022 02:50:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/24/2022 02:51:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/24/2022 02:51:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/24/2022 02:51:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/24/2022 02:51:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/24/2022 02:51:20 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.14516129032258066 on epoch=674
05/24/2022 02:51:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/24/2022 02:51:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/24/2022 02:51:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/24/2022 02:51:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/24/2022 02:51:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/24/2022 02:51:45 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.10683760683760683 on epoch=687
05/24/2022 02:51:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
05/24/2022 02:51:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/24/2022 02:51:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/24/2022 02:52:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/24/2022 02:52:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/24/2022 02:52:10 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.11384335154826959 on epoch=699
05/24/2022 02:52:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/24/2022 02:52:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/24/2022 02:52:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/24/2022 02:52:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/24/2022 02:52:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/24/2022 02:52:36 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.1313632441760138 on epoch=712
05/24/2022 02:52:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/24/2022 02:52:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/24/2022 02:52:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/24/2022 02:52:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/24/2022 02:52:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/24/2022 02:53:01 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.1530844155844156 on epoch=724
05/24/2022 02:53:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 02:53:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 02:53:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 02:53:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/24/2022 02:53:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
05/24/2022 02:53:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.10666666666666666 on epoch=737
05/24/2022 02:53:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/24/2022 02:53:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/24/2022 02:53:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/24/2022 02:53:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/24/2022 02:53:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/24/2022 02:53:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:53:50 - INFO - __main__ - Printing 3 examples
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:53:50 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:53:50 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 02:53:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 02:53:50 - INFO - __main__ - Printing 3 examples
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 02:53:50 - INFO - __main__ - ['refuted']
05/24/2022 02:53:50 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:53:50 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:53:50 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 02:53:51 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.15008210180623974 on epoch=749
05/24/2022 02:53:51 - INFO - __main__ - save last model!
05/24/2022 02:53:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 02:53:51 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 02:53:51 - INFO - __main__ - Printing 3 examples
05/24/2022 02:53:51 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:53:51 - INFO - __main__ - ['entailed']
05/24/2022 02:53:51 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:53:51 - INFO - __main__ - ['entailed']
05/24/2022 02:53:51 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 02:53:51 - INFO - __main__ - ['entailed']
05/24/2022 02:53:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 02:54:05 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 02:54:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 02:54:06 - INFO - __main__ - Starting training!
05/24/2022 02:54:15 - INFO - __main__ - Tokenizing Output ...
05/24/2022 02:54:28 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 03:03:19 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_42_0.4_8_predictions.txt
05/24/2022 03:03:19 - INFO - __main__ - Classification-F1 on test data: 0.0033
05/24/2022 03:03:19 - INFO - __main__ - prefix=tab_fact_32_42, lr=0.4, bsz=8, dev_performance=0.41075141075141075, test_performance=0.003323330492891457
05/24/2022 03:03:19 - INFO - __main__ - Running ... prefix=tab_fact_32_42, lr=0.3, bsz=8 ...
05/24/2022 03:03:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:03:20 - INFO - __main__ - Printing 3 examples
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:03:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:03:20 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 03:03:20 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:03:20 - INFO - __main__ - Printing 3 examples
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 03:03:20 - INFO - __main__ - ['refuted']
05/24/2022 03:03:20 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:03:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:03:20 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 03:03:35 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 03:03:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 03:03:36 - INFO - __main__ - Starting training!
05/24/2022 03:03:41 - INFO - __main__ - Step 10 Global step 10 Train loss 4.97 on epoch=2
05/24/2022 03:03:45 - INFO - __main__ - Step 20 Global step 20 Train loss 3.35 on epoch=4
05/24/2022 03:03:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.49 on epoch=7
05/24/2022 03:03:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.89 on epoch=9
05/24/2022 03:03:59 - INFO - __main__ - Step 50 Global step 50 Train loss 1.41 on epoch=12
05/24/2022 03:04:02 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.3333333333333333 on epoch=12
05/24/2022 03:04:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/24/2022 03:04:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=14
05/24/2022 03:04:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.78 on epoch=17
05/24/2022 03:04:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=19
05/24/2022 03:04:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=22
05/24/2022 03:04:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=24
05/24/2022 03:04:27 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 03:04:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=27
05/24/2022 03:04:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=29
05/24/2022 03:04:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=32
05/24/2022 03:04:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=34
05/24/2022 03:04:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=37
05/24/2022 03:04:51 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 03:04:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=39
05/24/2022 03:05:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=42
05/24/2022 03:05:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=44
05/24/2022 03:05:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=47
05/24/2022 03:05:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=49
05/24/2022 03:05:16 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 03:05:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=52
05/24/2022 03:05:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=54
05/24/2022 03:05:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=57
05/24/2022 03:05:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=59
05/24/2022 03:05:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=62
05/24/2022 03:05:41 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 03:05:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
05/24/2022 03:05:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=67
05/24/2022 03:05:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=69
05/24/2022 03:05:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=72
05/24/2022 03:06:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=74
05/24/2022 03:06:06 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 03:06:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=77
05/24/2022 03:06:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/24/2022 03:06:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=82
05/24/2022 03:06:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=84
05/24/2022 03:06:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=87
05/24/2022 03:06:31 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 03:06:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/24/2022 03:06:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
05/24/2022 03:06:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 03:06:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/24/2022 03:06:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=99
05/24/2022 03:06:57 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 03:07:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/24/2022 03:07:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=104
05/24/2022 03:07:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/24/2022 03:07:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/24/2022 03:07:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
05/24/2022 03:07:22 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 03:07:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
05/24/2022 03:07:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
05/24/2022 03:07:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
05/24/2022 03:07:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/24/2022 03:07:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
05/24/2022 03:07:47 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 03:07:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/24/2022 03:07:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/24/2022 03:08:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=132
05/24/2022 03:08:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/24/2022 03:08:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/24/2022 03:08:12 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.32631578947368417 on epoch=137
05/24/2022 03:08:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/24/2022 03:08:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=142
05/24/2022 03:08:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=144
05/24/2022 03:08:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
05/24/2022 03:08:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=149
05/24/2022 03:08:37 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3591989987484355 on epoch=149
05/24/2022 03:08:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3591989987484355 on epoch=149, global_step=600
05/24/2022 03:08:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/24/2022 03:08:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/24/2022 03:08:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=157
05/24/2022 03:08:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=159
05/24/2022 03:09:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/24/2022 03:09:03 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 03:09:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
05/24/2022 03:09:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
05/24/2022 03:09:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
05/24/2022 03:09:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
05/24/2022 03:09:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/24/2022 03:09:28 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.42840679919331603 on epoch=174
05/24/2022 03:09:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3591989987484355 -> 0.42840679919331603 on epoch=174, global_step=700
05/24/2022 03:09:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=177
05/24/2022 03:09:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=179
05/24/2022 03:09:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=182
05/24/2022 03:09:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=184
05/24/2022 03:09:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=187
05/24/2022 03:09:53 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.24242424242424243 on epoch=187
05/24/2022 03:09:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
05/24/2022 03:10:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=192
05/24/2022 03:10:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=194
05/24/2022 03:10:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=197
05/24/2022 03:10:15 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/24/2022 03:10:18 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.236999147485081 on epoch=199
05/24/2022 03:10:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=202
05/24/2022 03:10:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=204
05/24/2022 03:10:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
05/24/2022 03:10:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/24/2022 03:10:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
05/24/2022 03:10:43 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.30623534927332396 on epoch=212
05/24/2022 03:10:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
05/24/2022 03:10:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
05/24/2022 03:10:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
05/24/2022 03:11:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
05/24/2022 03:11:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=224
05/24/2022 03:11:08 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.21770334928229668 on epoch=224
05/24/2022 03:11:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
05/24/2022 03:11:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=229
05/24/2022 03:11:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
05/24/2022 03:11:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=234
05/24/2022 03:11:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/24/2022 03:11:33 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.16832627118644067 on epoch=237
05/24/2022 03:11:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=239
05/24/2022 03:11:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=242
05/24/2022 03:11:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
05/24/2022 03:11:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
05/24/2022 03:11:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=249
05/24/2022 03:11:59 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.15773049645390075 on epoch=249
05/24/2022 03:12:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
05/24/2022 03:12:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
05/24/2022 03:12:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
05/24/2022 03:12:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
05/24/2022 03:12:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=262
05/24/2022 03:12:24 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.18337368845843421 on epoch=262
05/24/2022 03:12:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
05/24/2022 03:12:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=267
05/24/2022 03:12:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/24/2022 03:12:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=272
05/24/2022 03:12:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
05/24/2022 03:12:49 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.18489326765188835 on epoch=274
05/24/2022 03:12:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=277
05/24/2022 03:12:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=279
05/24/2022 03:13:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/24/2022 03:13:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=284
05/24/2022 03:13:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=287
05/24/2022 03:13:14 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.2357218402961396 on epoch=287
05/24/2022 03:13:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
05/24/2022 03:13:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=292
05/24/2022 03:13:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
05/24/2022 03:13:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=297
05/24/2022 03:13:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=299
05/24/2022 03:13:39 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.19916457811194652 on epoch=299
05/24/2022 03:13:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/24/2022 03:13:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
05/24/2022 03:13:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=307
05/24/2022 03:13:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
05/24/2022 03:14:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
05/24/2022 03:14:04 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.1651158547710272 on epoch=312
05/24/2022 03:14:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
05/24/2022 03:14:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=317
05/24/2022 03:14:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/24/2022 03:14:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/24/2022 03:14:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
05/24/2022 03:14:30 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.0809667000779597 on epoch=324
05/24/2022 03:14:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
05/24/2022 03:14:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/24/2022 03:14:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/24/2022 03:14:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/24/2022 03:14:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/24/2022 03:14:55 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.10244343891402714 on epoch=337
05/24/2022 03:14:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/24/2022 03:15:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/24/2022 03:15:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/24/2022 03:15:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/24/2022 03:15:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/24/2022 03:15:20 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.08187134502923978 on epoch=349
05/24/2022 03:15:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/24/2022 03:15:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/24/2022 03:15:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
05/24/2022 03:15:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/24/2022 03:15:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/24/2022 03:15:45 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.0984090909090909 on epoch=362
05/24/2022 03:15:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/24/2022 03:15:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/24/2022 03:15:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
05/24/2022 03:16:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/24/2022 03:16:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
05/24/2022 03:16:10 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.10992548435171387 on epoch=374
05/24/2022 03:16:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/24/2022 03:16:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/24/2022 03:16:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/24/2022 03:16:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/24/2022 03:16:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/24/2022 03:16:36 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.10071301247771836 on epoch=387
05/24/2022 03:16:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/24/2022 03:16:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/24/2022 03:16:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/24/2022 03:16:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/24/2022 03:16:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/24/2022 03:17:01 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.14293981481481483 on epoch=399
05/24/2022 03:17:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/24/2022 03:17:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/24/2022 03:17:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/24/2022 03:17:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/24/2022 03:17:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/24/2022 03:17:26 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.17460317460317462 on epoch=412
05/24/2022 03:17:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
05/24/2022 03:17:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/24/2022 03:17:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/24/2022 03:17:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/24/2022 03:17:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/24/2022 03:17:51 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.14880952380952384 on epoch=424
05/24/2022 03:17:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/24/2022 03:18:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/24/2022 03:18:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/24/2022 03:18:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/24/2022 03:18:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/24/2022 03:18:16 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.12386621315192742 on epoch=437
05/24/2022 03:18:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/24/2022 03:18:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/24/2022 03:18:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/24/2022 03:18:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/24/2022 03:18:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/24/2022 03:18:41 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.10766889835666463 on epoch=449
05/24/2022 03:18:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/24/2022 03:18:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/24/2022 03:18:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
05/24/2022 03:18:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/24/2022 03:19:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/24/2022 03:19:06 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.13290163428923835 on epoch=462
05/24/2022 03:19:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/24/2022 03:19:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/24/2022 03:19:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/24/2022 03:19:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/24/2022 03:19:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/24/2022 03:19:32 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.12463256907701352 on epoch=474
05/24/2022 03:19:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/24/2022 03:19:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
05/24/2022 03:19:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/24/2022 03:19:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
05/24/2022 03:19:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/24/2022 03:19:57 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.10346320346320347 on epoch=487
05/24/2022 03:20:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/24/2022 03:20:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/24/2022 03:20:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/24/2022 03:20:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/24/2022 03:20:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/24/2022 03:20:22 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.25112205801860976 on epoch=499
05/24/2022 03:20:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/24/2022 03:20:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/24/2022 03:20:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/24/2022 03:20:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/24/2022 03:20:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/24/2022 03:20:47 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.10543478260869565 on epoch=512
05/24/2022 03:20:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/24/2022 03:20:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/24/2022 03:21:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/24/2022 03:21:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/24/2022 03:21:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/24/2022 03:21:12 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.14754689754689754 on epoch=524
05/24/2022 03:21:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/24/2022 03:21:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/24/2022 03:21:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
05/24/2022 03:21:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/24/2022 03:21:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/24/2022 03:21:37 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.10813839473315591 on epoch=537
05/24/2022 03:21:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/24/2022 03:21:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/24/2022 03:21:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/24/2022 03:21:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/24/2022 03:22:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/24/2022 03:22:02 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.08325297332047575 on epoch=549
05/24/2022 03:22:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/24/2022 03:22:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/24/2022 03:22:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/24/2022 03:22:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/24/2022 03:22:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/24/2022 03:22:27 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.09778184778184779 on epoch=562
05/24/2022 03:22:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/24/2022 03:22:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/24/2022 03:22:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/24/2022 03:22:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/24/2022 03:22:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/24/2022 03:22:52 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.08386305341808854 on epoch=574
05/24/2022 03:22:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/24/2022 03:23:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/24/2022 03:23:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/24/2022 03:23:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/24/2022 03:23:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/24/2022 03:23:17 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.12707627801967425 on epoch=587
05/24/2022 03:23:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/24/2022 03:23:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/24/2022 03:23:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/24/2022 03:23:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/24/2022 03:23:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/24/2022 03:23:43 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.10835459183673468 on epoch=599
05/24/2022 03:23:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/24/2022 03:23:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/24/2022 03:23:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/24/2022 03:24:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/24/2022 03:24:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/24/2022 03:24:08 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.08692810457516341 on epoch=612
05/24/2022 03:24:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/24/2022 03:24:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/24/2022 03:24:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/24/2022 03:24:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/24/2022 03:24:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/24/2022 03:24:33 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.12536687631027255 on epoch=624
05/24/2022 03:24:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/24/2022 03:24:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/24/2022 03:24:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/24/2022 03:24:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/24/2022 03:24:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/24/2022 03:24:58 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.08835623307592017 on epoch=637
05/24/2022 03:25:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/24/2022 03:25:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/24/2022 03:25:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/24/2022 03:25:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/24/2022 03:25:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/24/2022 03:25:23 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.06553017091035926 on epoch=649
05/24/2022 03:25:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/24/2022 03:25:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/24/2022 03:25:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/24/2022 03:25:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/24/2022 03:25:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/24/2022 03:25:48 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.10135642135642137 on epoch=662
05/24/2022 03:25:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/24/2022 03:25:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/24/2022 03:26:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/24/2022 03:26:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/24/2022 03:26:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/24/2022 03:26:13 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.10335949764521193 on epoch=674
05/24/2022 03:26:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/24/2022 03:26:22 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/24/2022 03:26:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/24/2022 03:26:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/24/2022 03:26:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/24/2022 03:26:38 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.15128968253968253 on epoch=687
05/24/2022 03:26:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/24/2022 03:26:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/24/2022 03:26:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/24/2022 03:26:56 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/24/2022 03:27:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/24/2022 03:27:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.09108087679516251 on epoch=699
05/24/2022 03:27:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/24/2022 03:27:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/24/2022 03:27:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/24/2022 03:27:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/24/2022 03:27:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/24/2022 03:27:28 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.20276497695852536 on epoch=712
05/24/2022 03:27:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/24/2022 03:27:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/24/2022 03:27:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/24/2022 03:27:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/24/2022 03:27:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 03:27:53 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.10955710955710957 on epoch=724
05/24/2022 03:27:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 03:28:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/24/2022 03:28:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/24/2022 03:28:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 03:28:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/24/2022 03:28:19 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.14100877192982456 on epoch=737
05/24/2022 03:28:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/24/2022 03:28:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/24/2022 03:28:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/24/2022 03:28:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/24/2022 03:28:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/24/2022 03:28:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:28:42 - INFO - __main__ - Printing 3 examples
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:28:42 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:28:42 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 03:28:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:28:42 - INFO - __main__ - Printing 3 examples
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 03:28:42 - INFO - __main__ - ['refuted']
05/24/2022 03:28:42 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:28:42 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:28:43 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 03:28:44 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.12321633867586392 on epoch=749
05/24/2022 03:28:44 - INFO - __main__ - save last model!
05/24/2022 03:28:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 03:28:44 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 03:28:44 - INFO - __main__ - Printing 3 examples
05/24/2022 03:28:44 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 03:28:44 - INFO - __main__ - ['entailed']
05/24/2022 03:28:44 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 03:28:44 - INFO - __main__ - ['entailed']
05/24/2022 03:28:44 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 03:28:44 - INFO - __main__ - ['entailed']
05/24/2022 03:28:44 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:28:58 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 03:28:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 03:28:59 - INFO - __main__ - Starting training!
05/24/2022 03:29:08 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:29:21 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 03:38:25 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_42_0.3_8_predictions.txt
05/24/2022 03:38:25 - INFO - __main__ - Classification-F1 on test data: 0.0037
05/24/2022 03:38:25 - INFO - __main__ - prefix=tab_fact_32_42, lr=0.3, bsz=8, dev_performance=0.42840679919331603, test_performance=0.003670914067377333
05/24/2022 03:38:25 - INFO - __main__ - Running ... prefix=tab_fact_32_42, lr=0.2, bsz=8 ...
05/24/2022 03:38:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:38:26 - INFO - __main__ - Printing 3 examples
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:38:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:38:26 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 03:38:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 03:38:26 - INFO - __main__ - Printing 3 examples
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: 5 - 4 be the agg when team 1 be l'etoile de morne - - l'eau [SEP] table_caption: 1987 concacaf champions' cup [SEP] table_text: team 1#agg#team 2#1st leg#2nd leg [n] golden star#4 - 0#uptown rebels#3 - 0#1 - 0 [n] mg renegades#3 - 4#club franciscain#2 - 0#1 - 4 [n] l'etoile de morne - ã  - l'eau#3 - 3 (4 - 3 pen)#harbour view fc#3 - 1#0 - 2 [n] vsadc#5 - 3#asg juventus de sainte - anne#5 - 1#0 - 2 [n] rick 's superstars#1 - 5#trintoc#1 - 2#0 - 3 [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: dallas star draft player austin , jamie , ondrej and luke in the third round [SEP] table_caption: 2007 - 08 dallas stars season [SEP] table_text: round#player#position#nationality#college / junior / club team (league) [n] 2#nico sacchetti#centre#united states#virginia high school ( ushs - mn ) [n] 3#sergei korostin#right wing#russia#dynamo moscow ( rus ) [n] 4#colton sceviour#right wing#canada#portland winter hawks ( whl ) [n] 5#austin smith#right wing#united states#gunnery prep [n] 5#jamie benn#left wing#canada#victoria grizzlies ( bchl ) [n] 5#ondrej roman#left wing#czech republic#spokane chiefs ( whl ) [n] 4#michael neal#left wing#canada#belleville bulls ( ohl ) [n] 5#luke gazdic#left wing#canada#erie otters ( ohl ) [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ -  [tab_fact] statement: wayne ferreira play in dubai , uae after he play in tokyo , japan [SEP] table_caption: wayne ferreira [SEP] table_text: outcome#date#championship#surface#opponent#score [n] runner - up#17 february 1992#memphis , us#hard (i)#malivai washington#3 - 6 , 2 - 6 [n] winner#15 june 1992#queen 's club , uk#grass#shuzo matsuoka#6 - 3 , 6 - 4 [n] runner - up#20 july 1992#stuttgart outdoor , germany#clay#andrei medvedev#1 - 6 , 4 - 6 , 7 - 6 (7 - 5) , 6 - 2 , 1 - 6 [n] winner#31 august 1992#schenectady , us#hard#jamie morgan#6 - 2 , 6 - 7 (5 - 7) , 6 - 2 [n] runner - up#8 march 1993#indian wells , us#hard#jim courier#3 - 6 , 3 - 6 , 1 - 6 [n] runner - up#14 june 1993#queen 's club , uk#grass#michael stich#3 - 6 , 4 - 6 [n] winner#10 january 1994#oahu , us#hard#richey reneberg#6 - 4 , 6 - 7 (3 - 7) , 6 - 1 [n] runner - up#28 february 1994#rotterdam , netherlands#carpet#michael stich#6 - 4 , 3 - 6 , 0 - 6 [n] runner - up#20 june 1994#manchester , uk#grass#patrick rafter#6 - 7 (5 - 7) , 6 - 7 (4 - 7) [n] winner#22 august 1994#indianapolis , us#hard#olivier delaître#6 - 2 , 6 - 1 [n] winner#19 september 1994#bordeaux , france#hard#jeff tarango#6 - 0 , 7 - 5 [n] winner#3 october 1994#basel , switzerland#hard (i)#patrick mcenroe#4 - 6 , 6 - 2 , 7 - 6 (9 - 7) , 6 - 3 [n] winner#17 october 1994#tel - aviv , israel#hard#amos mansdorf#7 - 6 (7 - 4) , 6 - 3 [n] winner#13 february 1995#dubai , uae#hard#andrea gaudenzi#6 - 3 , 6 - 3 [n] winner#8 may 1995#munich , germany#clay#michael stich#7 - 5 , 7 - 6 (8 - 6) [n] winner#16 october 1995#ostrava , czech republic#carpet#malivai washington#3 - 6 , 6 - 4 , 6 - 3 [n] winner#23 october 1995#lyon , france#carpet#pete sampras#7 - 6 (7 - 2) , 5 - 7 , 6 - 3 [n] winner#11 march 1996#scottsdale , us#hard#marcelo ríos#2 - 6 , 6 - 3 , 6 - 3 [n] runner - up#22 july 1996#washington , dc , us#hard#michael chang#2 - 6 , 4 - 6 [n] winner#26 august 1996#toronto , canada#hard#todd woodbridge#6 - 2 , 6 - 4 [n] runner - up#19 april 1999#tokyo , japan#hard#nicolas kiefer#6 - 7 (5 - 7) , 5 - 7 [n] winner#6 november 2000#stuttgart , germany#hard (i)#lleyton hewitt#7 - 6 (8 - 6) , 3 - 6 , 6 - 7 (5 - 7) , 7 - 6 (7 - 2) , 6 - 2 [n] winner#4 august 2003#los angeles , us#hard#lleyton hewitt#6 - 3 , 4 - 6 , 7 - 5 [n] 
05/24/2022 03:38:26 - INFO - __main__ - ['refuted']
05/24/2022 03:38:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 03:38:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 03:38:27 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 03:38:41 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 03:38:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 03:38:42 - INFO - __main__ - Starting training!
05/24/2022 03:38:47 - INFO - __main__ - Step 10 Global step 10 Train loss 5.43 on epoch=2
05/24/2022 03:38:52 - INFO - __main__ - Step 20 Global step 20 Train loss 3.82 on epoch=4
05/24/2022 03:38:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.91 on epoch=7
05/24/2022 03:39:00 - INFO - __main__ - Step 40 Global step 40 Train loss 2.52 on epoch=9
05/24/2022 03:39:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.15 on epoch=12
05/24/2022 03:39:08 - INFO - __main__ - Global step 50 Train loss 3.37 Classification-F1 0.0 on epoch=12
05/24/2022 03:39:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/24/2022 03:39:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.83 on epoch=14
05/24/2022 03:39:17 - INFO - __main__ - Step 70 Global step 70 Train loss 1.49 on epoch=17
05/24/2022 03:39:21 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=19
05/24/2022 03:39:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=22
05/24/2022 03:39:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
05/24/2022 03:39:33 - INFO - __main__ - Global step 100 Train loss 1.23 Classification-F1 0.21985815602836878 on epoch=24
05/24/2022 03:39:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.21985815602836878 on epoch=24, global_step=100
05/24/2022 03:39:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=27
05/24/2022 03:39:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=29
05/24/2022 03:39:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=32
05/24/2022 03:39:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=34
05/24/2022 03:39:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=37
05/24/2022 03:39:58 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 03:39:58 - INFO - __main__ - Saving model with best Classification-F1: 0.21985815602836878 -> 0.3333333333333333 on epoch=37, global_step=150
05/24/2022 03:40:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=39
05/24/2022 03:40:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=42
05/24/2022 03:40:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=44
05/24/2022 03:40:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=47
05/24/2022 03:40:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=49
05/24/2022 03:40:23 - INFO - __main__ - Global step 200 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 03:40:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=52
05/24/2022 03:40:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=54
05/24/2022 03:40:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=57
05/24/2022 03:40:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=59
05/24/2022 03:40:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=62
05/24/2022 03:40:48 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 03:40:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=64
05/24/2022 03:40:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=67
05/24/2022 03:41:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=69
05/24/2022 03:41:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=72
05/24/2022 03:41:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=74
05/24/2022 03:41:12 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 03:41:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=77
05/24/2022 03:41:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/24/2022 03:41:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=82
05/24/2022 03:41:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=84
05/24/2022 03:41:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=87
05/24/2022 03:41:37 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 03:41:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=89
05/24/2022 03:41:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=92
05/24/2022 03:41:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=94
05/24/2022 03:41:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/24/2022 03:41:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/24/2022 03:42:02 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 03:42:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/24/2022 03:42:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=104
05/24/2022 03:42:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/24/2022 03:42:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=109
05/24/2022 03:42:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=112
05/24/2022 03:42:27 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 03:42:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.18 on epoch=114
05/24/2022 03:42:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
05/24/2022 03:42:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=119
05/24/2022 03:42:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/24/2022 03:42:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=124
05/24/2022 03:42:52 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 03:42:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
05/24/2022 03:43:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/24/2022 03:43:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=132
05/24/2022 03:43:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/24/2022 03:43:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=137
05/24/2022 03:43:17 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 03:43:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=139
05/24/2022 03:43:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=142
05/24/2022 03:43:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=144
05/24/2022 03:43:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=147
05/24/2022 03:43:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=149
05/24/2022 03:43:42 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 03:43:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=152
05/24/2022 03:43:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=154
05/24/2022 03:43:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/24/2022 03:44:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
05/24/2022 03:44:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/24/2022 03:44:07 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 03:44:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=164
05/24/2022 03:44:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=167
05/24/2022 03:44:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
05/24/2022 03:44:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=172
05/24/2022 03:44:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/24/2022 03:44:32 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 03:44:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=177
05/24/2022 03:44:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=179
05/24/2022 03:44:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/24/2022 03:44:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=184
05/24/2022 03:44:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=187
05/24/2022 03:44:57 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=187
05/24/2022 03:45:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
05/24/2022 03:45:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/24/2022 03:45:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=194
05/24/2022 03:45:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=197
05/24/2022 03:45:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/24/2022 03:45:23 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 03:45:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=202
05/24/2022 03:45:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=204
05/24/2022 03:45:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=207
05/24/2022 03:45:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=209
05/24/2022 03:45:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
05/24/2022 03:45:48 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=212
05/24/2022 03:45:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=214
05/24/2022 03:45:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=217
05/24/2022 03:46:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=219
05/24/2022 03:46:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=222
05/24/2022 03:46:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=224
05/24/2022 03:46:13 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=224
05/24/2022 03:46:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=227
05/24/2022 03:46:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
05/24/2022 03:46:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=232
05/24/2022 03:46:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
05/24/2022 03:46:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
05/24/2022 03:46:38 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.32631578947368417 on epoch=237
05/24/2022 03:46:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=239
05/24/2022 03:46:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=242
05/24/2022 03:46:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=244
05/24/2022 03:46:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
05/24/2022 03:47:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=249
05/24/2022 03:47:03 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.36374269005847953 on epoch=249
05/24/2022 03:47:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=249, global_step=1000
05/24/2022 03:47:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=252
05/24/2022 03:47:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=254
05/24/2022 03:47:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=257
05/24/2022 03:47:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
05/24/2022 03:47:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=262
05/24/2022 03:47:28 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=262
05/24/2022 03:47:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=264
05/24/2022 03:47:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=267
05/24/2022 03:47:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=269
05/24/2022 03:47:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/24/2022 03:47:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=274
05/24/2022 03:47:54 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.28293545534924847 on epoch=274
05/24/2022 03:47:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
05/24/2022 03:48:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/24/2022 03:48:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
05/24/2022 03:48:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=284
05/24/2022 03:48:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=287
05/24/2022 03:48:19 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.28571428571428575 on epoch=287
05/24/2022 03:48:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=289
05/24/2022 03:48:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=292
05/24/2022 03:48:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=294
05/24/2022 03:48:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=297
05/24/2022 03:48:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
05/24/2022 03:48:44 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.19700582935877053 on epoch=299
05/24/2022 03:48:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=302
05/24/2022 03:48:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
05/24/2022 03:48:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=307
05/24/2022 03:49:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=309
05/24/2022 03:49:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=312
05/24/2022 03:49:09 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.18789115646258503 on epoch=312
05/24/2022 03:49:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=314
05/24/2022 03:49:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=317
05/24/2022 03:49:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=319
05/24/2022 03:49:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/24/2022 03:49:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=324
05/24/2022 03:49:34 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.19191512221326887 on epoch=324
05/24/2022 03:49:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=327
05/24/2022 03:49:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=329
05/24/2022 03:49:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=332
05/24/2022 03:49:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=334
05/24/2022 03:49:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=337
05/24/2022 03:50:00 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.2063342318059299 on epoch=337
05/24/2022 03:50:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=339
05/24/2022 03:50:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=342
05/24/2022 03:50:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=344
05/24/2022 03:50:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=347
05/24/2022 03:50:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=349
05/24/2022 03:50:25 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.1932876712328767 on epoch=349
05/24/2022 03:50:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
05/24/2022 03:50:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=354
05/24/2022 03:50:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=357
05/24/2022 03:50:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
05/24/2022 03:50:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/24/2022 03:50:50 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.2063342318059299 on epoch=362
05/24/2022 03:50:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=364
05/24/2022 03:50:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=367
05/24/2022 03:51:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=369
05/24/2022 03:51:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=372
05/24/2022 03:51:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=374
05/24/2022 03:51:15 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.29064498933901917 on epoch=374
05/24/2022 03:51:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=377
05/24/2022 03:51:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/24/2022 03:51:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=382
05/24/2022 03:51:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
05/24/2022 03:51:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=387
05/24/2022 03:51:40 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.2599206349206349 on epoch=387
05/24/2022 03:51:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=389
05/24/2022 03:51:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=392
05/24/2022 03:51:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
05/24/2022 03:51:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
05/24/2022 03:52:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=399
05/24/2022 03:52:05 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.17554112554112553 on epoch=399
05/24/2022 03:52:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=402
05/24/2022 03:52:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=404
05/24/2022 03:52:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=407
05/24/2022 03:52:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/24/2022 03:52:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=412
05/24/2022 03:52:30 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.22218623481781377 on epoch=412
05/24/2022 03:52:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=414
05/24/2022 03:52:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/24/2022 03:52:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/24/2022 03:52:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=422
05/24/2022 03:52:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=424
05/24/2022 03:52:56 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.20797829036635007 on epoch=424
05/24/2022 03:53:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
05/24/2022 03:53:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=429
05/24/2022 03:53:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=432
05/24/2022 03:53:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
05/24/2022 03:53:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
05/24/2022 03:53:21 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.1707672680882085 on epoch=437
05/24/2022 03:53:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
05/24/2022 03:53:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=442
05/24/2022 03:53:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/24/2022 03:53:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=447
05/24/2022 03:53:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=449
05/24/2022 03:53:46 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.2212211668928087 on epoch=449
05/24/2022 03:53:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
05/24/2022 03:53:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
05/24/2022 03:53:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
05/24/2022 03:54:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
05/24/2022 03:54:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/24/2022 03:54:11 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.1522645179216449 on epoch=462
05/24/2022 03:54:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=464
05/24/2022 03:54:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=467
05/24/2022 03:54:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=469
05/24/2022 03:54:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
05/24/2022 03:54:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=474
05/24/2022 03:54:36 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.20963142319074518 on epoch=474
05/24/2022 03:54:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
05/24/2022 03:54:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/24/2022 03:54:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/24/2022 03:54:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/24/2022 03:54:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
05/24/2022 03:55:01 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.2212211668928087 on epoch=487
05/24/2022 03:55:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
05/24/2022 03:55:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
05/24/2022 03:55:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=494
05/24/2022 03:55:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
05/24/2022 03:55:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/24/2022 03:55:27 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.19184981684981683 on epoch=499
05/24/2022 03:55:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
05/24/2022 03:55:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
05/24/2022 03:55:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=507
05/24/2022 03:55:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=509
05/24/2022 03:55:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=512
05/24/2022 03:55:52 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.21594827586206894 on epoch=512
05/24/2022 03:55:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
05/24/2022 03:56:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
05/24/2022 03:56:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
05/24/2022 03:56:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/24/2022 03:56:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/24/2022 03:56:17 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.21515151515151515 on epoch=524
05/24/2022 03:56:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
05/24/2022 03:56:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=529
05/24/2022 03:56:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
05/24/2022 03:56:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/24/2022 03:56:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/24/2022 03:56:42 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.20797829036635007 on epoch=537
05/24/2022 03:56:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/24/2022 03:56:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/24/2022 03:56:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/24/2022 03:57:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
05/24/2022 03:57:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
05/24/2022 03:57:07 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.21569928137092315 on epoch=549
05/24/2022 03:57:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
05/24/2022 03:57:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/24/2022 03:57:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/24/2022 03:57:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=559
05/24/2022 03:57:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=562
05/24/2022 03:57:32 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.148516452421376 on epoch=562
05/24/2022 03:57:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/24/2022 03:57:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/24/2022 03:57:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=569
05/24/2022 03:57:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/24/2022 03:57:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/24/2022 03:57:58 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.1271186440677966 on epoch=574
05/24/2022 03:58:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
05/24/2022 03:58:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/24/2022 03:58:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/24/2022 03:58:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/24/2022 03:58:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=587
05/24/2022 03:58:23 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.1459201905896367 on epoch=587
05/24/2022 03:58:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
05/24/2022 03:58:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/24/2022 03:58:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=594
05/24/2022 03:58:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/24/2022 03:58:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/24/2022 03:58:48 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.1873406193078324 on epoch=599
05/24/2022 03:58:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/24/2022 03:58:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/24/2022 03:59:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/24/2022 03:59:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/24/2022 03:59:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
05/24/2022 03:59:13 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.19940476190476195 on epoch=612
05/24/2022 03:59:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/24/2022 03:59:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/24/2022 03:59:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/24/2022 03:59:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/24/2022 03:59:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
05/24/2022 03:59:38 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.28404139433551195 on epoch=624
05/24/2022 03:59:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/24/2022 03:59:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/24/2022 03:59:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=632
05/24/2022 03:59:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/24/2022 04:00:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
05/24/2022 04:00:04 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.1898148148148148 on epoch=637
05/24/2022 04:00:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/24/2022 04:00:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=642
05/24/2022 04:00:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/24/2022 04:00:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/24/2022 04:00:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/24/2022 04:00:29 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.2248087431693989 on epoch=649
05/24/2022 04:00:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
05/24/2022 04:00:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/24/2022 04:00:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
05/24/2022 04:00:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/24/2022 04:00:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/24/2022 04:00:54 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.2557304611985639 on epoch=662
05/24/2022 04:00:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/24/2022 04:01:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/24/2022 04:01:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/24/2022 04:01:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=672
05/24/2022 04:01:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
05/24/2022 04:01:19 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.13983050847457626 on epoch=674
05/24/2022 04:01:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/24/2022 04:01:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/24/2022 04:01:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/24/2022 04:01:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/24/2022 04:01:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/24/2022 04:01:44 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.1584430267301946 on epoch=687
05/24/2022 04:01:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/24/2022 04:01:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/24/2022 04:01:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/24/2022 04:02:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/24/2022 04:02:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/24/2022 04:02:09 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.11700733752620544 on epoch=699
05/24/2022 04:02:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/24/2022 04:02:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/24/2022 04:02:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/24/2022 04:02:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/24/2022 04:02:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/24/2022 04:02:34 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.18303571428571427 on epoch=712
05/24/2022 04:02:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/24/2022 04:02:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/24/2022 04:02:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/24/2022 04:02:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/24/2022 04:02:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 04:03:00 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.13839285714285715 on epoch=724
05/24/2022 04:03:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/24/2022 04:03:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/24/2022 04:03:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 04:03:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/24/2022 04:03:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/24/2022 04:03:25 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.14293981481481483 on epoch=737
05/24/2022 04:03:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
05/24/2022 04:03:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/24/2022 04:03:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/24/2022 04:03:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/24/2022 04:03:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/24/2022 04:03:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:03:48 - INFO - __main__ - Printing 3 examples
05/24/2022 04:03:48 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 04:03:48 - INFO - __main__ - ['entailed']
05/24/2022 04:03:48 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 04:03:48 - INFO - __main__ - ['entailed']
05/24/2022 04:03:48 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 04:03:48 - INFO - __main__ - ['entailed']
05/24/2022 04:03:48 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:03:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:03:49 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 04:03:49 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:03:49 - INFO - __main__ - Printing 3 examples
05/24/2022 04:03:49 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 04:03:49 - INFO - __main__ - ['entailed']
05/24/2022 04:03:49 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 04:03:49 - INFO - __main__ - ['entailed']
05/24/2022 04:03:49 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 04:03:49 - INFO - __main__ - ['entailed']
05/24/2022 04:03:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:03:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:03:49 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 04:03:50 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.14293981481481483 on epoch=749
05/24/2022 04:03:50 - INFO - __main__ - save last model!
05/24/2022 04:03:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 04:03:50 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 04:03:50 - INFO - __main__ - Printing 3 examples
05/24/2022 04:03:50 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:03:50 - INFO - __main__ - ['entailed']
05/24/2022 04:03:50 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:03:50 - INFO - __main__ - ['entailed']
05/24/2022 04:03:50 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:03:50 - INFO - __main__ - ['entailed']
05/24/2022 04:03:50 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:04:04 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 04:04:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 04:04:05 - INFO - __main__ - Starting training!
05/24/2022 04:04:14 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:04:27 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 04:13:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_42_0.2_8_predictions.txt
05/24/2022 04:13:32 - INFO - __main__ - Classification-F1 on test data: 0.0044
05/24/2022 04:13:33 - INFO - __main__ - prefix=tab_fact_32_42, lr=0.2, bsz=8, dev_performance=0.36374269005847953, test_performance=0.004381328949528103
05/24/2022 04:13:33 - INFO - __main__ - Running ... prefix=tab_fact_32_87, lr=0.5, bsz=8 ...
05/24/2022 04:13:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:13:34 - INFO - __main__ - Printing 3 examples
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:13:34 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:13:34 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 04:13:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:13:34 - INFO - __main__ - Printing 3 examples
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 04:13:34 - INFO - __main__ - ['entailed']
05/24/2022 04:13:34 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:13:34 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:13:34 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 04:13:53 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 04:13:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 04:13:54 - INFO - __main__ - Starting training!
05/24/2022 04:13:59 - INFO - __main__ - Step 10 Global step 10 Train loss 4.60 on epoch=2
05/24/2022 04:14:03 - INFO - __main__ - Step 20 Global step 20 Train loss 2.76 on epoch=4
05/24/2022 04:14:08 - INFO - __main__ - Step 30 Global step 30 Train loss 1.97 on epoch=7
05/24/2022 04:14:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.34 on epoch=9
05/24/2022 04:14:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
05/24/2022 04:14:19 - INFO - __main__ - Global step 50 Train loss 2.33 Classification-F1 0.3333333333333333 on epoch=12
05/24/2022 04:14:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=12, global_step=50
05/24/2022 04:14:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=14
05/24/2022 04:14:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=17
05/24/2022 04:14:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=19
05/24/2022 04:14:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=22
05/24/2022 04:14:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=24
05/24/2022 04:14:44 - INFO - __main__ - Global step 100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 04:14:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=27
05/24/2022 04:14:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=29
05/24/2022 04:14:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=32
05/24/2022 04:15:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=34
05/24/2022 04:15:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=37
05/24/2022 04:15:09 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 04:15:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=39
05/24/2022 04:15:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=42
05/24/2022 04:15:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=44
05/24/2022 04:15:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=47
05/24/2022 04:15:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=49
05/24/2022 04:15:35 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 04:15:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=52
05/24/2022 04:15:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=54
05/24/2022 04:15:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=57
05/24/2022 04:15:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=59
05/24/2022 04:15:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=62
05/24/2022 04:16:00 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 04:16:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=64
05/24/2022 04:16:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=67
05/24/2022 04:16:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=69
05/24/2022 04:16:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=72
05/24/2022 04:16:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=74
05/24/2022 04:16:25 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 04:16:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=77
05/24/2022 04:16:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=79
05/24/2022 04:16:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=82
05/24/2022 04:16:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=84
05/24/2022 04:16:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=87
05/24/2022 04:16:51 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 04:16:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=89
05/24/2022 04:17:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=92
05/24/2022 04:17:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
05/24/2022 04:17:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/24/2022 04:17:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=99
05/24/2022 04:17:16 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 04:17:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=102
05/24/2022 04:17:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=104
05/24/2022 04:17:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.18 on epoch=107
05/24/2022 04:17:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=109
05/24/2022 04:17:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=112
05/24/2022 04:17:41 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 04:17:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/24/2022 04:17:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=117
05/24/2022 04:17:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/24/2022 04:17:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=122
05/24/2022 04:18:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=124
05/24/2022 04:18:07 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3347193347193347 on epoch=124
05/24/2022 04:18:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3347193347193347 on epoch=124, global_step=500
05/24/2022 04:18:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=127
05/24/2022 04:18:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.19 on epoch=129
05/24/2022 04:18:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
05/24/2022 04:18:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=134
05/24/2022 04:18:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=137
05/24/2022 04:18:32 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.5 on epoch=137
05/24/2022 04:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3347193347193347 -> 0.5 on epoch=137, global_step=550
05/24/2022 04:18:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/24/2022 04:18:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=142
05/24/2022 04:18:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/24/2022 04:18:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
05/24/2022 04:18:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
05/24/2022 04:18:58 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.5586206896551724 on epoch=149
05/24/2022 04:18:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5 -> 0.5586206896551724 on epoch=149, global_step=600
05/24/2022 04:19:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.16 on epoch=152
05/24/2022 04:19:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=154
05/24/2022 04:19:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
05/24/2022 04:19:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
05/24/2022 04:19:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/24/2022 04:19:23 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.5607843137254902 on epoch=162
05/24/2022 04:19:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5586206896551724 -> 0.5607843137254902 on epoch=162, global_step=650
05/24/2022 04:19:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/24/2022 04:19:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/24/2022 04:19:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=169
05/24/2022 04:19:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=172
05/24/2022 04:19:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/24/2022 04:19:48 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.32627450980392153 on epoch=174
05/24/2022 04:19:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=177
05/24/2022 04:19:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
05/24/2022 04:20:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/24/2022 04:20:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=184
05/24/2022 04:20:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
05/24/2022 04:20:13 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.22823529411764704 on epoch=187
05/24/2022 04:20:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
05/24/2022 04:20:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
05/24/2022 04:20:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/24/2022 04:20:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=197
05/24/2022 04:20:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
05/24/2022 04:20:39 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.15966666666666668 on epoch=199
05/24/2022 04:20:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/24/2022 04:20:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
05/24/2022 04:20:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
05/24/2022 04:20:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
05/24/2022 04:21:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=212
05/24/2022 04:21:04 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.2356546687532603 on epoch=212
05/24/2022 04:21:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/24/2022 04:21:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.11 on epoch=217
05/24/2022 04:21:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
05/24/2022 04:21:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
05/24/2022 04:21:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
05/24/2022 04:21:29 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.19585921325051756 on epoch=224
05/24/2022 04:21:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/24/2022 04:21:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/24/2022 04:21:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/24/2022 04:21:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/24/2022 04:21:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/24/2022 04:21:55 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.23808203991130822 on epoch=237
05/24/2022 04:21:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
05/24/2022 04:22:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
05/24/2022 04:22:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
05/24/2022 04:22:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/24/2022 04:22:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/24/2022 04:22:20 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.1299182808716707 on epoch=249
05/24/2022 04:22:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/24/2022 04:22:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
05/24/2022 04:22:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=257
05/24/2022 04:22:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
05/24/2022 04:22:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/24/2022 04:22:45 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.1379905808477237 on epoch=262
05/24/2022 04:22:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
05/24/2022 04:22:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/24/2022 04:22:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
05/24/2022 04:23:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/24/2022 04:23:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/24/2022 04:23:11 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.15795206971677558 on epoch=274
05/24/2022 04:23:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
05/24/2022 04:23:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/24/2022 04:23:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
05/24/2022 04:23:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/24/2022 04:23:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
05/24/2022 04:23:36 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.11790527531767424 on epoch=287
05/24/2022 04:23:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/24/2022 04:23:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/24/2022 04:23:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/24/2022 04:23:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/24/2022 04:23:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
05/24/2022 04:24:01 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.176 on epoch=299
05/24/2022 04:24:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/24/2022 04:24:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/24/2022 04:24:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/24/2022 04:24:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
05/24/2022 04:24:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/24/2022 04:24:26 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.09305555555555556 on epoch=312
05/24/2022 04:24:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/24/2022 04:24:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/24/2022 04:24:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/24/2022 04:24:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/24/2022 04:24:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/24/2022 04:24:51 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.14384546271338725 on epoch=324
05/24/2022 04:24:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/24/2022 04:25:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/24/2022 04:25:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/24/2022 04:25:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/24/2022 04:25:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/24/2022 04:25:16 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.05381944444444445 on epoch=337
05/24/2022 04:25:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/24/2022 04:25:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/24/2022 04:25:30 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/24/2022 04:25:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/24/2022 04:25:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/24/2022 04:25:41 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.0855924978687127 on epoch=349
05/24/2022 04:25:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/24/2022 04:25:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/24/2022 04:25:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/24/2022 04:25:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/24/2022 04:26:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
05/24/2022 04:26:07 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.11710418375717802 on epoch=362
05/24/2022 04:26:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/24/2022 04:26:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/24/2022 04:26:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/24/2022 04:26:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/24/2022 04:26:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/24/2022 04:26:32 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.11454018227009113 on epoch=374
05/24/2022 04:26:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/24/2022 04:26:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/24/2022 04:26:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/24/2022 04:26:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/24/2022 04:26:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/24/2022 04:26:57 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.11417748917748918 on epoch=387
05/24/2022 04:27:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/24/2022 04:27:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/24/2022 04:27:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/24/2022 04:27:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/24/2022 04:27:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/24/2022 04:27:22 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.09153936545240893 on epoch=399
05/24/2022 04:27:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/24/2022 04:27:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/24/2022 04:27:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/24/2022 04:27:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/24/2022 04:27:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/24/2022 04:27:47 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.09365079365079365 on epoch=412
05/24/2022 04:27:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/24/2022 04:27:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/24/2022 04:28:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/24/2022 04:28:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/24/2022 04:28:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/24/2022 04:28:13 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.09580303610154356 on epoch=424
05/24/2022 04:28:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/24/2022 04:28:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/24/2022 04:28:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/24/2022 04:28:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/24/2022 04:28:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/24/2022 04:28:38 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.07469919786096257 on epoch=437
05/24/2022 04:28:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/24/2022 04:28:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/24/2022 04:28:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/24/2022 04:28:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/24/2022 04:29:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/24/2022 04:29:03 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.09759988901220865 on epoch=449
05/24/2022 04:29:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/24/2022 04:29:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/24/2022 04:29:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/24/2022 04:29:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/24/2022 04:29:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/24/2022 04:29:29 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.08078703703703702 on epoch=462
05/24/2022 04:29:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/24/2022 04:29:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/24/2022 04:29:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/24/2022 04:29:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/24/2022 04:29:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/24/2022 04:29:54 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.06372549019607843 on epoch=474
05/24/2022 04:29:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/24/2022 04:30:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/24/2022 04:30:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/24/2022 04:30:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/24/2022 04:30:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/24/2022 04:30:19 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.07062146892655367 on epoch=487
05/24/2022 04:30:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/24/2022 04:30:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/24/2022 04:30:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/24/2022 04:30:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/24/2022 04:30:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/24/2022 04:30:44 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.06731934731934731 on epoch=499
05/24/2022 04:30:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/24/2022 04:30:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/24/2022 04:30:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/24/2022 04:31:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/24/2022 04:31:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/24/2022 04:31:10 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.09321077245605548 on epoch=512
05/24/2022 04:31:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/24/2022 04:31:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/24/2022 04:31:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/24/2022 04:31:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/24/2022 04:31:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/24/2022 04:31:35 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.060505980734624744 on epoch=524
05/24/2022 04:31:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/24/2022 04:31:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/24/2022 04:31:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/24/2022 04:31:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/24/2022 04:31:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/24/2022 04:32:00 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.05748917748917749 on epoch=537
05/24/2022 04:32:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/24/2022 04:32:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/24/2022 04:32:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/24/2022 04:32:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/24/2022 04:32:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/24/2022 04:32:26 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.06432525291086086 on epoch=549
05/24/2022 04:32:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/24/2022 04:32:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/24/2022 04:32:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/24/2022 04:32:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/24/2022 04:32:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/24/2022 04:32:51 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.08074866310160428 on epoch=562
05/24/2022 04:32:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/24/2022 04:33:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/24/2022 04:33:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/24/2022 04:33:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/24/2022 04:33:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/24/2022 04:33:16 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.09819504310344827 on epoch=574
05/24/2022 04:33:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/24/2022 04:33:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/24/2022 04:33:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/24/2022 04:33:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/24/2022 04:33:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/24/2022 04:33:41 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.06645833333333333 on epoch=587
05/24/2022 04:33:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/24/2022 04:33:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/24/2022 04:33:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/24/2022 04:33:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/24/2022 04:34:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/24/2022 04:34:07 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.08920780711825486 on epoch=599
05/24/2022 04:34:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/24/2022 04:34:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/24/2022 04:34:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/24/2022 04:34:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/24/2022 04:34:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/24/2022 04:34:32 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.0926800654502794 on epoch=612
05/24/2022 04:34:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/24/2022 04:34:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/24/2022 04:34:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/24/2022 04:34:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/24/2022 04:34:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/24/2022 04:34:57 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.07821853300343204 on epoch=624
05/24/2022 04:35:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/24/2022 04:35:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/24/2022 04:35:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/24/2022 04:35:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/24/2022 04:35:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/24/2022 04:35:22 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.07503628447024674 on epoch=637
05/24/2022 04:35:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/24/2022 04:35:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/24/2022 04:35:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/24/2022 04:35:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/24/2022 04:35:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/24/2022 04:35:48 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.0887052341597796 on epoch=649
05/24/2022 04:35:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/24/2022 04:35:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/24/2022 04:36:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/24/2022 04:36:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/24/2022 04:36:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/24/2022 04:36:13 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.06008277436848865 on epoch=662
05/24/2022 04:36:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/24/2022 04:36:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/24/2022 04:36:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/24/2022 04:36:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/24/2022 04:36:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/24/2022 04:36:38 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.060748093006157515 on epoch=674
05/24/2022 04:36:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/24/2022 04:36:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/24/2022 04:36:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/24/2022 04:36:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/24/2022 04:37:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/24/2022 04:37:03 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.10706504824151881 on epoch=687
05/24/2022 04:37:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/24/2022 04:37:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/24/2022 04:37:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/24/2022 04:37:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/24/2022 04:37:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/24/2022 04:37:29 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.08449729959826535 on epoch=699
05/24/2022 04:37:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/24/2022 04:37:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/24/2022 04:37:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/24/2022 04:37:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/24/2022 04:37:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/24/2022 04:37:54 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.06619928182328945 on epoch=712
05/24/2022 04:37:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/24/2022 04:38:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/24/2022 04:38:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/24/2022 04:38:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/24/2022 04:38:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 04:38:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.08319088319088318 on epoch=724
05/24/2022 04:38:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 04:38:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/24/2022 04:38:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 04:38:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 04:38:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/24/2022 04:38:45 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.09217391304347826 on epoch=737
05/24/2022 04:38:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/24/2022 04:38:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/24/2022 04:38:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/24/2022 04:39:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/24/2022 04:39:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/24/2022 04:39:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:39:08 - INFO - __main__ - Printing 3 examples
05/24/2022 04:39:08 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 04:39:08 - INFO - __main__ - ['entailed']
05/24/2022 04:39:08 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 04:39:08 - INFO - __main__ - ['entailed']
05/24/2022 04:39:08 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 04:39:08 - INFO - __main__ - ['entailed']
05/24/2022 04:39:08 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:39:08 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:39:09 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 04:39:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:39:09 - INFO - __main__ - Printing 3 examples
05/24/2022 04:39:09 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 04:39:09 - INFO - __main__ - ['entailed']
05/24/2022 04:39:09 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 04:39:09 - INFO - __main__ - ['entailed']
05/24/2022 04:39:09 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 04:39:09 - INFO - __main__ - ['entailed']
05/24/2022 04:39:09 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:39:09 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:39:09 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 04:39:10 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.07823551488206935 on epoch=749
05/24/2022 04:39:10 - INFO - __main__ - save last model!
05/24/2022 04:39:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 04:39:10 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 04:39:10 - INFO - __main__ - Printing 3 examples
05/24/2022 04:39:10 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:39:10 - INFO - __main__ - ['entailed']
05/24/2022 04:39:10 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:39:10 - INFO - __main__ - ['entailed']
05/24/2022 04:39:10 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 04:39:10 - INFO - __main__ - ['entailed']
05/24/2022 04:39:10 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:39:26 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 04:39:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 04:39:27 - INFO - __main__ - Starting training!
05/24/2022 04:39:36 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:39:49 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 04:48:54 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_87_0.5_8_predictions.txt
05/24/2022 04:48:54 - INFO - __main__ - Classification-F1 on test data: 0.0021
05/24/2022 04:48:54 - INFO - __main__ - prefix=tab_fact_32_87, lr=0.5, bsz=8, dev_performance=0.5607843137254902, test_performance=0.002127651084340332
05/24/2022 04:48:54 - INFO - __main__ - Running ... prefix=tab_fact_32_87, lr=0.4, bsz=8 ...
05/24/2022 04:48:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:48:55 - INFO - __main__ - Printing 3 examples
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:48:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:48:55 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 04:48:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 04:48:55 - INFO - __main__ - Printing 3 examples
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 04:48:55 - INFO - __main__ - ['entailed']
05/24/2022 04:48:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 04:48:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 04:48:56 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 04:49:15 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 04:49:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 04:49:16 - INFO - __main__ - Starting training!
05/24/2022 04:49:21 - INFO - __main__ - Step 10 Global step 10 Train loss 5.10 on epoch=2
05/24/2022 04:49:25 - INFO - __main__ - Step 20 Global step 20 Train loss 3.02 on epoch=4
05/24/2022 04:49:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.38 on epoch=7
05/24/2022 04:49:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=9
05/24/2022 04:49:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.28 on epoch=12
05/24/2022 04:49:41 - INFO - __main__ - Global step 50 Train loss 2.70 Classification-F1 0.10857142857142857 on epoch=12
05/24/2022 04:49:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10857142857142857 on epoch=12, global_step=50
05/24/2022 04:49:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=14
05/24/2022 04:49:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=17
05/24/2022 04:49:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=19
05/24/2022 04:50:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=22
05/24/2022 04:50:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=24
05/24/2022 04:50:07 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 04:50:07 - INFO - __main__ - Saving model with best Classification-F1: 0.10857142857142857 -> 0.3333333333333333 on epoch=24, global_step=100
05/24/2022 04:50:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=27
05/24/2022 04:50:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=29
05/24/2022 04:50:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=32
05/24/2022 04:50:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=34
05/24/2022 04:50:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=37
05/24/2022 04:50:32 - INFO - __main__ - Global step 150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 04:50:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=39
05/24/2022 04:50:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=42
05/24/2022 04:50:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=44
05/24/2022 04:50:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=47
05/24/2022 04:50:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=49
05/24/2022 04:50:57 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 04:51:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=52
05/24/2022 04:51:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=54
05/24/2022 04:51:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=57
05/24/2022 04:51:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=59
05/24/2022 04:51:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=62
05/24/2022 04:51:23 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 04:51:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=64
05/24/2022 04:51:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=67
05/24/2022 04:51:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=69
05/24/2022 04:51:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=72
05/24/2022 04:51:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/24/2022 04:51:48 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 04:51:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=77
05/24/2022 04:51:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=79
05/24/2022 04:52:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=82
05/24/2022 04:52:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=84
05/24/2022 04:52:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=87
05/24/2022 04:52:14 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 04:52:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
05/24/2022 04:52:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=92
05/24/2022 04:52:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 04:52:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=97
05/24/2022 04:52:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=99
05/24/2022 04:52:39 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 04:52:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/24/2022 04:52:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=104
05/24/2022 04:52:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=107
05/24/2022 04:52:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/24/2022 04:53:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/24/2022 04:53:05 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 04:53:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=114
05/24/2022 04:53:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
05/24/2022 04:53:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=119
05/24/2022 04:53:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=122
05/24/2022 04:53:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
05/24/2022 04:53:30 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.3671451355661882 on epoch=124
05/24/2022 04:53:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3671451355661882 on epoch=124, global_step=500
05/24/2022 04:53:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/24/2022 04:53:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=129
05/24/2022 04:53:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=132
05/24/2022 04:53:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=134
05/24/2022 04:53:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=137
05/24/2022 04:53:56 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.34299516908212563 on epoch=137
05/24/2022 04:54:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=139
05/24/2022 04:54:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=142
05/24/2022 04:54:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=144
05/24/2022 04:54:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=147
05/24/2022 04:54:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=149
05/24/2022 04:54:21 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.34299516908212563 on epoch=149
05/24/2022 04:54:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=152
05/24/2022 04:54:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/24/2022 04:54:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/24/2022 04:54:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
05/24/2022 04:54:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/24/2022 04:54:47 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.4079058031959629 on epoch=162
05/24/2022 04:54:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3671451355661882 -> 0.4079058031959629 on epoch=162, global_step=650
05/24/2022 04:54:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/24/2022 04:54:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
05/24/2022 04:55:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/24/2022 04:55:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
05/24/2022 04:55:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
05/24/2022 04:55:12 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.41125541125541126 on epoch=174
05/24/2022 04:55:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4079058031959629 -> 0.41125541125541126 on epoch=174, global_step=700
05/24/2022 04:55:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=177
05/24/2022 04:55:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=179
05/24/2022 04:55:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/24/2022 04:55:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/24/2022 04:55:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/24/2022 04:55:38 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.37356767467907304 on epoch=187
05/24/2022 04:55:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
05/24/2022 04:55:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
05/24/2022 04:55:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=194
05/24/2022 04:55:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
05/24/2022 04:56:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
05/24/2022 04:56:03 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.2848469516822666 on epoch=199
05/24/2022 04:56:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
05/24/2022 04:56:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=204
05/24/2022 04:56:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
05/24/2022 04:56:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
05/24/2022 04:56:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=212
05/24/2022 04:56:28 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.2717725409836066 on epoch=212
05/24/2022 04:56:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=214
05/24/2022 04:56:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
05/24/2022 04:56:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
05/24/2022 04:56:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
05/24/2022 04:56:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=224
05/24/2022 04:56:54 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.3518566382355475 on epoch=224
05/24/2022 04:56:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
05/24/2022 04:57:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
05/24/2022 04:57:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/24/2022 04:57:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
05/24/2022 04:57:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/24/2022 04:57:19 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.12457912457912458 on epoch=237
05/24/2022 04:57:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=239
05/24/2022 04:57:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
05/24/2022 04:57:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
05/24/2022 04:57:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
05/24/2022 04:57:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/24/2022 04:57:44 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.16834011759384893 on epoch=249
05/24/2022 04:57:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
05/24/2022 04:57:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
05/24/2022 04:57:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=257
05/24/2022 04:58:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
05/24/2022 04:58:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=262
05/24/2022 04:58:10 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.17209005560113025 on epoch=262
05/24/2022 04:58:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
05/24/2022 04:58:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=267
05/24/2022 04:58:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=269
05/24/2022 04:58:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/24/2022 04:58:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/24/2022 04:58:35 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.09948666997847326 on epoch=274
05/24/2022 04:58:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=277
05/24/2022 04:58:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
05/24/2022 04:58:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/24/2022 04:58:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=284
05/24/2022 04:58:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
05/24/2022 04:59:00 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.09332422853054985 on epoch=287
05/24/2022 04:59:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
05/24/2022 04:59:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
05/24/2022 04:59:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
05/24/2022 04:59:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/24/2022 04:59:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/24/2022 04:59:26 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.1083443375463749 on epoch=299
05/24/2022 04:59:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/24/2022 04:59:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/24/2022 04:59:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/24/2022 04:59:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/24/2022 04:59:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/24/2022 04:59:51 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.0600907029478458 on epoch=312
05/24/2022 04:59:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
05/24/2022 05:00:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.10 on epoch=317
05/24/2022 05:00:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/24/2022 05:00:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/24/2022 05:00:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/24/2022 05:00:16 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.09173789173789174 on epoch=324
05/24/2022 05:00:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/24/2022 05:00:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/24/2022 05:00:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/24/2022 05:00:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/24/2022 05:00:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/24/2022 05:00:41 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.070749791144528 on epoch=337
05/24/2022 05:00:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/24/2022 05:00:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/24/2022 05:00:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
05/24/2022 05:00:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/24/2022 05:01:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/24/2022 05:01:07 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.05982905982905983 on epoch=349
05/24/2022 05:01:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
05/24/2022 05:01:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/24/2022 05:01:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/24/2022 05:01:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/24/2022 05:01:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
05/24/2022 05:01:32 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.05672131147540984 on epoch=362
05/24/2022 05:01:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/24/2022 05:01:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/24/2022 05:01:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/24/2022 05:01:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/24/2022 05:01:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/24/2022 05:01:57 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.07202380952380953 on epoch=374
05/24/2022 05:02:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/24/2022 05:02:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/24/2022 05:02:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/24/2022 05:02:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/24/2022 05:02:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/24/2022 05:02:23 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.047169811320754707 on epoch=387
05/24/2022 05:02:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/24/2022 05:02:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/24/2022 05:02:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/24/2022 05:02:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/24/2022 05:02:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
05/24/2022 05:02:48 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.0333952528379773 on epoch=399
05/24/2022 05:02:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/24/2022 05:02:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/24/2022 05:03:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/24/2022 05:03:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/24/2022 05:03:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
05/24/2022 05:03:13 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.035406698564593296 on epoch=412
05/24/2022 05:03:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/24/2022 05:03:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/24/2022 05:03:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/24/2022 05:03:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/24/2022 05:03:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/24/2022 05:03:39 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.04348884381338742 on epoch=424
05/24/2022 05:03:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/24/2022 05:03:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/24/2022 05:03:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/24/2022 05:03:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/24/2022 05:04:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/24/2022 05:04:04 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.06342780026990553 on epoch=437
05/24/2022 05:04:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/24/2022 05:04:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/24/2022 05:04:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/24/2022 05:04:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/24/2022 05:04:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/24/2022 05:04:29 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.0440251572327044 on epoch=449
05/24/2022 05:04:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/24/2022 05:04:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/24/2022 05:04:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
05/24/2022 05:04:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/24/2022 05:04:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/24/2022 05:04:55 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.02679549114331723 on epoch=462
05/24/2022 05:04:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/24/2022 05:05:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/24/2022 05:05:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/24/2022 05:05:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/24/2022 05:05:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/24/2022 05:05:20 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.031849103277674706 on epoch=474
05/24/2022 05:05:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/24/2022 05:05:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/24/2022 05:05:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/24/2022 05:05:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/24/2022 05:05:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/24/2022 05:05:45 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.045536858811321135 on epoch=487
05/24/2022 05:05:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/24/2022 05:05:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/24/2022 05:05:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/24/2022 05:06:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/24/2022 05:06:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/24/2022 05:06:11 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.0418151579947731 on epoch=499
05/24/2022 05:06:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/24/2022 05:06:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/24/2022 05:06:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/24/2022 05:06:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/24/2022 05:06:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/24/2022 05:06:36 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.04531237782469309 on epoch=512
05/24/2022 05:06:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/24/2022 05:06:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/24/2022 05:06:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/24/2022 05:06:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/24/2022 05:06:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/24/2022 05:07:01 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.056251402918069585 on epoch=524
05/24/2022 05:07:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/24/2022 05:07:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/24/2022 05:07:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/24/2022 05:07:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/24/2022 05:07:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/24/2022 05:07:26 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.03364984174579377 on epoch=537
05/24/2022 05:07:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/24/2022 05:07:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/24/2022 05:07:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/24/2022 05:07:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/24/2022 05:07:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/24/2022 05:07:51 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.03414471228463095 on epoch=549
05/24/2022 05:07:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/24/2022 05:08:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/24/2022 05:08:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/24/2022 05:08:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/24/2022 05:08:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/24/2022 05:08:17 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.06867079280872385 on epoch=562
05/24/2022 05:08:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/24/2022 05:08:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/24/2022 05:08:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/24/2022 05:08:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/24/2022 05:08:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/24/2022 05:08:42 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.058086392451717525 on epoch=574
05/24/2022 05:08:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/24/2022 05:08:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/24/2022 05:08:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
05/24/2022 05:09:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
05/24/2022 05:09:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/24/2022 05:09:07 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.06257341646335954 on epoch=587
05/24/2022 05:09:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/24/2022 05:09:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/24/2022 05:09:21 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/24/2022 05:09:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/24/2022 05:09:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/24/2022 05:09:33 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.05497802569303584 on epoch=599
05/24/2022 05:09:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/24/2022 05:09:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/24/2022 05:09:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/24/2022 05:09:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/24/2022 05:09:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/24/2022 05:09:58 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.05553283875615788 on epoch=612
05/24/2022 05:10:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/24/2022 05:10:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/24/2022 05:10:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/24/2022 05:10:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/24/2022 05:10:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/24/2022 05:10:24 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.09228070175438596 on epoch=624
05/24/2022 05:10:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/24/2022 05:10:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/24/2022 05:10:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/24/2022 05:10:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/24/2022 05:10:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/24/2022 05:10:49 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.050362080315997364 on epoch=637
05/24/2022 05:10:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/24/2022 05:10:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/24/2022 05:11:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/24/2022 05:11:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/24/2022 05:11:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/24/2022 05:11:14 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.05410714285714286 on epoch=649
05/24/2022 05:11:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/24/2022 05:11:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/24/2022 05:11:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/24/2022 05:11:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/24/2022 05:11:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/24/2022 05:11:39 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.07684946632315054 on epoch=662
05/24/2022 05:11:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/24/2022 05:11:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/24/2022 05:11:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/24/2022 05:11:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/24/2022 05:12:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/24/2022 05:12:05 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.0514456630109671 on epoch=674
05/24/2022 05:12:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/24/2022 05:12:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/24/2022 05:12:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/24/2022 05:12:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/24/2022 05:12:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/24/2022 05:12:30 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.054833449802003255 on epoch=687
05/24/2022 05:12:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/24/2022 05:12:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/24/2022 05:12:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/24/2022 05:12:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/24/2022 05:12:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/24/2022 05:12:55 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.0420823300080266 on epoch=699
05/24/2022 05:13:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/24/2022 05:13:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/24/2022 05:13:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/24/2022 05:13:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/24/2022 05:13:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/24/2022 05:13:21 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.04021390374331551 on epoch=712
05/24/2022 05:13:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/24/2022 05:13:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/24/2022 05:13:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/24/2022 05:13:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/24/2022 05:13:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/24/2022 05:13:46 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.04990364940302362 on epoch=724
05/24/2022 05:13:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 05:13:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 05:13:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 05:14:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 05:14:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/24/2022 05:14:11 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.03453647416413373 on epoch=737
05/24/2022 05:14:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=739
05/24/2022 05:14:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/24/2022 05:14:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/24/2022 05:14:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/24/2022 05:14:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/24/2022 05:14:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:14:35 - INFO - __main__ - Printing 3 examples
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:14:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:14:35 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 05:14:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:14:35 - INFO - __main__ - Printing 3 examples
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 05:14:35 - INFO - __main__ - ['entailed']
05/24/2022 05:14:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:14:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:14:35 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 05:14:36 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.03257575757575758 on epoch=749
05/24/2022 05:14:36 - INFO - __main__ - save last model!
05/24/2022 05:14:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 05:14:36 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 05:14:36 - INFO - __main__ - Printing 3 examples
05/24/2022 05:14:36 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:14:36 - INFO - __main__ - ['entailed']
05/24/2022 05:14:36 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:14:36 - INFO - __main__ - ['entailed']
05/24/2022 05:14:36 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:14:36 - INFO - __main__ - ['entailed']
05/24/2022 05:14:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:14:51 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 05:14:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 05:14:52 - INFO - __main__ - Starting training!
05/24/2022 05:15:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:15:13 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 05:23:41 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_87_0.4_8_predictions.txt
05/24/2022 05:23:41 - INFO - __main__ - Classification-F1 on test data: 0.0014
05/24/2022 05:23:41 - INFO - __main__ - prefix=tab_fact_32_87, lr=0.4, bsz=8, dev_performance=0.41125541125541126, test_performance=0.0014213261576771507
05/24/2022 05:23:41 - INFO - __main__ - Running ... prefix=tab_fact_32_87, lr=0.3, bsz=8 ...
05/24/2022 05:23:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:23:42 - INFO - __main__ - Printing 3 examples
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:23:42 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:23:42 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 05:23:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:23:42 - INFO - __main__ - Printing 3 examples
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 05:23:42 - INFO - __main__ - ['entailed']
05/24/2022 05:23:42 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:23:42 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:23:42 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 05:23:58 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 05:23:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 05:23:58 - INFO - __main__ - Starting training!
05/24/2022 05:24:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.53 on epoch=2
05/24/2022 05:24:08 - INFO - __main__ - Step 20 Global step 20 Train loss 3.35 on epoch=4
05/24/2022 05:24:12 - INFO - __main__ - Step 30 Global step 30 Train loss 2.69 on epoch=7
05/24/2022 05:24:17 - INFO - __main__ - Step 40 Global step 40 Train loss 2.19 on epoch=9
05/24/2022 05:24:21 - INFO - __main__ - Step 50 Global step 50 Train loss 1.78 on epoch=12
05/24/2022 05:24:24 - INFO - __main__ - Global step 50 Train loss 3.11 Classification-F1 0.0791139240506329 on epoch=12
05/24/2022 05:24:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0791139240506329 on epoch=12, global_step=50
05/24/2022 05:24:28 - INFO - __main__ - Step 60 Global step 60 Train loss 1.44 on epoch=14
05/24/2022 05:24:33 - INFO - __main__ - Step 70 Global step 70 Train loss 1.17 on epoch=17
05/24/2022 05:24:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=19
05/24/2022 05:24:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=22
05/24/2022 05:24:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=24
05/24/2022 05:24:49 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=24
05/24/2022 05:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.0791139240506329 -> 0.3333333333333333 on epoch=24, global_step=100
05/24/2022 05:24:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=27
05/24/2022 05:24:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=29
05/24/2022 05:25:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=32
05/24/2022 05:25:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=34
05/24/2022 05:25:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=37
05/24/2022 05:25:14 - INFO - __main__ - Global step 150 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 05:25:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=39
05/24/2022 05:25:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=42
05/24/2022 05:25:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=44
05/24/2022 05:25:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.30 on epoch=47
05/24/2022 05:25:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=49
05/24/2022 05:25:38 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 05:25:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=52
05/24/2022 05:25:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=54
05/24/2022 05:25:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=57
05/24/2022 05:25:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=59
05/24/2022 05:26:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=62
05/24/2022 05:26:03 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 05:26:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=64
05/24/2022 05:26:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=67
05/24/2022 05:26:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=69
05/24/2022 05:26:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=72
05/24/2022 05:26:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
05/24/2022 05:26:28 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 05:26:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=77
05/24/2022 05:26:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=79
05/24/2022 05:26:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=82
05/24/2022 05:26:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=84
05/24/2022 05:26:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/24/2022 05:26:53 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 05:26:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/24/2022 05:27:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=92
05/24/2022 05:27:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/24/2022 05:27:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/24/2022 05:27:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/24/2022 05:27:18 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 05:27:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=102
05/24/2022 05:27:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=104
05/24/2022 05:27:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=107
05/24/2022 05:27:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=109
05/24/2022 05:27:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/24/2022 05:27:42 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 05:27:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=114
05/24/2022 05:27:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=117
05/24/2022 05:27:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=119
05/24/2022 05:28:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=122
05/24/2022 05:28:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=124
05/24/2022 05:28:07 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 05:28:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=127
05/24/2022 05:28:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=129
05/24/2022 05:28:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
05/24/2022 05:28:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=134
05/24/2022 05:28:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=137
05/24/2022 05:28:32 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 05:28:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/24/2022 05:28:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=142
05/24/2022 05:28:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=144
05/24/2022 05:28:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
05/24/2022 05:28:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=149
05/24/2022 05:28:57 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 05:29:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=152
05/24/2022 05:29:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
05/24/2022 05:29:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/24/2022 05:29:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=159
05/24/2022 05:29:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
05/24/2022 05:29:22 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 05:29:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/24/2022 05:29:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=167
05/24/2022 05:29:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/24/2022 05:29:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=172
05/24/2022 05:29:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/24/2022 05:29:48 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 05:29:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=177
05/24/2022 05:29:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/24/2022 05:30:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/24/2022 05:30:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
05/24/2022 05:30:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=187
05/24/2022 05:30:13 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.40116959064327484 on epoch=187
05/24/2022 05:30:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.40116959064327484 on epoch=187, global_step=750
05/24/2022 05:30:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
05/24/2022 05:30:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=192
05/24/2022 05:30:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=194
05/24/2022 05:30:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=197
05/24/2022 05:30:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
05/24/2022 05:30:39 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 05:30:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=202
05/24/2022 05:30:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
05/24/2022 05:30:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=207
05/24/2022 05:30:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
05/24/2022 05:31:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
05/24/2022 05:31:04 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.3818181818181818 on epoch=212
05/24/2022 05:31:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=214
05/24/2022 05:31:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=217
05/24/2022 05:31:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=219
05/24/2022 05:31:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=222
05/24/2022 05:31:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=224
05/24/2022 05:31:30 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.34299516908212563 on epoch=224
05/24/2022 05:31:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
05/24/2022 05:31:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/24/2022 05:31:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=232
05/24/2022 05:31:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
05/24/2022 05:31:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=237
05/24/2022 05:31:55 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.42840679919331603 on epoch=237
05/24/2022 05:31:55 - INFO - __main__ - Saving model with best Classification-F1: 0.40116959064327484 -> 0.42840679919331603 on epoch=237, global_step=950
05/24/2022 05:32:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=239
05/24/2022 05:32:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=242
05/24/2022 05:32:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
05/24/2022 05:32:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=247
05/24/2022 05:32:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=249
05/24/2022 05:32:21 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.5 on epoch=249
05/24/2022 05:32:21 - INFO - __main__ - Saving model with best Classification-F1: 0.42840679919331603 -> 0.5 on epoch=249, global_step=1000
05/24/2022 05:32:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=252
05/24/2022 05:32:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=254
05/24/2022 05:32:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
05/24/2022 05:32:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=259
05/24/2022 05:32:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
05/24/2022 05:32:46 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.3306397306397306 on epoch=262
05/24/2022 05:32:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=264
05/24/2022 05:32:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=267
05/24/2022 05:32:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=269
05/24/2022 05:33:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=272
05/24/2022 05:33:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=274
05/24/2022 05:33:11 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.3399289700659563 on epoch=274
05/24/2022 05:33:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=277
05/24/2022 05:33:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=279
05/24/2022 05:33:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=282
05/24/2022 05:33:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=284
05/24/2022 05:33:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
05/24/2022 05:33:36 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.3425553319919517 on epoch=287
05/24/2022 05:33:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=289
05/24/2022 05:33:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/24/2022 05:33:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=294
05/24/2022 05:33:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
05/24/2022 05:33:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
05/24/2022 05:34:02 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.327553969063403 on epoch=299
05/24/2022 05:34:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=302
05/24/2022 05:34:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
05/24/2022 05:34:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=307
05/24/2022 05:34:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
05/24/2022 05:34:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
05/24/2022 05:34:27 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.24473684210526314 on epoch=312
05/24/2022 05:34:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=314
05/24/2022 05:34:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
05/24/2022 05:34:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=319
05/24/2022 05:34:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=322
05/24/2022 05:34:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=324
05/24/2022 05:34:52 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.3069557362240289 on epoch=324
05/24/2022 05:34:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
05/24/2022 05:35:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
05/24/2022 05:35:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
05/24/2022 05:35:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
05/24/2022 05:35:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
05/24/2022 05:35:18 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.27098039215686276 on epoch=337
05/24/2022 05:35:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/24/2022 05:35:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=342
05/24/2022 05:35:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=344
05/24/2022 05:35:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/24/2022 05:35:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=349
05/24/2022 05:35:43 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.2590268886043534 on epoch=349
05/24/2022 05:35:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
05/24/2022 05:35:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
05/24/2022 05:35:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/24/2022 05:36:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/24/2022 05:36:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=362
05/24/2022 05:36:08 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.1677858357955054 on epoch=362
05/24/2022 05:36:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
05/24/2022 05:36:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
05/24/2022 05:36:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/24/2022 05:36:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=372
05/24/2022 05:36:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/24/2022 05:36:34 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.14332399626517273 on epoch=374
05/24/2022 05:36:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/24/2022 05:36:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/24/2022 05:36:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
05/24/2022 05:36:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/24/2022 05:36:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=387
05/24/2022 05:36:58 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.1673992673992674 on epoch=387
05/24/2022 05:37:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
05/24/2022 05:37:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/24/2022 05:37:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/24/2022 05:37:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/24/2022 05:37:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/24/2022 05:37:23 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.16448801742919392 on epoch=399
05/24/2022 05:37:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=402
05/24/2022 05:37:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
05/24/2022 05:37:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/24/2022 05:37:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=409
05/24/2022 05:37:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/24/2022 05:37:48 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.1677858357955054 on epoch=412
05/24/2022 05:37:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/24/2022 05:37:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/24/2022 05:38:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
05/24/2022 05:38:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/24/2022 05:38:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/24/2022 05:38:13 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.1673992673992674 on epoch=424
05/24/2022 05:38:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/24/2022 05:38:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/24/2022 05:38:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/24/2022 05:38:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/24/2022 05:38:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/24/2022 05:38:38 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.20737618545837724 on epoch=437
05/24/2022 05:38:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/24/2022 05:38:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/24/2022 05:38:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/24/2022 05:38:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/24/2022 05:39:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/24/2022 05:39:02 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.21116803278688528 on epoch=449
05/24/2022 05:39:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/24/2022 05:39:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/24/2022 05:39:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/24/2022 05:39:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/24/2022 05:39:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/24/2022 05:39:27 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.16723004694835683 on epoch=462
05/24/2022 05:39:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/24/2022 05:39:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/24/2022 05:39:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/24/2022 05:39:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/24/2022 05:39:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=474
05/24/2022 05:39:53 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.2468253968253968 on epoch=474
05/24/2022 05:39:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/24/2022 05:40:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/24/2022 05:40:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/24/2022 05:40:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/24/2022 05:40:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/24/2022 05:40:17 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.20627450980392154 on epoch=487
05/24/2022 05:40:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/24/2022 05:40:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/24/2022 05:40:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/24/2022 05:40:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/24/2022 05:40:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/24/2022 05:40:43 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.14942778817283844 on epoch=499
05/24/2022 05:40:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/24/2022 05:40:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/24/2022 05:40:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/24/2022 05:41:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/24/2022 05:41:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/24/2022 05:41:08 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.14448979591836736 on epoch=512
05/24/2022 05:41:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/24/2022 05:41:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/24/2022 05:41:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/24/2022 05:41:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/24/2022 05:41:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
05/24/2022 05:41:33 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.1423426513462528 on epoch=524
05/24/2022 05:41:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/24/2022 05:41:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/24/2022 05:41:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/24/2022 05:41:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/24/2022 05:41:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
05/24/2022 05:41:58 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.14388059701492537 on epoch=537
05/24/2022 05:42:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/24/2022 05:42:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/24/2022 05:42:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/24/2022 05:42:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/24/2022 05:42:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
05/24/2022 05:42:23 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.11286195286195284 on epoch=549
05/24/2022 05:42:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/24/2022 05:42:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/24/2022 05:42:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/24/2022 05:42:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/24/2022 05:42:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/24/2022 05:42:48 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.10282352941176472 on epoch=562
05/24/2022 05:42:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/24/2022 05:42:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/24/2022 05:43:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/24/2022 05:43:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/24/2022 05:43:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/24/2022 05:43:14 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.14146301459734295 on epoch=574
05/24/2022 05:43:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/24/2022 05:43:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/24/2022 05:43:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/24/2022 05:43:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/24/2022 05:43:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/24/2022 05:43:39 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.09689969604863222 on epoch=587
05/24/2022 05:43:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/24/2022 05:43:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/24/2022 05:43:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/24/2022 05:43:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/24/2022 05:44:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=599
05/24/2022 05:44:04 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.10794928299958481 on epoch=599
05/24/2022 05:44:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/24/2022 05:44:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/24/2022 05:44:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/24/2022 05:44:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/24/2022 05:44:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/24/2022 05:44:30 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.10326797385620916 on epoch=612
05/24/2022 05:44:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/24/2022 05:44:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/24/2022 05:44:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/24/2022 05:44:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/24/2022 05:44:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/24/2022 05:44:55 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.10051781906792567 on epoch=624
05/24/2022 05:44:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/24/2022 05:45:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/24/2022 05:45:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/24/2022 05:45:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/24/2022 05:45:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/24/2022 05:45:20 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.11519274376417234 on epoch=637
05/24/2022 05:45:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/24/2022 05:45:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/24/2022 05:45:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/24/2022 05:45:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/24/2022 05:45:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/24/2022 05:45:45 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.07758445258445258 on epoch=649
05/24/2022 05:45:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/24/2022 05:45:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/24/2022 05:45:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/24/2022 05:46:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/24/2022 05:46:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/24/2022 05:46:10 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.06425644028103045 on epoch=662
05/24/2022 05:46:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/24/2022 05:46:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/24/2022 05:46:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/24/2022 05:46:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/24/2022 05:46:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/24/2022 05:46:35 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.10157575757575757 on epoch=674
05/24/2022 05:46:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/24/2022 05:46:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/24/2022 05:46:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/24/2022 05:46:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/24/2022 05:46:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/24/2022 05:47:00 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.11131535947712419 on epoch=687
05/24/2022 05:47:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/24/2022 05:47:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/24/2022 05:47:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/24/2022 05:47:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/24/2022 05:47:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/24/2022 05:47:25 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.09544494720965309 on epoch=699
05/24/2022 05:47:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/24/2022 05:47:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/24/2022 05:47:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/24/2022 05:47:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/24/2022 05:47:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/24/2022 05:47:50 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.10598602731025722 on epoch=712
05/24/2022 05:47:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/24/2022 05:47:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/24/2022 05:48:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/24/2022 05:48:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/24/2022 05:48:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/24/2022 05:48:15 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.08617604617604618 on epoch=724
05/24/2022 05:48:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/24/2022 05:48:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/24/2022 05:48:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/24/2022 05:48:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/24/2022 05:48:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/24/2022 05:48:40 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.08692082111436951 on epoch=737
05/24/2022 05:48:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/24/2022 05:48:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/24/2022 05:48:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/24/2022 05:48:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/24/2022 05:49:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/24/2022 05:49:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:49:03 - INFO - __main__ - Printing 3 examples
05/24/2022 05:49:03 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 05:49:03 - INFO - __main__ - ['entailed']
05/24/2022 05:49:03 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 05:49:03 - INFO - __main__ - ['entailed']
05/24/2022 05:49:03 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 05:49:03 - INFO - __main__ - ['entailed']
05/24/2022 05:49:03 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:49:04 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:49:04 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 05:49:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:49:04 - INFO - __main__ - Printing 3 examples
05/24/2022 05:49:04 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 05:49:04 - INFO - __main__ - ['entailed']
05/24/2022 05:49:04 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 05:49:04 - INFO - __main__ - ['entailed']
05/24/2022 05:49:04 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 05:49:04 - INFO - __main__ - ['entailed']
05/24/2022 05:49:04 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:49:04 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:49:04 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 05:49:05 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.08879765395894428 on epoch=749
05/24/2022 05:49:05 - INFO - __main__ - save last model!
05/24/2022 05:49:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 05:49:05 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 05:49:05 - INFO - __main__ - Printing 3 examples
05/24/2022 05:49:05 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:49:05 - INFO - __main__ - ['entailed']
05/24/2022 05:49:05 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:49:05 - INFO - __main__ - ['entailed']
05/24/2022 05:49:05 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 05:49:05 - INFO - __main__ - ['entailed']
05/24/2022 05:49:05 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:49:19 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 05:49:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 05:49:20 - INFO - __main__ - Starting training!
05/24/2022 05:49:29 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:49:42 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 05:58:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_87_0.3_8_predictions.txt
05/24/2022 05:58:43 - INFO - __main__ - Classification-F1 on test data: 0.0055
05/24/2022 05:58:43 - INFO - __main__ - prefix=tab_fact_32_87, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.005460676076225448
05/24/2022 05:58:43 - INFO - __main__ - Running ... prefix=tab_fact_32_87, lr=0.2, bsz=8 ...
05/24/2022 05:58:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:58:44 - INFO - __main__ - Printing 3 examples
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:58:44 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:58:44 - INFO - __main__ - Loaded 64 examples from train data
05/24/2022 05:58:44 - INFO - __main__ - Start tokenizing ... 64 instances
05/24/2022 05:58:44 - INFO - __main__ - Printing 3 examples
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: the average attendance of each game during december of the 2009 - 10 boston celtic season be 18293.23 [SEP] table_caption: 2009 - 10 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 18#december 1#charlotte#w 108 - 90 (ot)#ray allen (27)#kendrick perkins (12)#rajon rondo (9)#time warner cable arena 15129#14 - 4 [n] 19#december 3#san antonio#w 90 - 83 (ot)#kevin garnett (20)#kevin garnett , kendrick perkins (7)#rajon rondo (12)#at&t center 18581#15 - 4 [n] 20#december 4#oklahoma city#w 105 - 87 (ot)#kevin garnett (23)#kevin garnett , kendrick perkins (8)#rajon rondo (6)#ford center 18203#16 - 4 [n] 21#december 8#milwaukee#w 98 - 89 (ot)#kevin garnett (25)#kevin garnett , rajon rondo (9)#rajon rondo (13)#td garden 18624#17 - 4 [n] 22#december 10#washington#w 104 - 102 (ot)#rajon rondo (21)#kendrick perkins (11)#rajon rondo (11)#verizon center 20173#18 - 4 [n] 23#december 12#chicago#w 106 - 80 (ot)#rajon rondo (16)#kevin garnett (10)#rajon rondo (14)#united center 21257#19 - 4 [n] 24#december 14#memphis#w 110 - 105 (ot)#paul pierce (19)#kevin garnett (8)#rajon rondo (9)#fedexforum 14193#20 - 4 [n] 25#december 18#philadelphia#l 97 - 98 (ot)#kevin garnett (21)#kendrick perkins (16)#rajon rondo (10)#td garden 18624#20 - 5 [n] 26#december 20#minnesota#w 122 - 104 (ot)#paul pierce (29)#kendrick perkins (11)#rajon rondo (15)#td garden 18624#21 - 5 [n] 27#december 22#indiana#w 103 - 94 (ot)#ray allen (23)#rasheed wallace (13)#rajon rondo (9)#td garden 18624#22 - 5 [n] 28#december 25#orlando#w 86 - 77 (ot)#ray allen (18)#rajon rondo (13)#rajon rondo (8)#amway arena 17461#23 - 5 [n] 29#december 27#la clippers#l 90 - 92 (ot)#rajon rondo (20)#tony allen (10)#rajon rondo (6)#staples center 19060#23 - 6 [n] 30#december 28#golden state#l 99 - 103 (ot)#rajon rondo (30)#kendrick perkins (14)#rajon rondo (15)#oracle arena 19259#23 - 7 [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: the opponent be new york ranger on october 8 [SEP] table_caption: 2009 - 10 washington capitals season [SEP] table_text: game#date#opponent#score#location#attendance#record#points [n] 1#october 1#boston bruins#4 - 1#td garden#17565#1 - 0 - 0#2 [n] 2#october 3#toronto maple leafs#6 - 4#verizon center#18277#2 - 0 - 0#4 [n] 3#october 6#philadelphia flyers#6 - 5 ot#wachovia center#19567#2 - 0 - 1#5 [n] 4#october 8#new york rangers#4 - 3#verizon center#18277#2 - 1 - 1#5 [n] 5#october 10#detroit red wings#3 - 2#joe louis arena#19122#2 - 2 - 1#5 [n] 6#october 12#new jersey devils#3 - 2 so#verizon center#18277#2 - 2 - 2#6 [n] 7#october 15#san jose sharks#4 - 1#verizon center#18277#3 - 2 - 2#8 [n] 8#october 17#nashville predators#3 - 2 so#verizon center#18277#4 - 2 - 2#10 [n] 9#october 22#atlanta thrashers#5 - 4#philips arena#13192#5 - 2 - 2#12 [n] 10#october 24#new york islanders#3 - 2 ot#nassau veterans memorial coliseum#11541#6 - 2 - 2#14 [n] 11#october 27#philadelphia flyers#4 - 2#verizon center#18277#7 - 2 - 2#16 [n] 12#october 29#atlanta thrashers#4 - 3#philips arena#12893#8 - 2 - 2#18 [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ -  [tab_fact] statement: 4 of the 6 cooper - climax car have mechanical issue that cause them to drop out of the race [SEP] table_caption: 1957 german grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] juan manuel fangio#maserati#22#3:30:38.3#1 [n] mike hawthorn#ferrari#22#+ 3.6 secs#2 [n] peter collins#ferrari#22#+ 35.6 secs#4 [n] luigi musso#ferrari#22#+ 3:37.6#8 [n] stirling moss#vanwall#22#+ 4:37.2#7 [n] jean behra#maserati#22#+ 4:38.5#3 [n] harry schell#maserati#22#+ 6:47.5#6 [n] masten gregory#maserati#21#+ 1 lap#10 [n] tony brooks#vanwall#21#+ 1 lap#5 [n] giorgio scarlatti#maserati#21#+ 1 lap#13 [n] bruce halford#maserati#21#+ 1 lap#16 [n] edgar barth#porsche#21#+ 1 lap#12 [n] brian naylor#cooper - climax#20#+ 2 laps#17 [n] carel godin de beaufort#porsche#20#+ 2 laps#20 [n] tony marsh#cooper - climax#17#+ 5 laps#22 [n] hans herrmann#maserati#14#chassis#11 [n] umberto maglioli#porsche#13#engine#15 [n] roy salvadori#cooper - climax#11#suspension#14 [n] paco godia#maserati#11#steering#21 [n] stuart lewis - evans#vanwall#10#gearbox#9 [n] jack brabham#cooper - climax#6#transmission#18 [n] paul england#cooper - climax#4#distributor#23 [n] dick gibson#cooper - climax#3#steering#24 [n] horace gould#maserati#2#axle#19 [n] 
05/24/2022 05:58:44 - INFO - __main__ - ['entailed']
05/24/2022 05:58:44 - INFO - __main__ - Tokenizing Input ...
05/24/2022 05:58:44 - INFO - __main__ - Tokenizing Output ...
05/24/2022 05:58:44 - INFO - __main__ - Loaded 64 examples from dev data
05/24/2022 05:58:59 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 05:59:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 05:59:00 - INFO - __main__ - Starting training!
05/24/2022 05:59:05 - INFO - __main__ - Step 10 Global step 10 Train loss 5.26 on epoch=2
05/24/2022 05:59:09 - INFO - __main__ - Step 20 Global step 20 Train loss 3.99 on epoch=4
05/24/2022 05:59:14 - INFO - __main__ - Step 30 Global step 30 Train loss 3.11 on epoch=7
05/24/2022 05:59:18 - INFO - __main__ - Step 40 Global step 40 Train loss 2.73 on epoch=9
05/24/2022 05:59:22 - INFO - __main__ - Step 50 Global step 50 Train loss 2.20 on epoch=12
05/24/2022 05:59:25 - INFO - __main__ - Global step 50 Train loss 3.46 Classification-F1 0.0 on epoch=12
05/24/2022 05:59:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/24/2022 05:59:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.97 on epoch=14
05/24/2022 05:59:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.77 on epoch=17
05/24/2022 05:59:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.46 on epoch=19
05/24/2022 05:59:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.20 on epoch=22
05/24/2022 05:59:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=24
05/24/2022 05:59:50 - INFO - __main__ - Global step 100 Train loss 1.50 Classification-F1 0.16489361702127658 on epoch=24
05/24/2022 05:59:50 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.16489361702127658 on epoch=24, global_step=100
05/24/2022 05:59:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.78 on epoch=27
05/24/2022 05:59:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=29
05/24/2022 06:00:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=32
05/24/2022 06:00:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=34
05/24/2022 06:00:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=37
05/24/2022 06:00:15 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=37
05/24/2022 06:00:15 - INFO - __main__ - Saving model with best Classification-F1: 0.16489361702127658 -> 0.3333333333333333 on epoch=37, global_step=150
05/24/2022 06:00:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=39
05/24/2022 06:00:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=42
05/24/2022 06:00:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=44
05/24/2022 06:00:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=47
05/24/2022 06:00:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=49
05/24/2022 06:00:39 - INFO - __main__ - Global step 200 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=49
05/24/2022 06:00:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=52
05/24/2022 06:00:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=54
05/24/2022 06:00:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=57
05/24/2022 06:00:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=59
05/24/2022 06:01:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=62
05/24/2022 06:01:04 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=62
05/24/2022 06:01:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=64
05/24/2022 06:01:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=67
05/24/2022 06:01:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=69
05/24/2022 06:01:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=72
05/24/2022 06:01:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=74
05/24/2022 06:01:29 - INFO - __main__ - Global step 300 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=74
05/24/2022 06:01:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=77
05/24/2022 06:01:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/24/2022 06:01:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=82
05/24/2022 06:01:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=84
05/24/2022 06:01:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=87
05/24/2022 06:01:54 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=87
05/24/2022 06:01:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
05/24/2022 06:02:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=92
05/24/2022 06:02:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=94
05/24/2022 06:02:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/24/2022 06:02:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=99
05/24/2022 06:02:19 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=99
05/24/2022 06:02:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=102
05/24/2022 06:02:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=104
05/24/2022 06:02:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=107
05/24/2022 06:02:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=109
05/24/2022 06:02:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=112
05/24/2022 06:02:44 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=112
05/24/2022 06:02:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=114
05/24/2022 06:02:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=117
05/24/2022 06:02:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=119
05/24/2022 06:03:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/24/2022 06:03:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=124
05/24/2022 06:03:09 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=124
05/24/2022 06:03:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=127
05/24/2022 06:03:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/24/2022 06:03:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=132
05/24/2022 06:03:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=134
05/24/2022 06:03:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=137
05/24/2022 06:03:34 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=137
05/24/2022 06:03:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=139
05/24/2022 06:03:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=142
05/24/2022 06:03:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=144
05/24/2022 06:03:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
05/24/2022 06:03:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=149
05/24/2022 06:03:59 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=149
05/24/2022 06:04:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=152
05/24/2022 06:04:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
05/24/2022 06:04:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/24/2022 06:04:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=159
05/24/2022 06:04:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=162
05/24/2022 06:04:24 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=162
05/24/2022 06:04:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=164
05/24/2022 06:04:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=167
05/24/2022 06:04:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/24/2022 06:04:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=172
05/24/2022 06:04:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/24/2022 06:04:49 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=174
05/24/2022 06:04:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=177
05/24/2022 06:04:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/24/2022 06:05:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
05/24/2022 06:05:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=184
05/24/2022 06:05:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
05/24/2022 06:05:14 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=187
05/24/2022 06:05:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=189
05/24/2022 06:05:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=192
05/24/2022 06:05:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=194
05/24/2022 06:05:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=197
05/24/2022 06:05:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=199
05/24/2022 06:05:39 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=199
05/24/2022 06:05:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=202
05/24/2022 06:05:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=204
05/24/2022 06:05:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
05/24/2022 06:05:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=209
05/24/2022 06:06:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=212
05/24/2022 06:06:04 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=212
05/24/2022 06:06:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=214
05/24/2022 06:06:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=217
05/24/2022 06:06:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=219
05/24/2022 06:06:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
05/24/2022 06:06:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=224
05/24/2022 06:06:29 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=224
05/24/2022 06:06:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=227
05/24/2022 06:06:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=229
05/24/2022 06:06:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
05/24/2022 06:06:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=234
05/24/2022 06:06:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=237
05/24/2022 06:06:54 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=237
05/24/2022 06:06:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
05/24/2022 06:07:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
05/24/2022 06:07:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=244
05/24/2022 06:07:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
05/24/2022 06:07:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=249
05/24/2022 06:07:19 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=249
05/24/2022 06:07:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=252
05/24/2022 06:07:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=254
05/24/2022 06:07:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
05/24/2022 06:07:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=259
05/24/2022 06:07:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=262
05/24/2022 06:07:44 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=262
05/24/2022 06:07:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=264
05/24/2022 06:07:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=267
05/24/2022 06:07:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=269
05/24/2022 06:08:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/24/2022 06:08:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
05/24/2022 06:08:09 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=274
05/24/2022 06:08:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=277
05/24/2022 06:08:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/24/2022 06:08:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=282
05/24/2022 06:08:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=284
05/24/2022 06:08:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=287
05/24/2022 06:08:35 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=287
05/24/2022 06:08:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=289
05/24/2022 06:08:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/24/2022 06:08:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=294
05/24/2022 06:08:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=297
05/24/2022 06:08:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=299
05/24/2022 06:09:00 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=299
05/24/2022 06:09:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
05/24/2022 06:09:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=304
05/24/2022 06:09:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=307
05/24/2022 06:09:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=309
05/24/2022 06:09:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=312
05/24/2022 06:09:25 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3511520737327189 on epoch=312
05/24/2022 06:09:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3511520737327189 on epoch=312, global_step=1250
05/24/2022 06:09:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
05/24/2022 06:09:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=317
05/24/2022 06:09:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=319
05/24/2022 06:09:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=322
05/24/2022 06:09:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=324
05/24/2022 06:09:50 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.36374269005847953 on epoch=324
05/24/2022 06:09:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3511520737327189 -> 0.36374269005847953 on epoch=324, global_step=1300
05/24/2022 06:09:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=327
05/24/2022 06:09:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
05/24/2022 06:10:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=332
05/24/2022 06:10:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=334
05/24/2022 06:10:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=337
05/24/2022 06:10:16 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.3888888888888889 on epoch=337
05/24/2022 06:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.3888888888888889 on epoch=337, global_step=1350
05/24/2022 06:10:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
05/24/2022 06:10:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=342
05/24/2022 06:10:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=344
05/24/2022 06:10:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=347
05/24/2022 06:10:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=349
05/24/2022 06:10:41 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.35415625520573046 on epoch=349
05/24/2022 06:10:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=352
05/24/2022 06:10:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=354
05/24/2022 06:10:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=357
05/24/2022 06:10:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=359
05/24/2022 06:11:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=362
05/24/2022 06:11:06 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3543171654626763 on epoch=362
05/24/2022 06:11:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=364
05/24/2022 06:11:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=367
05/24/2022 06:11:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=369
05/24/2022 06:11:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=372
05/24/2022 06:11:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=374
05/24/2022 06:11:31 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.5607843137254902 on epoch=374
05/24/2022 06:11:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3888888888888889 -> 0.5607843137254902 on epoch=374, global_step=1500
05/24/2022 06:11:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=377
05/24/2022 06:11:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=379
05/24/2022 06:11:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=382
05/24/2022 06:11:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=384
05/24/2022 06:11:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=387
05/24/2022 06:11:57 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.2733264409408116 on epoch=387
05/24/2022 06:12:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=389
05/24/2022 06:12:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=392
05/24/2022 06:12:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=394
05/24/2022 06:12:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=397
05/24/2022 06:12:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=399
05/24/2022 06:12:22 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.5730170496664195 on epoch=399
05/24/2022 06:12:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5730170496664195 on epoch=399, global_step=1600
05/24/2022 06:12:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=402
05/24/2022 06:12:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=404
05/24/2022 06:12:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=407
05/24/2022 06:12:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=409
05/24/2022 06:12:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=412
05/24/2022 06:12:47 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.26389247867666066 on epoch=412
05/24/2022 06:12:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
05/24/2022 06:12:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=417
05/24/2022 06:13:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=419
05/24/2022 06:13:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=422
05/24/2022 06:13:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/24/2022 06:13:12 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.21285563751317174 on epoch=424
05/24/2022 06:13:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=427
05/24/2022 06:13:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=429
05/24/2022 06:13:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
05/24/2022 06:13:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
05/24/2022 06:13:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=437
05/24/2022 06:13:37 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.24491392801251954 on epoch=437
05/24/2022 06:13:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=439
05/24/2022 06:13:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=442
05/24/2022 06:13:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/24/2022 06:13:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=447
05/24/2022 06:14:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=449
05/24/2022 06:14:03 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.16712630998345288 on epoch=449
05/24/2022 06:14:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=452
05/24/2022 06:14:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=454
05/24/2022 06:14:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=457
05/24/2022 06:14:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
05/24/2022 06:14:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=462
05/24/2022 06:14:28 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.20429931972789114 on epoch=462
05/24/2022 06:14:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=464
05/24/2022 06:14:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
05/24/2022 06:14:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=469
05/24/2022 06:14:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=472
05/24/2022 06:14:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=474
05/24/2022 06:14:53 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.3365811965811966 on epoch=474
05/24/2022 06:14:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=477
05/24/2022 06:15:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=479
05/24/2022 06:15:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
05/24/2022 06:15:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/24/2022 06:15:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=487
05/24/2022 06:15:18 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.2580128205128205 on epoch=487
05/24/2022 06:15:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/24/2022 06:15:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=492
05/24/2022 06:15:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
05/24/2022 06:15:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=497
05/24/2022 06:15:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=499
05/24/2022 06:15:43 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.12453900709219858 on epoch=499
05/24/2022 06:15:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
05/24/2022 06:15:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
05/24/2022 06:15:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=507
05/24/2022 06:16:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=509
05/24/2022 06:16:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/24/2022 06:16:09 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.35445887445887453 on epoch=512
05/24/2022 06:16:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=514
05/24/2022 06:16:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/24/2022 06:16:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=519
05/24/2022 06:16:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/24/2022 06:16:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/24/2022 06:16:34 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.2602291325695581 on epoch=524
05/24/2022 06:16:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
05/24/2022 06:16:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=529
05/24/2022 06:16:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/24/2022 06:16:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
05/24/2022 06:16:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
05/24/2022 06:16:59 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.20615901455767077 on epoch=537
05/24/2022 06:17:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/24/2022 06:17:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
05/24/2022 06:17:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/24/2022 06:17:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
05/24/2022 06:17:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
05/24/2022 06:17:24 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.20225225225225224 on epoch=549
05/24/2022 06:17:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=552
05/24/2022 06:17:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
05/24/2022 06:17:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/24/2022 06:17:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/24/2022 06:17:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
05/24/2022 06:17:49 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.16515837104072398 on epoch=562
05/24/2022 06:17:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=564
05/24/2022 06:17:58 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
05/24/2022 06:18:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
05/24/2022 06:18:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=572
05/24/2022 06:18:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
05/24/2022 06:18:15 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.17446727549467278 on epoch=574
05/24/2022 06:18:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
05/24/2022 06:18:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/24/2022 06:18:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
05/24/2022 06:18:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/24/2022 06:18:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=587
05/24/2022 06:18:40 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.14695752009184845 on epoch=587
05/24/2022 06:18:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
05/24/2022 06:18:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/24/2022 06:18:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=594
05/24/2022 06:18:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/24/2022 06:19:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
05/24/2022 06:19:05 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.15122191241594224 on epoch=599
05/24/2022 06:19:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
05/24/2022 06:19:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
05/24/2022 06:19:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/24/2022 06:19:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/24/2022 06:19:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/24/2022 06:19:30 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.12962686567164178 on epoch=612
05/24/2022 06:19:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=614
05/24/2022 06:19:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=617
05/24/2022 06:19:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=619
05/24/2022 06:19:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
05/24/2022 06:19:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/24/2022 06:19:55 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.1334503950834065 on epoch=624
05/24/2022 06:20:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/24/2022 06:20:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
05/24/2022 06:20:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/24/2022 06:20:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/24/2022 06:20:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/24/2022 06:20:21 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.11626064567241039 on epoch=637
05/24/2022 06:20:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
05/24/2022 06:20:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
05/24/2022 06:20:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/24/2022 06:20:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/24/2022 06:20:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/24/2022 06:20:46 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.11953836260872507 on epoch=649
05/24/2022 06:20:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/24/2022 06:20:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
05/24/2022 06:20:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/24/2022 06:21:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=659
05/24/2022 06:21:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=662
05/24/2022 06:21:11 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.12696969696969695 on epoch=662
05/24/2022 06:21:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
05/24/2022 06:21:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/24/2022 06:21:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/24/2022 06:21:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/24/2022 06:21:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/24/2022 06:21:36 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.12858621709553386 on epoch=674
05/24/2022 06:21:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=677
05/24/2022 06:21:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
05/24/2022 06:21:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
05/24/2022 06:21:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
05/24/2022 06:21:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
05/24/2022 06:22:02 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.07602977667493796 on epoch=687
05/24/2022 06:22:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/24/2022 06:22:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/24/2022 06:22:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/24/2022 06:22:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/24/2022 06:22:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/24/2022 06:22:27 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.10246153846153847 on epoch=699
05/24/2022 06:22:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/24/2022 06:22:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
05/24/2022 06:22:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/24/2022 06:22:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=709
05/24/2022 06:22:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=712
05/24/2022 06:22:52 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.09067064083457527 on epoch=712
05/24/2022 06:22:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
05/24/2022 06:23:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
05/24/2022 06:23:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=719
05/24/2022 06:23:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/24/2022 06:23:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/24/2022 06:23:17 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.09561290322580647 on epoch=724
05/24/2022 06:23:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/24/2022 06:23:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/24/2022 06:23:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/24/2022 06:23:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/24/2022 06:23:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/24/2022 06:23:42 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.09365126676602088 on epoch=737
05/24/2022 06:23:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/24/2022 06:23:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=742
05/24/2022 06:23:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/24/2022 06:24:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/24/2022 06:24:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/24/2022 06:24:07 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.10189155107187893 on epoch=749
05/24/2022 06:24:07 - INFO - __main__ - save last model!
05/24/2022 06:24:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 06:24:08 - INFO - __main__ - Start tokenizing ... 12792 instances
05/24/2022 06:24:08 - INFO - __main__ - Printing 3 examples
05/24/2022 06:24:08 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 06:24:08 - INFO - __main__ - ['entailed']
05/24/2022 06:24:08 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 06:24:08 - INFO - __main__ - ['entailed']
05/24/2022 06:24:08 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/24/2022 06:24:08 - INFO - __main__ - ['entailed']
05/24/2022 06:24:08 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:24:32 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:24:45 - INFO - __main__ - Loaded 12792 examples from test data
05/24/2022 06:33:41 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-tab_fact/tab_fact_32_87_0.2_8_predictions.txt
05/24/2022 06:33:41 - INFO - __main__ - Classification-F1 on test data: 0.0053
05/24/2022 06:33:42 - INFO - __main__ - prefix=tab_fact_32_87, lr=0.2, bsz=8, dev_performance=0.5730170496664195, test_performance=0.005282018459200036
